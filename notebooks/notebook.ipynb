{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI1CsdAZN7PS"
      },
      "source": [
        "### Startup\n",
        "\n",
        "This code is meant to be executed on Google Colab.\n",
        "To use it locally change *COLAB_MODE* to False.\n",
        "\n",
        "**Note**: remember to change *workdir* accordingly, the notebook must be runned inside the root project folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzSZjcNvoPgs",
        "outputId": "13bebaa2-3950-48c0-d7b6-73a9bb887b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fra/AIGCDetection-CI-CD/src\n"
          ]
        }
      ],
      "source": [
        "workdir = \"./src\"\n",
        "%cd $workdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MlHM4qAGEMR"
      },
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "The code incorporates elements derived from the code originally published in the research paper, which can be found here: https://github.com/alsgkals2/CoReD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEuLI5ANwixJ",
        "outputId": "b27e1eb0-c412-403d-cfd4-f89a869c9159"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from utils.common_functions import *\n",
        "from utils.cored_functions import *\n",
        "from utils.data_loader import create_dataloader\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "\n",
        "from utils.data_loader import create_dataloader\n",
        "from utils.model_loader import load_models\n",
        "from utils.train_utils import *\n",
        "\n",
        "\n",
        "def kd_train(args, log = None):\n",
        "\n",
        "    # Init\n",
        "    torch.cuda.empty_cache()\n",
        "    device = 'cuda' if args.num_gpu else 'cpu'\n",
        "    lr = args.lr\n",
        "    KD_alpha = args.KD_alpha\n",
        "    num_class = args.num_class\n",
        "    num_store_per=5\n",
        "\n",
        "\n",
        "    # Load datasets and models\n",
        "    dicLoader, dicCoReD, dicSourceName = initialization(args)\n",
        "    print(\"Dataset available in dicLoader: \", \" / \".join([n for n in dicLoader]))\n",
        "    print(\"Dataset available in dicCoReD: \", \" / \".join([n for n in dicCoReD]))\n",
        "    teacher_model, student_model = load_models(args.weight, args.network, num_gpu = args.num_gpu)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=0.1)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    if args.lr_schedule == \"cosine\":\n",
        "        print(\"Apply Cosine learning rate schedule\")\n",
        "        lr_scheduler = CosineAnnealingLR(optimizer=optimizer,\n",
        "                                        T_max=10,\n",
        "                                        eta_min=1e-5,\n",
        "                                        verbose=True)\n",
        "    else:\n",
        "        print(f\"Input: {args.lr_schedule}, No learning rate schedule applied ... \")\n",
        "\n",
        "\n",
        "    # Pre-evaluation\n",
        "    print(\"Loading train target for correcting ...  \")\n",
        "    _list_correct, _ = func_correct(teacher_model.to(device), dicCoReD['train_target_forCorrect'])\n",
        "    _correct_loaders, already_correct_ratio = GetSplitLoaders_BinaryClasses(_list_correct, dicCoReD['train_target_dataset'], get_augs(args)[0], num_store_per)\n",
        "    print(\"Ratio of already correctly predicted in training set: {:.3f}\".format(already_correct_ratio))\n",
        "    list_features = GetListTeacherFeatureFakeReal(teacher_model.module if ',' in args.num_gpu else teacher_model ,_correct_loaders, mode=args.network)\n",
        "    list_features = np.array(list_features)\n",
        "    print(\"List feature size: \", list_features.shape)\n",
        "\n",
        "    # Initial validation\n",
        "    _, _, test_acc = Test(dicLoader['val_target'], student_model, criterion, log = log, source_name = args.name_target)\n",
        "    total_acc = test_acc\n",
        "    print(\"[VAL Acc] Target: {:.2f}%\".format( test_acc))\n",
        "    cnt = 1\n",
        "    for name in dicLoader:\n",
        "        if 'val_dataset' in name or 'val_source' in name:\n",
        "            if 'val_source' in name:\n",
        "                source_name = name.split(\"_\")[2]\n",
        "            else:\n",
        "                source_name = \"<source name>\"\n",
        "\n",
        "            _, _, source_acc = Test(dicLoader[name], student_model, criterion, log = log, source_name = source_name)\n",
        "            total_acc += source_acc\n",
        "            print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "            cnt += 1\n",
        "\n",
        "    print(\"[VAL Acc] Avg {:.2f}%\\n Save initial model weight\".format(total_acc / cnt))\n",
        "    best_acc = total_acc\n",
        "    \n",
        "    is_best_acc = False\n",
        "    cur_patience = 0 # Early stop and saving\n",
        "    l_weight = 1.0 # reduce the conservation when performance does not gain much\n",
        "    print(f\"Start training in {args.epochs} epochs\")\n",
        "\n",
        "\n",
        "    # ------- START TRAINING ------- #\n",
        "    for epoch in range(args.epochs):\n",
        "        correct,total = 0,0\n",
        "        teacher_model.eval()\n",
        "        student_model.train()\n",
        "        disp = {}\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dicLoader['train_target']):\n",
        "            # Load data\n",
        "            step = (batch_idx+1) * (epoch+1)\n",
        "            inputs = inputs.to(device).to(torch.float32)\n",
        "            targets = targets.to(device).to(torch.long)\n",
        "            if torch.isnan(inputs).any() or torch.isnan(targets).any():\n",
        "                raise ValueError(\"There is Nan values in input or target\")\n",
        "\n",
        "            # Forward\n",
        "            teacher_outputs = teacher_model(inputs)\n",
        "            penul_ft, outputs = student_model(inputs, True)\n",
        "\n",
        "            # Losses\n",
        "            loss_main = criterion(outputs, targets)\n",
        "            loss_kd = loss_fn_kd(outputs, targets, teacher_outputs)\n",
        "            loss_kd = loss_clampping(loss_kd, 0, 1800)\n",
        "\n",
        "            #REP loss\n",
        "            list_features_std = [list(), list()]\n",
        "            rep_ft_partitions = correct_binary_simple(inputs=inputs, penul_ft=penul_ft, outputs=outputs, targets=targets) # rep_ft_partitions : 5 x 2\n",
        "            for j in range(num_store_per):\n",
        "                for i in range(num_class):\n",
        "                    if(np.count_nonzero(list_features[i][j])==0 or len(rep_ft_partitions[j][i])==0):\n",
        "                      continue\n",
        "                    feat = torch.stack(rep_ft_partitions[j][i], dim=0).mean(dim=0)\n",
        "                    assert feat.size(-1) == 2048 or feat.size(-1) == 512 or feat.size(-1) == 1280\n",
        "                    rep_loss = (feat.to(torch.float32)  - torch.tensor(list_features[i][j]).to(device).to(torch.float32)).pow(2).mean()\n",
        "                    list_features_std[i].append(rep_loss)\n",
        "            sne_loss = 0.0\n",
        "            for fs in list_features_std:\n",
        "                for ss in fs:\n",
        "                    if ss.requires_grad:\n",
        "                        sne_loss += ss\n",
        "            sne_loss = loss_clampping(sne_loss, 0, 1) # REP Loss is clampped in this project\n",
        "\n",
        "            # Total loss\n",
        "            loss = loss_main  + l_weight*(loss_kd + sne_loss)\n",
        "            sne_item = sne_loss if type(sne_loss) == float else sne_loss.item()\n",
        "\n",
        "            # Log and display\n",
        "            disp[\"CE\"] = loss_main.item()\n",
        "            disp[\"KD\"] = loss_kd.item() if loss_kd > 0 else 0.0\n",
        "            disp[\"REP\"] = sne_item if sne_loss > 0 else 0.0\n",
        "            call = ' | '.join([\"{}: {:.4f}\".format(k, v) for k, v in disp.items()])\n",
        "            print(\"Train Epoch: {e:03d} Batch: {batch:05d}/{size:05d} | Loss: {loss:.4f} | {call}\"\n",
        "                            .format(e=epoch+1, batch=batch_idx+1, size=len(dicLoader['train_target']), loss=loss.item(), call=call))\n",
        "\n",
        "            # Learn!\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if args.lr_schedule == \"onecycle\":\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            # Predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += len(targets)\n",
        "\n",
        "        if args.lr_schedule == \"cosine\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "\n",
        "        # ----- Validation ------ #\n",
        "\n",
        "        # Current task\n",
        "        _, _, test_acc = Test(dicLoader['val_target'], student_model, criterion, log = None, source_name = args.name_target)\n",
        "        total_acc = test_acc\n",
        "        print(\"[VAL Acc] Target: {:.2f}%\".format( test_acc))\n",
        "\n",
        "        # Past tasks\n",
        "        cnt = 1\n",
        "        for name in dicLoader:\n",
        "            if 'val_dataset' in name or 'val_source' in name:\n",
        "                if 'val_dataset' in name:\n",
        "                    source_name = dicSourceName[f'source{cnt}']\n",
        "                else:\n",
        "                    source_name = dicSourceName['source']\n",
        "\n",
        "                _, _, source_acc = Test(dicLoader[name], student_model, criterion, log = None, source_name = source_name)\n",
        "                total_acc += source_acc\n",
        "                print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "                cnt += 1\n",
        "        print(\"[VAL Acc] Avg {:.2f}%\".format(total_acc / cnt))\n",
        "\n",
        "        # Early stop\n",
        "        is_best_acc = total_acc > best_acc\n",
        "        if is_best_acc:\n",
        "                print(\"VAL Acc improve from {:.2f}% to {:.2f}%\".format(best_acc/cnt, total_acc/cnt))\n",
        "                cur_patience = 0\n",
        "        else:\n",
        "            cur_patience += 1\n",
        "        if args.loss_schedule and (cur_patience > 0 and cur_patience % 4 == 0):\n",
        "                l_weight = ReduceWeightOnPlateau(l_weight, args.decay_factor)\n",
        "\n",
        "        # Save\n",
        "        best_acc = max(total_acc,best_acc)\n",
        "        if  is_best_acc:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': student_model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict()},\n",
        "            checkpoint = savepath,\n",
        "            filename = 'epoch_{}'.format( epoch+1 if (epoch+1)%10==0 else ''),\n",
        "            ACC_BEST=is_best_acc\n",
        "            )\n",
        "            print('Save best model' if is_best_acc else f'Save checkpoint model @ {epoch+1}')\n",
        "        if args.early_stop and (cur_patience == args.patience):\n",
        "            print(\"Early stopping ...\")\n",
        "            return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7csYl99zAH"
      },
      "source": [
        "# Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avxhBq4HFhOO",
        "outputId": "60506fd7-31dc-443e-8a10-30b5a64132fa"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from common_functions import initialization, load_models, AverageMeter\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, average_precision_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def evaluate(args, global_writer=None):\n",
        "\n",
        "    # Config\n",
        "    setattr(args,\"name_sources\", \"\")\n",
        "    setattr(args,\"name_target\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "    # Load model\n",
        "    _, model = load_models(args.weight, args.network, args.num_gpu, not args.test)\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Load datasets\n",
        "    tot_avg_acc, real_avg_acc, fake_avg_acc = 0.0, 0.0 ,0.0\n",
        "    for ds_name in args.ds_cfg[\"fake_ds\"]:\n",
        "      data_folder = f\"{args.dataroot}/{ds_name}/test\"\n",
        "      setattr(args,\"data\", data_folder)\n",
        "      dicLoader,_, dicSourceName = initialization(args)\n",
        "\n",
        "\n",
        "      for key, name in zip(dicLoader, dicSourceName):\n",
        "        # Init\n",
        "        global best_acc\n",
        "        correct, total =0,0\n",
        "        losses = AverageMeter()\n",
        "        arc = AverageMeter()\n",
        "        acc_real = AverageMeter()\n",
        "        acc_fake = AverageMeter()\n",
        "        sum_of_AUROC=[]\n",
        "        target=[]\n",
        "        output = []\n",
        "        y_true=np.zeros((0,2),dtype=np.int8)\n",
        "        y_pred=np.zeros((0,2),dtype=np.int8)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          model.eval()\n",
        "          model.cuda()\n",
        "\n",
        "          for (inputs, targets) in tqdm(dicLoader[key], ncols=50):\n",
        "              # Predict\n",
        "              inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, targets)\n",
        "              _, predicted = torch.max(outputs, 1)\n",
        "              correct = (predicted == targets).squeeze()\n",
        "              total += len(targets)\n",
        "              losses.update(loss.data.tolist(), inputs.size(0))\n",
        "              _y_pred = outputs.cpu().detach()\n",
        "              _y_gt = targets.cpu().detach().numpy()\n",
        "              acc = [0, 0]\n",
        "              class_total = [0, 0]\n",
        "              for i in range(len(targets)):\n",
        "                  label = targets[i]\n",
        "                  acc[label] += 1 if correct[i].item() == True else 0\n",
        "                  class_total[label] += 1\n",
        "\n",
        "              losses.update(loss.data.tolist(), inputs.size(0))\n",
        "              if (class_total[0] != 0):\n",
        "                  acc_real.update(acc[0] / class_total[0])\n",
        "              if (class_total[1] != 0):\n",
        "                  acc_fake.update(acc[1] / class_total[1])\n",
        "\n",
        "              target.append(_y_gt)\n",
        "              output.append(_y_pred.numpy()[:,1])\n",
        "              auroc=None\n",
        "              try:\n",
        "                  auroc = roc_auc_score(_y_gt, outputs[:,1].cpu().detach().numpy())\n",
        "              except ValueError:\n",
        "                  pass\n",
        "              sum_of_AUROC.append(auroc)\n",
        "              _y_true = np.array(torch.zeros(targets.shape[0],2), dtype=np.int8)\n",
        "              _y_gt = _y_gt.astype(int)\n",
        "              for _ in range(len(targets)):\n",
        "                  _y_true[_][_y_gt[_]] = 1\n",
        "              y_true = np.concatenate((y_true,_y_true))\n",
        "              a = _y_pred.argmax(1)\n",
        "              _y_pred = np.array(torch.zeros(_y_pred.shape).scatter(1, a.unsqueeze(1), 1),dtype=np.int8)\n",
        "              y_pred = np.concatenate((y_pred,_y_pred))\n",
        "\n",
        "          n_real_samples = np.count_nonzero(y_true, axis=0)[0]\n",
        "          n_fake_samples = np.count_nonzero(y_true, axis=0)[1]\n",
        "          acc = accuracy_score(y_true, y_pred)\n",
        "          ap = average_precision_score(y_true, y_pred)\n",
        "\n",
        "          result = classification_report(y_true, y_pred,\n",
        "                                              labels=None,\n",
        "                                              target_names=None,\n",
        "                                              sample_weight=None,\n",
        "                                              digits=4,\n",
        "                                              output_dict=False,\n",
        "                                              zero_division='warn')\n",
        "\n",
        "\n",
        "          print(f\"\\nLoss:{losses.avg:.4f} | Acc:{acc:.4f} | Acc Real:{acc_real.avg:.4f} | Acc Fake:{acc_fake.avg:.4f} | Ap:{ap:.4f}\")\n",
        "          print(f'Num reals: {n_real_samples}, Num fakes: {n_fake_samples}')\n",
        "          print(\"\\n\\n\",result)\n",
        "\n",
        "          tot_avg_acc += acc\n",
        "          real_avg_acc += acc_real.avg\n",
        "          fake_avg_acc += acc_fake.avg\n",
        "\n",
        "        \n",
        "    total_ds = len(args.ds_cfg[\"fake_ds\"])\n",
        "    print(f\"Avg: | Acc:{tot_avg_acc/total_ds:.4f} | Acc Real:{real_avg_acc/total_ds:.4f} | Acc Fake:{fake_avg_acc/total_ds:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RE9O_Ego1iy"
      },
      "source": [
        "# Workspace\n",
        "\n",
        "In this section it is possible to run trainings and evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO7-OzjtSrus"
      },
      "source": [
        "## Traning\n",
        "\n",
        "\n",
        "1.   Configurate the training\n",
        "2.   Build the dataset\n",
        "3.   Train!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrQgEkqyqu8O"
      },
      "source": [
        "### Knowledge distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-BWn0-vizej7"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild\",diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"cyclegan\"] # List of all datasets from first to current task\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    \"name\":         \"tkd_gau_big_cycle\",    # Used for tagging the experiment on logs\n",
        "    \"data\":         \"../datasets\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"../checkpoints/model_best_accuracy.pth\", # load weights of task i-1 from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"name_sources\": \"biggan_cyclegan\",        # Ordered list of previous task: dataset1_dataset2_dataseti-1\n",
        "    \"name_target\":   \"cyclegan\",            # Task i dataset\n",
        "    \"checkpoint_path\": \"../checkpoints/test\", # Save folder (the task subfolder is automatically created)\n",
        "    \"lr_schedule\":  \"cosine\",               # cosine, onecycle\n",
        "    \"test\":         False,                  # False\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"loss_schedule\": True,                  # True, False\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"KD_alpha\":     1,                    # alpha factor for kd loss\n",
        "    \"num_store\":    5,                      # Stores for representation loss\n",
        "    \"lr\":           0.005,                  # Learning rate\n",
        "    \"decay_factor\": 0.9,\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"epochs\":       5,                    # Traning epochs\n",
        "    \"early_stop\":   True,                   # True, False\n",
        "    \"patience\":     25,                     # Early stop patience\n",
        "    \"ds_cfg\":       ds_cfg,\n",
        "\n",
        "    \"source_datasets\":{\"cyclegan\": \"../datasets/cyclegan\"},                      \n",
        "    \"target_dataset_name\":\"biggan\",\n",
        "    \"target_dataset_dir\":\"../datasets/biggan\",\n",
        "    \"train\":True\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cfg = SimpleNamespace(**train_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(config_file='../model_config.conf', target_name='biggan', target_dir='../datasets/biggan', source_datasets={'cyclegan': '../datasets/cyclegan'}, num_gpu='0', checkpoint_path='./', output_path='./', epochs='5                            # Training epochs', early_stop='25                       # Early stop patience: use \"\" to disable', batch_size='64', resolution='128', lr='0.005', kd_alpha='0.5', decay_factor='0.9', flip='False')\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Making Loader for Continual Learning..\n",
            "===> Making Loader : cyclegan\n",
            "DATASET PATHS\n",
            "val_source_dir  {'cyclegan': '../datasets/cyclegan'}\n",
            "val_target_dir  ../datasets/biggan/val\n",
            "train_dir  ../datasets/biggan/train\n",
            "Dataset available in train_loaders:  train / val\n",
            "Dataset available in val_loaders:  cyclegan\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fra/AIGCDetection-CI-CD/src/train.py\", line 183, in <module>\n",
            "    train(args)\n",
            "  File \"/home/fra/AIGCDetection-CI-CD/src/train.py\", line 63, in train\n",
            "    teacher_model, student_model = load_models(args.weight, args.network, num_gpu = args.num_gpu)\n",
            "                                               ^^^^^^^^^^^\n",
            "AttributeError: 'Namespace' object has no attribute 'weight'\n"
          ]
        }
      ],
      "source": [
        "!python train.py -c ../model_config.conf --input_model ../checkpoints/model_best_accuracy.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykvxXn34S2dg",
        "outputId": "40d83eca-dedb-4861-8ead-11286f6718f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "Source Name : biggan\n",
            "Source Name : cyclegan\n",
            "\n",
            "===> Making Loader for Continual Learning..\n",
            "===> Making Loader : ../datasets/biggan/val\n",
            "===> Making Loader : ../datasets/cyclegan/val\n",
            "DATASET PATHS\n",
            "val_source_dir  ['../datasets/biggan/val', '../datasets/cyclegan/val']\n",
            "val_target_dir  ../datasets/cyclegan/val\n",
            "train_dir  ../datasets/cyclegan/train/\n",
            "Dataset available in dicLoader:  train_target / val_target / val_dataset1 / val_dataset2\n",
            "Dataset available in dicCoReD:  train_target_dataset / train_target_forCorrect\n",
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet from ../checkpoints/model_best_accuracy.pth\n",
            "Loaded\n",
            "Apply Cosine learning rate schedule\n",
            "Adjusting learning rate of group 0 to 5.0000e-03.\n",
            "Loading train target for correcting ...  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                      | 0/25 [00:00<?, ?it/s]/home/fra/miniconda3/envs/AIGCdetection/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706726118919/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "100%|█████████████| 25/25 [00:03<00:00,  7.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "list_length_realfakeloader : [[23, 26, 26, 51, 615], [15, 26, 56, 125, 520]]\n",
            "Ratio of already correctly predicted in training set: 0.946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List feature size:  (2, 5, 2048)\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 13.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.2208 | MainLoss:0.2208 | top:91.4122\n",
            "[VAL Acc] Target: 91.41%\n",
            "===> Starting the dataset <source name>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 14.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.3038 | MainLoss:0.3038 | top:89.0000\n",
            "[VAL Acc] Source 1-th: 89.00%\n",
            "===> Starting the dataset <source name>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 15.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1932 | MainLoss:0.1932 | top:91.4122\n",
            "[VAL Acc] Source 2-th: 91.41%\n",
            "[VAL Acc] Avg 90.61%\n",
            " Save initial model weight\n",
            "Start training in 5 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'loss_clampping' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kd_train(cfg)\n",
            "Cell \u001b[0;32mIn[7], line 103\u001b[0m, in \u001b[0;36mkd_train\u001b[0;34m(args, log)\u001b[0m\n\u001b[1;32m    101\u001b[0m loss_main \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m    102\u001b[0m loss_kd \u001b[38;5;241m=\u001b[39m loss_fn_kd(outputs, targets, teacher_outputs)\n\u001b[0;32m--> 103\u001b[0m loss_kd \u001b[38;5;241m=\u001b[39m loss_clampping(loss_kd, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1800\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#REP loss\u001b[39;00m\n\u001b[1;32m    106\u001b[0m list_features_std \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m()]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_clampping' is not defined"
          ]
        }
      ],
      "source": [
        "kd_train(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjqcB8Vn_Dkj"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lqLXKz3P_GuJ"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild,diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"cyclegan\"] # List of all datasets to test\n",
        "}\n",
        "\n",
        "\n",
        "evaluate_cfg = {\n",
        "\n",
        "    \"name\":         \"ekd_gau_big_cycle\",    # Used for tagging the experiment on logs\n",
        "    \"dataroot\":     \"../datasets\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"../checkpoints/model_best_accuracy.pth\", # load weights of task i from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"test\":         True,                   # True\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"ds_cfg\":        ds_cfg,\n",
        "}\n",
        "\n",
        "evaluate_cfg = SimpleNamespace(**evaluate_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1hdy8PL_LyX",
        "outputId": "27ea677a-9f95-493b-ed4b-fa5c5e9e7e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet from ../checkpoints/model_best_accuracy.pth\n",
            "Loaded\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from ../datasets/cyclegan/test\n",
            "Source: \n",
            "Target: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████| 9/9 [00:00<00:00, 11.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss:0.1773 | Acc:0.9286 | Acc Real:0.9108 | Acc Fake:0.9460 | Ap:0.8983\n",
            "Num reals: 273, Num fakes: 273\n",
            "\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9466    0.9084    0.9271       273\n",
            "           1     0.9120    0.9487    0.9300       273\n",
            "\n",
            "   micro avg     0.9286    0.9286    0.9286       546\n",
            "   macro avg     0.9293    0.9286    0.9285       546\n",
            "weighted avg     0.9293    0.9286    0.9285       546\n",
            " samples avg     0.9286    0.9286    0.9286       546\n",
            "\n",
            "Avg: | Acc:0.9286 | Acc Real:0.9108 | Acc Fake:0.9460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(evaluate_cfg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
