{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI1CsdAZN7PS"
      },
      "source": [
        "### Startup\n",
        "\n",
        "This code is meant to be executed on Google Colab.\n",
        "To use it locally change *COLAB_MODE* to False.\n",
        "\n",
        "**Note**: remember to change *workdir* accordingly, the notebook must be runned inside the root project folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzSZjcNvoPgs",
        "outputId": "13bebaa2-3950-48c0-d7b6-73a9bb887b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fra/AIGCDetection-CI-CD/src\n"
          ]
        }
      ],
      "source": [
        "workdir = \"./src\"\n",
        "%cd $workdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MlHM4qAGEMR"
      },
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "The code incorporates elements derived from the code originally published in the research paper, which can be found here: https://github.com/alsgkals2/CoReD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEuLI5ANwixJ",
        "outputId": "b27e1eb0-c412-403d-cfd4-f89a869c9159"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from utils.common_functions import *\n",
        "from utils.cored_functions import *\n",
        "from utils.data_loader import create_dataloader\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "\n",
        "\n",
        "from utils.data_loader import create_dataloader\n",
        "from utils.model_loader import load_models\n",
        "from utils.train_utils import *\n",
        "\n",
        "\n",
        "def kd_train(args, log = None):\n",
        "\n",
        "    # Init\n",
        "    torch.cuda.empty_cache()\n",
        "    device = 'cuda' if args.num_gpu else 'cpu'\n",
        "    lr = args.lr\n",
        "    KD_alpha = args.KD_alpha\n",
        "    num_class = args.num_class\n",
        "    num_store_per=5\n",
        "\n",
        "\n",
        "    # Load datasets and models\n",
        "    dicLoader, dicCoReD, dicSourceName = initialization(args)\n",
        "    print(\"Dataset available in dicLoader: \", \" / \".join([n for n in dicLoader]))\n",
        "    print(\"Dataset available in dicCoReD: \", \" / \".join([n for n in dicCoReD]))\n",
        "    teacher_model, student_model = load_models(args.weight, args.network, num_gpu = args.num_gpu)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=0.1)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    if args.lr_schedule == \"cosine\":\n",
        "        print(\"Apply Cosine learning rate schedule\")\n",
        "        lr_scheduler = CosineAnnealingLR(optimizer=optimizer,\n",
        "                                        T_max=10,\n",
        "                                        eta_min=1e-5,\n",
        "                                        verbose=True)\n",
        "    else:\n",
        "        print(f\"Input: {args.lr_schedule}, No learning rate schedule applied ... \")\n",
        "\n",
        "\n",
        "    # Pre-evaluation\n",
        "    print(\"Loading train target for correcting ...  \")\n",
        "    _list_correct, _ = func_correct(teacher_model.to(device), dicCoReD['train_target_forCorrect'])\n",
        "    _correct_loaders, already_correct_ratio = GetSplitLoaders_BinaryClasses(_list_correct, dicCoReD['train_target_dataset'], get_augs(args)[0], num_store_per)\n",
        "    print(\"Ratio of already correctly predicted in training set: {:.3f}\".format(already_correct_ratio))\n",
        "    list_features = GetListTeacherFeatureFakeReal(teacher_model.module if ',' in args.num_gpu else teacher_model ,_correct_loaders, mode=args.network)\n",
        "    list_features = np.array(list_features)\n",
        "    print(\"List feature size: \", list_features.shape)\n",
        "\n",
        "    # Initial validation\n",
        "    _, _, test_acc = Test(dicLoader['val_target'], student_model, criterion, log = log, source_name = args.name_target)\n",
        "    total_acc = test_acc\n",
        "    print(\"[VAL Acc] Target: {:.2f}%\".format( test_acc))\n",
        "    cnt = 1\n",
        "    for name in dicLoader:\n",
        "        if 'val_dataset' in name or 'val_source' in name:\n",
        "            if 'val_source' in name:\n",
        "                source_name = name.split(\"_\")[2]\n",
        "            else:\n",
        "                source_name = \"<source name>\"\n",
        "\n",
        "            _, _, source_acc = Test(dicLoader[name], student_model, criterion, log = log, source_name = source_name)\n",
        "            total_acc += source_acc\n",
        "            print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "            cnt += 1\n",
        "\n",
        "    print(\"[VAL Acc] Avg {:.2f}%\\n Save initial model weight\".format(total_acc / cnt))\n",
        "    best_acc = total_acc\n",
        "    \n",
        "    is_best_acc = False\n",
        "    cur_patience = 0 # Early stop and saving\n",
        "    l_weight = 1.0 # reduce the conservation when performance does not gain much\n",
        "    print(f\"Start training in {args.epochs} epochs\")\n",
        "\n",
        "\n",
        "    # ------- START TRAINING ------- #\n",
        "    for epoch in range(args.epochs):\n",
        "        correct,total = 0,0\n",
        "        teacher_model.eval()\n",
        "        student_model.train()\n",
        "        disp = {}\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dicLoader['train_target']):\n",
        "            # Load data\n",
        "            step = (batch_idx+1) * (epoch+1)\n",
        "            inputs = inputs.to(device).to(torch.float32)\n",
        "            targets = targets.to(device).to(torch.long)\n",
        "            if torch.isnan(inputs).any() or torch.isnan(targets).any():\n",
        "                raise ValueError(\"There is Nan values in input or target\")\n",
        "\n",
        "            # Forward\n",
        "            teacher_outputs = teacher_model(inputs)\n",
        "            penul_ft, outputs = student_model(inputs, True)\n",
        "\n",
        "            # Losses\n",
        "            loss_main = criterion(outputs, targets)\n",
        "            loss_kd = loss_fn_kd(outputs, targets, teacher_outputs)\n",
        "            loss_kd = loss_clampping(loss_kd, 0, 1800)\n",
        "\n",
        "            #REP loss\n",
        "            list_features_std = [list(), list()]\n",
        "            rep_ft_partitions = correct_binary_simple(inputs=inputs, penul_ft=penul_ft, outputs=outputs, targets=targets) # rep_ft_partitions : 5 x 2\n",
        "            for j in range(num_store_per):\n",
        "                for i in range(num_class):\n",
        "                    if(np.count_nonzero(list_features[i][j])==0 or len(rep_ft_partitions[j][i])==0):\n",
        "                      continue\n",
        "                    feat = torch.stack(rep_ft_partitions[j][i], dim=0).mean(dim=0)\n",
        "                    assert feat.size(-1) == 2048 or feat.size(-1) == 512 or feat.size(-1) == 1280\n",
        "                    rep_loss = (feat.to(torch.float32)  - torch.tensor(list_features[i][j]).to(device).to(torch.float32)).pow(2).mean()\n",
        "                    list_features_std[i].append(rep_loss)\n",
        "            sne_loss = 0.0\n",
        "            for fs in list_features_std:\n",
        "                for ss in fs:\n",
        "                    if ss.requires_grad:\n",
        "                        sne_loss += ss\n",
        "            sne_loss = loss_clampping(sne_loss, 0, 1) # REP Loss is clampped in this project\n",
        "\n",
        "            # Total loss\n",
        "            loss = loss_main  + l_weight*(loss_kd + sne_loss)\n",
        "            sne_item = sne_loss if type(sne_loss) == float else sne_loss.item()\n",
        "\n",
        "            # Log and display\n",
        "            disp[\"CE\"] = loss_main.item()\n",
        "            disp[\"KD\"] = loss_kd.item() if loss_kd > 0 else 0.0\n",
        "            disp[\"REP\"] = sne_item if sne_loss > 0 else 0.0\n",
        "            call = ' | '.join([\"{}: {:.4f}\".format(k, v) for k, v in disp.items()])\n",
        "            print(\"Train Epoch: {e:03d} Batch: {batch:05d}/{size:05d} | Loss: {loss:.4f} | {call}\"\n",
        "                            .format(e=epoch+1, batch=batch_idx+1, size=len(dicLoader['train_target']), loss=loss.item(), call=call))\n",
        "\n",
        "            # Learn!\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if args.lr_schedule == \"onecycle\":\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            # Predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += len(targets)\n",
        "\n",
        "        if args.lr_schedule == \"cosine\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "\n",
        "        # ----- Validation ------ #\n",
        "\n",
        "        # Current task\n",
        "        _, _, test_acc = Test(dicLoader['val_target'], student_model, criterion, log = None, source_name = args.name_target)\n",
        "        total_acc = test_acc\n",
        "        print(\"[VAL Acc] Target: {:.2f}%\".format( test_acc))\n",
        "\n",
        "        # Past tasks\n",
        "        cnt = 1\n",
        "        for name in dicLoader:\n",
        "            if 'val_dataset' in name or 'val_source' in name:\n",
        "                if 'val_dataset' in name:\n",
        "                    source_name = dicSourceName[f'source{cnt}']\n",
        "                else:\n",
        "                    source_name = dicSourceName['source']\n",
        "\n",
        "                _, _, source_acc = Test(dicLoader[name], student_model, criterion, log = None, source_name = source_name)\n",
        "                total_acc += source_acc\n",
        "                print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "                cnt += 1\n",
        "        print(\"[VAL Acc] Avg {:.2f}%\".format(total_acc / cnt))\n",
        "\n",
        "        # Early stop\n",
        "        is_best_acc = total_acc > best_acc\n",
        "        if is_best_acc:\n",
        "                print(\"VAL Acc improve from {:.2f}% to {:.2f}%\".format(best_acc/cnt, total_acc/cnt))\n",
        "                cur_patience = 0\n",
        "        else:\n",
        "            cur_patience += 1\n",
        "        if args.loss_schedule and (cur_patience > 0 and cur_patience % 4 == 0):\n",
        "                l_weight = ReduceWeightOnPlateau(l_weight, args.decay_factor)\n",
        "\n",
        "        # Save\n",
        "        best_acc = max(total_acc,best_acc)\n",
        "        if  is_best_acc:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': student_model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict()},\n",
        "            checkpoint = savepath,\n",
        "            filename = 'epoch_{}'.format( epoch+1 if (epoch+1)%10==0 else ''),\n",
        "            ACC_BEST=is_best_acc\n",
        "            )\n",
        "            print('Save best model' if is_best_acc else f'Save checkpoint model @ {epoch+1}')\n",
        "        if args.early_stop and (cur_patience == args.patience):\n",
        "            print(\"Early stopping ...\")\n",
        "            return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7csYl99zAH"
      },
      "source": [
        "# Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avxhBq4HFhOO",
        "outputId": "60506fd7-31dc-443e-8a10-30b5a64132fa"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from common_functions import initialization, load_models, AverageMeter\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, average_precision_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def evaluate(args, global_writer=None):\n",
        "\n",
        "    # Config\n",
        "    setattr(args,\"name_sources\", \"\")\n",
        "    setattr(args,\"name_target\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "    # Load model\n",
        "    _, model = load_models(args.weight, args.network, args.num_gpu, not args.test)\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Load datasets\n",
        "    tot_avg_acc, real_avg_acc, fake_avg_acc = 0.0, 0.0 ,0.0\n",
        "    for ds_name in args.ds_cfg[\"fake_ds\"]:\n",
        "      data_folder = f\"{args.dataroot}/{ds_name}/test\"\n",
        "      setattr(args,\"data\", data_folder)\n",
        "      dicLoader,_, dicSourceName = initialization(args)\n",
        "\n",
        "\n",
        "      for key, name in zip(dicLoader, dicSourceName):\n",
        "        # Init\n",
        "        global best_acc\n",
        "        correct, total =0,0\n",
        "        losses = AverageMeter()\n",
        "        arc = AverageMeter()\n",
        "        acc_real = AverageMeter()\n",
        "        acc_fake = AverageMeter()\n",
        "        sum_of_AUROC=[]\n",
        "        target=[]\n",
        "        output = []\n",
        "        y_true=np.zeros((0,2),dtype=np.int8)\n",
        "        y_pred=np.zeros((0,2),dtype=np.int8)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          model.eval()\n",
        "          model.cuda()\n",
        "\n",
        "          for (inputs, targets) in tqdm(dicLoader[key], ncols=50):\n",
        "              # Predict\n",
        "              inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, targets)\n",
        "              _, predicted = torch.max(outputs, 1)\n",
        "              correct = (predicted == targets).squeeze()\n",
        "              total += len(targets)\n",
        "              losses.update(loss.data.tolist(), inputs.size(0))\n",
        "              _y_pred = outputs.cpu().detach()\n",
        "              _y_gt = targets.cpu().detach().numpy()\n",
        "              acc = [0, 0]\n",
        "              class_total = [0, 0]\n",
        "              for i in range(len(targets)):\n",
        "                  label = targets[i]\n",
        "                  acc[label] += 1 if correct[i].item() == True else 0\n",
        "                  class_total[label] += 1\n",
        "\n",
        "              losses.update(loss.data.tolist(), inputs.size(0))\n",
        "              if (class_total[0] != 0):\n",
        "                  acc_real.update(acc[0] / class_total[0])\n",
        "              if (class_total[1] != 0):\n",
        "                  acc_fake.update(acc[1] / class_total[1])\n",
        "\n",
        "              target.append(_y_gt)\n",
        "              output.append(_y_pred.numpy()[:,1])\n",
        "              auroc=None\n",
        "              try:\n",
        "                  auroc = roc_auc_score(_y_gt, outputs[:,1].cpu().detach().numpy())\n",
        "              except ValueError:\n",
        "                  pass\n",
        "              sum_of_AUROC.append(auroc)\n",
        "              _y_true = np.array(torch.zeros(targets.shape[0],2), dtype=np.int8)\n",
        "              _y_gt = _y_gt.astype(int)\n",
        "              for _ in range(len(targets)):\n",
        "                  _y_true[_][_y_gt[_]] = 1\n",
        "              y_true = np.concatenate((y_true,_y_true))\n",
        "              a = _y_pred.argmax(1)\n",
        "              _y_pred = np.array(torch.zeros(_y_pred.shape).scatter(1, a.unsqueeze(1), 1),dtype=np.int8)\n",
        "              y_pred = np.concatenate((y_pred,_y_pred))\n",
        "\n",
        "          n_real_samples = np.count_nonzero(y_true, axis=0)[0]\n",
        "          n_fake_samples = np.count_nonzero(y_true, axis=0)[1]\n",
        "          acc = accuracy_score(y_true, y_pred)\n",
        "          ap = average_precision_score(y_true, y_pred)\n",
        "\n",
        "          result = classification_report(y_true, y_pred,\n",
        "                                              labels=None,\n",
        "                                              target_names=None,\n",
        "                                              sample_weight=None,\n",
        "                                              digits=4,\n",
        "                                              output_dict=False,\n",
        "                                              zero_division='warn')\n",
        "\n",
        "\n",
        "          print(f\"\\nLoss:{losses.avg:.4f} | Acc:{acc:.4f} | Acc Real:{acc_real.avg:.4f} | Acc Fake:{acc_fake.avg:.4f} | Ap:{ap:.4f}\")\n",
        "          print(f'Num reals: {n_real_samples}, Num fakes: {n_fake_samples}')\n",
        "          print(\"\\n\\n\",result)\n",
        "\n",
        "          tot_avg_acc += acc\n",
        "          real_avg_acc += acc_real.avg\n",
        "          fake_avg_acc += acc_fake.avg\n",
        "\n",
        "        \n",
        "    total_ds = len(args.ds_cfg[\"fake_ds\"])\n",
        "    print(f\"Avg: | Acc:{tot_avg_acc/total_ds:.4f} | Acc Real:{real_avg_acc/total_ds:.4f} | Acc Fake:{fake_avg_acc/total_ds:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RE9O_Ego1iy"
      },
      "source": [
        "# Workspace\n",
        "\n",
        "In this section it is possible to run trainings and evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO7-OzjtSrus"
      },
      "source": [
        "## Traning\n",
        "\n",
        "\n",
        "1.   Configurate the training\n",
        "2.   Build the dataset\n",
        "3.   Train!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrQgEkqyqu8O"
      },
      "source": [
        "### Knowledge distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-BWn0-vizej7"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild\",diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"cyclegan\"] # List of all datasets from first to current task\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    \"name\":         \"tkd_gau_big_cycle\",    # Used for tagging the experiment on logs\n",
        "    \"data\":         \"../datasets\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"../checkpoints/model_best_accuracy.pth\", # load weights of task i-1 from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"name_sources\": \"biggan_cyclegan\",        # Ordered list of previous task: dataset1_dataset2_dataseti-1\n",
        "    \"name_target\":   \"cyclegan\",            # Task i dataset\n",
        "    \"checkpoint_path\": \"../checkpoints/test\", # Save folder (the task subfolder is automatically created)\n",
        "    \"lr_schedule\":  \"cosine\",               # cosine, onecycle\n",
        "    \"test\":         False,                  # False\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"loss_schedule\": True,                  # True, False\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"KD_alpha\":     1,                    # alpha factor for kd loss\n",
        "    \"num_store\":    5,                      # Stores for representation loss\n",
        "    \"lr\":           0.005,                  # Learning rate\n",
        "    \"decay_factor\": 0.9,\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"epochs\":       5,                    # Traning epochs\n",
        "    \"early_stop\":   True,                   # True, False\n",
        "    \"patience\":     25,                     # Early stop patience\n",
        "    \"ds_cfg\":       ds_cfg,\n",
        "\n",
        "    \"source_datasets\":{\"cyclegan\": \"../datasets/cyclegan\"},                      \n",
        "    \"target_dataset_name\":\"biggan\",\n",
        "    \"target_dataset_dir\":\"../datasets/biggan\",\n",
        "    \"train\":True\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cfg = SimpleNamespace(**train_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Making Loader for Continual Learning..\n",
            "===> Making Loader : cyclegan\n",
            "DATASET PATHS\n",
            "val_source_dir  {'cyclegan': '../datasets/cyclegan'}\n",
            "val_target_dir  ../datasets/biggan/val/\n",
            "train_dir  ../datasets/biggan/train/\n",
            "Dataset available in train_loaders:  train / val\n",
            "Dataset available in val_loaders:  cyclegan\n",
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet from ../checkpoints/model_best_accuracy.pth\n",
            "Loaded\n",
            "Apply Cosine learning rate schedule\n",
            "Adjusting learning rate of group 0 to 5.0000e-03.\n",
            "Start training in 5 epochs\n",
            "Train Epoch: 001 Batch: 00001/00038 | Loss: 1060.6809 | CE: 0.2185 | KD: 1060.4624\n",
            "Train Epoch: 001 Batch: 00002/00038 | Loss: 1060.1849 | CE: 0.1700 | KD: 1060.0149\n",
            "Train Epoch: 001 Batch: 00003/00038 | Loss: 1060.1692 | CE: 0.1514 | KD: 1060.0177\n",
            "Train Epoch: 001 Batch: 00004/00038 | Loss: 1060.0497 | CE: 0.1415 | KD: 1059.9082\n",
            "Train Epoch: 001 Batch: 00005/00038 | Loss: 1060.1500 | CE: 0.1805 | KD: 1059.9696\n",
            "Train Epoch: 001 Batch: 00006/00038 | Loss: 1060.2673 | CE: 0.1605 | KD: 1060.1068\n",
            "Train Epoch: 001 Batch: 00007/00038 | Loss: 1060.0050 | CE: 0.1231 | KD: 1059.8820\n",
            "Train Epoch: 001 Batch: 00008/00038 | Loss: 1060.1447 | CE: 0.1895 | KD: 1059.9552\n",
            "Train Epoch: 001 Batch: 00009/00038 | Loss: 1060.2324 | CE: 0.1764 | KD: 1060.0560\n",
            "Train Epoch: 001 Batch: 00010/00038 | Loss: 1059.9323 | CE: 0.0837 | KD: 1059.8485\n",
            "Train Epoch: 001 Batch: 00011/00038 | Loss: 1060.0358 | CE: 0.1418 | KD: 1059.8939\n",
            "Train Epoch: 001 Batch: 00012/00038 | Loss: 1060.1169 | CE: 0.1773 | KD: 1059.9397\n",
            "Train Epoch: 001 Batch: 00013/00038 | Loss: 1060.1754 | CE: 0.2130 | KD: 1059.9624\n",
            "Train Epoch: 001 Batch: 00014/00038 | Loss: 1060.0931 | CE: 0.1430 | KD: 1059.9502\n",
            "Train Epoch: 001 Batch: 00015/00038 | Loss: 1060.2130 | CE: 0.2087 | KD: 1060.0043\n",
            "Train Epoch: 001 Batch: 00016/00038 | Loss: 1060.1725 | CE: 0.1306 | KD: 1060.0419\n",
            "Train Epoch: 001 Batch: 00017/00038 | Loss: 1060.1593 | CE: 0.2006 | KD: 1059.9587\n",
            "Train Epoch: 001 Batch: 00018/00038 | Loss: 1060.2017 | CE: 0.1333 | KD: 1060.0684\n",
            "Train Epoch: 001 Batch: 00019/00038 | Loss: 1060.1796 | CE: 0.1366 | KD: 1060.0430\n",
            "Train Epoch: 001 Batch: 00020/00038 | Loss: 1061.0104 | CE: 0.5255 | KD: 1060.4849\n",
            "Train Epoch: 001 Batch: 00021/00038 | Loss: 1060.3262 | CE: 0.2419 | KD: 1060.0844\n",
            "Train Epoch: 001 Batch: 00022/00038 | Loss: 1060.0836 | CE: 0.0790 | KD: 1060.0046\n",
            "Train Epoch: 001 Batch: 00023/00038 | Loss: 1060.1968 | CE: 0.1734 | KD: 1060.0233\n",
            "Train Epoch: 001 Batch: 00024/00038 | Loss: 1060.0529 | CE: 0.1495 | KD: 1059.9033\n",
            "Train Epoch: 001 Batch: 00025/00038 | Loss: 1060.0365 | CE: 0.0658 | KD: 1059.9707\n",
            "Train Epoch: 001 Batch: 00026/00038 | Loss: 1060.0259 | CE: 0.1258 | KD: 1059.9001\n",
            "Train Epoch: 001 Batch: 00027/00038 | Loss: 1060.0375 | CE: 0.1205 | KD: 1059.9170\n",
            "Train Epoch: 001 Batch: 00028/00038 | Loss: 1060.1749 | CE: 0.1596 | KD: 1060.0154\n",
            "Train Epoch: 001 Batch: 00029/00038 | Loss: 1060.0989 | CE: 0.1383 | KD: 1059.9606\n",
            "Train Epoch: 001 Batch: 00030/00038 | Loss: 1060.0015 | CE: 0.0880 | KD: 1059.9135\n",
            "Train Epoch: 001 Batch: 00031/00038 | Loss: 1060.1814 | CE: 0.1898 | KD: 1059.9916\n",
            "Train Epoch: 001 Batch: 00032/00038 | Loss: 1060.0680 | CE: 0.1392 | KD: 1059.9288\n",
            "Train Epoch: 001 Batch: 00033/00038 | Loss: 1060.0795 | CE: 0.1661 | KD: 1059.9133\n",
            "Train Epoch: 001 Batch: 00034/00038 | Loss: 1059.9802 | CE: 0.0895 | KD: 1059.8907\n",
            "Train Epoch: 001 Batch: 00035/00038 | Loss: 1060.1182 | CE: 0.1286 | KD: 1059.9895\n",
            "Train Epoch: 001 Batch: 00036/00038 | Loss: 1060.0068 | CE: 0.1238 | KD: 1059.8831\n",
            "Train Epoch: 001 Batch: 00037/00038 | Loss: 1060.0084 | CE: 0.1022 | KD: 1059.9061\n",
            "Train Epoch: 001 Batch: 00038/00038 | Loss: 1060.3730 | CE: 0.2332 | KD: 1060.1399\n",
            "Adjusting learning rate of group 0 to 4.8779e-03.\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.2060 | acc:91.6250\n",
            "[VAL Acc] Target: 91.62%\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.2247 | acc:91.9847\n",
            "[VAL Acc] Source cyclegan: 91.98%\n",
            "[VAL Acc] Avg 91.80%\n",
            "VAL Acc improve from 0.00% to 91.80%\n",
            "Save best model\n",
            "Train Epoch: 002 Batch: 00001/00038 | Loss: 1060.2858 | CE: 0.2122 | KD: 1060.0736\n",
            "Train Epoch: 002 Batch: 00002/00038 | Loss: 1060.2515 | CE: 0.0962 | KD: 1060.1553\n",
            "Train Epoch: 002 Batch: 00003/00038 | Loss: 1060.1381 | CE: 0.1071 | KD: 1060.0310\n",
            "Train Epoch: 002 Batch: 00004/00038 | Loss: 1060.1709 | CE: 0.1906 | KD: 1059.9803\n",
            "Train Epoch: 002 Batch: 00005/00038 | Loss: 1060.0564 | CE: 0.0914 | KD: 1059.9651\n",
            "Train Epoch: 002 Batch: 00006/00038 | Loss: 1060.3228 | CE: 0.1643 | KD: 1060.1584\n",
            "Train Epoch: 002 Batch: 00007/00038 | Loss: 1060.2075 | CE: 0.2241 | KD: 1059.9834\n",
            "Train Epoch: 002 Batch: 00008/00038 | Loss: 1060.0010 | CE: 0.1499 | KD: 1059.8511\n",
            "Train Epoch: 002 Batch: 00009/00038 | Loss: 1060.2194 | CE: 0.1348 | KD: 1060.0846\n",
            "Train Epoch: 002 Batch: 00010/00038 | Loss: 1060.1224 | CE: 0.1830 | KD: 1059.9395\n",
            "Train Epoch: 002 Batch: 00011/00038 | Loss: 1060.2343 | CE: 0.2476 | KD: 1059.9866\n",
            "Train Epoch: 002 Batch: 00012/00038 | Loss: 1060.0214 | CE: 0.1360 | KD: 1059.8854\n",
            "Train Epoch: 002 Batch: 00013/00038 | Loss: 1060.1249 | CE: 0.1269 | KD: 1059.9980\n",
            "Train Epoch: 002 Batch: 00014/00038 | Loss: 1060.2073 | CE: 0.1372 | KD: 1060.0701\n",
            "Train Epoch: 002 Batch: 00015/00038 | Loss: 1060.2278 | CE: 0.1856 | KD: 1060.0422\n",
            "Train Epoch: 002 Batch: 00016/00038 | Loss: 1059.9626 | CE: 0.0845 | KD: 1059.8781\n",
            "Train Epoch: 002 Batch: 00017/00038 | Loss: 1060.0781 | CE: 0.1571 | KD: 1059.9210\n",
            "Train Epoch: 002 Batch: 00018/00038 | Loss: 1060.0710 | CE: 0.1235 | KD: 1059.9475\n",
            "Train Epoch: 002 Batch: 00019/00038 | Loss: 1059.9688 | CE: 0.0874 | KD: 1059.8813\n",
            "Train Epoch: 002 Batch: 00020/00038 | Loss: 1060.1721 | CE: 0.0987 | KD: 1060.0735\n",
            "Train Epoch: 002 Batch: 00021/00038 | Loss: 1060.0134 | CE: 0.0807 | KD: 1059.9327\n",
            "Train Epoch: 002 Batch: 00022/00038 | Loss: 1060.1700 | CE: 0.1681 | KD: 1060.0020\n",
            "Train Epoch: 002 Batch: 00023/00038 | Loss: 1059.9968 | CE: 0.1052 | KD: 1059.8916\n",
            "Train Epoch: 002 Batch: 00024/00038 | Loss: 1059.9714 | CE: 0.1108 | KD: 1059.8606\n",
            "Train Epoch: 002 Batch: 00025/00038 | Loss: 1060.0797 | CE: 0.1533 | KD: 1059.9264\n",
            "Train Epoch: 002 Batch: 00026/00038 | Loss: 1060.1068 | CE: 0.0813 | KD: 1060.0255\n",
            "Train Epoch: 002 Batch: 00027/00038 | Loss: 1060.2783 | CE: 0.2964 | KD: 1059.9819\n",
            "Train Epoch: 002 Batch: 00028/00038 | Loss: 1060.0184 | CE: 0.1761 | KD: 1059.8423\n",
            "Train Epoch: 002 Batch: 00029/00038 | Loss: 1060.0691 | CE: 0.0963 | KD: 1059.9728\n",
            "Train Epoch: 002 Batch: 00030/00038 | Loss: 1060.0835 | CE: 0.1832 | KD: 1059.9003\n",
            "Train Epoch: 002 Batch: 00031/00038 | Loss: 1060.1206 | CE: 0.2486 | KD: 1059.8721\n",
            "Train Epoch: 002 Batch: 00032/00038 | Loss: 1060.4896 | CE: 0.1353 | KD: 1060.3544\n",
            "Train Epoch: 002 Batch: 00033/00038 | Loss: 1060.2773 | CE: 0.1007 | KD: 1060.1766\n",
            "Train Epoch: 002 Batch: 00034/00038 | Loss: 1059.9949 | CE: 0.1338 | KD: 1059.8611\n",
            "Train Epoch: 002 Batch: 00035/00038 | Loss: 1060.1154 | CE: 0.1715 | KD: 1059.9438\n",
            "Train Epoch: 002 Batch: 00036/00038 | Loss: 1060.1202 | CE: 0.1295 | KD: 1059.9907\n",
            "Train Epoch: 002 Batch: 00037/00038 | Loss: 1060.3895 | CE: 0.2356 | KD: 1060.1539\n",
            "Train Epoch: 002 Batch: 00038/00038 | Loss: 1060.0931 | CE: 0.1299 | KD: 1059.9633\n",
            "Adjusting learning rate of group 0 to 4.5235e-03.\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.1934 | acc:92.5000\n",
            "[VAL Acc] Target: 92.50%\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.2787 | acc:87.4046\n",
            "[VAL Acc] Source cyclegan: 87.40%\n",
            "[VAL Acc] Avg 89.95%\n",
            "Train Epoch: 003 Batch: 00001/00038 | Loss: 1060.0347 | CE: 0.0644 | KD: 1059.9702\n",
            "Train Epoch: 003 Batch: 00002/00038 | Loss: 1060.0558 | CE: 0.0927 | KD: 1059.9630\n",
            "Train Epoch: 003 Batch: 00003/00038 | Loss: 1060.0344 | CE: 0.1538 | KD: 1059.8806\n",
            "Train Epoch: 003 Batch: 00004/00038 | Loss: 1059.9637 | CE: 0.1246 | KD: 1059.8391\n",
            "Train Epoch: 003 Batch: 00005/00038 | Loss: 1060.0039 | CE: 0.1327 | KD: 1059.8712\n",
            "Train Epoch: 003 Batch: 00006/00038 | Loss: 1059.9550 | CE: 0.1184 | KD: 1059.8365\n",
            "Train Epoch: 003 Batch: 00007/00038 | Loss: 1060.1471 | CE: 0.1360 | KD: 1060.0111\n",
            "Train Epoch: 003 Batch: 00008/00038 | Loss: 1060.0049 | CE: 0.1255 | KD: 1059.8794\n",
            "Train Epoch: 003 Batch: 00009/00038 | Loss: 1060.0471 | CE: 0.1095 | KD: 1059.9376\n",
            "Train Epoch: 003 Batch: 00010/00038 | Loss: 1060.1519 | CE: 0.1742 | KD: 1059.9777\n",
            "Train Epoch: 003 Batch: 00011/00038 | Loss: 1060.0375 | CE: 0.1261 | KD: 1059.9114\n",
            "Train Epoch: 003 Batch: 00012/00038 | Loss: 1059.9197 | CE: 0.0814 | KD: 1059.8383\n",
            "Train Epoch: 003 Batch: 00013/00038 | Loss: 1060.1606 | CE: 0.1961 | KD: 1059.9646\n",
            "Train Epoch: 003 Batch: 00014/00038 | Loss: 1060.0421 | CE: 0.1272 | KD: 1059.9149\n",
            "Train Epoch: 003 Batch: 00015/00038 | Loss: 1060.0295 | CE: 0.0933 | KD: 1059.9363\n",
            "Train Epoch: 003 Batch: 00016/00038 | Loss: 1060.0247 | CE: 0.0444 | KD: 1059.9802\n",
            "Train Epoch: 003 Batch: 00017/00038 | Loss: 1059.9683 | CE: 0.0855 | KD: 1059.8828\n",
            "Train Epoch: 003 Batch: 00018/00038 | Loss: 1060.1526 | CE: 0.1228 | KD: 1060.0298\n",
            "Train Epoch: 003 Batch: 00019/00038 | Loss: 1060.0769 | CE: 0.1425 | KD: 1059.9344\n",
            "Train Epoch: 003 Batch: 00020/00038 | Loss: 1060.0028 | CE: 0.0928 | KD: 1059.9100\n",
            "Train Epoch: 003 Batch: 00021/00038 | Loss: 1060.0419 | CE: 0.1267 | KD: 1059.9152\n",
            "Train Epoch: 003 Batch: 00022/00038 | Loss: 1060.2388 | CE: 0.2801 | KD: 1059.9586\n",
            "Train Epoch: 003 Batch: 00023/00038 | Loss: 1060.1278 | CE: 0.1886 | KD: 1059.9392\n",
            "Train Epoch: 003 Batch: 00024/00038 | Loss: 1060.1826 | CE: 0.1904 | KD: 1059.9922\n",
            "Train Epoch: 003 Batch: 00025/00038 | Loss: 1059.9818 | CE: 0.1206 | KD: 1059.8612\n",
            "Train Epoch: 003 Batch: 00026/00038 | Loss: 1060.1449 | CE: 0.1225 | KD: 1060.0225\n",
            "Train Epoch: 003 Batch: 00027/00038 | Loss: 1060.0465 | CE: 0.1785 | KD: 1059.8680\n",
            "Train Epoch: 003 Batch: 00028/00038 | Loss: 1060.1694 | CE: 0.1279 | KD: 1060.0415\n",
            "Train Epoch: 003 Batch: 00029/00038 | Loss: 1060.0441 | CE: 0.1205 | KD: 1059.9236\n",
            "Train Epoch: 003 Batch: 00030/00038 | Loss: 1060.1135 | CE: 0.1444 | KD: 1059.9691\n",
            "Train Epoch: 003 Batch: 00031/00038 | Loss: 1060.0599 | CE: 0.1775 | KD: 1059.8824\n",
            "Train Epoch: 003 Batch: 00032/00038 | Loss: 1060.0378 | CE: 0.1368 | KD: 1059.9010\n",
            "Train Epoch: 003 Batch: 00033/00038 | Loss: 1060.0312 | CE: 0.0845 | KD: 1059.9468\n",
            "Train Epoch: 003 Batch: 00034/00038 | Loss: 1060.0161 | CE: 0.0803 | KD: 1059.9358\n",
            "Train Epoch: 003 Batch: 00035/00038 | Loss: 1060.0353 | CE: 0.1308 | KD: 1059.9045\n",
            "Train Epoch: 003 Batch: 00036/00038 | Loss: 1060.0656 | CE: 0.1197 | KD: 1059.9458\n",
            "Train Epoch: 003 Batch: 00037/00038 | Loss: 1060.0374 | CE: 0.0688 | KD: 1059.9685\n",
            "Train Epoch: 003 Batch: 00038/00038 | Loss: 1060.1960 | CE: 0.1885 | KD: 1060.0076\n",
            "Adjusting learning rate of group 0 to 3.9715e-03.\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.2989 | acc:89.6250\n",
            "[VAL Acc] Target: 89.62%\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.2997 | acc:88.7405\n",
            "[VAL Acc] Source cyclegan: 88.74%\n",
            "[VAL Acc] Avg 89.18%\n",
            "Train Epoch: 004 Batch: 00001/00038 | Loss: 1059.9666 | CE: 0.0956 | KD: 1059.8710\n",
            "Train Epoch: 004 Batch: 00002/00038 | Loss: 1059.9979 | CE: 0.1225 | KD: 1059.8755\n",
            "Train Epoch: 004 Batch: 00003/00038 | Loss: 1060.0044 | CE: 0.0998 | KD: 1059.9045\n",
            "Train Epoch: 004 Batch: 00004/00038 | Loss: 1060.0663 | CE: 0.1607 | KD: 1059.9055\n",
            "Train Epoch: 004 Batch: 00005/00038 | Loss: 1060.2250 | CE: 0.1452 | KD: 1060.0797\n",
            "Train Epoch: 004 Batch: 00006/00038 | Loss: 1059.9086 | CE: 0.0803 | KD: 1059.8282\n",
            "Train Epoch: 004 Batch: 00007/00038 | Loss: 1060.0743 | CE: 0.0876 | KD: 1059.9867\n",
            "Train Epoch: 004 Batch: 00008/00038 | Loss: 1060.1642 | CE: 0.1745 | KD: 1059.9896\n",
            "Train Epoch: 004 Batch: 00009/00038 | Loss: 1060.0441 | CE: 0.1046 | KD: 1059.9395\n",
            "Train Epoch: 004 Batch: 00010/00038 | Loss: 1060.0085 | CE: 0.0866 | KD: 1059.9220\n",
            "Train Epoch: 004 Batch: 00011/00038 | Loss: 1060.0737 | CE: 0.1926 | KD: 1059.8811\n",
            "Train Epoch: 004 Batch: 00012/00038 | Loss: 1059.9761 | CE: 0.1093 | KD: 1059.8667\n",
            "Train Epoch: 004 Batch: 00013/00038 | Loss: 1060.1986 | CE: 0.0765 | KD: 1060.1221\n",
            "Train Epoch: 004 Batch: 00014/00038 | Loss: 1059.9781 | CE: 0.1186 | KD: 1059.8596\n",
            "Train Epoch: 004 Batch: 00015/00038 | Loss: 1059.9189 | CE: 0.0951 | KD: 1059.8239\n",
            "Train Epoch: 004 Batch: 00016/00038 | Loss: 1059.9265 | CE: 0.0889 | KD: 1059.8376\n",
            "Train Epoch: 004 Batch: 00017/00038 | Loss: 1060.0704 | CE: 0.1988 | KD: 1059.8716\n",
            "Train Epoch: 004 Batch: 00018/00038 | Loss: 1060.1270 | CE: 0.1637 | KD: 1059.9633\n",
            "Train Epoch: 004 Batch: 00019/00038 | Loss: 1060.1610 | CE: 0.1226 | KD: 1060.0385\n",
            "Train Epoch: 004 Batch: 00020/00038 | Loss: 1060.1094 | CE: 0.1816 | KD: 1059.9277\n",
            "Train Epoch: 004 Batch: 00021/00038 | Loss: 1060.0865 | CE: 0.1131 | KD: 1059.9734\n",
            "Train Epoch: 004 Batch: 00022/00038 | Loss: 1060.0585 | CE: 0.1529 | KD: 1059.9055\n",
            "Train Epoch: 004 Batch: 00023/00038 | Loss: 1060.1505 | CE: 0.2112 | KD: 1059.9392\n",
            "Train Epoch: 004 Batch: 00024/00038 | Loss: 1060.0924 | CE: 0.0862 | KD: 1060.0062\n",
            "Train Epoch: 004 Batch: 00025/00038 | Loss: 1060.2213 | CE: 0.1770 | KD: 1060.0443\n",
            "Train Epoch: 004 Batch: 00026/00038 | Loss: 1060.1648 | CE: 0.2233 | KD: 1059.9415\n",
            "Train Epoch: 004 Batch: 00027/00038 | Loss: 1060.0494 | CE: 0.0984 | KD: 1059.9510\n",
            "Train Epoch: 004 Batch: 00028/00038 | Loss: 1060.0895 | CE: 0.1344 | KD: 1059.9551\n",
            "Train Epoch: 004 Batch: 00029/00038 | Loss: 1059.8706 | CE: 0.0708 | KD: 1059.7998\n",
            "Train Epoch: 004 Batch: 00030/00038 | Loss: 1059.9435 | CE: 0.1342 | KD: 1059.8093\n",
            "Train Epoch: 004 Batch: 00031/00038 | Loss: 1059.9308 | CE: 0.0914 | KD: 1059.8394\n",
            "Train Epoch: 004 Batch: 00032/00038 | Loss: 1059.9487 | CE: 0.0748 | KD: 1059.8739\n",
            "Train Epoch: 004 Batch: 00033/00038 | Loss: 1060.0183 | CE: 0.1104 | KD: 1059.9080\n",
            "Train Epoch: 004 Batch: 00034/00038 | Loss: 1060.0741 | CE: 0.1596 | KD: 1059.9144\n",
            "Train Epoch: 004 Batch: 00035/00038 | Loss: 1060.2114 | CE: 0.0828 | KD: 1060.1285\n",
            "Train Epoch: 004 Batch: 00036/00038 | Loss: 1060.1663 | CE: 0.2187 | KD: 1059.9475\n",
            "Train Epoch: 004 Batch: 00037/00038 | Loss: 1059.9586 | CE: 0.1184 | KD: 1059.8402\n",
            "Train Epoch: 004 Batch: 00038/00038 | Loss: 1059.8789 | CE: 0.0612 | KD: 1059.8177\n",
            "Adjusting learning rate of group 0 to 3.2760e-03.\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.1997 | acc:92.0000\n",
            "[VAL Acc] Target: 92.00%\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.2520 | acc:88.1679\n",
            "[VAL Acc] Source cyclegan: 88.17%\n",
            "[VAL Acc] Avg 90.08%\n",
            "Train Epoch: 005 Batch: 00001/00038 | Loss: 1060.1481 | CE: 0.1781 | KD: 1059.9700\n",
            "Train Epoch: 005 Batch: 00002/00038 | Loss: 1060.0544 | CE: 0.1470 | KD: 1059.9075\n",
            "Train Epoch: 005 Batch: 00003/00038 | Loss: 1060.0278 | CE: 0.1331 | KD: 1059.8947\n",
            "Train Epoch: 005 Batch: 00004/00038 | Loss: 1060.0126 | CE: 0.0741 | KD: 1059.9385\n",
            "Train Epoch: 005 Batch: 00005/00038 | Loss: 1060.0823 | CE: 0.1636 | KD: 1059.9186\n",
            "Train Epoch: 005 Batch: 00006/00038 | Loss: 1059.9150 | CE: 0.0381 | KD: 1059.8770\n",
            "Train Epoch: 005 Batch: 00007/00038 | Loss: 1060.1567 | CE: 0.1520 | KD: 1060.0048\n",
            "Train Epoch: 005 Batch: 00008/00038 | Loss: 1060.0258 | CE: 0.1229 | KD: 1059.9028\n",
            "Train Epoch: 005 Batch: 00009/00038 | Loss: 1059.9297 | CE: 0.1079 | KD: 1059.8218\n",
            "Train Epoch: 005 Batch: 00010/00038 | Loss: 1060.2170 | CE: 0.2480 | KD: 1059.9690\n",
            "Train Epoch: 005 Batch: 00011/00038 | Loss: 1060.0415 | CE: 0.1300 | KD: 1059.9115\n",
            "Train Epoch: 005 Batch: 00012/00038 | Loss: 1060.2511 | CE: 0.2284 | KD: 1060.0227\n",
            "Train Epoch: 005 Batch: 00013/00038 | Loss: 1060.0220 | CE: 0.1738 | KD: 1059.8481\n",
            "Train Epoch: 005 Batch: 00014/00038 | Loss: 1059.9847 | CE: 0.1067 | KD: 1059.8781\n",
            "Train Epoch: 005 Batch: 00015/00038 | Loss: 1060.0248 | CE: 0.0974 | KD: 1059.9274\n",
            "Train Epoch: 005 Batch: 00016/00038 | Loss: 1060.0446 | CE: 0.1581 | KD: 1059.8865\n",
            "Train Epoch: 005 Batch: 00017/00038 | Loss: 1060.0220 | CE: 0.0996 | KD: 1059.9224\n",
            "Train Epoch: 005 Batch: 00018/00038 | Loss: 1060.0204 | CE: 0.0676 | KD: 1059.9528\n",
            "Train Epoch: 005 Batch: 00019/00038 | Loss: 1059.9384 | CE: 0.1010 | KD: 1059.8373\n",
            "Train Epoch: 005 Batch: 00020/00038 | Loss: 1060.1124 | CE: 0.1850 | KD: 1059.9275\n",
            "Train Epoch: 005 Batch: 00021/00038 | Loss: 1060.1163 | CE: 0.1528 | KD: 1059.9636\n",
            "Train Epoch: 005 Batch: 00022/00038 | Loss: 1060.1328 | CE: 0.1771 | KD: 1059.9557\n",
            "Train Epoch: 005 Batch: 00023/00038 | Loss: 1060.1086 | CE: 0.1425 | KD: 1059.9662\n",
            "Train Epoch: 005 Batch: 00024/00038 | Loss: 1059.9797 | CE: 0.0923 | KD: 1059.8875\n",
            "Train Epoch: 005 Batch: 00025/00038 | Loss: 1060.0917 | CE: 0.1477 | KD: 1059.9440\n",
            "Train Epoch: 005 Batch: 00026/00038 | Loss: 1060.0514 | CE: 0.1757 | KD: 1059.8757\n",
            "Train Epoch: 005 Batch: 00027/00038 | Loss: 1059.8787 | CE: 0.0839 | KD: 1059.7948\n",
            "Train Epoch: 005 Batch: 00028/00038 | Loss: 1060.1941 | CE: 0.1782 | KD: 1060.0160\n",
            "Train Epoch: 005 Batch: 00029/00038 | Loss: 1060.0559 | CE: 0.1427 | KD: 1059.9132\n",
            "Train Epoch: 005 Batch: 00030/00038 | Loss: 1060.0205 | CE: 0.0784 | KD: 1059.9420\n",
            "Train Epoch: 005 Batch: 00031/00038 | Loss: 1059.9314 | CE: 0.0763 | KD: 1059.8551\n",
            "Train Epoch: 005 Batch: 00032/00038 | Loss: 1059.9951 | CE: 0.1490 | KD: 1059.8462\n",
            "Train Epoch: 005 Batch: 00033/00038 | Loss: 1059.9426 | CE: 0.0810 | KD: 1059.8617\n",
            "Train Epoch: 005 Batch: 00034/00038 | Loss: 1059.9966 | CE: 0.1137 | KD: 1059.8828\n",
            "Train Epoch: 005 Batch: 00035/00038 | Loss: 1059.9119 | CE: 0.0925 | KD: 1059.8193\n",
            "Train Epoch: 005 Batch: 00036/00038 | Loss: 1060.0179 | CE: 0.1310 | KD: 1059.8870\n",
            "Train Epoch: 005 Batch: 00037/00038 | Loss: 1060.1134 | CE: 0.0642 | KD: 1060.0492\n",
            "Train Epoch: 005 Batch: 00038/00038 | Loss: 1060.0262 | CE: 0.1604 | KD: 1059.8658\n",
            "Adjusting learning rate of group 0 to 2.5050e-03.\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.1939 | acc:91.6250\n",
            "[VAL Acc] Target: 91.62%\n",
            "===> Starting the dataset cyclegan\n",
            "\n",
            "Test results | Loss:0.2449 | acc:89.6947\n",
            "[VAL Acc] Source cyclegan: 89.69%\n",
            "[VAL Acc] Avg 90.66%\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'types.SimpleNamespace' object has no attribute 'decay_factor'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m----> 3\u001b[0m train(cfg)\n",
            "File \u001b[0;32m~/AIGCDetection-CI-CD/src/train.py:144\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    142\u001b[0m     cur_patience \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mlr_schedule \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (cur_patience \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cur_patience \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m         alpha_kd \u001b[38;5;241m=\u001b[39m ReduceWeightOnPlateau(alpha_kd, args\u001b[38;5;241m.\u001b[39mdecay_factor)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Save \u001b[39;00m\n\u001b[1;32m    147\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(total_acc,best_acc)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'types.SimpleNamespace' object has no attribute 'decay_factor'"
          ]
        }
      ],
      "source": [
        "from train import train\n",
        "\n",
        "train(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykvxXn34S2dg",
        "outputId": "40d83eca-dedb-4861-8ead-11286f6718f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "Source Name : biggan\n",
            "Source Name : cyclegan\n",
            "\n",
            "===> Making Loader for Continual Learning..\n",
            "===> Making Loader : ../datasets/biggan/val\n",
            "===> Making Loader : ../datasets/cyclegan/val\n",
            "DATASET PATHS\n",
            "val_source_dir  ['../datasets/biggan/val', '../datasets/cyclegan/val']\n",
            "val_target_dir  ../datasets/cyclegan/val\n",
            "train_dir  ../datasets/cyclegan/train/\n",
            "Dataset available in dicLoader:  train_target / val_target / val_dataset1 / val_dataset2\n",
            "Dataset available in dicCoReD:  train_target_dataset / train_target_forCorrect\n",
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet from ../checkpoints/model_best_accuracy.pth\n",
            "Loaded\n",
            "Apply Cosine learning rate schedule\n",
            "Adjusting learning rate of group 0 to 5.0000e-03.\n",
            "Loading train target for correcting ...  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                      | 0/25 [00:00<?, ?it/s]/home/fra/miniconda3/envs/AIGCdetection/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706726118919/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "100%|█████████████| 25/25 [00:03<00:00,  7.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "list_length_realfakeloader : [[23, 26, 26, 51, 615], [15, 26, 56, 125, 520]]\n",
            "Ratio of already correctly predicted in training set: 0.946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List feature size:  (2, 5, 2048)\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 13.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.2208 | MainLoss:0.2208 | top:91.4122\n",
            "[VAL Acc] Target: 91.41%\n",
            "===> Starting the dataset <source name>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 14.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.3038 | MainLoss:0.3038 | top:89.0000\n",
            "[VAL Acc] Source 1-th: 89.00%\n",
            "===> Starting the dataset <source name>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 15.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1932 | MainLoss:0.1932 | top:91.4122\n",
            "[VAL Acc] Source 2-th: 91.41%\n",
            "[VAL Acc] Avg 90.61%\n",
            " Save initial model weight\n",
            "Start training in 5 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'loss_clampping' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kd_train(cfg)\n",
            "Cell \u001b[0;32mIn[7], line 103\u001b[0m, in \u001b[0;36mkd_train\u001b[0;34m(args, log)\u001b[0m\n\u001b[1;32m    101\u001b[0m loss_main \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m    102\u001b[0m loss_kd \u001b[38;5;241m=\u001b[39m loss_fn_kd(outputs, targets, teacher_outputs)\n\u001b[0;32m--> 103\u001b[0m loss_kd \u001b[38;5;241m=\u001b[39m loss_clampping(loss_kd, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1800\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#REP loss\u001b[39;00m\n\u001b[1;32m    106\u001b[0m list_features_std \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m()]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_clampping' is not defined"
          ]
        }
      ],
      "source": [
        "kd_train(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjqcB8Vn_Dkj"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lqLXKz3P_GuJ"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild,diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"cyclegan\"] # List of all datasets to test\n",
        "}\n",
        "\n",
        "\n",
        "evaluate_cfg = {\n",
        "\n",
        "    \"name\":         \"ekd_gau_big_cycle\",    # Used for tagging the experiment on logs\n",
        "    \"dataroot\":     \"../datasets\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"../checkpoints/model_best_accuracy.pth\", # load weights of task i from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"test\":         True,                   # True\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"ds_cfg\":        ds_cfg,\n",
        "}\n",
        "\n",
        "evaluate_cfg = SimpleNamespace(**evaluate_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1hdy8PL_LyX",
        "outputId": "27ea677a-9f95-493b-ed4b-fa5c5e9e7e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet from ../checkpoints/model_best_accuracy.pth\n",
            "Loaded\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from ../datasets/cyclegan/test\n",
            "Source: \n",
            "Target: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████| 9/9 [00:00<00:00, 11.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss:0.1773 | Acc:0.9286 | Acc Real:0.9108 | Acc Fake:0.9460 | Ap:0.8983\n",
            "Num reals: 273, Num fakes: 273\n",
            "\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9466    0.9084    0.9271       273\n",
            "           1     0.9120    0.9487    0.9300       273\n",
            "\n",
            "   micro avg     0.9286    0.9286    0.9286       546\n",
            "   macro avg     0.9293    0.9286    0.9285       546\n",
            "weighted avg     0.9293    0.9286    0.9285       546\n",
            " samples avg     0.9286    0.9286    0.9286       546\n",
            "\n",
            "Avg: | Acc:0.9286 | Acc Real:0.9108 | Acc Fake:0.9460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(evaluate_cfg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
