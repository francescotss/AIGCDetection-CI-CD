{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI1CsdAZN7PS"
      },
      "source": [
        "### Startup\n",
        "\n",
        "This code is meant to be executed on Google Colab.\n",
        "To use it locally change *COLAB_MODE* to False.\n",
        "\n",
        "**Note**: remember to change *workdir* accordingly, the notebook must be runned inside the root project folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzSZjcNvoPgs",
        "outputId": "13bebaa2-3950-48c0-d7b6-73a9bb887b86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/fra/AIGCDetection-CI-CD/src\n"
          ]
        }
      ],
      "source": [
        "workdir = \"../src\"\n",
        "%cd $workdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MlHM4qAGEMR"
      },
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "The code incorporates elements derived from the code originally published in the research paper, which can be found here: https://github.com/alsgkals2/CoReD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEuLI5ANwixJ",
        "outputId": "b27e1eb0-c412-403d-cfd4-f89a869c9159"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from common_functions import *\n",
        "from cored_functions import *\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "from train_utils import loss_clampping, ReduceWeightOnPlateau\n",
        "\n",
        "\n",
        "def kd_train(args, log = None):\n",
        "\n",
        "    # Init\n",
        "    torch.cuda.empty_cache()\n",
        "    device = 'cuda' if args.num_gpu else 'cpu'\n",
        "    lr = args.lr\n",
        "    KD_alpha = args.KD_alpha\n",
        "    num_class = args.num_class\n",
        "    num_store_per = args.num_store\n",
        "    savepath = f\"{args.checkpoints_dir}/{args.name_sources}_{args.name_target}/\"\n",
        "    savepath = savepath.replace('//','/')\n",
        "    if not os.path.isdir(savepath):\n",
        "        os.makedirs(savepath)\n",
        "    print(f'save path: {savepath}')\n",
        "\n",
        "    # Load datasets and models\n",
        "    dicLoader, dicCoReD, dicSourceName = initialization(args)\n",
        "    print(\"Dataset available in dicLoader: \", \" / \".join([n for n in dicLoader]))\n",
        "    print(\"Dataset available in dicCoReD: \", \" / \".join([n for n in dicCoReD]))\n",
        "    teacher_model, student_model = load_models(args.weight, args.network, num_gpu = args.num_gpu)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=0.1)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    if args.lr_schedule == \"cosine\":\n",
        "        print(\"Apply Cosine learning rate schedule\")\n",
        "        lr_scheduler = CosineAnnealingLR(optimizer=optimizer,\n",
        "                                        T_max=10,\n",
        "                                        eta_min=1e-5,\n",
        "                                        verbose=True)\n",
        "    elif args.lr_schedule == \"onecycle\":\n",
        "        print(\"Apply OneCycle learning rate schedule\")\n",
        "        lr_scheduler = OneCycleLR(optimizer=optimizer,\n",
        "                                    max_lr=lr,\n",
        "                                    epochs=args.epochs,\n",
        "                                    steps_per_epoch=len(dicLoader['train_target']),\n",
        "                                    pct_start=0.05,\n",
        "                                    total_steps=None,\n",
        "                                    verbose=False)\n",
        "    else:\n",
        "\n",
        "        print(f\"Input: {args.lr_schedule}, No learning rate schedule applied ... \")\n",
        "        return\n",
        "    watching_step = len(dicLoader['train_target']) // 10\n",
        "\n",
        "\n",
        "    # Pre-evaluation\n",
        "    print(\"Loading train target for correcting ...  \")\n",
        "    _list_correct, _ = func_correct(teacher_model.to(device), dicCoReD['train_target_forCorrect'])\n",
        "    _correct_loaders, already_correct_ratio = GetSplitLoaders_BinaryClasses(_list_correct, dicCoReD['train_target_dataset'], get_augs(args)[0], num_store_per)\n",
        "    print(\"Ratio of already correctly predicted in training set: {:.3f}\".format(already_correct_ratio))\n",
        "    list_features = GetListTeacherFeatureFakeReal(teacher_model.module if ',' in args.num_gpu else teacher_model ,_correct_loaders, mode=args.network)\n",
        "    list_features = np.array(list_features)\n",
        "    print(\"List feature size: \", list_features.shape)\n",
        "\n",
        "    # Initial validation\n",
        "    _, _, test_acc = Test(dicLoader['val_target'], student_model, criterion, log = log, source_name = args.name_target)\n",
        "    total_acc = test_acc\n",
        "    print(\"[VAL Acc] Target: {:.2f}%\".format( test_acc))\n",
        "    cnt = 1\n",
        "    for name in dicLoader:\n",
        "        if 'val_dataset' in name or 'val_source' in name:\n",
        "            if 'val_dataset' in name:\n",
        "                source_name = dicSourceName[f'source{cnt}']\n",
        "            else:\n",
        "                source_name = dicSourceName['source']\n",
        "\n",
        "            _, _, source_acc = Test(dicLoader[name], student_model, criterion, log = log, source_name = source_name)\n",
        "            total_acc += source_acc\n",
        "            print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "            cnt += 1\n",
        "\n",
        "    print(\"[VAL Acc] Avg {:.2f}%\\n Save initial model weight\".format(total_acc / cnt))\n",
        "    best_acc = total_acc\n",
        "    save_checkpoint({\n",
        "            'epoch': 0,\n",
        "            'state_dict': student_model.state_dict(),\n",
        "            'best_acc': best_acc,\n",
        "            'optimizer': optimizer.state_dict()},\n",
        "                checkpoint = savepath,\n",
        "                filename = '',\n",
        "                ACC_BEST=True\n",
        "                )\n",
        "\n",
        "    is_best_acc = False\n",
        "    cur_patience = 0 # Early stop and saving\n",
        "    l_weight = 1.0 # reduce the conservation when performance does not gain much\n",
        "    print(f\"Start training in {args.epochs} epochs\")\n",
        "\n",
        "\n",
        "    # ------- START TRAINING ------- #\n",
        "    for epoch in range(args.epochs):\n",
        "        correct,total = 0,0\n",
        "        teacher_model.eval()\n",
        "        student_model.train()\n",
        "        disp = {}\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dicLoader['train_target']):\n",
        "            # Load data\n",
        "            step = (batch_idx+1) * (epoch+1)\n",
        "            inputs = inputs.to(device).to(torch.float32)\n",
        "            targets = targets.to(device).to(torch.long)\n",
        "            if torch.isnan(inputs).any() or torch.isnan(targets).any():\n",
        "                raise ValueError(\"There is Nan values in input or target\")\n",
        "\n",
        "            # Forward\n",
        "            teacher_outputs = teacher_model(inputs)\n",
        "            penul_ft, outputs = student_model(inputs, True)\n",
        "\n",
        "            # Losses\n",
        "            loss_main = criterion(outputs, targets)\n",
        "            loss_kd = loss_fn_kd(outputs, targets, teacher_outputs)\n",
        "            loss_kd = loss_clampping(loss_kd, 0, 1800)\n",
        "\n",
        "            #REP loss\n",
        "            list_features_std = [list(), list()]\n",
        "            rep_ft_partitions = correct_binary_simple(inputs=inputs, penul_ft=penul_ft, outputs=outputs, targets=targets) # rep_ft_partitions : 5 x 2\n",
        "            for j in range(num_store_per):\n",
        "                for i in range(num_class):\n",
        "                    if(np.count_nonzero(list_features[i][j])==0 or len(rep_ft_partitions[j][i])==0):\n",
        "                      continue\n",
        "                    feat = torch.stack(rep_ft_partitions[j][i], dim=0).mean(dim=0)\n",
        "                    assert feat.size(-1) == 2048 or feat.size(-1) == 512 or feat.size(-1) == 1280\n",
        "                    rep_loss = (feat.to(torch.float32)  - torch.tensor(list_features[i][j]).to(device).to(torch.float32)).pow(2).mean()\n",
        "                    list_features_std[i].append(rep_loss)\n",
        "            sne_loss = 0.0\n",
        "            for fs in list_features_std:\n",
        "                for ss in fs:\n",
        "                    if ss.requires_grad:\n",
        "                        sne_loss += ss\n",
        "            sne_loss = loss_clampping(sne_loss, 0, 1) # REP Loss is clampped in this project\n",
        "\n",
        "            # Total loss\n",
        "            loss = loss_main  + l_weight*(loss_kd + sne_loss)\n",
        "            sne_item = sne_loss if type(sne_loss) == float else sne_loss.item()\n",
        "\n",
        "            # Log and display\n",
        "            disp[\"CE\"] = loss_main.item()\n",
        "            disp[\"KD\"] = loss_kd.item() if loss_kd > 0 else 0.0\n",
        "            disp[\"REP\"] = sne_item if sne_loss > 0 else 0.0\n",
        "            call = ' | '.join([\"{}: {:.4f}\".format(k, v) for k, v in disp.items()])\n",
        "            print(\"Train Epoch: {e:03d} Batch: {batch:05d}/{size:05d} | Loss: {loss:.4f} | {call}\"\n",
        "                            .format(e=epoch+1, batch=batch_idx+1, size=len(dicLoader['train_target']), loss=loss.item(), call=call))\n",
        "\n",
        "            # Learn!\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if args.lr_schedule == \"onecycle\":\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            # Predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += len(targets)\n",
        "\n",
        "        if args.lr_schedule == \"cosine\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "\n",
        "        # ----- Validation ------ #\n",
        "\n",
        "        # Current task\n",
        "        _, _, test_acc = Test(dicLoader['val_target'], student_model, criterion, log = None, source_name = args.name_target)\n",
        "        total_acc = test_acc\n",
        "        print(\"[VAL Acc] Target: {:.2f}%\".format( test_acc))\n",
        "\n",
        "        # Past tasks\n",
        "        cnt = 1\n",
        "        for name in dicLoader:\n",
        "            if 'val_dataset' in name or 'val_source' in name:\n",
        "                if 'val_dataset' in name:\n",
        "                    source_name = dicSourceName[f'source{cnt}']\n",
        "                else:\n",
        "                    source_name = dicSourceName['source']\n",
        "\n",
        "                _, _, source_acc = Test(dicLoader[name], student_model, criterion, log = None, source_name = source_name)\n",
        "                total_acc += source_acc\n",
        "                print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "                cnt += 1\n",
        "        print(\"[VAL Acc] Avg {:.2f}%\".format(total_acc / cnt))\n",
        "\n",
        "        # Early stop\n",
        "        is_best_acc = total_acc > best_acc\n",
        "        if is_best_acc:\n",
        "                print(\"VAL Acc improve from {:.2f}% to {:.2f}%\".format(best_acc/cnt, total_acc/cnt))\n",
        "                cur_patience = 0\n",
        "        else:\n",
        "            cur_patience += 1\n",
        "        if args.loss_schedule and (cur_patience > 0 and cur_patience % 4 == 0):\n",
        "                l_weight = ReduceWeightOnPlateau(l_weight, args.decay_factor)\n",
        "\n",
        "        # Save\n",
        "        best_acc = max(total_acc,best_acc)\n",
        "        if  is_best_acc:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': student_model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict()},\n",
        "            checkpoint = savepath,\n",
        "            filename = 'epoch_{}'.format( epoch+1 if (epoch+1)%10==0 else ''),\n",
        "            ACC_BEST=is_best_acc\n",
        "            )\n",
        "            print('Save best model' if is_best_acc else f'Save checkpoint model @ {epoch+1}')\n",
        "        if args.early_stop and (cur_patience == args.patience):\n",
        "            print(\"Early stopping ...\")\n",
        "            return\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7csYl99zAH"
      },
      "source": [
        "# Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avxhBq4HFhOO",
        "outputId": "60506fd7-31dc-443e-8a10-30b5a64132fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from common_functions import initialization, load_models, AverageMeter\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, average_precision_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def evaluate(args, global_writer=None):\n",
        "\n",
        "    # Config\n",
        "    setattr(args,\"name_sources\", \"\")\n",
        "    setattr(args,\"name_target\", \"\")\n",
        "\n",
        "\n",
        "\n",
        "    # Load model\n",
        "    _, model = load_models(args.weight, args.network, args.num_gpu, not args.test)\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Load datasets\n",
        "    tot_avg_acc, real_avg_acc, fake_avg_acc = 0.0, 0.0 ,0.0\n",
        "    for ds_name in args.ds_cfg[\"fake_ds\"]:\n",
        "      data_folder = f\"{args.dataroot}/{ds_name}/test\"\n",
        "      setattr(args,\"data\", data_folder)\n",
        "      dicLoader,_, dicSourceName = initialization(args)\n",
        "\n",
        "\n",
        "      for key, name in zip(dicLoader, dicSourceName):\n",
        "        # Init\n",
        "        global best_acc\n",
        "        correct, total =0,0\n",
        "        losses = AverageMeter()\n",
        "        arc = AverageMeter()\n",
        "        acc_real = AverageMeter()\n",
        "        acc_fake = AverageMeter()\n",
        "        sum_of_AUROC=[]\n",
        "        target=[]\n",
        "        output = []\n",
        "        y_true=np.zeros((0,2),dtype=np.int8)\n",
        "        y_pred=np.zeros((0,2),dtype=np.int8)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          model.eval()\n",
        "          model.cuda()\n",
        "\n",
        "          for (inputs, targets) in tqdm(dicLoader[key], ncols=50):\n",
        "              # Predict\n",
        "              inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, targets)\n",
        "              _, predicted = torch.max(outputs, 1)\n",
        "              correct = (predicted == targets).squeeze()\n",
        "              total += len(targets)\n",
        "              losses.update(loss.data.tolist(), inputs.size(0))\n",
        "              _y_pred = outputs.cpu().detach()\n",
        "              _y_gt = targets.cpu().detach().numpy()\n",
        "              acc = [0, 0]\n",
        "              class_total = [0, 0]\n",
        "              for i in range(len(targets)):\n",
        "                  label = targets[i]\n",
        "                  acc[label] += 1 if correct[i].item() == True else 0\n",
        "                  class_total[label] += 1\n",
        "\n",
        "              losses.update(loss.data.tolist(), inputs.size(0))\n",
        "              if (class_total[0] != 0):\n",
        "                  acc_real.update(acc[0] / class_total[0])\n",
        "              if (class_total[1] != 0):\n",
        "                  acc_fake.update(acc[1] / class_total[1])\n",
        "\n",
        "              target.append(_y_gt)\n",
        "              output.append(_y_pred.numpy()[:,1])\n",
        "              auroc=None\n",
        "              try:\n",
        "                  auroc = roc_auc_score(_y_gt, outputs[:,1].cpu().detach().numpy())\n",
        "              except ValueError:\n",
        "                  pass\n",
        "              sum_of_AUROC.append(auroc)\n",
        "              _y_true = np.array(torch.zeros(targets.shape[0],2), dtype=np.int8)\n",
        "              _y_gt = _y_gt.astype(int)\n",
        "              for _ in range(len(targets)):\n",
        "                  _y_true[_][_y_gt[_]] = 1\n",
        "              y_true = np.concatenate((y_true,_y_true))\n",
        "              a = _y_pred.argmax(1)\n",
        "              _y_pred = np.array(torch.zeros(_y_pred.shape).scatter(1, a.unsqueeze(1), 1),dtype=np.int8)\n",
        "              y_pred = np.concatenate((y_pred,_y_pred))\n",
        "\n",
        "          n_real_samples = np.count_nonzero(y_true, axis=0)[0]\n",
        "          n_fake_samples = np.count_nonzero(y_true, axis=0)[1]\n",
        "          acc = accuracy_score(y_true, y_pred)\n",
        "          ap = average_precision_score(y_true, y_pred)\n",
        "\n",
        "          result = classification_report(y_true, y_pred,\n",
        "                                              labels=None,\n",
        "                                              target_names=None,\n",
        "                                              sample_weight=None,\n",
        "                                              digits=4,\n",
        "                                              output_dict=False,\n",
        "                                              zero_division='warn')\n",
        "\n",
        "\n",
        "          print(f\"\\nLoss:{losses.avg:.4f} | Acc:{acc:.4f} | Acc Real:{acc_real.avg:.4f} | Acc Fake:{acc_fake.avg:.4f} | Ap:{ap:.4f}\")\n",
        "          print(f'Num reals: {n_real_samples}, Num fakes: {n_fake_samples}')\n",
        "          print(\"\\n\\n\",result)\n",
        "\n",
        "          tot_avg_acc += acc\n",
        "          real_avg_acc += acc_real.avg\n",
        "          fake_avg_acc += acc_fake.avg\n",
        "\n",
        "        \n",
        "    total_ds = len(args.ds_cfg[\"fake_ds\"])\n",
        "    print(f\"Avg: | Acc:{tot_avg_acc/total_ds:.4f} | Acc Real:{real_avg_acc/total_ds:.4f} | Acc Fake:{fake_avg_acc/total_ds:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RE9O_Ego1iy"
      },
      "source": [
        "# Workspace\n",
        "\n",
        "In this section it is possible to run trainings and evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO7-OzjtSrus"
      },
      "source": [
        "## Traning\n",
        "\n",
        "\n",
        "1.   Configurate the training\n",
        "2.   Build the dataset\n",
        "3.   Train!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrQgEkqyqu8O"
      },
      "source": [
        "### Knowledge distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-BWn0-vizej7"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild\",diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"cyclegan\"] # List of all datasets from first to current task\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    \"name\":         \"tkd_gau_big_cycle\",    # Used for tagging the experiment on logs\n",
        "    \"data\":         \"../datasets\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"../checkpoints/model_best_accuracy.pth\", # load weights of task i-1 from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"name_sources\": \"cyclegan\",        # Ordered list of previous task: dataset1_dataset2_dataseti-1\n",
        "    \"name_target\":   \"cyclegan\",            # Task i dataset\n",
        "    \"checkpoints_dir\": \"../checkpoints/test\", # Save folder (the task subfolder is automatically created)\n",
        "    \"lr_schedule\":  \"cosine\",               # cosine, onecycle\n",
        "    \"test\":         False,                  # False\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"loss_schedule\": True,                  # True, False\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"KD_alpha\":     0.5,                    # alpha factor for kd loss\n",
        "    \"num_store\":    5,                      # Stores for representation loss\n",
        "    \"lr\":           0.005,                  # Learning rate\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"epochs\":       5,                    # Traning epochs\n",
        "    \"early_stop\":   True,                   # True, False\n",
        "    \"patience\":     25,                     # Early stop patience\n",
        "    \"ds_cfg\":       ds_cfg,\n",
        "}\n",
        "\n",
        "cfg = SimpleNamespace(**train_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykvxXn34S2dg",
        "outputId": "40d83eca-dedb-4861-8ead-11286f6718f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "save path: ./checkpoints/test/cyclegan_cyclegan/\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from ../datasets\n",
            "Source: cyclegan\n",
            "Target: cyclegan\n",
            "\n",
            "---DATASET PATHS---\n",
            "Train dir: ../datasets/cyclegan/train\n",
            "Validation Source dir ../datasets/cyclegan/val\n",
            "Validation Target dir ../datasets/cyclegan/val\n",
            "Dataset available in dicLoader:  train_target / val_source / val_target / val_target_mix\n",
            "Dataset available in dicCoReD:  train_target_dataset / train_target_forCorrect\n",
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet from ../checkpoints/model_best_accuracy.pth\n",
            "Loaded\n",
            "Apply Cosine learning rate schedule\n",
            "Adjusting learning rate of group 0 to 5.0000e-03.\n",
            "Loading train target for correcting ...  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████| 25/25 [00:02<00:00, 10.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "list_length_realfakeloader : [[23, 26, 26, 51, 615], [15, 26, 56, 125, 520]]\n",
            "Ratio of already correctly predicted in training set: 0.946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List feature size:  (2, 5, 2048)\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.2208 | MainLoss:0.2208 | top:91.4122\n",
            "[VAL Acc] Target: 91.41%\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 13.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.2014 | MainLoss:0.2014 | top:91.0305\n",
            "[VAL Acc] Source 1-th: 91.03%\n",
            "[VAL Acc] Avg 91.22%\n",
            " Save initial model weight\n",
            "Start training in 5 epochs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 001 Batch: 00001/00025 | Loss: 1060.8955 | CE: 0.1236 | KD: 1059.7719 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00002/00025 | Loss: 1060.9996 | CE: 0.1482 | KD: 1059.8514 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00003/00025 | Loss: 1061.2261 | CE: 0.3285 | KD: 1059.8976 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00004/00025 | Loss: 1060.9740 | CE: 0.1721 | KD: 1059.8019 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00005/00025 | Loss: 1061.0328 | CE: 0.1907 | KD: 1059.8422 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00006/00025 | Loss: 1061.4423 | CE: 0.2576 | KD: 1060.1847 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00007/00025 | Loss: 1060.9247 | CE: 0.1125 | KD: 1059.8123 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00008/00025 | Loss: 1061.1953 | CE: 0.1304 | KD: 1060.0649 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00009/00025 | Loss: 1060.8483 | CE: 0.0766 | KD: 1059.7716 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00010/00025 | Loss: 1061.0870 | CE: 0.1634 | KD: 1059.9236 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00011/00025 | Loss: 1061.0034 | CE: 0.1418 | KD: 1059.8616 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00012/00025 | Loss: 1061.2161 | CE: 0.2433 | KD: 1059.9728 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00013/00025 | Loss: 1060.9924 | CE: 0.1615 | KD: 1059.8309 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00014/00025 | Loss: 1061.2754 | CE: 0.2419 | KD: 1060.0334 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00015/00025 | Loss: 1060.9336 | CE: 0.1162 | KD: 1059.8174 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00016/00025 | Loss: 1061.0037 | CE: 0.1354 | KD: 1059.8682 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00017/00025 | Loss: 1061.0055 | CE: 0.1382 | KD: 1059.8673 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00018/00025 | Loss: 1060.9844 | CE: 0.1174 | KD: 1059.8669 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00019/00025 | Loss: 1061.4111 | CE: 0.3413 | KD: 1060.0698 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00020/00025 | Loss: 1061.3101 | CE: 0.1971 | KD: 1060.1129 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00021/00025 | Loss: 1061.2413 | CE: 0.3324 | KD: 1059.9089 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00022/00025 | Loss: 1060.9451 | CE: 0.1458 | KD: 1059.7993 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00023/00025 | Loss: 1061.0160 | CE: 0.1879 | KD: 1059.8281 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00024/00025 | Loss: 1060.8848 | CE: 0.0934 | KD: 1059.7914 | REP: 1.0000\n",
            "Train Epoch: 001 Batch: 00025/00025 | Loss: 1060.9534 | CE: 0.0926 | KD: 1059.8608 | REP: 1.0000\n",
            "Adjusting learning rate of group 0 to 4.8779e-03.\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1675 | MainLoss:0.1675 | top:92.3664\n",
            "[VAL Acc] Target: 92.37%\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1560 | MainLoss:0.1560 | top:94.0840\n",
            "[VAL Acc] Source 1-th: 94.08%\n",
            "[VAL Acc] Avg 93.23%\n",
            "VAL Acc improve from 91.22% to 93.23%\n",
            "Save best model\n",
            "Train Epoch: 002 Batch: 00001/00025 | Loss: 1060.9445 | CE: 0.0911 | KD: 1059.8534 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00002/00025 | Loss: 1061.0112 | CE: 0.1484 | KD: 1059.8628 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00003/00025 | Loss: 1060.8883 | CE: 0.0854 | KD: 1059.8030 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00004/00025 | Loss: 1060.9493 | CE: 0.1384 | KD: 1059.8109 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00005/00025 | Loss: 1060.9307 | CE: 0.1312 | KD: 1059.7994 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00006/00025 | Loss: 1060.9868 | CE: 0.1427 | KD: 1059.8441 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00007/00025 | Loss: 1060.9958 | CE: 0.1486 | KD: 1059.8472 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00008/00025 | Loss: 1060.9888 | CE: 0.1470 | KD: 1059.8418 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00009/00025 | Loss: 1061.0446 | CE: 0.1256 | KD: 1059.9189 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00010/00025 | Loss: 1061.0110 | CE: 0.1133 | KD: 1059.8977 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00011/00025 | Loss: 1060.8743 | CE: 0.0802 | KD: 1059.7941 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00012/00025 | Loss: 1061.0050 | CE: 0.1738 | KD: 1059.8312 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00013/00025 | Loss: 1061.0723 | CE: 0.1876 | KD: 1059.8846 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00014/00025 | Loss: 1060.9404 | CE: 0.1211 | KD: 1059.8193 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00015/00025 | Loss: 1061.0710 | CE: 0.2114 | KD: 1059.8596 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00016/00025 | Loss: 1060.9578 | CE: 0.0882 | KD: 1059.8696 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00017/00025 | Loss: 1061.4248 | CE: 0.2539 | KD: 1060.1709 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00018/00025 | Loss: 1060.9376 | CE: 0.1033 | KD: 1059.8344 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00019/00025 | Loss: 1061.0452 | CE: 0.1391 | KD: 1059.9060 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00020/00025 | Loss: 1061.1742 | CE: 0.2119 | KD: 1059.9623 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00021/00025 | Loss: 1061.1846 | CE: 0.2400 | KD: 1059.9446 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00022/00025 | Loss: 1061.0565 | CE: 0.1424 | KD: 1059.9141 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00023/00025 | Loss: 1061.0681 | CE: 0.1100 | KD: 1059.9581 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00024/00025 | Loss: 1060.9828 | CE: 0.1328 | KD: 1059.8500 | REP: 1.0000\n",
            "Train Epoch: 002 Batch: 00025/00025 | Loss: 1060.9163 | CE: 0.1049 | KD: 1059.8114 | REP: 1.0000\n",
            "Adjusting learning rate of group 0 to 4.5235e-03.\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1523 | MainLoss:0.1523 | top:94.0840\n",
            "[VAL Acc] Target: 94.08%\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1667 | MainLoss:0.1667 | top:93.3206\n",
            "[VAL Acc] Source 1-th: 93.32%\n",
            "[VAL Acc] Avg 93.70%\n",
            "VAL Acc improve from 93.23% to 93.70%\n",
            "Save best model\n",
            "Train Epoch: 003 Batch: 00001/00025 | Loss: 1061.0481 | CE: 0.2010 | KD: 1059.8470 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00002/00025 | Loss: 1061.0742 | CE: 0.1848 | KD: 1059.8894 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00003/00025 | Loss: 1061.2377 | CE: 0.2129 | KD: 1060.0248 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00004/00025 | Loss: 1060.9454 | CE: 0.1500 | KD: 1059.7954 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00005/00025 | Loss: 1060.8910 | CE: 0.0612 | KD: 1059.8297 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00006/00025 | Loss: 1060.8983 | CE: 0.0890 | KD: 1059.8093 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00007/00025 | Loss: 1061.0049 | CE: 0.1332 | KD: 1059.8717 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00008/00025 | Loss: 1061.0377 | CE: 0.1521 | KD: 1059.8856 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00009/00025 | Loss: 1060.9254 | CE: 0.1039 | KD: 1059.8215 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00010/00025 | Loss: 1061.0089 | CE: 0.1122 | KD: 1059.8967 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00011/00025 | Loss: 1060.8480 | CE: 0.0730 | KD: 1059.7750 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00012/00025 | Loss: 1061.1010 | CE: 0.1817 | KD: 1059.9192 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00013/00025 | Loss: 1060.9692 | CE: 0.1095 | KD: 1059.8597 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00014/00025 | Loss: 1061.0260 | CE: 0.1695 | KD: 1059.8564 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00015/00025 | Loss: 1061.0258 | CE: 0.1899 | KD: 1059.8358 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00016/00025 | Loss: 1060.8463 | CE: 0.0657 | KD: 1059.7806 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00017/00025 | Loss: 1060.8829 | CE: 0.0939 | KD: 1059.7891 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00018/00025 | Loss: 1060.9758 | CE: 0.0970 | KD: 1059.8788 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00019/00025 | Loss: 1061.1400 | CE: 0.1635 | KD: 1059.9766 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00020/00025 | Loss: 1061.0521 | CE: 0.1713 | KD: 1059.8809 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00021/00025 | Loss: 1060.9928 | CE: 0.1593 | KD: 1059.8335 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00022/00025 | Loss: 1061.0781 | CE: 0.1948 | KD: 1059.8833 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00023/00025 | Loss: 1060.9799 | CE: 0.1483 | KD: 1059.8315 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00024/00025 | Loss: 1060.9594 | CE: 0.1257 | KD: 1059.8336 | REP: 1.0000\n",
            "Train Epoch: 003 Batch: 00025/00025 | Loss: 1060.9742 | CE: 0.0861 | KD: 1059.8881 | REP: 1.0000\n",
            "Adjusting learning rate of group 0 to 3.9715e-03.\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1540 | MainLoss:0.1540 | top:94.8473\n",
            "[VAL Acc] Target: 94.85%\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 15.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1583 | MainLoss:0.1583 | top:92.3664\n",
            "[VAL Acc] Source 1-th: 92.37%\n",
            "[VAL Acc] Avg 93.61%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 004 Batch: 00001/00025 | Loss: 1060.8822 | CE: 0.0754 | KD: 1059.8068 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00002/00025 | Loss: 1060.9719 | CE: 0.1540 | KD: 1059.8988 | REP: 0.9191\n",
            "Train Epoch: 004 Batch: 00003/00025 | Loss: 1060.8918 | CE: 0.0976 | KD: 1059.7942 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00004/00025 | Loss: 1061.0485 | CE: 0.2187 | KD: 1059.8855 | REP: 0.9442\n",
            "Train Epoch: 004 Batch: 00005/00025 | Loss: 1061.3699 | CE: 0.2317 | KD: 1060.1382 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00006/00025 | Loss: 1061.1526 | CE: 0.1752 | KD: 1059.9774 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00007/00025 | Loss: 1060.9722 | CE: 0.1236 | KD: 1059.8485 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00008/00025 | Loss: 1060.9728 | CE: 0.1261 | KD: 1059.8467 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00009/00025 | Loss: 1061.1147 | CE: 0.1938 | KD: 1059.9209 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00010/00025 | Loss: 1061.1776 | CE: 0.2268 | KD: 1059.9508 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00011/00025 | Loss: 1060.9829 | CE: 0.0937 | KD: 1059.8893 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00012/00025 | Loss: 1061.0488 | CE: 0.1133 | KD: 1059.9355 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00013/00025 | Loss: 1061.0934 | CE: 0.2114 | KD: 1059.8820 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00014/00025 | Loss: 1061.0576 | CE: 0.1841 | KD: 1059.8735 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00015/00025 | Loss: 1060.9615 | CE: 0.1405 | KD: 1059.8210 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00016/00025 | Loss: 1061.0565 | CE: 0.1854 | KD: 1059.8711 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00017/00025 | Loss: 1060.9086 | CE: 0.1013 | KD: 1059.8073 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00018/00025 | Loss: 1060.9073 | CE: 0.0578 | KD: 1059.8495 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00019/00025 | Loss: 1060.9283 | CE: 0.1180 | KD: 1059.8104 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00020/00025 | Loss: 1061.1576 | CE: 0.1492 | KD: 1060.0084 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00021/00025 | Loss: 1061.0211 | CE: 0.1641 | KD: 1059.8569 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00022/00025 | Loss: 1060.9091 | CE: 0.1125 | KD: 1059.7965 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00023/00025 | Loss: 1061.0057 | CE: 0.1113 | KD: 1059.8944 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00024/00025 | Loss: 1061.0485 | CE: 0.2045 | KD: 1059.8439 | REP: 1.0000\n",
            "Train Epoch: 004 Batch: 00025/00025 | Loss: 1061.1942 | CE: 0.1528 | KD: 1060.0415 | REP: 1.0000\n",
            "Adjusting learning rate of group 0 to 3.2760e-03.\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1214 | MainLoss:0.1214 | top:95.0382\n",
            "[VAL Acc] Target: 95.04%\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1392 | MainLoss:0.1392 | top:95.4198\n",
            "[VAL Acc] Source 1-th: 95.42%\n",
            "[VAL Acc] Avg 95.23%\n",
            "VAL Acc improve from 93.70% to 95.23%\n",
            "Save best model\n",
            "Train Epoch: 005 Batch: 00001/00025 | Loss: 1061.1187 | CE: 0.2308 | KD: 1059.8878 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00002/00025 | Loss: 1061.0809 | CE: 0.1511 | KD: 1059.9298 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00003/00025 | Loss: 1061.0363 | CE: 0.1849 | KD: 1059.8513 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00004/00025 | Loss: 1061.0763 | CE: 0.1584 | KD: 1059.9178 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00005/00025 | Loss: 1060.9271 | CE: 0.1221 | KD: 1059.8051 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00006/00025 | Loss: 1060.9316 | CE: 0.0819 | KD: 1059.8497 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00007/00025 | Loss: 1061.1552 | CE: 0.2226 | KD: 1059.9325 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00008/00025 | Loss: 1060.9799 | CE: 0.1332 | KD: 1059.8467 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00009/00025 | Loss: 1060.9005 | CE: 0.0955 | KD: 1059.8049 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00010/00025 | Loss: 1060.9680 | CE: 0.1333 | KD: 1059.8347 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00011/00025 | Loss: 1061.0225 | CE: 0.1624 | KD: 1059.8601 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00012/00025 | Loss: 1061.1051 | CE: 0.1531 | KD: 1059.9520 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00013/00025 | Loss: 1060.9442 | CE: 0.1140 | KD: 1059.8302 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00014/00025 | Loss: 1060.8947 | CE: 0.0740 | KD: 1059.8207 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00015/00025 | Loss: 1061.0557 | CE: 0.1994 | KD: 1059.8563 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00016/00025 | Loss: 1060.9354 | CE: 0.0847 | KD: 1059.8507 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00017/00025 | Loss: 1061.1807 | CE: 0.1705 | KD: 1060.0101 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00018/00025 | Loss: 1060.8549 | CE: 0.0660 | KD: 1059.7888 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00019/00025 | Loss: 1061.0439 | CE: 0.1673 | KD: 1059.8766 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00020/00025 | Loss: 1061.0625 | CE: 0.1014 | KD: 1059.9611 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00021/00025 | Loss: 1060.8597 | CE: 0.0740 | KD: 1059.7858 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00022/00025 | Loss: 1061.0396 | CE: 0.1895 | KD: 1059.8501 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00023/00025 | Loss: 1060.9498 | CE: 0.0836 | KD: 1059.8662 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00024/00025 | Loss: 1060.9346 | CE: 0.1120 | KD: 1059.8225 | REP: 1.0000\n",
            "Train Epoch: 005 Batch: 00025/00025 | Loss: 1060.8422 | CE: 0.1524 | KD: 1059.8711 | REP: 0.8186\n",
            "Adjusting learning rate of group 0 to 2.5050e-03.\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1613 | MainLoss:0.1613 | top:93.7023\n",
            "[VAL Acc] Target: 93.70%\n",
            "===> Starting the dataset cyclegan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 14.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.1541 | MainLoss:0.1541 | top:93.5115\n",
            "[VAL Acc] Source 1-th: 93.51%\n",
            "[VAL Acc] Avg 93.61%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "kd_train(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjqcB8Vn_Dkj"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lqLXKz3P_GuJ"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild,diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"cyclegan\"] # List of all datasets to test\n",
        "}\n",
        "\n",
        "\n",
        "evaluate_cfg = {\n",
        "\n",
        "    \"name\":         \"ekd_gau_big_cycle\",    # Used for tagging the experiment on logs\n",
        "    \"dataroot\":     \"../datasets\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"../checkpoints/model_best_accuracy.pth\", # load weights of task i from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"test\":         True,                   # True\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"ds_cfg\":        ds_cfg,\n",
        "}\n",
        "\n",
        "evaluate_cfg = SimpleNamespace(**evaluate_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1hdy8PL_LyX",
        "outputId": "27ea677a-9f95-493b-ed4b-fa5c5e9e7e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet from ../checkpoints/model_best_accuracy.pth\n",
            "Loaded\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from ../datasets/cyclegan/test\n",
            "Source: \n",
            "Target: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████| 9/9 [00:00<00:00, 11.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss:0.1773 | Acc:0.9286 | Acc Real:0.9108 | Acc Fake:0.9460 | Ap:0.8983\n",
            "Num reals: 273, Num fakes: 273\n",
            "\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9466    0.9084    0.9271       273\n",
            "           1     0.9120    0.9487    0.9300       273\n",
            "\n",
            "   micro avg     0.9286    0.9286    0.9286       546\n",
            "   macro avg     0.9293    0.9286    0.9285       546\n",
            "weighted avg     0.9293    0.9286    0.9285       546\n",
            " samples avg     0.9286    0.9286    0.9286       546\n",
            "\n",
            "Avg: | Acc:0.9286 | Acc Real:0.9108 | Acc Fake:0.9460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(evaluate_cfg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
