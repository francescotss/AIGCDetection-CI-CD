{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='model_config.conf', network='ResNet18', input_model='../KD-AIGC-Detection/checkpoints/KD_r18/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild', output_dir='../KD-AIGC-Detection/checkpoints/KD_r18/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild', target_dataset='../KD-AIGC-Detection/datasets/custom/diffusionshort', use_comet=True, comet_name='t18_diff', num_gpu='0', output_model_version='3', epochs='250', early_stop='35', batch_size='64', resolution='128', lr_schedule='cosine', lr='0.005', kd_alpha='0.5', decay_factor='0.9', flip='False')\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/28f50f2b2b104d2fba71d4af73f78b99\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/imle\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/crn\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/wild\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/train\n",
      "Dataset available in train_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan / ../KD-AIGC-Detection/datasets/custom/biggan / ../KD-AIGC-Detection/datasets/custom/cyclegan / ../KD-AIGC-Detection/datasets/custom/imle / ../KD-AIGC-Detection/datasets/custom/faceforensics / ../KD-AIGC-Detection/datasets/custom/crn / ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Loading ResNet18 from ../KD-AIGC-Detection/checkpoints/KD_r18/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild/model_best_accuracy.pth\n",
      "Loaded\n",
      "Apply Cosine learning rate schedule\n",
      "/home/fra/miniconda3/envs/paper/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:1.7067 | acc:52.8500\n",
      "Start training in 250 epochs\n",
      "Train Epoch: 001 Batch: 00001/00094 | Loss: 533.0706 | CE: 1.8892 | KD: 1062.3628\n",
      "Train Epoch: 001 Batch: 00002/00094 | Loss: 531.8826 | CE: 1.2652 | KD: 1061.2347\n",
      "Train Epoch: 001 Batch: 00003/00094 | Loss: 531.6676 | CE: 1.0344 | KD: 1061.2665\n",
      "Train Epoch: 001 Batch: 00004/00094 | Loss: 531.6237 | CE: 1.1033 | KD: 1061.0409\n",
      "Train Epoch: 001 Batch: 00005/00094 | Loss: 531.2487 | CE: 0.8363 | KD: 1060.8247\n",
      "Train Epoch: 001 Batch: 00006/00094 | Loss: 531.4571 | CE: 0.8061 | KD: 1061.3020\n",
      "Train Epoch: 001 Batch: 00007/00094 | Loss: 531.0767 | CE: 0.7250 | KD: 1060.7034\n",
      "Train Epoch: 001 Batch: 00008/00094 | Loss: 531.1317 | CE: 0.6209 | KD: 1061.0215\n",
      "Train Epoch: 001 Batch: 00009/00094 | Loss: 531.0664 | CE: 0.6070 | KD: 1060.9188\n",
      "Train Epoch: 001 Batch: 00010/00094 | Loss: 531.2557 | CE: 0.8329 | KD: 1060.8456\n",
      "Train Epoch: 001 Batch: 00011/00094 | Loss: 531.0792 | CE: 0.5596 | KD: 1061.0392\n",
      "Train Epoch: 001 Batch: 00012/00094 | Loss: 531.1607 | CE: 0.6317 | KD: 1061.0580\n",
      "Train Epoch: 001 Batch: 00013/00094 | Loss: 530.8296 | CE: 0.5140 | KD: 1060.6312\n",
      "Train Epoch: 001 Batch: 00014/00094 | Loss: 530.8860 | CE: 0.4559 | KD: 1060.8602\n",
      "Train Epoch: 001 Batch: 00015/00094 | Loss: 531.0988 | CE: 0.7461 | KD: 1060.7056\n",
      "Train Epoch: 001 Batch: 00016/00094 | Loss: 530.7785 | CE: 0.4869 | KD: 1060.5833\n",
      "Train Epoch: 001 Batch: 00017/00094 | Loss: 530.9815 | CE: 0.6507 | KD: 1060.6617\n",
      "Train Epoch: 001 Batch: 00018/00094 | Loss: 530.8276 | CE: 0.4650 | KD: 1060.7252\n",
      "Train Epoch: 001 Batch: 00019/00094 | Loss: 530.8693 | CE: 0.5181 | KD: 1060.7023\n",
      "Train Epoch: 001 Batch: 00020/00094 | Loss: 531.1317 | CE: 0.6960 | KD: 1060.8715\n",
      "Train Epoch: 001 Batch: 00021/00094 | Loss: 531.2689 | CE: 0.6906 | KD: 1061.1566\n",
      "Train Epoch: 001 Batch: 00022/00094 | Loss: 530.9460 | CE: 0.5267 | KD: 1060.8385\n",
      "Train Epoch: 001 Batch: 00023/00094 | Loss: 530.9501 | CE: 0.4417 | KD: 1061.0167\n",
      "Train Epoch: 001 Batch: 00024/00094 | Loss: 530.8637 | CE: 0.4642 | KD: 1060.7991\n",
      "Train Epoch: 001 Batch: 00025/00094 | Loss: 530.9550 | CE: 0.6123 | KD: 1060.6853\n",
      "Train Epoch: 001 Batch: 00026/00094 | Loss: 531.0045 | CE: 0.6627 | KD: 1060.6836\n",
      "Train Epoch: 001 Batch: 00027/00094 | Loss: 530.9241 | CE: 0.5778 | KD: 1060.6925\n",
      "Train Epoch: 001 Batch: 00028/00094 | Loss: 530.7545 | CE: 0.4271 | KD: 1060.6547\n",
      "Train Epoch: 001 Batch: 00029/00094 | Loss: 530.9626 | CE: 0.5189 | KD: 1060.8875\n",
      "Train Epoch: 001 Batch: 00030/00094 | Loss: 531.1122 | CE: 0.6145 | KD: 1060.9954\n",
      "Train Epoch: 001 Batch: 00031/00094 | Loss: 530.8928 | CE: 0.4620 | KD: 1060.8615\n",
      "Train Epoch: 001 Batch: 00032/00094 | Loss: 530.8087 | CE: 0.5370 | KD: 1060.5432\n",
      "Train Epoch: 001 Batch: 00033/00094 | Loss: 531.2883 | CE: 0.8611 | KD: 1060.8545\n",
      "Train Epoch: 001 Batch: 00034/00094 | Loss: 530.8311 | CE: 0.4601 | KD: 1060.7418\n",
      "Train Epoch: 001 Batch: 00035/00094 | Loss: 530.9011 | CE: 0.5347 | KD: 1060.7327\n",
      "Train Epoch: 001 Batch: 00036/00094 | Loss: 530.6902 | CE: 0.4229 | KD: 1060.5347\n",
      "Train Epoch: 001 Batch: 00037/00094 | Loss: 530.8234 | CE: 0.4799 | KD: 1060.6870\n",
      "Train Epoch: 001 Batch: 00038/00094 | Loss: 530.8689 | CE: 0.5077 | KD: 1060.7224\n",
      "Train Epoch: 001 Batch: 00039/00094 | Loss: 530.7562 | CE: 0.4586 | KD: 1060.5951\n",
      "Train Epoch: 001 Batch: 00040/00094 | Loss: 531.0771 | CE: 0.7146 | KD: 1060.7252\n",
      "Train Epoch: 001 Batch: 00041/00094 | Loss: 530.8398 | CE: 0.4397 | KD: 1060.8003\n",
      "Train Epoch: 001 Batch: 00042/00094 | Loss: 530.9087 | CE: 0.4860 | KD: 1060.8453\n",
      "Train Epoch: 001 Batch: 00043/00094 | Loss: 530.8054 | CE: 0.4760 | KD: 1060.6587\n",
      "Train Epoch: 001 Batch: 00044/00094 | Loss: 530.9115 | CE: 0.4275 | KD: 1060.9679\n",
      "Train Epoch: 001 Batch: 00045/00094 | Loss: 530.7878 | CE: 0.4469 | KD: 1060.6818\n",
      "Train Epoch: 001 Batch: 00046/00094 | Loss: 530.7479 | CE: 0.4086 | KD: 1060.6787\n",
      "Train Epoch: 001 Batch: 00047/00094 | Loss: 530.6769 | CE: 0.4658 | KD: 1060.4222\n",
      "Train Epoch: 001 Batch: 00048/00094 | Loss: 530.6072 | CE: 0.3815 | KD: 1060.4513\n",
      "Train Epoch: 001 Batch: 00049/00094 | Loss: 530.7149 | CE: 0.4630 | KD: 1060.5039\n",
      "Train Epoch: 001 Batch: 00050/00094 | Loss: 530.6426 | CE: 0.3677 | KD: 1060.5499\n",
      "Train Epoch: 001 Batch: 00051/00094 | Loss: 530.6399 | CE: 0.4086 | KD: 1060.4625\n",
      "Train Epoch: 001 Batch: 00052/00094 | Loss: 530.7285 | CE: 0.4138 | KD: 1060.6293\n",
      "Train Epoch: 001 Batch: 00053/00094 | Loss: 530.8074 | CE: 0.4470 | KD: 1060.7208\n",
      "Train Epoch: 001 Batch: 00054/00094 | Loss: 530.9381 | CE: 0.4999 | KD: 1060.8763\n",
      "Train Epoch: 001 Batch: 00055/00094 | Loss: 530.6728 | CE: 0.4441 | KD: 1060.4574\n",
      "Train Epoch: 001 Batch: 00056/00094 | Loss: 530.6994 | CE: 0.4181 | KD: 1060.5626\n",
      "Train Epoch: 001 Batch: 00057/00094 | Loss: 530.6420 | CE: 0.4064 | KD: 1060.4712\n",
      "Train Epoch: 001 Batch: 00058/00094 | Loss: 530.8087 | CE: 0.4904 | KD: 1060.6365\n",
      "Train Epoch: 001 Batch: 00059/00094 | Loss: 530.9086 | CE: 0.4864 | KD: 1060.8442\n",
      "Train Epoch: 001 Batch: 00060/00094 | Loss: 530.5810 | CE: 0.3975 | KD: 1060.3669\n",
      "Train Epoch: 001 Batch: 00061/00094 | Loss: 530.8466 | CE: 0.4831 | KD: 1060.7269\n",
      "Train Epoch: 001 Batch: 00062/00094 | Loss: 530.9244 | CE: 0.4818 | KD: 1060.8854\n",
      "Train Epoch: 001 Batch: 00063/00094 | Loss: 530.8307 | CE: 0.4857 | KD: 1060.6899\n",
      "Train Epoch: 001 Batch: 00064/00094 | Loss: 530.7263 | CE: 0.3605 | KD: 1060.7314\n",
      "Train Epoch: 001 Batch: 00065/00094 | Loss: 530.7977 | CE: 0.4520 | KD: 1060.6913\n",
      "Train Epoch: 001 Batch: 00066/00094 | Loss: 530.8196 | CE: 0.5796 | KD: 1060.4800\n",
      "Train Epoch: 001 Batch: 00067/00094 | Loss: 530.8234 | CE: 0.4424 | KD: 1060.7620\n",
      "Train Epoch: 001 Batch: 00068/00094 | Loss: 530.5881 | CE: 0.3934 | KD: 1060.3895\n",
      "Train Epoch: 001 Batch: 00069/00094 | Loss: 530.6838 | CE: 0.3758 | KD: 1060.6160\n",
      "Train Epoch: 001 Batch: 00070/00094 | Loss: 530.6528 | CE: 0.3377 | KD: 1060.6302\n",
      "Train Epoch: 001 Batch: 00071/00094 | Loss: 530.8135 | CE: 0.4969 | KD: 1060.6333\n",
      "Train Epoch: 001 Batch: 00072/00094 | Loss: 530.7286 | CE: 0.3324 | KD: 1060.7924\n",
      "Train Epoch: 001 Batch: 00073/00094 | Loss: 530.7122 | CE: 0.3700 | KD: 1060.6843\n",
      "Train Epoch: 001 Batch: 00074/00094 | Loss: 530.6833 | CE: 0.4671 | KD: 1060.4325\n",
      "Train Epoch: 001 Batch: 00075/00094 | Loss: 530.9252 | CE: 0.4980 | KD: 1060.8544\n",
      "Train Epoch: 001 Batch: 00076/00094 | Loss: 530.8412 | CE: 0.4849 | KD: 1060.7126\n",
      "Train Epoch: 001 Batch: 00077/00094 | Loss: 530.6432 | CE: 0.3264 | KD: 1060.6338\n",
      "Train Epoch: 001 Batch: 00078/00094 | Loss: 530.6493 | CE: 0.3584 | KD: 1060.5817\n",
      "Train Epoch: 001 Batch: 00079/00094 | Loss: 530.7345 | CE: 0.4021 | KD: 1060.6649\n",
      "Train Epoch: 001 Batch: 00080/00094 | Loss: 530.8525 | CE: 0.5302 | KD: 1060.6445\n",
      "Train Epoch: 001 Batch: 00081/00094 | Loss: 530.5861 | CE: 0.4027 | KD: 1060.3667\n",
      "Train Epoch: 001 Batch: 00082/00094 | Loss: 530.9108 | CE: 0.5404 | KD: 1060.7408\n",
      "Train Epoch: 001 Batch: 00083/00094 | Loss: 530.7930 | CE: 0.3832 | KD: 1060.8197\n",
      "Train Epoch: 001 Batch: 00084/00094 | Loss: 530.6733 | CE: 0.3721 | KD: 1060.6023\n",
      "Train Epoch: 001 Batch: 00085/00094 | Loss: 530.6268 | CE: 0.3106 | KD: 1060.6324\n",
      "Train Epoch: 001 Batch: 00086/00094 | Loss: 530.5617 | CE: 0.3294 | KD: 1060.4646\n",
      "Train Epoch: 001 Batch: 00087/00094 | Loss: 530.9062 | CE: 0.3981 | KD: 1061.0161\n",
      "Train Epoch: 001 Batch: 00088/00094 | Loss: 530.7182 | CE: 0.3598 | KD: 1060.7168\n",
      "Train Epoch: 001 Batch: 00089/00094 | Loss: 530.6754 | CE: 0.3555 | KD: 1060.6396\n",
      "Train Epoch: 001 Batch: 00090/00094 | Loss: 530.7121 | CE: 0.4024 | KD: 1060.6193\n",
      "Train Epoch: 001 Batch: 00091/00094 | Loss: 530.7548 | CE: 0.3225 | KD: 1060.8647\n",
      "Train Epoch: 001 Batch: 00092/00094 | Loss: 530.6886 | CE: 0.4170 | KD: 1060.5432\n",
      "Train Epoch: 001 Batch: 00093/00094 | Loss: 530.7511 | CE: 0.4438 | KD: 1060.6146\n",
      "Train Epoch: 001 Batch: 00094/00094 | Loss: 530.7004 | CE: 0.3569 | KD: 1060.6871\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3792 | acc:85.7000\n",
      "[VAL Acc] Target: 85.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7339 | acc:51.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0221 | acc:59.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0093 | acc:51.5267\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 51.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1656 | acc:55.4075\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7517 | acc:61.2754\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 61.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1431 | acc:56.3871\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 56.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7982 | acc:65.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 65.31%\n",
      "[VAL Acc] Avg 60.85%\n",
      "VAL Acc improve from 0.00% to 60.85%\n",
      "Save best model\n",
      "Train Epoch: 002 Batch: 00001/00094 | Loss: 530.7260 | CE: 0.4152 | KD: 1060.6216\n",
      "Train Epoch: 002 Batch: 00002/00094 | Loss: 530.8387 | CE: 0.4213 | KD: 1060.8348\n",
      "Train Epoch: 002 Batch: 00003/00094 | Loss: 530.7982 | CE: 0.4452 | KD: 1060.7059\n",
      "Train Epoch: 002 Batch: 00004/00094 | Loss: 530.8675 | CE: 0.4925 | KD: 1060.7499\n",
      "Train Epoch: 002 Batch: 00005/00094 | Loss: 530.7334 | CE: 0.3442 | KD: 1060.7783\n",
      "Train Epoch: 002 Batch: 00006/00094 | Loss: 530.6789 | CE: 0.3137 | KD: 1060.7305\n",
      "Train Epoch: 002 Batch: 00007/00094 | Loss: 530.7084 | CE: 0.3412 | KD: 1060.7343\n",
      "Train Epoch: 002 Batch: 00008/00094 | Loss: 530.6159 | CE: 0.2927 | KD: 1060.6464\n",
      "Train Epoch: 002 Batch: 00009/00094 | Loss: 530.6560 | CE: 0.3554 | KD: 1060.6012\n",
      "Train Epoch: 002 Batch: 00010/00094 | Loss: 530.6934 | CE: 0.3674 | KD: 1060.6520\n",
      "Train Epoch: 002 Batch: 00011/00094 | Loss: 530.5804 | CE: 0.2531 | KD: 1060.6547\n",
      "Train Epoch: 002 Batch: 00012/00094 | Loss: 530.6132 | CE: 0.3385 | KD: 1060.5494\n",
      "Train Epoch: 002 Batch: 00013/00094 | Loss: 530.8162 | CE: 0.3763 | KD: 1060.8796\n",
      "Train Epoch: 002 Batch: 00014/00094 | Loss: 530.7408 | CE: 0.3207 | KD: 1060.8402\n",
      "Train Epoch: 002 Batch: 00015/00094 | Loss: 530.5980 | CE: 0.2914 | KD: 1060.6130\n",
      "Train Epoch: 002 Batch: 00016/00094 | Loss: 530.7689 | CE: 0.3945 | KD: 1060.7487\n",
      "Train Epoch: 002 Batch: 00017/00094 | Loss: 530.6112 | CE: 0.3527 | KD: 1060.5170\n",
      "Train Epoch: 002 Batch: 00018/00094 | Loss: 530.7266 | CE: 0.3685 | KD: 1060.7162\n",
      "Train Epoch: 002 Batch: 00019/00094 | Loss: 530.5692 | CE: 0.2598 | KD: 1060.6189\n",
      "Train Epoch: 002 Batch: 00020/00094 | Loss: 530.6492 | CE: 0.4094 | KD: 1060.4796\n",
      "Train Epoch: 002 Batch: 00021/00094 | Loss: 530.5803 | CE: 0.2794 | KD: 1060.6018\n",
      "Train Epoch: 002 Batch: 00022/00094 | Loss: 530.7660 | CE: 0.4119 | KD: 1060.7081\n",
      "Train Epoch: 002 Batch: 00023/00094 | Loss: 530.6421 | CE: 0.2496 | KD: 1060.7849\n",
      "Train Epoch: 002 Batch: 00024/00094 | Loss: 530.8134 | CE: 0.3961 | KD: 1060.8346\n",
      "Train Epoch: 002 Batch: 00025/00094 | Loss: 530.6282 | CE: 0.3260 | KD: 1060.6044\n",
      "Train Epoch: 002 Batch: 00026/00094 | Loss: 530.5688 | CE: 0.3702 | KD: 1060.3972\n",
      "Train Epoch: 002 Batch: 00027/00094 | Loss: 530.8116 | CE: 0.3895 | KD: 1060.8442\n",
      "Train Epoch: 002 Batch: 00028/00094 | Loss: 530.4532 | CE: 0.2901 | KD: 1060.3262\n",
      "Train Epoch: 002 Batch: 00029/00094 | Loss: 530.8163 | CE: 0.4542 | KD: 1060.7244\n",
      "Train Epoch: 002 Batch: 00030/00094 | Loss: 530.5246 | CE: 0.3069 | KD: 1060.4354\n",
      "Train Epoch: 002 Batch: 00031/00094 | Loss: 530.7870 | CE: 0.4648 | KD: 1060.6445\n",
      "Train Epoch: 002 Batch: 00032/00094 | Loss: 530.7531 | CE: 0.3552 | KD: 1060.7957\n",
      "Train Epoch: 002 Batch: 00033/00094 | Loss: 530.7067 | CE: 0.3433 | KD: 1060.7268\n",
      "Train Epoch: 002 Batch: 00034/00094 | Loss: 530.5338 | CE: 0.2931 | KD: 1060.4812\n",
      "Train Epoch: 002 Batch: 00035/00094 | Loss: 530.6283 | CE: 0.3135 | KD: 1060.6296\n",
      "Train Epoch: 002 Batch: 00036/00094 | Loss: 530.7833 | CE: 0.4620 | KD: 1060.6426\n",
      "Train Epoch: 002 Batch: 00037/00094 | Loss: 530.7010 | CE: 0.4253 | KD: 1060.5515\n",
      "Train Epoch: 002 Batch: 00038/00094 | Loss: 530.6412 | CE: 0.3831 | KD: 1060.5162\n",
      "Train Epoch: 002 Batch: 00039/00094 | Loss: 530.6130 | CE: 0.3094 | KD: 1060.6072\n",
      "Train Epoch: 002 Batch: 00040/00094 | Loss: 530.5874 | CE: 0.3259 | KD: 1060.5231\n",
      "Train Epoch: 002 Batch: 00041/00094 | Loss: 530.6877 | CE: 0.3496 | KD: 1060.6763\n",
      "Train Epoch: 002 Batch: 00042/00094 | Loss: 530.8315 | CE: 0.4305 | KD: 1060.8021\n",
      "Train Epoch: 002 Batch: 00043/00094 | Loss: 530.6748 | CE: 0.3008 | KD: 1060.7480\n",
      "Train Epoch: 002 Batch: 00044/00094 | Loss: 530.6984 | CE: 0.4210 | KD: 1060.5548\n",
      "Train Epoch: 002 Batch: 00045/00094 | Loss: 530.8745 | CE: 0.4124 | KD: 1060.9240\n",
      "Train Epoch: 002 Batch: 00046/00094 | Loss: 530.5539 | CE: 0.2506 | KD: 1060.6066\n",
      "Train Epoch: 002 Batch: 00047/00094 | Loss: 530.6272 | CE: 0.3542 | KD: 1060.5460\n",
      "Train Epoch: 002 Batch: 00048/00094 | Loss: 530.6097 | CE: 0.3644 | KD: 1060.4906\n",
      "Train Epoch: 002 Batch: 00049/00094 | Loss: 530.7808 | CE: 0.4279 | KD: 1060.7057\n",
      "Train Epoch: 002 Batch: 00050/00094 | Loss: 530.4974 | CE: 0.2509 | KD: 1060.4929\n",
      "Train Epoch: 002 Batch: 00051/00094 | Loss: 530.7945 | CE: 0.3161 | KD: 1060.9567\n",
      "Train Epoch: 002 Batch: 00052/00094 | Loss: 530.7701 | CE: 0.4186 | KD: 1060.7030\n",
      "Train Epoch: 002 Batch: 00053/00094 | Loss: 530.6842 | CE: 0.2865 | KD: 1060.7954\n",
      "Train Epoch: 002 Batch: 00054/00094 | Loss: 530.8275 | CE: 0.4277 | KD: 1060.7997\n",
      "Train Epoch: 002 Batch: 00055/00094 | Loss: 530.6329 | CE: 0.2792 | KD: 1060.7075\n",
      "Train Epoch: 002 Batch: 00056/00094 | Loss: 530.8917 | CE: 0.4118 | KD: 1060.9596\n",
      "Train Epoch: 002 Batch: 00057/00094 | Loss: 530.7621 | CE: 0.3805 | KD: 1060.7632\n",
      "Train Epoch: 002 Batch: 00058/00094 | Loss: 530.5364 | CE: 0.2637 | KD: 1060.5454\n",
      "Train Epoch: 002 Batch: 00059/00094 | Loss: 530.7908 | CE: 0.3514 | KD: 1060.8787\n",
      "Train Epoch: 002 Batch: 00060/00094 | Loss: 530.5098 | CE: 0.3438 | KD: 1060.3320\n",
      "Train Epoch: 002 Batch: 00061/00094 | Loss: 530.6314 | CE: 0.3311 | KD: 1060.6006\n",
      "Train Epoch: 002 Batch: 00062/00094 | Loss: 530.6946 | CE: 0.3834 | KD: 1060.6224\n",
      "Train Epoch: 002 Batch: 00063/00094 | Loss: 530.8875 | CE: 0.2806 | KD: 1061.2136\n",
      "Train Epoch: 002 Batch: 00064/00094 | Loss: 530.4431 | CE: 0.2378 | KD: 1060.4106\n",
      "Train Epoch: 002 Batch: 00065/00094 | Loss: 530.5821 | CE: 0.2878 | KD: 1060.5885\n",
      "Train Epoch: 002 Batch: 00066/00094 | Loss: 530.3827 | CE: 0.2553 | KD: 1060.2549\n",
      "Train Epoch: 002 Batch: 00067/00094 | Loss: 530.5638 | CE: 0.2790 | KD: 1060.5696\n",
      "Train Epoch: 002 Batch: 00068/00094 | Loss: 531.2203 | CE: 0.5578 | KD: 1061.3250\n",
      "Train Epoch: 002 Batch: 00069/00094 | Loss: 530.7949 | CE: 0.3226 | KD: 1060.9447\n",
      "Train Epoch: 002 Batch: 00070/00094 | Loss: 530.6061 | CE: 0.3276 | KD: 1060.5571\n",
      "Train Epoch: 002 Batch: 00071/00094 | Loss: 530.8281 | CE: 0.3773 | KD: 1060.9015\n",
      "Train Epoch: 002 Batch: 00072/00094 | Loss: 530.5283 | CE: 0.3107 | KD: 1060.4352\n",
      "Train Epoch: 002 Batch: 00073/00094 | Loss: 530.6066 | CE: 0.3050 | KD: 1060.6031\n",
      "Train Epoch: 002 Batch: 00074/00094 | Loss: 530.5863 | CE: 0.2622 | KD: 1060.6482\n",
      "Train Epoch: 002 Batch: 00075/00094 | Loss: 530.8449 | CE: 0.3958 | KD: 1060.8983\n",
      "Train Epoch: 002 Batch: 00076/00094 | Loss: 530.7381 | CE: 0.3339 | KD: 1060.8083\n",
      "Train Epoch: 002 Batch: 00077/00094 | Loss: 530.7004 | CE: 0.3473 | KD: 1060.7063\n",
      "Train Epoch: 002 Batch: 00078/00094 | Loss: 530.6266 | CE: 0.3325 | KD: 1060.5883\n",
      "Train Epoch: 002 Batch: 00079/00094 | Loss: 530.6678 | CE: 0.3448 | KD: 1060.6461\n",
      "Train Epoch: 002 Batch: 00080/00094 | Loss: 530.7001 | CE: 0.4430 | KD: 1060.5143\n",
      "Train Epoch: 002 Batch: 00081/00094 | Loss: 530.6916 | CE: 0.3236 | KD: 1060.7361\n",
      "Train Epoch: 002 Batch: 00082/00094 | Loss: 530.6520 | CE: 0.2858 | KD: 1060.7323\n",
      "Train Epoch: 002 Batch: 00083/00094 | Loss: 530.5524 | CE: 0.2948 | KD: 1060.5153\n",
      "Train Epoch: 002 Batch: 00084/00094 | Loss: 530.5869 | CE: 0.2950 | KD: 1060.5837\n",
      "Train Epoch: 002 Batch: 00085/00094 | Loss: 530.5982 | CE: 0.2502 | KD: 1060.6960\n",
      "Train Epoch: 002 Batch: 00086/00094 | Loss: 530.7605 | CE: 0.3285 | KD: 1060.8640\n",
      "Train Epoch: 002 Batch: 00087/00094 | Loss: 530.4714 | CE: 0.2288 | KD: 1060.4852\n",
      "Train Epoch: 002 Batch: 00088/00094 | Loss: 530.6190 | CE: 0.3600 | KD: 1060.5181\n",
      "Train Epoch: 002 Batch: 00089/00094 | Loss: 530.5854 | CE: 0.2767 | KD: 1060.6173\n",
      "Train Epoch: 002 Batch: 00090/00094 | Loss: 530.6011 | CE: 0.2453 | KD: 1060.7117\n",
      "Train Epoch: 002 Batch: 00091/00094 | Loss: 530.5696 | CE: 0.2363 | KD: 1060.6666\n",
      "Train Epoch: 002 Batch: 00092/00094 | Loss: 530.8429 | CE: 0.3091 | KD: 1061.0676\n",
      "Train Epoch: 002 Batch: 00093/00094 | Loss: 530.5018 | CE: 0.2396 | KD: 1060.5244\n",
      "Train Epoch: 002 Batch: 00094/00094 | Loss: 530.5945 | CE: 0.2868 | KD: 1060.6154\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3096 | acc:89.3000\n",
      "[VAL Acc] Target: 89.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5264 | acc:51.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0551 | acc:60.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0099 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.0606 | acc:56.6223\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7028 | acc:65.7116\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 65.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9611 | acc:60.2665\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 60.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7778 | acc:62.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.38%\n",
      "[VAL Acc] Avg 62.08%\n",
      "VAL Acc improve from 60.85% to 62.08%\n",
      "Save best model\n",
      "Train Epoch: 003 Batch: 00001/00094 | Loss: 530.6210 | CE: 0.3242 | KD: 1060.5938\n",
      "Train Epoch: 003 Batch: 00002/00094 | Loss: 530.6622 | CE: 0.2885 | KD: 1060.7476\n",
      "Train Epoch: 003 Batch: 00003/00094 | Loss: 530.7776 | CE: 0.3701 | KD: 1060.8149\n",
      "Train Epoch: 003 Batch: 00004/00094 | Loss: 530.6005 | CE: 0.2414 | KD: 1060.7181\n",
      "Train Epoch: 003 Batch: 00005/00094 | Loss: 530.5574 | CE: 0.3404 | KD: 1060.4341\n",
      "Train Epoch: 003 Batch: 00006/00094 | Loss: 530.5223 | CE: 0.2715 | KD: 1060.5016\n",
      "Train Epoch: 003 Batch: 00007/00094 | Loss: 530.5526 | CE: 0.2235 | KD: 1060.6581\n",
      "Train Epoch: 003 Batch: 00008/00094 | Loss: 530.4579 | CE: 0.2130 | KD: 1060.4899\n",
      "Train Epoch: 003 Batch: 00009/00094 | Loss: 530.6591 | CE: 0.3080 | KD: 1060.7021\n",
      "Train Epoch: 003 Batch: 00010/00094 | Loss: 530.5857 | CE: 0.2675 | KD: 1060.6364\n",
      "Train Epoch: 003 Batch: 00011/00094 | Loss: 530.5967 | CE: 0.2551 | KD: 1060.6832\n",
      "Train Epoch: 003 Batch: 00012/00094 | Loss: 530.5521 | CE: 0.3169 | KD: 1060.4705\n",
      "Train Epoch: 003 Batch: 00013/00094 | Loss: 530.7853 | CE: 0.3215 | KD: 1060.9276\n",
      "Train Epoch: 003 Batch: 00014/00094 | Loss: 530.6373 | CE: 0.2954 | KD: 1060.6838\n",
      "Train Epoch: 003 Batch: 00015/00094 | Loss: 530.6789 | CE: 0.3860 | KD: 1060.5858\n",
      "Train Epoch: 003 Batch: 00016/00094 | Loss: 530.6464 | CE: 0.2456 | KD: 1060.8015\n",
      "Train Epoch: 003 Batch: 00017/00094 | Loss: 530.6948 | CE: 0.3050 | KD: 1060.7795\n",
      "Train Epoch: 003 Batch: 00018/00094 | Loss: 530.6270 | CE: 0.3797 | KD: 1060.4948\n",
      "Train Epoch: 003 Batch: 00019/00094 | Loss: 530.8095 | CE: 0.3737 | KD: 1060.8717\n",
      "Train Epoch: 003 Batch: 00020/00094 | Loss: 530.6721 | CE: 0.3179 | KD: 1060.7084\n",
      "Train Epoch: 003 Batch: 00021/00094 | Loss: 530.6942 | CE: 0.3072 | KD: 1060.7739\n",
      "Train Epoch: 003 Batch: 00022/00094 | Loss: 530.5862 | CE: 0.2134 | KD: 1060.7456\n",
      "Train Epoch: 003 Batch: 00023/00094 | Loss: 530.5004 | CE: 0.2771 | KD: 1060.4464\n",
      "Train Epoch: 003 Batch: 00024/00094 | Loss: 530.6132 | CE: 0.3136 | KD: 1060.5994\n",
      "Train Epoch: 003 Batch: 00025/00094 | Loss: 530.5504 | CE: 0.2237 | KD: 1060.6534\n",
      "Train Epoch: 003 Batch: 00026/00094 | Loss: 530.7264 | CE: 0.3109 | KD: 1060.8309\n",
      "Train Epoch: 003 Batch: 00027/00094 | Loss: 530.5631 | CE: 0.2866 | KD: 1060.5530\n",
      "Train Epoch: 003 Batch: 00028/00094 | Loss: 530.6081 | CE: 0.2947 | KD: 1060.6268\n",
      "Train Epoch: 003 Batch: 00029/00094 | Loss: 530.8665 | CE: 0.4393 | KD: 1060.8544\n",
      "Train Epoch: 003 Batch: 00030/00094 | Loss: 530.5589 | CE: 0.2409 | KD: 1060.6360\n",
      "Train Epoch: 003 Batch: 00031/00094 | Loss: 530.8457 | CE: 0.3621 | KD: 1060.9673\n",
      "Train Epoch: 003 Batch: 00032/00094 | Loss: 530.7627 | CE: 0.3828 | KD: 1060.7598\n",
      "Train Epoch: 003 Batch: 00033/00094 | Loss: 530.6660 | CE: 0.3330 | KD: 1060.6659\n",
      "Train Epoch: 003 Batch: 00034/00094 | Loss: 530.3977 | CE: 0.2242 | KD: 1060.3470\n",
      "Train Epoch: 003 Batch: 00035/00094 | Loss: 530.6750 | CE: 0.2722 | KD: 1060.8058\n",
      "Train Epoch: 003 Batch: 00036/00094 | Loss: 530.6641 | CE: 0.2525 | KD: 1060.8231\n",
      "Train Epoch: 003 Batch: 00037/00094 | Loss: 530.7306 | CE: 0.3153 | KD: 1060.8306\n",
      "Train Epoch: 003 Batch: 00038/00094 | Loss: 530.6381 | CE: 0.2836 | KD: 1060.7090\n",
      "Train Epoch: 003 Batch: 00039/00094 | Loss: 530.4652 | CE: 0.2472 | KD: 1060.4360\n",
      "Train Epoch: 003 Batch: 00040/00094 | Loss: 530.5777 | CE: 0.2488 | KD: 1060.6578\n",
      "Train Epoch: 003 Batch: 00041/00094 | Loss: 530.7512 | CE: 0.3060 | KD: 1060.8903\n",
      "Train Epoch: 003 Batch: 00042/00094 | Loss: 530.4259 | CE: 0.2866 | KD: 1060.2787\n",
      "Train Epoch: 003 Batch: 00043/00094 | Loss: 530.4351 | CE: 0.1883 | KD: 1060.4935\n",
      "Train Epoch: 003 Batch: 00044/00094 | Loss: 530.4609 | CE: 0.1929 | KD: 1060.5361\n",
      "Train Epoch: 003 Batch: 00045/00094 | Loss: 530.6974 | CE: 0.2864 | KD: 1060.8221\n",
      "Train Epoch: 003 Batch: 00046/00094 | Loss: 530.7299 | CE: 0.4056 | KD: 1060.6486\n",
      "Train Epoch: 003 Batch: 00047/00094 | Loss: 530.5399 | CE: 0.2558 | KD: 1060.5681\n",
      "Train Epoch: 003 Batch: 00048/00094 | Loss: 530.7774 | CE: 0.3958 | KD: 1060.7632\n",
      "Train Epoch: 003 Batch: 00049/00094 | Loss: 530.4731 | CE: 0.2860 | KD: 1060.3743\n",
      "Train Epoch: 003 Batch: 00050/00094 | Loss: 530.7932 | CE: 0.2644 | KD: 1061.0575\n",
      "Train Epoch: 003 Batch: 00051/00094 | Loss: 530.5262 | CE: 0.2965 | KD: 1060.4595\n",
      "Train Epoch: 003 Batch: 00052/00094 | Loss: 530.5660 | CE: 0.2862 | KD: 1060.5598\n",
      "Train Epoch: 003 Batch: 00053/00094 | Loss: 530.5555 | CE: 0.2759 | KD: 1060.5593\n",
      "Train Epoch: 003 Batch: 00054/00094 | Loss: 530.5518 | CE: 0.2265 | KD: 1060.6508\n",
      "Train Epoch: 003 Batch: 00055/00094 | Loss: 530.4904 | CE: 0.2698 | KD: 1060.4410\n",
      "Train Epoch: 003 Batch: 00056/00094 | Loss: 530.5015 | CE: 0.2663 | KD: 1060.4705\n",
      "Train Epoch: 003 Batch: 00057/00094 | Loss: 530.5978 | CE: 0.2891 | KD: 1060.6173\n",
      "Train Epoch: 003 Batch: 00058/00094 | Loss: 530.4180 | CE: 0.2244 | KD: 1060.3872\n",
      "Train Epoch: 003 Batch: 00059/00094 | Loss: 530.7000 | CE: 0.3421 | KD: 1060.7158\n",
      "Train Epoch: 003 Batch: 00060/00094 | Loss: 530.6188 | CE: 0.2841 | KD: 1060.6693\n",
      "Train Epoch: 003 Batch: 00061/00094 | Loss: 530.5860 | CE: 0.2422 | KD: 1060.6876\n",
      "Train Epoch: 003 Batch: 00062/00094 | Loss: 530.6629 | CE: 0.3654 | KD: 1060.5951\n",
      "Train Epoch: 003 Batch: 00063/00094 | Loss: 530.5231 | CE: 0.2063 | KD: 1060.6334\n",
      "Train Epoch: 003 Batch: 00064/00094 | Loss: 530.8626 | CE: 0.5335 | KD: 1060.6582\n",
      "Train Epoch: 003 Batch: 00065/00094 | Loss: 530.7486 | CE: 0.2799 | KD: 1060.9374\n",
      "Train Epoch: 003 Batch: 00066/00094 | Loss: 530.6998 | CE: 0.2007 | KD: 1060.9980\n",
      "Train Epoch: 003 Batch: 00067/00094 | Loss: 530.6284 | CE: 0.2581 | KD: 1060.7406\n",
      "Train Epoch: 003 Batch: 00068/00094 | Loss: 530.7768 | CE: 0.3823 | KD: 1060.7889\n",
      "Train Epoch: 003 Batch: 00069/00094 | Loss: 530.5995 | CE: 0.2646 | KD: 1060.6700\n",
      "Train Epoch: 003 Batch: 00070/00094 | Loss: 530.6558 | CE: 0.3093 | KD: 1060.6930\n",
      "Train Epoch: 003 Batch: 00071/00094 | Loss: 530.4350 | CE: 0.2090 | KD: 1060.4519\n",
      "Train Epoch: 003 Batch: 00072/00094 | Loss: 530.7903 | CE: 0.3079 | KD: 1060.9647\n",
      "Train Epoch: 003 Batch: 00073/00094 | Loss: 530.5707 | CE: 0.2470 | KD: 1060.6476\n",
      "Train Epoch: 003 Batch: 00074/00094 | Loss: 530.5790 | CE: 0.2582 | KD: 1060.6416\n",
      "Train Epoch: 003 Batch: 00075/00094 | Loss: 530.4268 | CE: 0.2482 | KD: 1060.3572\n",
      "Train Epoch: 003 Batch: 00076/00094 | Loss: 531.1759 | CE: 0.6140 | KD: 1061.1239\n",
      "Train Epoch: 003 Batch: 00077/00094 | Loss: 530.7706 | CE: 0.3017 | KD: 1060.9377\n",
      "Train Epoch: 003 Batch: 00078/00094 | Loss: 530.7023 | CE: 0.3102 | KD: 1060.7843\n",
      "Train Epoch: 003 Batch: 00079/00094 | Loss: 530.4680 | CE: 0.2594 | KD: 1060.4172\n",
      "Train Epoch: 003 Batch: 00080/00094 | Loss: 530.6521 | CE: 0.3176 | KD: 1060.6689\n",
      "Train Epoch: 003 Batch: 00081/00094 | Loss: 530.5029 | CE: 0.2355 | KD: 1060.5348\n",
      "Train Epoch: 003 Batch: 00082/00094 | Loss: 530.8533 | CE: 0.3263 | KD: 1061.0541\n",
      "Train Epoch: 003 Batch: 00083/00094 | Loss: 530.8070 | CE: 0.4441 | KD: 1060.7257\n",
      "Train Epoch: 003 Batch: 00084/00094 | Loss: 530.6331 | CE: 0.2095 | KD: 1060.8472\n",
      "Train Epoch: 003 Batch: 00085/00094 | Loss: 530.5314 | CE: 0.2320 | KD: 1060.5989\n",
      "Train Epoch: 003 Batch: 00086/00094 | Loss: 530.5834 | CE: 0.2725 | KD: 1060.6218\n",
      "Train Epoch: 003 Batch: 00087/00094 | Loss: 530.6447 | CE: 0.2684 | KD: 1060.7526\n",
      "Train Epoch: 003 Batch: 00088/00094 | Loss: 530.6968 | CE: 0.3378 | KD: 1060.7180\n",
      "Train Epoch: 003 Batch: 00089/00094 | Loss: 530.6645 | CE: 0.3669 | KD: 1060.5951\n",
      "Train Epoch: 003 Batch: 00090/00094 | Loss: 530.6063 | CE: 0.3356 | KD: 1060.5414\n",
      "Train Epoch: 003 Batch: 00091/00094 | Loss: 530.4816 | CE: 0.2126 | KD: 1060.5380\n",
      "Train Epoch: 003 Batch: 00092/00094 | Loss: 530.5681 | CE: 0.2904 | KD: 1060.5555\n",
      "Train Epoch: 003 Batch: 00093/00094 | Loss: 530.9219 | CE: 0.4148 | KD: 1061.0140\n",
      "Train Epoch: 003 Batch: 00094/00094 | Loss: 530.5867 | CE: 0.2091 | KD: 1060.7551\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2595 | acc:92.2000\n",
      "[VAL Acc] Target: 92.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6150 | acc:50.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0855 | acc:59.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9665 | acc:51.9084\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 51.91%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2365 | acc:54.8981\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6024 | acc:70.5176\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0822 | acc:58.6207\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 58.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7632 | acc:61.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.81%\n",
      "[VAL Acc] Avg 62.47%\n",
      "VAL Acc improve from 62.08% to 62.47%\n",
      "Save best model\n",
      "Train Epoch: 004 Batch: 00001/00094 | Loss: 530.8495 | CE: 0.2841 | KD: 1061.1309\n",
      "Train Epoch: 004 Batch: 00002/00094 | Loss: 530.6056 | CE: 0.2432 | KD: 1060.7247\n",
      "Train Epoch: 004 Batch: 00003/00094 | Loss: 530.6009 | CE: 0.2820 | KD: 1060.6378\n",
      "Train Epoch: 004 Batch: 00004/00094 | Loss: 530.4586 | CE: 0.2336 | KD: 1060.4498\n",
      "Train Epoch: 004 Batch: 00005/00094 | Loss: 530.6009 | CE: 0.2771 | KD: 1060.6476\n",
      "Train Epoch: 004 Batch: 00006/00094 | Loss: 530.5400 | CE: 0.2381 | KD: 1060.6039\n",
      "Train Epoch: 004 Batch: 00007/00094 | Loss: 530.6373 | CE: 0.2627 | KD: 1060.7493\n",
      "Train Epoch: 004 Batch: 00008/00094 | Loss: 530.5433 | CE: 0.2188 | KD: 1060.6492\n",
      "Train Epoch: 004 Batch: 00009/00094 | Loss: 530.6516 | CE: 0.2273 | KD: 1060.8484\n",
      "Train Epoch: 004 Batch: 00010/00094 | Loss: 530.6879 | CE: 0.2711 | KD: 1060.8336\n",
      "Train Epoch: 004 Batch: 00011/00094 | Loss: 530.6713 | CE: 0.3159 | KD: 1060.7107\n",
      "Train Epoch: 004 Batch: 00012/00094 | Loss: 530.9543 | CE: 0.5384 | KD: 1060.8317\n",
      "Train Epoch: 004 Batch: 00013/00094 | Loss: 530.5612 | CE: 0.3059 | KD: 1060.5105\n",
      "Train Epoch: 004 Batch: 00014/00094 | Loss: 530.5444 | CE: 0.2258 | KD: 1060.6372\n",
      "Train Epoch: 004 Batch: 00015/00094 | Loss: 530.7191 | CE: 0.3108 | KD: 1060.8165\n",
      "Train Epoch: 004 Batch: 00016/00094 | Loss: 530.7183 | CE: 0.2742 | KD: 1060.8881\n",
      "Train Epoch: 004 Batch: 00017/00094 | Loss: 530.9645 | CE: 0.3549 | KD: 1061.2192\n",
      "Train Epoch: 004 Batch: 00018/00094 | Loss: 530.6094 | CE: 0.3342 | KD: 1060.5505\n",
      "Train Epoch: 004 Batch: 00019/00094 | Loss: 530.6665 | CE: 0.2296 | KD: 1060.8738\n",
      "Train Epoch: 004 Batch: 00020/00094 | Loss: 530.3970 | CE: 0.2299 | KD: 1060.3342\n",
      "Train Epoch: 004 Batch: 00021/00094 | Loss: 530.7397 | CE: 0.3221 | KD: 1060.8352\n",
      "Train Epoch: 004 Batch: 00022/00094 | Loss: 530.5796 | CE: 0.2482 | KD: 1060.6627\n",
      "Train Epoch: 004 Batch: 00023/00094 | Loss: 530.5457 | CE: 0.2115 | KD: 1060.6685\n",
      "Train Epoch: 004 Batch: 00024/00094 | Loss: 530.5490 | CE: 0.3066 | KD: 1060.4846\n",
      "Train Epoch: 004 Batch: 00025/00094 | Loss: 530.5630 | CE: 0.2527 | KD: 1060.6208\n",
      "Train Epoch: 004 Batch: 00026/00094 | Loss: 530.7508 | CE: 0.3008 | KD: 1060.9000\n",
      "Train Epoch: 004 Batch: 00027/00094 | Loss: 530.6902 | CE: 0.2450 | KD: 1060.8906\n",
      "Train Epoch: 004 Batch: 00028/00094 | Loss: 530.6479 | CE: 0.3124 | KD: 1060.6710\n",
      "Train Epoch: 004 Batch: 00029/00094 | Loss: 530.5858 | CE: 0.2043 | KD: 1060.7631\n",
      "Train Epoch: 004 Batch: 00030/00094 | Loss: 530.7189 | CE: 0.2816 | KD: 1060.8748\n",
      "Train Epoch: 004 Batch: 00031/00094 | Loss: 530.6934 | CE: 0.3957 | KD: 1060.5953\n",
      "Train Epoch: 004 Batch: 00032/00094 | Loss: 530.6983 | CE: 0.4037 | KD: 1060.5891\n",
      "Train Epoch: 004 Batch: 00033/00094 | Loss: 530.5418 | CE: 0.1993 | KD: 1060.6851\n",
      "Train Epoch: 004 Batch: 00034/00094 | Loss: 530.6722 | CE: 0.2972 | KD: 1060.7500\n",
      "Train Epoch: 004 Batch: 00035/00094 | Loss: 530.7961 | CE: 0.3597 | KD: 1060.8727\n",
      "Train Epoch: 004 Batch: 00036/00094 | Loss: 530.6083 | CE: 0.2512 | KD: 1060.7142\n",
      "Train Epoch: 004 Batch: 00037/00094 | Loss: 530.6370 | CE: 0.2795 | KD: 1060.7148\n",
      "Train Epoch: 004 Batch: 00038/00094 | Loss: 530.5677 | CE: 0.2362 | KD: 1060.6632\n",
      "Train Epoch: 004 Batch: 00039/00094 | Loss: 530.7117 | CE: 0.3483 | KD: 1060.7268\n",
      "Train Epoch: 004 Batch: 00040/00094 | Loss: 530.5990 | CE: 0.3247 | KD: 1060.5486\n",
      "Train Epoch: 004 Batch: 00041/00094 | Loss: 530.5137 | CE: 0.2084 | KD: 1060.6106\n",
      "Train Epoch: 004 Batch: 00042/00094 | Loss: 530.7140 | CE: 0.2852 | KD: 1060.8575\n",
      "Train Epoch: 004 Batch: 00043/00094 | Loss: 530.4703 | CE: 0.2264 | KD: 1060.4877\n",
      "Train Epoch: 004 Batch: 00044/00094 | Loss: 530.6844 | CE: 0.2801 | KD: 1060.8087\n",
      "Train Epoch: 004 Batch: 00045/00094 | Loss: 530.8863 | CE: 0.4861 | KD: 1060.8004\n",
      "Train Epoch: 004 Batch: 00046/00094 | Loss: 530.6131 | CE: 0.2836 | KD: 1060.6589\n",
      "Train Epoch: 004 Batch: 00047/00094 | Loss: 530.6479 | CE: 0.2627 | KD: 1060.7704\n",
      "Train Epoch: 004 Batch: 00048/00094 | Loss: 530.6833 | CE: 0.2924 | KD: 1060.7820\n",
      "Train Epoch: 004 Batch: 00049/00094 | Loss: 530.6049 | CE: 0.2683 | KD: 1060.6732\n",
      "Train Epoch: 004 Batch: 00050/00094 | Loss: 530.5795 | CE: 0.2527 | KD: 1060.6537\n",
      "Train Epoch: 004 Batch: 00051/00094 | Loss: 530.6901 | CE: 0.2452 | KD: 1060.8896\n",
      "Train Epoch: 004 Batch: 00052/00094 | Loss: 530.4557 | CE: 0.2540 | KD: 1060.4034\n",
      "Train Epoch: 004 Batch: 00053/00094 | Loss: 530.5796 | CE: 0.2825 | KD: 1060.5942\n",
      "Train Epoch: 004 Batch: 00054/00094 | Loss: 530.5862 | CE: 0.2801 | KD: 1060.6122\n",
      "Train Epoch: 004 Batch: 00055/00094 | Loss: 530.5186 | CE: 0.2596 | KD: 1060.5179\n",
      "Train Epoch: 004 Batch: 00056/00094 | Loss: 530.5614 | CE: 0.2261 | KD: 1060.6707\n",
      "Train Epoch: 004 Batch: 00057/00094 | Loss: 530.7061 | CE: 0.2790 | KD: 1060.8541\n",
      "Train Epoch: 004 Batch: 00058/00094 | Loss: 530.6286 | CE: 0.3224 | KD: 1060.6124\n",
      "Train Epoch: 004 Batch: 00059/00094 | Loss: 530.8057 | CE: 0.3993 | KD: 1060.8130\n",
      "Train Epoch: 004 Batch: 00060/00094 | Loss: 530.8300 | CE: 0.3404 | KD: 1060.9792\n",
      "Train Epoch: 004 Batch: 00061/00094 | Loss: 530.6565 | CE: 0.2307 | KD: 1060.8517\n",
      "Train Epoch: 004 Batch: 00062/00094 | Loss: 530.5477 | CE: 0.2351 | KD: 1060.6251\n",
      "Train Epoch: 004 Batch: 00063/00094 | Loss: 530.8265 | CE: 0.2859 | KD: 1061.0812\n",
      "Train Epoch: 004 Batch: 00064/00094 | Loss: 530.5927 | CE: 0.2405 | KD: 1060.7045\n",
      "Train Epoch: 004 Batch: 00065/00094 | Loss: 530.5361 | CE: 0.2540 | KD: 1060.5641\n",
      "Train Epoch: 004 Batch: 00066/00094 | Loss: 530.5109 | CE: 0.2474 | KD: 1060.5271\n",
      "Train Epoch: 004 Batch: 00067/00094 | Loss: 530.6472 | CE: 0.2193 | KD: 1060.8557\n",
      "Train Epoch: 004 Batch: 00068/00094 | Loss: 530.7514 | CE: 0.3802 | KD: 1060.7423\n",
      "Train Epoch: 004 Batch: 00069/00094 | Loss: 530.6553 | CE: 0.3006 | KD: 1060.7096\n",
      "Train Epoch: 004 Batch: 00070/00094 | Loss: 530.6502 | CE: 0.2650 | KD: 1060.7704\n",
      "Train Epoch: 004 Batch: 00071/00094 | Loss: 530.5544 | CE: 0.1954 | KD: 1060.7179\n",
      "Train Epoch: 004 Batch: 00072/00094 | Loss: 530.5684 | CE: 0.3147 | KD: 1060.5074\n",
      "Train Epoch: 004 Batch: 00073/00094 | Loss: 530.6592 | CE: 0.2688 | KD: 1060.7809\n",
      "Train Epoch: 004 Batch: 00074/00094 | Loss: 530.7055 | CE: 0.2933 | KD: 1060.8245\n",
      "Train Epoch: 004 Batch: 00075/00094 | Loss: 530.5699 | CE: 0.2592 | KD: 1060.6213\n",
      "Train Epoch: 004 Batch: 00076/00094 | Loss: 530.5084 | CE: 0.3005 | KD: 1060.4158\n",
      "Train Epoch: 004 Batch: 00077/00094 | Loss: 530.6042 | CE: 0.3734 | KD: 1060.4618\n",
      "Train Epoch: 004 Batch: 00078/00094 | Loss: 530.5195 | CE: 0.2846 | KD: 1060.4697\n",
      "Train Epoch: 004 Batch: 00079/00094 | Loss: 530.6184 | CE: 0.2855 | KD: 1060.6658\n",
      "Train Epoch: 004 Batch: 00080/00094 | Loss: 530.5925 | CE: 0.2519 | KD: 1060.6812\n",
      "Train Epoch: 004 Batch: 00081/00094 | Loss: 530.6567 | CE: 0.4237 | KD: 1060.4662\n",
      "Train Epoch: 004 Batch: 00082/00094 | Loss: 530.4666 | CE: 0.2397 | KD: 1060.4536\n",
      "Train Epoch: 004 Batch: 00083/00094 | Loss: 530.6551 | CE: 0.2582 | KD: 1060.7937\n",
      "Train Epoch: 004 Batch: 00084/00094 | Loss: 530.6995 | CE: 0.2813 | KD: 1060.8364\n",
      "Train Epoch: 004 Batch: 00085/00094 | Loss: 530.5716 | CE: 0.3110 | KD: 1060.5212\n",
      "Train Epoch: 004 Batch: 00086/00094 | Loss: 530.5927 | CE: 0.2010 | KD: 1060.7833\n",
      "Train Epoch: 004 Batch: 00087/00094 | Loss: 530.4474 | CE: 0.2009 | KD: 1060.4930\n",
      "Train Epoch: 004 Batch: 00088/00094 | Loss: 530.8008 | CE: 0.3672 | KD: 1060.8672\n",
      "Train Epoch: 004 Batch: 00089/00094 | Loss: 530.6002 | CE: 0.2781 | KD: 1060.6442\n",
      "Train Epoch: 004 Batch: 00090/00094 | Loss: 530.7013 | CE: 0.2374 | KD: 1060.9277\n",
      "Train Epoch: 004 Batch: 00091/00094 | Loss: 530.8964 | CE: 0.3505 | KD: 1061.0918\n",
      "Train Epoch: 004 Batch: 00092/00094 | Loss: 530.7946 | CE: 0.3100 | KD: 1060.9690\n",
      "Train Epoch: 004 Batch: 00093/00094 | Loss: 530.5597 | CE: 0.2442 | KD: 1060.6310\n",
      "Train Epoch: 004 Batch: 00094/00094 | Loss: 530.3804 | CE: 0.1869 | KD: 1060.3871\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2531 | acc:92.5500\n",
      "[VAL Acc] Target: 92.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5452 | acc:50.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9276 | acc:59.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0047 | acc:50.3817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1166 | acc:56.2304\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6117 | acc:68.0222\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8808 | acc:63.7931\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.6980 | acc:62.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.38%\n",
      "[VAL Acc] Avg 62.91%\n",
      "VAL Acc improve from 62.47% to 62.91%\n",
      "Save best model\n",
      "Train Epoch: 005 Batch: 00001/00094 | Loss: 530.6289 | CE: 0.2469 | KD: 1060.7642\n",
      "Train Epoch: 005 Batch: 00002/00094 | Loss: 530.5813 | CE: 0.2466 | KD: 1060.6694\n",
      "Train Epoch: 005 Batch: 00003/00094 | Loss: 530.5097 | CE: 0.2541 | KD: 1060.5111\n",
      "Train Epoch: 005 Batch: 00004/00094 | Loss: 530.6115 | CE: 0.2896 | KD: 1060.6438\n",
      "Train Epoch: 005 Batch: 00005/00094 | Loss: 530.5465 | CE: 0.2416 | KD: 1060.6099\n",
      "Train Epoch: 005 Batch: 00006/00094 | Loss: 530.5869 | CE: 0.2650 | KD: 1060.6436\n",
      "Train Epoch: 005 Batch: 00007/00094 | Loss: 530.6052 | CE: 0.3092 | KD: 1060.5920\n",
      "Train Epoch: 005 Batch: 00008/00094 | Loss: 530.6742 | CE: 0.2198 | KD: 1060.9088\n",
      "Train Epoch: 005 Batch: 00009/00094 | Loss: 530.5204 | CE: 0.2577 | KD: 1060.5254\n",
      "Train Epoch: 005 Batch: 00010/00094 | Loss: 530.6364 | CE: 0.2325 | KD: 1060.8079\n",
      "Train Epoch: 005 Batch: 00011/00094 | Loss: 530.3841 | CE: 0.1855 | KD: 1060.3972\n",
      "Train Epoch: 005 Batch: 00012/00094 | Loss: 530.5754 | CE: 0.2081 | KD: 1060.7346\n",
      "Train Epoch: 005 Batch: 00013/00094 | Loss: 530.7651 | CE: 0.2896 | KD: 1060.9510\n",
      "Train Epoch: 005 Batch: 00014/00094 | Loss: 530.7026 | CE: 0.2472 | KD: 1060.9109\n",
      "Train Epoch: 005 Batch: 00015/00094 | Loss: 530.6125 | CE: 0.2908 | KD: 1060.6434\n",
      "Train Epoch: 005 Batch: 00016/00094 | Loss: 530.4754 | CE: 0.2741 | KD: 1060.4027\n",
      "Train Epoch: 005 Batch: 00017/00094 | Loss: 530.4562 | CE: 0.2514 | KD: 1060.4094\n",
      "Train Epoch: 005 Batch: 00018/00094 | Loss: 530.5158 | CE: 0.2360 | KD: 1060.5596\n",
      "Train Epoch: 005 Batch: 00019/00094 | Loss: 530.5234 | CE: 0.2156 | KD: 1060.6156\n",
      "Train Epoch: 005 Batch: 00020/00094 | Loss: 530.6338 | CE: 0.2829 | KD: 1060.7018\n",
      "Train Epoch: 005 Batch: 00021/00094 | Loss: 530.7552 | CE: 0.3441 | KD: 1060.8221\n",
      "Train Epoch: 005 Batch: 00022/00094 | Loss: 530.7319 | CE: 0.4143 | KD: 1060.6351\n",
      "Train Epoch: 005 Batch: 00023/00094 | Loss: 530.7612 | CE: 0.4115 | KD: 1060.6993\n",
      "Train Epoch: 005 Batch: 00024/00094 | Loss: 530.5028 | CE: 0.1790 | KD: 1060.6476\n",
      "Train Epoch: 005 Batch: 00025/00094 | Loss: 530.6071 | CE: 0.3087 | KD: 1060.5968\n",
      "Train Epoch: 005 Batch: 00026/00094 | Loss: 530.5201 | CE: 0.1853 | KD: 1060.6697\n",
      "Train Epoch: 005 Batch: 00027/00094 | Loss: 530.7123 | CE: 0.2742 | KD: 1060.8761\n",
      "Train Epoch: 005 Batch: 00028/00094 | Loss: 530.4861 | CE: 0.1912 | KD: 1060.5900\n",
      "Train Epoch: 005 Batch: 00029/00094 | Loss: 530.8126 | CE: 0.3076 | KD: 1061.0100\n",
      "Train Epoch: 005 Batch: 00030/00094 | Loss: 530.6730 | CE: 0.3478 | KD: 1060.6505\n",
      "Train Epoch: 005 Batch: 00031/00094 | Loss: 530.5930 | CE: 0.2599 | KD: 1060.6661\n",
      "Train Epoch: 005 Batch: 00032/00094 | Loss: 530.5460 | CE: 0.2347 | KD: 1060.6224\n",
      "Train Epoch: 005 Batch: 00033/00094 | Loss: 530.5903 | CE: 0.2793 | KD: 1060.6221\n",
      "Train Epoch: 005 Batch: 00034/00094 | Loss: 530.7159 | CE: 0.2807 | KD: 1060.8705\n",
      "Train Epoch: 005 Batch: 00035/00094 | Loss: 530.7480 | CE: 0.2618 | KD: 1060.9724\n",
      "Train Epoch: 005 Batch: 00036/00094 | Loss: 530.5163 | CE: 0.2463 | KD: 1060.5399\n",
      "Train Epoch: 005 Batch: 00037/00094 | Loss: 530.5640 | CE: 0.2773 | KD: 1060.5735\n",
      "Train Epoch: 005 Batch: 00038/00094 | Loss: 530.7315 | CE: 0.3244 | KD: 1060.8143\n",
      "Train Epoch: 005 Batch: 00039/00094 | Loss: 530.8133 | CE: 0.3982 | KD: 1060.8302\n",
      "Train Epoch: 005 Batch: 00040/00094 | Loss: 530.5466 | CE: 0.2957 | KD: 1060.5020\n",
      "Train Epoch: 005 Batch: 00041/00094 | Loss: 530.6072 | CE: 0.2437 | KD: 1060.7271\n",
      "Train Epoch: 005 Batch: 00042/00094 | Loss: 530.7034 | CE: 0.2298 | KD: 1060.9471\n",
      "Train Epoch: 005 Batch: 00043/00094 | Loss: 530.6527 | CE: 0.2597 | KD: 1060.7861\n",
      "Train Epoch: 005 Batch: 00044/00094 | Loss: 530.4405 | CE: 0.1763 | KD: 1060.5284\n",
      "Train Epoch: 005 Batch: 00045/00094 | Loss: 530.6683 | CE: 0.2879 | KD: 1060.7609\n",
      "Train Epoch: 005 Batch: 00046/00094 | Loss: 530.4865 | CE: 0.2649 | KD: 1060.4432\n",
      "Train Epoch: 005 Batch: 00047/00094 | Loss: 530.4936 | CE: 0.2548 | KD: 1060.4775\n",
      "Train Epoch: 005 Batch: 00048/00094 | Loss: 530.5138 | CE: 0.1959 | KD: 1060.6359\n",
      "Train Epoch: 005 Batch: 00049/00094 | Loss: 530.7176 | CE: 0.2732 | KD: 1060.8888\n",
      "Train Epoch: 005 Batch: 00050/00094 | Loss: 530.4796 | CE: 0.2451 | KD: 1060.4691\n",
      "Train Epoch: 005 Batch: 00051/00094 | Loss: 530.5944 | CE: 0.2087 | KD: 1060.7714\n",
      "Train Epoch: 005 Batch: 00052/00094 | Loss: 530.5208 | CE: 0.3104 | KD: 1060.4208\n",
      "Train Epoch: 005 Batch: 00053/00094 | Loss: 530.5317 | CE: 0.2370 | KD: 1060.5896\n",
      "Train Epoch: 005 Batch: 00054/00094 | Loss: 530.5811 | CE: 0.2178 | KD: 1060.7266\n",
      "Train Epoch: 005 Batch: 00055/00094 | Loss: 530.5855 | CE: 0.2071 | KD: 1060.7568\n",
      "Train Epoch: 005 Batch: 00056/00094 | Loss: 530.4250 | CE: 0.2681 | KD: 1060.3137\n",
      "Train Epoch: 005 Batch: 00057/00094 | Loss: 530.7891 | CE: 0.3824 | KD: 1060.8135\n",
      "Train Epoch: 005 Batch: 00058/00094 | Loss: 530.5508 | CE: 0.2358 | KD: 1060.6301\n",
      "Train Epoch: 005 Batch: 00059/00094 | Loss: 530.5369 | CE: 0.2024 | KD: 1060.6691\n",
      "Train Epoch: 005 Batch: 00060/00094 | Loss: 530.5941 | CE: 0.2881 | KD: 1060.6119\n",
      "Train Epoch: 005 Batch: 00061/00094 | Loss: 530.7713 | CE: 0.2723 | KD: 1060.9979\n",
      "Train Epoch: 005 Batch: 00062/00094 | Loss: 530.7905 | CE: 0.3195 | KD: 1060.9419\n",
      "Train Epoch: 005 Batch: 00063/00094 | Loss: 530.5921 | CE: 0.2522 | KD: 1060.6798\n",
      "Train Epoch: 005 Batch: 00064/00094 | Loss: 530.6688 | CE: 0.2744 | KD: 1060.7888\n",
      "Train Epoch: 005 Batch: 00065/00094 | Loss: 530.7423 | CE: 0.2937 | KD: 1060.8973\n",
      "Train Epoch: 005 Batch: 00066/00094 | Loss: 530.6325 | CE: 0.2120 | KD: 1060.8409\n",
      "Train Epoch: 005 Batch: 00067/00094 | Loss: 530.4843 | CE: 0.2184 | KD: 1060.5317\n",
      "Train Epoch: 005 Batch: 00068/00094 | Loss: 530.5251 | CE: 0.1846 | KD: 1060.6809\n",
      "Train Epoch: 005 Batch: 00069/00094 | Loss: 530.5424 | CE: 0.2691 | KD: 1060.5468\n",
      "Train Epoch: 005 Batch: 00070/00094 | Loss: 530.4951 | CE: 0.2588 | KD: 1060.4725\n",
      "Train Epoch: 005 Batch: 00071/00094 | Loss: 530.5548 | CE: 0.2097 | KD: 1060.6902\n",
      "Train Epoch: 005 Batch: 00072/00094 | Loss: 530.4926 | CE: 0.1710 | KD: 1060.6433\n",
      "Train Epoch: 005 Batch: 00073/00094 | Loss: 530.4997 | CE: 0.1874 | KD: 1060.6245\n",
      "Train Epoch: 005 Batch: 00074/00094 | Loss: 530.4752 | CE: 0.2062 | KD: 1060.5380\n",
      "Train Epoch: 005 Batch: 00075/00094 | Loss: 530.6981 | CE: 0.3065 | KD: 1060.7832\n",
      "Train Epoch: 005 Batch: 00076/00094 | Loss: 530.5312 | CE: 0.2461 | KD: 1060.5702\n",
      "Train Epoch: 005 Batch: 00077/00094 | Loss: 530.7048 | CE: 0.2540 | KD: 1060.9015\n",
      "Train Epoch: 005 Batch: 00078/00094 | Loss: 530.6490 | CE: 0.3411 | KD: 1060.6158\n",
      "Train Epoch: 005 Batch: 00079/00094 | Loss: 530.5712 | CE: 0.2237 | KD: 1060.6951\n",
      "Train Epoch: 005 Batch: 00080/00094 | Loss: 530.6589 | CE: 0.2907 | KD: 1060.7365\n",
      "Train Epoch: 005 Batch: 00081/00094 | Loss: 530.8315 | CE: 0.4070 | KD: 1060.8490\n",
      "Train Epoch: 005 Batch: 00082/00094 | Loss: 530.4222 | CE: 0.2110 | KD: 1060.4225\n",
      "Train Epoch: 005 Batch: 00083/00094 | Loss: 530.5844 | CE: 0.2395 | KD: 1060.6897\n",
      "Train Epoch: 005 Batch: 00084/00094 | Loss: 530.5643 | CE: 0.1766 | KD: 1060.7755\n",
      "Train Epoch: 005 Batch: 00085/00094 | Loss: 530.8055 | CE: 0.2263 | KD: 1061.1583\n",
      "Train Epoch: 005 Batch: 00086/00094 | Loss: 530.5854 | CE: 0.2964 | KD: 1060.5780\n",
      "Train Epoch: 005 Batch: 00087/00094 | Loss: 530.6161 | CE: 0.2589 | KD: 1060.7145\n",
      "Train Epoch: 005 Batch: 00088/00094 | Loss: 530.7874 | CE: 0.1834 | KD: 1061.2079\n",
      "Train Epoch: 005 Batch: 00089/00094 | Loss: 530.5917 | CE: 0.2167 | KD: 1060.7500\n",
      "Train Epoch: 005 Batch: 00090/00094 | Loss: 530.5072 | CE: 0.2320 | KD: 1060.5503\n",
      "Train Epoch: 005 Batch: 00091/00094 | Loss: 530.7444 | CE: 0.2727 | KD: 1060.9435\n",
      "Train Epoch: 005 Batch: 00092/00094 | Loss: 530.6918 | CE: 0.1820 | KD: 1061.0198\n",
      "Train Epoch: 005 Batch: 00093/00094 | Loss: 530.4400 | CE: 0.2455 | KD: 1060.3889\n",
      "Train Epoch: 005 Batch: 00094/00094 | Loss: 530.8032 | CE: 0.2535 | KD: 1061.0994\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2557 | acc:92.3500\n",
      "[VAL Acc] Target: 92.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5915 | acc:50.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0363 | acc:57.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 57.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0112 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2395 | acc:53.3307\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5967 | acc:68.2994\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9738 | acc:60.6975\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 60.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7772 | acc:59.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.13%\n",
      "[VAL Acc] Avg 61.32%\n",
      "Train Epoch: 006 Batch: 00001/00094 | Loss: 530.7941 | CE: 0.2703 | KD: 1061.0475\n",
      "Train Epoch: 006 Batch: 00002/00094 | Loss: 530.6572 | CE: 0.2574 | KD: 1060.7996\n",
      "Train Epoch: 006 Batch: 00003/00094 | Loss: 530.6499 | CE: 0.2944 | KD: 1060.7109\n",
      "Train Epoch: 006 Batch: 00004/00094 | Loss: 530.5135 | CE: 0.2614 | KD: 1060.5043\n",
      "Train Epoch: 006 Batch: 00005/00094 | Loss: 530.5999 | CE: 0.2803 | KD: 1060.6390\n",
      "Train Epoch: 006 Batch: 00006/00094 | Loss: 530.4540 | CE: 0.2420 | KD: 1060.4241\n",
      "Train Epoch: 006 Batch: 00007/00094 | Loss: 530.5152 | CE: 0.2035 | KD: 1060.6234\n",
      "Train Epoch: 006 Batch: 00008/00094 | Loss: 530.5708 | CE: 0.2272 | KD: 1060.6871\n",
      "Train Epoch: 006 Batch: 00009/00094 | Loss: 530.5883 | CE: 0.2651 | KD: 1060.6462\n",
      "Train Epoch: 006 Batch: 00010/00094 | Loss: 530.6262 | CE: 0.2925 | KD: 1060.6672\n",
      "Train Epoch: 006 Batch: 00011/00094 | Loss: 530.7090 | CE: 0.3414 | KD: 1060.7352\n",
      "Train Epoch: 006 Batch: 00012/00094 | Loss: 530.6155 | CE: 0.2420 | KD: 1060.7468\n",
      "Train Epoch: 006 Batch: 00013/00094 | Loss: 530.5018 | CE: 0.2362 | KD: 1060.5312\n",
      "Train Epoch: 006 Batch: 00014/00094 | Loss: 530.5790 | CE: 0.2365 | KD: 1060.6851\n",
      "Train Epoch: 006 Batch: 00015/00094 | Loss: 530.6069 | CE: 0.2861 | KD: 1060.6416\n",
      "Train Epoch: 006 Batch: 00016/00094 | Loss: 530.5118 | CE: 0.2966 | KD: 1060.4303\n",
      "Train Epoch: 006 Batch: 00017/00094 | Loss: 530.6282 | CE: 0.2595 | KD: 1060.7374\n",
      "Train Epoch: 006 Batch: 00018/00094 | Loss: 530.5101 | CE: 0.2213 | KD: 1060.5776\n",
      "Train Epoch: 006 Batch: 00019/00094 | Loss: 530.6009 | CE: 0.2756 | KD: 1060.6505\n",
      "Train Epoch: 006 Batch: 00020/00094 | Loss: 530.5176 | CE: 0.2246 | KD: 1060.5859\n",
      "Train Epoch: 006 Batch: 00021/00094 | Loss: 530.5580 | CE: 0.2737 | KD: 1060.5687\n",
      "Train Epoch: 006 Batch: 00022/00094 | Loss: 530.7413 | CE: 0.2421 | KD: 1060.9983\n",
      "Train Epoch: 006 Batch: 00023/00094 | Loss: 530.7622 | CE: 0.2622 | KD: 1061.0000\n",
      "Train Epoch: 006 Batch: 00024/00094 | Loss: 530.7131 | CE: 0.3484 | KD: 1060.7292\n",
      "Train Epoch: 006 Batch: 00025/00094 | Loss: 530.6919 | CE: 0.2631 | KD: 1060.8577\n",
      "Train Epoch: 006 Batch: 00026/00094 | Loss: 530.6970 | CE: 0.3316 | KD: 1060.7308\n",
      "Train Epoch: 006 Batch: 00027/00094 | Loss: 530.3918 | CE: 0.2167 | KD: 1060.3503\n",
      "Train Epoch: 006 Batch: 00028/00094 | Loss: 530.5713 | CE: 0.2505 | KD: 1060.6416\n",
      "Train Epoch: 006 Batch: 00029/00094 | Loss: 530.6041 | CE: 0.2779 | KD: 1060.6523\n",
      "Train Epoch: 006 Batch: 00030/00094 | Loss: 530.6085 | CE: 0.2151 | KD: 1060.7866\n",
      "Train Epoch: 006 Batch: 00031/00094 | Loss: 530.7264 | CE: 0.3514 | KD: 1060.7499\n",
      "Train Epoch: 006 Batch: 00032/00094 | Loss: 530.7646 | CE: 0.2826 | KD: 1060.9640\n",
      "Train Epoch: 006 Batch: 00033/00094 | Loss: 530.5378 | CE: 0.1877 | KD: 1060.7002\n",
      "Train Epoch: 006 Batch: 00034/00094 | Loss: 530.4768 | CE: 0.2439 | KD: 1060.4657\n",
      "Train Epoch: 006 Batch: 00035/00094 | Loss: 530.5806 | CE: 0.2336 | KD: 1060.6940\n",
      "Train Epoch: 006 Batch: 00036/00094 | Loss: 530.5024 | CE: 0.2442 | KD: 1060.5164\n",
      "Train Epoch: 006 Batch: 00037/00094 | Loss: 530.7484 | CE: 0.2865 | KD: 1060.9238\n",
      "Train Epoch: 006 Batch: 00038/00094 | Loss: 530.6280 | CE: 0.2488 | KD: 1060.7583\n",
      "Train Epoch: 006 Batch: 00039/00094 | Loss: 530.5388 | CE: 0.1460 | KD: 1060.7855\n",
      "Train Epoch: 006 Batch: 00040/00094 | Loss: 530.5712 | CE: 0.2256 | KD: 1060.6912\n",
      "Train Epoch: 006 Batch: 00041/00094 | Loss: 530.5784 | CE: 0.2165 | KD: 1060.7239\n",
      "Train Epoch: 006 Batch: 00042/00094 | Loss: 530.6999 | CE: 0.2282 | KD: 1060.9432\n",
      "Train Epoch: 006 Batch: 00043/00094 | Loss: 530.6240 | CE: 0.2412 | KD: 1060.7656\n",
      "Train Epoch: 006 Batch: 00044/00094 | Loss: 530.4821 | CE: 0.1768 | KD: 1060.6105\n",
      "Train Epoch: 006 Batch: 00045/00094 | Loss: 530.8383 | CE: 0.3038 | KD: 1061.0688\n",
      "Train Epoch: 006 Batch: 00046/00094 | Loss: 530.4776 | CE: 0.1815 | KD: 1060.5922\n",
      "Train Epoch: 006 Batch: 00047/00094 | Loss: 530.5737 | CE: 0.2796 | KD: 1060.5883\n",
      "Train Epoch: 006 Batch: 00048/00094 | Loss: 530.6191 | CE: 0.3061 | KD: 1060.6260\n",
      "Train Epoch: 006 Batch: 00049/00094 | Loss: 530.4268 | CE: 0.2423 | KD: 1060.3688\n",
      "Train Epoch: 006 Batch: 00050/00094 | Loss: 530.6511 | CE: 0.2141 | KD: 1060.8741\n",
      "Train Epoch: 006 Batch: 00051/00094 | Loss: 530.5529 | CE: 0.2987 | KD: 1060.5084\n",
      "Train Epoch: 006 Batch: 00052/00094 | Loss: 530.9186 | CE: 0.3218 | KD: 1061.1936\n",
      "Train Epoch: 006 Batch: 00053/00094 | Loss: 530.6406 | CE: 0.3487 | KD: 1060.5839\n",
      "Train Epoch: 006 Batch: 00054/00094 | Loss: 530.7325 | CE: 0.2878 | KD: 1060.8895\n",
      "Train Epoch: 006 Batch: 00055/00094 | Loss: 530.4450 | CE: 0.2117 | KD: 1060.4666\n",
      "Train Epoch: 006 Batch: 00056/00094 | Loss: 530.5600 | CE: 0.3080 | KD: 1060.5039\n",
      "Train Epoch: 006 Batch: 00057/00094 | Loss: 530.7824 | CE: 0.3351 | KD: 1060.8947\n",
      "Train Epoch: 006 Batch: 00058/00094 | Loss: 530.5400 | CE: 0.2144 | KD: 1060.6511\n",
      "Train Epoch: 006 Batch: 00059/00094 | Loss: 530.5505 | CE: 0.2680 | KD: 1060.5649\n",
      "Train Epoch: 006 Batch: 00060/00094 | Loss: 530.6436 | CE: 0.2891 | KD: 1060.7090\n",
      "Train Epoch: 006 Batch: 00061/00094 | Loss: 530.5550 | CE: 0.2237 | KD: 1060.6626\n",
      "Train Epoch: 006 Batch: 00062/00094 | Loss: 530.7271 | CE: 0.2720 | KD: 1060.9102\n",
      "Train Epoch: 006 Batch: 00063/00094 | Loss: 530.5311 | CE: 0.1964 | KD: 1060.6693\n",
      "Train Epoch: 006 Batch: 00064/00094 | Loss: 530.9442 | CE: 0.5074 | KD: 1060.8735\n",
      "Train Epoch: 006 Batch: 00065/00094 | Loss: 530.5414 | CE: 0.2381 | KD: 1060.6067\n",
      "Train Epoch: 006 Batch: 00066/00094 | Loss: 530.7144 | CE: 0.2777 | KD: 1060.8733\n",
      "Train Epoch: 006 Batch: 00067/00094 | Loss: 530.5680 | CE: 0.2428 | KD: 1060.6504\n",
      "Train Epoch: 006 Batch: 00068/00094 | Loss: 530.4852 | CE: 0.1880 | KD: 1060.5945\n",
      "Train Epoch: 006 Batch: 00069/00094 | Loss: 530.5929 | CE: 0.2800 | KD: 1060.6259\n",
      "Train Epoch: 006 Batch: 00070/00094 | Loss: 530.4720 | CE: 0.1990 | KD: 1060.5460\n",
      "Train Epoch: 006 Batch: 00071/00094 | Loss: 530.5819 | CE: 0.2261 | KD: 1060.7117\n",
      "Train Epoch: 006 Batch: 00072/00094 | Loss: 530.5800 | CE: 0.2347 | KD: 1060.6904\n",
      "Train Epoch: 006 Batch: 00073/00094 | Loss: 530.6799 | CE: 0.3642 | KD: 1060.6313\n",
      "Train Epoch: 006 Batch: 00074/00094 | Loss: 530.4321 | CE: 0.1775 | KD: 1060.5092\n",
      "Train Epoch: 006 Batch: 00075/00094 | Loss: 530.6202 | CE: 0.2564 | KD: 1060.7275\n",
      "Train Epoch: 006 Batch: 00076/00094 | Loss: 530.9030 | CE: 0.2833 | KD: 1061.2393\n",
      "Train Epoch: 006 Batch: 00077/00094 | Loss: 530.7785 | CE: 0.2561 | KD: 1061.0448\n",
      "Train Epoch: 006 Batch: 00078/00094 | Loss: 530.7797 | CE: 0.3031 | KD: 1060.9532\n",
      "Train Epoch: 006 Batch: 00079/00094 | Loss: 530.6753 | CE: 0.1794 | KD: 1060.9917\n",
      "Train Epoch: 006 Batch: 00080/00094 | Loss: 530.5703 | CE: 0.2386 | KD: 1060.6633\n",
      "Train Epoch: 006 Batch: 00081/00094 | Loss: 530.5654 | CE: 0.2056 | KD: 1060.7195\n",
      "Train Epoch: 006 Batch: 00082/00094 | Loss: 530.6652 | CE: 0.2729 | KD: 1060.7847\n",
      "Train Epoch: 006 Batch: 00083/00094 | Loss: 530.5723 | CE: 0.2156 | KD: 1060.7135\n",
      "Train Epoch: 006 Batch: 00084/00094 | Loss: 530.4985 | CE: 0.1740 | KD: 1060.6489\n",
      "Train Epoch: 006 Batch: 00085/00094 | Loss: 530.5361 | CE: 0.2437 | KD: 1060.5847\n",
      "Train Epoch: 006 Batch: 00086/00094 | Loss: 530.7236 | CE: 0.3959 | KD: 1060.6553\n",
      "Train Epoch: 006 Batch: 00087/00094 | Loss: 530.6179 | CE: 0.2403 | KD: 1060.7551\n",
      "Train Epoch: 006 Batch: 00088/00094 | Loss: 530.4611 | CE: 0.2702 | KD: 1060.3818\n",
      "Train Epoch: 006 Batch: 00089/00094 | Loss: 530.6039 | CE: 0.2570 | KD: 1060.6938\n",
      "Train Epoch: 006 Batch: 00090/00094 | Loss: 530.3737 | CE: 0.1717 | KD: 1060.4038\n",
      "Train Epoch: 006 Batch: 00091/00094 | Loss: 530.5011 | CE: 0.2222 | KD: 1060.5579\n",
      "Train Epoch: 006 Batch: 00092/00094 | Loss: 530.5237 | CE: 0.2511 | KD: 1060.5452\n",
      "Train Epoch: 006 Batch: 00093/00094 | Loss: 530.5116 | CE: 0.1722 | KD: 1060.6788\n",
      "Train Epoch: 006 Batch: 00094/00094 | Loss: 530.5628 | CE: 0.1969 | KD: 1060.7318\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2362 | acc:93.4500\n",
      "[VAL Acc] Target: 93.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7149 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0845 | acc:56.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0134 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3639 | acc:52.5862\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.59%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5906 | acc:67.9298\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 67.93%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0836 | acc:58.1113\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 58.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7586 | acc:60.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.81%\n",
      "[VAL Acc] Avg 61.14%\n",
      "Train Epoch: 007 Batch: 00001/00094 | Loss: 530.4880 | CE: 0.2123 | KD: 1060.5514\n",
      "Train Epoch: 007 Batch: 00002/00094 | Loss: 530.5724 | CE: 0.2459 | KD: 1060.6531\n",
      "Train Epoch: 007 Batch: 00003/00094 | Loss: 530.4790 | CE: 0.1858 | KD: 1060.5864\n",
      "Train Epoch: 007 Batch: 00004/00094 | Loss: 530.4117 | CE: 0.2094 | KD: 1060.4045\n",
      "Train Epoch: 007 Batch: 00005/00094 | Loss: 530.5461 | CE: 0.2691 | KD: 1060.5540\n",
      "Train Epoch: 007 Batch: 00006/00094 | Loss: 530.4930 | CE: 0.1841 | KD: 1060.6178\n",
      "Train Epoch: 007 Batch: 00007/00094 | Loss: 530.5305 | CE: 0.2315 | KD: 1060.5979\n",
      "Train Epoch: 007 Batch: 00008/00094 | Loss: 530.5543 | CE: 0.2045 | KD: 1060.6997\n",
      "Train Epoch: 007 Batch: 00009/00094 | Loss: 530.7260 | CE: 0.3741 | KD: 1060.7039\n",
      "Train Epoch: 007 Batch: 00010/00094 | Loss: 530.6450 | CE: 0.2159 | KD: 1060.8582\n",
      "Train Epoch: 007 Batch: 00011/00094 | Loss: 530.7236 | CE: 0.4026 | KD: 1060.6420\n",
      "Train Epoch: 007 Batch: 00012/00094 | Loss: 530.7088 | CE: 0.3254 | KD: 1060.7667\n",
      "Train Epoch: 007 Batch: 00013/00094 | Loss: 530.3997 | CE: 0.2144 | KD: 1060.3705\n",
      "Train Epoch: 007 Batch: 00014/00094 | Loss: 530.7785 | CE: 0.3154 | KD: 1060.9263\n",
      "Train Epoch: 007 Batch: 00015/00094 | Loss: 530.4944 | CE: 0.2394 | KD: 1060.5100\n",
      "Train Epoch: 007 Batch: 00016/00094 | Loss: 530.5793 | CE: 0.2357 | KD: 1060.6873\n",
      "Train Epoch: 007 Batch: 00017/00094 | Loss: 530.4708 | CE: 0.2280 | KD: 1060.4856\n",
      "Train Epoch: 007 Batch: 00018/00094 | Loss: 530.5095 | CE: 0.2318 | KD: 1060.5555\n",
      "Train Epoch: 007 Batch: 00019/00094 | Loss: 530.4764 | CE: 0.2376 | KD: 1060.4778\n",
      "Train Epoch: 007 Batch: 00020/00094 | Loss: 530.5066 | CE: 0.2038 | KD: 1060.6056\n",
      "Train Epoch: 007 Batch: 00021/00094 | Loss: 530.7630 | CE: 0.2152 | KD: 1061.0956\n",
      "Train Epoch: 007 Batch: 00022/00094 | Loss: 530.6330 | CE: 0.2206 | KD: 1060.8247\n",
      "Train Epoch: 007 Batch: 00023/00094 | Loss: 530.4335 | CE: 0.1771 | KD: 1060.5127\n",
      "Train Epoch: 007 Batch: 00024/00094 | Loss: 530.4716 | CE: 0.2021 | KD: 1060.5391\n",
      "Train Epoch: 007 Batch: 00025/00094 | Loss: 530.5910 | CE: 0.2950 | KD: 1060.5920\n",
      "Train Epoch: 007 Batch: 00026/00094 | Loss: 530.4582 | CE: 0.2347 | KD: 1060.4469\n",
      "Train Epoch: 007 Batch: 00027/00094 | Loss: 530.5043 | CE: 0.2144 | KD: 1060.5798\n",
      "Train Epoch: 007 Batch: 00028/00094 | Loss: 530.5262 | CE: 0.2460 | KD: 1060.5603\n",
      "Train Epoch: 007 Batch: 00029/00094 | Loss: 530.5071 | CE: 0.1686 | KD: 1060.6770\n",
      "Train Epoch: 007 Batch: 00030/00094 | Loss: 530.5460 | CE: 0.2704 | KD: 1060.5513\n",
      "Train Epoch: 007 Batch: 00031/00094 | Loss: 530.4708 | CE: 0.1824 | KD: 1060.5767\n",
      "Train Epoch: 007 Batch: 00032/00094 | Loss: 530.9089 | CE: 0.2773 | KD: 1061.2632\n",
      "Train Epoch: 007 Batch: 00033/00094 | Loss: 530.6525 | CE: 0.3146 | KD: 1060.6758\n",
      "Train Epoch: 007 Batch: 00034/00094 | Loss: 530.5864 | CE: 0.1873 | KD: 1060.7982\n",
      "Train Epoch: 007 Batch: 00035/00094 | Loss: 530.5031 | CE: 0.2908 | KD: 1060.4247\n",
      "Train Epoch: 007 Batch: 00036/00094 | Loss: 530.5988 | CE: 0.1980 | KD: 1060.8016\n",
      "Train Epoch: 007 Batch: 00037/00094 | Loss: 530.7360 | CE: 0.2825 | KD: 1060.9070\n",
      "Train Epoch: 007 Batch: 00038/00094 | Loss: 530.4484 | CE: 0.2261 | KD: 1060.4447\n",
      "Train Epoch: 007 Batch: 00039/00094 | Loss: 531.3410 | CE: 0.7765 | KD: 1061.1292\n",
      "Train Epoch: 007 Batch: 00040/00094 | Loss: 530.8154 | CE: 0.2526 | KD: 1061.1256\n",
      "Train Epoch: 007 Batch: 00041/00094 | Loss: 530.5623 | CE: 0.2364 | KD: 1060.6519\n",
      "Train Epoch: 007 Batch: 00042/00094 | Loss: 530.5573 | CE: 0.2193 | KD: 1060.6759\n",
      "Train Epoch: 007 Batch: 00043/00094 | Loss: 530.4557 | CE: 0.2371 | KD: 1060.4371\n",
      "Train Epoch: 007 Batch: 00044/00094 | Loss: 530.5625 | CE: 0.1997 | KD: 1060.7257\n",
      "Train Epoch: 007 Batch: 00045/00094 | Loss: 530.5153 | CE: 0.2230 | KD: 1060.5846\n",
      "Train Epoch: 007 Batch: 00046/00094 | Loss: 530.7606 | CE: 0.3177 | KD: 1060.8856\n",
      "Train Epoch: 007 Batch: 00047/00094 | Loss: 530.5917 | CE: 0.2623 | KD: 1060.6587\n",
      "Train Epoch: 007 Batch: 00048/00094 | Loss: 530.7358 | CE: 0.3045 | KD: 1060.8627\n",
      "Train Epoch: 007 Batch: 00049/00094 | Loss: 530.5599 | CE: 0.2072 | KD: 1060.7056\n",
      "Train Epoch: 007 Batch: 00050/00094 | Loss: 530.6260 | CE: 0.2340 | KD: 1060.7839\n",
      "Train Epoch: 007 Batch: 00051/00094 | Loss: 530.7249 | CE: 0.2849 | KD: 1060.8799\n",
      "Train Epoch: 007 Batch: 00052/00094 | Loss: 530.6655 | CE: 0.2228 | KD: 1060.8854\n",
      "Train Epoch: 007 Batch: 00053/00094 | Loss: 530.4116 | CE: 0.1823 | KD: 1060.4585\n",
      "Train Epoch: 007 Batch: 00054/00094 | Loss: 530.6924 | CE: 0.2866 | KD: 1060.8116\n",
      "Train Epoch: 007 Batch: 00055/00094 | Loss: 530.6635 | CE: 0.2260 | KD: 1060.8750\n",
      "Train Epoch: 007 Batch: 00056/00094 | Loss: 530.5609 | CE: 0.2158 | KD: 1060.6902\n",
      "Train Epoch: 007 Batch: 00057/00094 | Loss: 530.5672 | CE: 0.2607 | KD: 1060.6129\n",
      "Train Epoch: 007 Batch: 00058/00094 | Loss: 530.8849 | CE: 0.4372 | KD: 1060.8954\n",
      "Train Epoch: 007 Batch: 00059/00094 | Loss: 530.5275 | CE: 0.2402 | KD: 1060.5747\n",
      "Train Epoch: 007 Batch: 00060/00094 | Loss: 530.4213 | CE: 0.1494 | KD: 1060.5437\n",
      "Train Epoch: 007 Batch: 00061/00094 | Loss: 530.5894 | CE: 0.2490 | KD: 1060.6809\n",
      "Train Epoch: 007 Batch: 00062/00094 | Loss: 530.6407 | CE: 0.2988 | KD: 1060.6838\n",
      "Train Epoch: 007 Batch: 00063/00094 | Loss: 530.4509 | CE: 0.1780 | KD: 1060.5458\n",
      "Train Epoch: 007 Batch: 00064/00094 | Loss: 530.4927 | CE: 0.2067 | KD: 1060.5721\n",
      "Train Epoch: 007 Batch: 00065/00094 | Loss: 530.5366 | CE: 0.2427 | KD: 1060.5876\n",
      "Train Epoch: 007 Batch: 00066/00094 | Loss: 530.5146 | CE: 0.1698 | KD: 1060.6897\n",
      "Train Epoch: 007 Batch: 00067/00094 | Loss: 530.4854 | CE: 0.2344 | KD: 1060.5020\n",
      "Train Epoch: 007 Batch: 00068/00094 | Loss: 530.3859 | CE: 0.2374 | KD: 1060.2971\n",
      "Train Epoch: 007 Batch: 00069/00094 | Loss: 530.5456 | CE: 0.2713 | KD: 1060.5487\n",
      "Train Epoch: 007 Batch: 00070/00094 | Loss: 530.6368 | CE: 0.2592 | KD: 1060.7551\n",
      "Train Epoch: 007 Batch: 00071/00094 | Loss: 530.5438 | CE: 0.2280 | KD: 1060.6316\n",
      "Train Epoch: 007 Batch: 00072/00094 | Loss: 530.7150 | CE: 0.3224 | KD: 1060.7853\n",
      "Train Epoch: 007 Batch: 00073/00094 | Loss: 530.5970 | CE: 0.2679 | KD: 1060.6583\n",
      "Train Epoch: 007 Batch: 00074/00094 | Loss: 530.7244 | CE: 0.2095 | KD: 1061.0299\n",
      "Train Epoch: 007 Batch: 00075/00094 | Loss: 530.8074 | CE: 0.2711 | KD: 1061.0725\n",
      "Train Epoch: 007 Batch: 00076/00094 | Loss: 530.8501 | CE: 0.3269 | KD: 1061.0465\n",
      "Train Epoch: 007 Batch: 00077/00094 | Loss: 530.7092 | CE: 0.2692 | KD: 1060.8799\n",
      "Train Epoch: 007 Batch: 00078/00094 | Loss: 530.4862 | CE: 0.2578 | KD: 1060.4568\n",
      "Train Epoch: 007 Batch: 00079/00094 | Loss: 530.4993 | CE: 0.2302 | KD: 1060.5382\n",
      "Train Epoch: 007 Batch: 00080/00094 | Loss: 530.7426 | CE: 0.2591 | KD: 1060.9672\n",
      "Train Epoch: 007 Batch: 00081/00094 | Loss: 530.5281 | CE: 0.1884 | KD: 1060.6794\n",
      "Train Epoch: 007 Batch: 00082/00094 | Loss: 530.6056 | CE: 0.2606 | KD: 1060.6901\n",
      "Train Epoch: 007 Batch: 00083/00094 | Loss: 530.6624 | CE: 0.3091 | KD: 1060.7065\n",
      "Train Epoch: 007 Batch: 00084/00094 | Loss: 530.7067 | CE: 0.2675 | KD: 1060.8785\n",
      "Train Epoch: 007 Batch: 00085/00094 | Loss: 530.3903 | CE: 0.1997 | KD: 1060.3812\n",
      "Train Epoch: 007 Batch: 00086/00094 | Loss: 530.5981 | CE: 0.2837 | KD: 1060.6288\n",
      "Train Epoch: 007 Batch: 00087/00094 | Loss: 530.5566 | CE: 0.2103 | KD: 1060.6926\n",
      "Train Epoch: 007 Batch: 00088/00094 | Loss: 530.6032 | CE: 0.2654 | KD: 1060.6757\n",
      "Train Epoch: 007 Batch: 00089/00094 | Loss: 530.4471 | CE: 0.2039 | KD: 1060.4865\n",
      "Train Epoch: 007 Batch: 00090/00094 | Loss: 530.3316 | CE: 0.2006 | KD: 1060.2620\n",
      "Train Epoch: 007 Batch: 00091/00094 | Loss: 530.5251 | CE: 0.2078 | KD: 1060.6346\n",
      "Train Epoch: 007 Batch: 00092/00094 | Loss: 530.4247 | CE: 0.2435 | KD: 1060.3624\n",
      "Train Epoch: 007 Batch: 00093/00094 | Loss: 530.4968 | CE: 0.2403 | KD: 1060.5129\n",
      "Train Epoch: 007 Batch: 00094/00094 | Loss: 530.8133 | CE: 0.2207 | KD: 1061.1853\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2431 | acc:93.7000\n",
      "[VAL Acc] Target: 93.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6956 | acc:50.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0703 | acc:59.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0512 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2994 | acc:53.9185\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5870 | acc:69.2237\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0496 | acc:59.7571\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 59.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7481 | acc:60.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.81%\n",
      "[VAL Acc] Avg 62.08%\n",
      "Train Epoch: 008 Batch: 00001/00094 | Loss: 530.6935 | CE: 0.3500 | KD: 1060.6869\n",
      "Train Epoch: 008 Batch: 00002/00094 | Loss: 530.4070 | CE: 0.1898 | KD: 1060.4343\n",
      "Train Epoch: 008 Batch: 00003/00094 | Loss: 530.3833 | CE: 0.1659 | KD: 1060.4349\n",
      "Train Epoch: 008 Batch: 00004/00094 | Loss: 530.7411 | CE: 0.2853 | KD: 1060.9116\n",
      "Train Epoch: 008 Batch: 00005/00094 | Loss: 530.4865 | CE: 0.2154 | KD: 1060.5422\n",
      "Train Epoch: 008 Batch: 00006/00094 | Loss: 530.6317 | CE: 0.2010 | KD: 1060.8613\n",
      "Train Epoch: 008 Batch: 00007/00094 | Loss: 530.5876 | CE: 0.2334 | KD: 1060.7084\n",
      "Train Epoch: 008 Batch: 00008/00094 | Loss: 530.7828 | CE: 0.3271 | KD: 1060.9114\n",
      "Train Epoch: 008 Batch: 00009/00094 | Loss: 530.7106 | CE: 0.2712 | KD: 1060.8788\n",
      "Train Epoch: 008 Batch: 00010/00094 | Loss: 530.5720 | CE: 0.2033 | KD: 1060.7373\n",
      "Train Epoch: 008 Batch: 00011/00094 | Loss: 530.4863 | CE: 0.1732 | KD: 1060.6261\n",
      "Train Epoch: 008 Batch: 00012/00094 | Loss: 530.4581 | CE: 0.1911 | KD: 1060.5338\n",
      "Train Epoch: 008 Batch: 00013/00094 | Loss: 530.5824 | CE: 0.2127 | KD: 1060.7394\n",
      "Train Epoch: 008 Batch: 00014/00094 | Loss: 530.4675 | CE: 0.1728 | KD: 1060.5894\n",
      "Train Epoch: 008 Batch: 00015/00094 | Loss: 530.4746 | CE: 0.1638 | KD: 1060.6217\n",
      "Train Epoch: 008 Batch: 00016/00094 | Loss: 530.8599 | CE: 0.4601 | KD: 1060.7997\n",
      "Train Epoch: 008 Batch: 00017/00094 | Loss: 530.4384 | CE: 0.2004 | KD: 1060.4760\n",
      "Train Epoch: 008 Batch: 00018/00094 | Loss: 530.6478 | CE: 0.3023 | KD: 1060.6909\n",
      "Train Epoch: 008 Batch: 00019/00094 | Loss: 530.4458 | CE: 0.2481 | KD: 1060.3954\n",
      "Train Epoch: 008 Batch: 00020/00094 | Loss: 530.3550 | CE: 0.1859 | KD: 1060.3383\n",
      "Train Epoch: 008 Batch: 00021/00094 | Loss: 530.5766 | CE: 0.2272 | KD: 1060.6989\n",
      "Train Epoch: 008 Batch: 00022/00094 | Loss: 530.5289 | CE: 0.2756 | KD: 1060.5066\n",
      "Train Epoch: 008 Batch: 00023/00094 | Loss: 530.8531 | CE: 0.3038 | KD: 1061.0985\n",
      "Train Epoch: 008 Batch: 00024/00094 | Loss: 530.6415 | CE: 0.3064 | KD: 1060.6702\n",
      "Train Epoch: 008 Batch: 00025/00094 | Loss: 530.4327 | CE: 0.2509 | KD: 1060.3635\n",
      "Train Epoch: 008 Batch: 00026/00094 | Loss: 530.5162 | CE: 0.1614 | KD: 1060.7096\n",
      "Train Epoch: 008 Batch: 00027/00094 | Loss: 530.6478 | CE: 0.2797 | KD: 1060.7361\n",
      "Train Epoch: 008 Batch: 00028/00094 | Loss: 530.5308 | CE: 0.2374 | KD: 1060.5868\n",
      "Train Epoch: 008 Batch: 00029/00094 | Loss: 530.5500 | CE: 0.1755 | KD: 1060.7490\n",
      "Train Epoch: 008 Batch: 00030/00094 | Loss: 530.6844 | CE: 0.2823 | KD: 1060.8042\n",
      "Train Epoch: 008 Batch: 00031/00094 | Loss: 530.7209 | CE: 0.2926 | KD: 1060.8566\n",
      "Train Epoch: 008 Batch: 00032/00094 | Loss: 530.5517 | CE: 0.2554 | KD: 1060.5925\n",
      "Train Epoch: 008 Batch: 00033/00094 | Loss: 530.6421 | CE: 0.2152 | KD: 1060.8538\n",
      "Train Epoch: 008 Batch: 00034/00094 | Loss: 530.6514 | CE: 0.3015 | KD: 1060.6998\n",
      "Train Epoch: 008 Batch: 00035/00094 | Loss: 530.4373 | CE: 0.2238 | KD: 1060.4269\n",
      "Train Epoch: 008 Batch: 00036/00094 | Loss: 530.5972 | CE: 0.2448 | KD: 1060.7047\n",
      "Train Epoch: 008 Batch: 00037/00094 | Loss: 530.7608 | CE: 0.3761 | KD: 1060.7694\n",
      "Train Epoch: 008 Batch: 00038/00094 | Loss: 530.4930 | CE: 0.2203 | KD: 1060.5454\n",
      "Train Epoch: 008 Batch: 00039/00094 | Loss: 530.6569 | CE: 0.2344 | KD: 1060.8450\n",
      "Train Epoch: 008 Batch: 00040/00094 | Loss: 530.4507 | CE: 0.1944 | KD: 1060.5126\n",
      "Train Epoch: 008 Batch: 00041/00094 | Loss: 530.5689 | CE: 0.2009 | KD: 1060.7361\n",
      "Train Epoch: 008 Batch: 00042/00094 | Loss: 530.5392 | CE: 0.2452 | KD: 1060.5879\n",
      "Train Epoch: 008 Batch: 00043/00094 | Loss: 530.6866 | CE: 0.3571 | KD: 1060.6591\n",
      "Train Epoch: 008 Batch: 00044/00094 | Loss: 530.4234 | CE: 0.1443 | KD: 1060.5582\n",
      "Train Epoch: 008 Batch: 00045/00094 | Loss: 530.5002 | CE: 0.2500 | KD: 1060.5005\n",
      "Train Epoch: 008 Batch: 00046/00094 | Loss: 530.4582 | CE: 0.2014 | KD: 1060.5135\n",
      "Train Epoch: 008 Batch: 00047/00094 | Loss: 530.7380 | CE: 0.3136 | KD: 1060.8489\n",
      "Train Epoch: 008 Batch: 00048/00094 | Loss: 530.6542 | CE: 0.2799 | KD: 1060.7488\n",
      "Train Epoch: 008 Batch: 00049/00094 | Loss: 530.5971 | CE: 0.2331 | KD: 1060.7281\n",
      "Train Epoch: 008 Batch: 00050/00094 | Loss: 530.6144 | CE: 0.1995 | KD: 1060.8297\n",
      "Train Epoch: 008 Batch: 00051/00094 | Loss: 530.5944 | CE: 0.2400 | KD: 1060.7087\n",
      "Train Epoch: 008 Batch: 00052/00094 | Loss: 530.6091 | CE: 0.1991 | KD: 1060.8202\n",
      "Train Epoch: 008 Batch: 00053/00094 | Loss: 530.6238 | CE: 0.1555 | KD: 1060.9365\n",
      "Train Epoch: 008 Batch: 00054/00094 | Loss: 530.5161 | CE: 0.2277 | KD: 1060.5767\n",
      "Train Epoch: 008 Batch: 00055/00094 | Loss: 530.5778 | CE: 0.2874 | KD: 1060.5809\n",
      "Train Epoch: 008 Batch: 00056/00094 | Loss: 530.4696 | CE: 0.2497 | KD: 1060.4398\n",
      "Train Epoch: 008 Batch: 00057/00094 | Loss: 530.6922 | CE: 0.2549 | KD: 1060.8745\n",
      "Train Epoch: 008 Batch: 00058/00094 | Loss: 530.5584 | CE: 0.1975 | KD: 1060.7217\n",
      "Train Epoch: 008 Batch: 00059/00094 | Loss: 530.4764 | CE: 0.2028 | KD: 1060.5471\n",
      "Train Epoch: 008 Batch: 00060/00094 | Loss: 530.5417 | CE: 0.2249 | KD: 1060.6337\n",
      "Train Epoch: 008 Batch: 00061/00094 | Loss: 530.5750 | CE: 0.1969 | KD: 1060.7561\n",
      "Train Epoch: 008 Batch: 00062/00094 | Loss: 530.6554 | CE: 0.2043 | KD: 1060.9022\n",
      "Train Epoch: 008 Batch: 00063/00094 | Loss: 530.7038 | CE: 0.2966 | KD: 1060.8143\n",
      "Train Epoch: 008 Batch: 00064/00094 | Loss: 530.5589 | CE: 0.3303 | KD: 1060.4572\n",
      "Train Epoch: 008 Batch: 00065/00094 | Loss: 530.5717 | CE: 0.1560 | KD: 1060.8314\n",
      "Train Epoch: 008 Batch: 00066/00094 | Loss: 530.6195 | CE: 0.2186 | KD: 1060.8019\n",
      "Train Epoch: 008 Batch: 00067/00094 | Loss: 530.7331 | CE: 0.4355 | KD: 1060.5952\n",
      "Train Epoch: 008 Batch: 00068/00094 | Loss: 530.6461 | CE: 0.2344 | KD: 1060.8234\n",
      "Train Epoch: 008 Batch: 00069/00094 | Loss: 530.4438 | CE: 0.1757 | KD: 1060.5361\n",
      "Train Epoch: 008 Batch: 00070/00094 | Loss: 530.5092 | CE: 0.2108 | KD: 1060.5968\n",
      "Train Epoch: 008 Batch: 00071/00094 | Loss: 530.7604 | CE: 0.2978 | KD: 1060.9252\n",
      "Train Epoch: 008 Batch: 00072/00094 | Loss: 530.5847 | CE: 0.2756 | KD: 1060.6183\n",
      "Train Epoch: 008 Batch: 00073/00094 | Loss: 530.7046 | CE: 0.2852 | KD: 1060.8389\n",
      "Train Epoch: 008 Batch: 00074/00094 | Loss: 530.4620 | CE: 0.2549 | KD: 1060.4143\n",
      "Train Epoch: 008 Batch: 00075/00094 | Loss: 530.6866 | CE: 0.2606 | KD: 1060.8521\n",
      "Train Epoch: 008 Batch: 00076/00094 | Loss: 530.5913 | CE: 0.2271 | KD: 1060.7285\n",
      "Train Epoch: 008 Batch: 00077/00094 | Loss: 530.5705 | CE: 0.2538 | KD: 1060.6333\n",
      "Train Epoch: 008 Batch: 00078/00094 | Loss: 530.4697 | CE: 0.1622 | KD: 1060.6149\n",
      "Train Epoch: 008 Batch: 00079/00094 | Loss: 530.4747 | CE: 0.1909 | KD: 1060.5676\n",
      "Train Epoch: 008 Batch: 00080/00094 | Loss: 530.6251 | CE: 0.1670 | KD: 1060.9160\n",
      "Train Epoch: 008 Batch: 00081/00094 | Loss: 530.8878 | CE: 0.3475 | KD: 1061.0807\n",
      "Train Epoch: 008 Batch: 00082/00094 | Loss: 530.6271 | CE: 0.2259 | KD: 1060.8024\n",
      "Train Epoch: 008 Batch: 00083/00094 | Loss: 530.5308 | CE: 0.1864 | KD: 1060.6887\n",
      "Train Epoch: 008 Batch: 00084/00094 | Loss: 530.8801 | CE: 0.3095 | KD: 1061.1411\n",
      "Train Epoch: 008 Batch: 00085/00094 | Loss: 530.5745 | CE: 0.2566 | KD: 1060.6359\n",
      "Train Epoch: 008 Batch: 00086/00094 | Loss: 530.5710 | CE: 0.2297 | KD: 1060.6827\n",
      "Train Epoch: 008 Batch: 00087/00094 | Loss: 530.6196 | CE: 0.2192 | KD: 1060.8008\n",
      "Train Epoch: 008 Batch: 00088/00094 | Loss: 530.4501 | CE: 0.2198 | KD: 1060.4607\n",
      "Train Epoch: 008 Batch: 00089/00094 | Loss: 530.5811 | CE: 0.3391 | KD: 1060.4839\n",
      "Train Epoch: 008 Batch: 00090/00094 | Loss: 530.3350 | CE: 0.1837 | KD: 1060.3026\n",
      "Train Epoch: 008 Batch: 00091/00094 | Loss: 530.4092 | CE: 0.2064 | KD: 1060.4056\n",
      "Train Epoch: 008 Batch: 00092/00094 | Loss: 530.4932 | CE: 0.2103 | KD: 1060.5658\n",
      "Train Epoch: 008 Batch: 00093/00094 | Loss: 530.4720 | CE: 0.1833 | KD: 1060.5775\n",
      "Train Epoch: 008 Batch: 00094/00094 | Loss: 530.5634 | CE: 0.2316 | KD: 1060.6637\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2270 | acc:93.9500\n",
      "[VAL Acc] Target: 93.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6118 | acc:50.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0590 | acc:57.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 57.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0140 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2271 | acc:54.8589\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5560 | acc:71.1645\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9428 | acc:63.3229\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.32%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7257 | acc:62.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.12%\n",
      "[VAL Acc] Avg 62.63%\n",
      "Train Epoch: 009 Batch: 00001/00094 | Loss: 477.5905 | CE: 0.2533 | KD: 1060.7494\n",
      "Train Epoch: 009 Batch: 00002/00094 | Loss: 477.6336 | CE: 0.2269 | KD: 1060.9038\n",
      "Train Epoch: 009 Batch: 00003/00094 | Loss: 477.5414 | CE: 0.3352 | KD: 1060.4584\n",
      "Train Epoch: 009 Batch: 00004/00094 | Loss: 477.5816 | CE: 0.2591 | KD: 1060.7167\n",
      "Train Epoch: 009 Batch: 00005/00094 | Loss: 477.5494 | CE: 0.2399 | KD: 1060.6879\n",
      "Train Epoch: 009 Batch: 00006/00094 | Loss: 477.6690 | CE: 0.2701 | KD: 1060.8864\n",
      "Train Epoch: 009 Batch: 00007/00094 | Loss: 477.5586 | CE: 0.3284 | KD: 1060.5116\n",
      "Train Epoch: 009 Batch: 00008/00094 | Loss: 477.6608 | CE: 0.2095 | KD: 1061.0028\n",
      "Train Epoch: 009 Batch: 00009/00094 | Loss: 477.4333 | CE: 0.1878 | KD: 1060.5454\n",
      "Train Epoch: 009 Batch: 00010/00094 | Loss: 477.5868 | CE: 0.2542 | KD: 1060.7391\n",
      "Train Epoch: 009 Batch: 00011/00094 | Loss: 477.5237 | CE: 0.1913 | KD: 1060.7386\n",
      "Train Epoch: 009 Batch: 00012/00094 | Loss: 477.3898 | CE: 0.2538 | KD: 1060.3022\n",
      "Train Epoch: 009 Batch: 00013/00094 | Loss: 477.3716 | CE: 0.1622 | KD: 1060.4652\n",
      "Train Epoch: 009 Batch: 00014/00094 | Loss: 477.6514 | CE: 0.2271 | KD: 1060.9431\n",
      "Train Epoch: 009 Batch: 00015/00094 | Loss: 477.4325 | CE: 0.1759 | KD: 1060.5703\n",
      "Train Epoch: 009 Batch: 00016/00094 | Loss: 477.4707 | CE: 0.2270 | KD: 1060.5417\n",
      "Train Epoch: 009 Batch: 00017/00094 | Loss: 477.5312 | CE: 0.2497 | KD: 1060.6256\n",
      "Train Epoch: 009 Batch: 00018/00094 | Loss: 477.6279 | CE: 0.2618 | KD: 1060.8137\n",
      "Train Epoch: 009 Batch: 00019/00094 | Loss: 477.7033 | CE: 0.4011 | KD: 1060.6715\n",
      "Train Epoch: 009 Batch: 00020/00094 | Loss: 477.4947 | CE: 0.1668 | KD: 1060.7286\n",
      "Train Epoch: 009 Batch: 00021/00094 | Loss: 477.4907 | CE: 0.2345 | KD: 1060.5692\n",
      "Train Epoch: 009 Batch: 00022/00094 | Loss: 477.4541 | CE: 0.2017 | KD: 1060.5608\n",
      "Train Epoch: 009 Batch: 00023/00094 | Loss: 477.6592 | CE: 0.3123 | KD: 1060.7708\n",
      "Train Epoch: 009 Batch: 00024/00094 | Loss: 477.5974 | CE: 0.2228 | KD: 1060.8324\n",
      "Train Epoch: 009 Batch: 00025/00094 | Loss: 477.6032 | CE: 0.2732 | KD: 1060.7333\n",
      "Train Epoch: 009 Batch: 00026/00094 | Loss: 477.5676 | CE: 0.2585 | KD: 1060.6869\n",
      "Train Epoch: 009 Batch: 00027/00094 | Loss: 477.4483 | CE: 0.2288 | KD: 1060.4879\n",
      "Train Epoch: 009 Batch: 00028/00094 | Loss: 477.6049 | CE: 0.2773 | KD: 1060.7279\n",
      "Train Epoch: 009 Batch: 00029/00094 | Loss: 477.8259 | CE: 0.3845 | KD: 1060.9811\n",
      "Train Epoch: 009 Batch: 00030/00094 | Loss: 477.6740 | CE: 0.2046 | KD: 1061.0430\n",
      "Train Epoch: 009 Batch: 00031/00094 | Loss: 477.5383 | CE: 0.2589 | KD: 1060.6207\n",
      "Train Epoch: 009 Batch: 00032/00094 | Loss: 477.4109 | CE: 0.2112 | KD: 1060.4436\n",
      "Train Epoch: 009 Batch: 00033/00094 | Loss: 477.5827 | CE: 0.2055 | KD: 1060.8384\n",
      "Train Epoch: 009 Batch: 00034/00094 | Loss: 477.6112 | CE: 0.2496 | KD: 1060.8037\n",
      "Train Epoch: 009 Batch: 00035/00094 | Loss: 477.5702 | CE: 0.1861 | KD: 1060.8535\n",
      "Train Epoch: 009 Batch: 00036/00094 | Loss: 477.4779 | CE: 0.2053 | KD: 1060.6058\n",
      "Train Epoch: 009 Batch: 00037/00094 | Loss: 477.6754 | CE: 0.3082 | KD: 1060.8160\n",
      "Train Epoch: 009 Batch: 00038/00094 | Loss: 477.3438 | CE: 0.1820 | KD: 1060.3596\n",
      "Train Epoch: 009 Batch: 00039/00094 | Loss: 477.5995 | CE: 0.2871 | KD: 1060.6945\n",
      "Train Epoch: 009 Batch: 00040/00094 | Loss: 477.7955 | CE: 0.4720 | KD: 1060.7191\n",
      "Train Epoch: 009 Batch: 00041/00094 | Loss: 477.6445 | CE: 0.2690 | KD: 1060.8346\n",
      "Train Epoch: 009 Batch: 00042/00094 | Loss: 477.5168 | CE: 0.2202 | KD: 1060.6589\n",
      "Train Epoch: 009 Batch: 00043/00094 | Loss: 477.5462 | CE: 0.2478 | KD: 1060.6632\n",
      "Train Epoch: 009 Batch: 00044/00094 | Loss: 477.4648 | CE: 0.2029 | KD: 1060.5819\n",
      "Train Epoch: 009 Batch: 00045/00094 | Loss: 477.5735 | CE: 0.3194 | KD: 1060.5646\n",
      "Train Epoch: 009 Batch: 00046/00094 | Loss: 477.6099 | CE: 0.2291 | KD: 1060.8463\n",
      "Train Epoch: 009 Batch: 00047/00094 | Loss: 477.5222 | CE: 0.2173 | KD: 1060.6775\n",
      "Train Epoch: 009 Batch: 00048/00094 | Loss: 477.5769 | CE: 0.3004 | KD: 1060.6145\n",
      "Train Epoch: 009 Batch: 00049/00094 | Loss: 477.5899 | CE: 0.3825 | KD: 1060.4608\n",
      "Train Epoch: 009 Batch: 00050/00094 | Loss: 477.3901 | CE: 0.2007 | KD: 1060.4209\n",
      "Train Epoch: 009 Batch: 00051/00094 | Loss: 477.5371 | CE: 0.2284 | KD: 1060.6862\n",
      "Train Epoch: 009 Batch: 00052/00094 | Loss: 477.5304 | CE: 0.2230 | KD: 1060.6830\n",
      "Train Epoch: 009 Batch: 00053/00094 | Loss: 477.6943 | CE: 0.3477 | KD: 1060.7703\n",
      "Train Epoch: 009 Batch: 00054/00094 | Loss: 477.5388 | CE: 0.2177 | KD: 1060.7134\n",
      "Train Epoch: 009 Batch: 00055/00094 | Loss: 477.5229 | CE: 0.2094 | KD: 1060.6967\n",
      "Train Epoch: 009 Batch: 00056/00094 | Loss: 477.5530 | CE: 0.1951 | KD: 1060.7953\n",
      "Train Epoch: 009 Batch: 00057/00094 | Loss: 477.6931 | CE: 0.3078 | KD: 1060.8564\n",
      "Train Epoch: 009 Batch: 00058/00094 | Loss: 477.4756 | CE: 0.1846 | KD: 1060.6466\n",
      "Train Epoch: 009 Batch: 00059/00094 | Loss: 477.6393 | CE: 0.2290 | KD: 1060.9119\n",
      "Train Epoch: 009 Batch: 00060/00094 | Loss: 477.6198 | CE: 0.2171 | KD: 1060.8949\n",
      "Train Epoch: 009 Batch: 00061/00094 | Loss: 477.4210 | CE: 0.1879 | KD: 1060.5179\n",
      "Train Epoch: 009 Batch: 00062/00094 | Loss: 477.4738 | CE: 0.2434 | KD: 1060.5121\n",
      "Train Epoch: 009 Batch: 00063/00094 | Loss: 477.7789 | CE: 0.2356 | KD: 1061.2074\n",
      "Train Epoch: 009 Batch: 00064/00094 | Loss: 477.5452 | CE: 0.2596 | KD: 1060.6346\n",
      "Train Epoch: 009 Batch: 00065/00094 | Loss: 477.5557 | CE: 0.3037 | KD: 1060.5602\n",
      "Train Epoch: 009 Batch: 00066/00094 | Loss: 477.6212 | CE: 0.2275 | KD: 1060.8750\n",
      "Train Epoch: 009 Batch: 00067/00094 | Loss: 477.4603 | CE: 0.2338 | KD: 1060.5033\n",
      "Train Epoch: 009 Batch: 00068/00094 | Loss: 477.4184 | CE: 0.1956 | KD: 1060.4952\n",
      "Train Epoch: 009 Batch: 00069/00094 | Loss: 477.5083 | CE: 0.2682 | KD: 1060.5336\n",
      "Train Epoch: 009 Batch: 00070/00094 | Loss: 477.4218 | CE: 0.2298 | KD: 1060.4266\n",
      "Train Epoch: 009 Batch: 00071/00094 | Loss: 477.5816 | CE: 0.2125 | KD: 1060.8203\n",
      "Train Epoch: 009 Batch: 00072/00094 | Loss: 477.6932 | CE: 0.3026 | KD: 1060.8680\n",
      "Train Epoch: 009 Batch: 00073/00094 | Loss: 477.7652 | CE: 0.3238 | KD: 1060.9808\n",
      "Train Epoch: 009 Batch: 00074/00094 | Loss: 477.6378 | CE: 0.2265 | KD: 1060.9139\n",
      "Train Epoch: 009 Batch: 00075/00094 | Loss: 477.5259 | CE: 0.2593 | KD: 1060.5924\n",
      "Train Epoch: 009 Batch: 00076/00094 | Loss: 477.6473 | CE: 0.2923 | KD: 1060.7889\n",
      "Train Epoch: 009 Batch: 00077/00094 | Loss: 477.6880 | CE: 0.2265 | KD: 1061.0256\n",
      "Train Epoch: 009 Batch: 00078/00094 | Loss: 477.6111 | CE: 0.2756 | KD: 1060.7455\n",
      "Train Epoch: 009 Batch: 00079/00094 | Loss: 477.4861 | CE: 0.2603 | KD: 1060.5018\n",
      "Train Epoch: 009 Batch: 00080/00094 | Loss: 477.5128 | CE: 0.2517 | KD: 1060.5802\n",
      "Train Epoch: 009 Batch: 00081/00094 | Loss: 477.3905 | CE: 0.1842 | KD: 1060.4584\n",
      "Train Epoch: 009 Batch: 00082/00094 | Loss: 477.4875 | CE: 0.2816 | KD: 1060.4574\n",
      "Train Epoch: 009 Batch: 00083/00094 | Loss: 477.4917 | CE: 0.2271 | KD: 1060.5880\n",
      "Train Epoch: 009 Batch: 00084/00094 | Loss: 477.4550 | CE: 0.2457 | KD: 1060.4651\n",
      "Train Epoch: 009 Batch: 00085/00094 | Loss: 477.5206 | CE: 0.1737 | KD: 1060.7708\n",
      "Train Epoch: 009 Batch: 00086/00094 | Loss: 477.5932 | CE: 0.2005 | KD: 1060.8727\n",
      "Train Epoch: 009 Batch: 00087/00094 | Loss: 477.5481 | CE: 0.2148 | KD: 1060.7406\n",
      "Train Epoch: 009 Batch: 00088/00094 | Loss: 477.4817 | CE: 0.2195 | KD: 1060.5826\n",
      "Train Epoch: 009 Batch: 00089/00094 | Loss: 477.5029 | CE: 0.2394 | KD: 1060.5856\n",
      "Train Epoch: 009 Batch: 00090/00094 | Loss: 477.5412 | CE: 0.2355 | KD: 1060.6794\n",
      "Train Epoch: 009 Batch: 00091/00094 | Loss: 477.5114 | CE: 0.2759 | KD: 1060.5234\n",
      "Train Epoch: 009 Batch: 00092/00094 | Loss: 477.4635 | CE: 0.1837 | KD: 1060.6217\n",
      "Train Epoch: 009 Batch: 00093/00094 | Loss: 477.6551 | CE: 0.2465 | KD: 1060.9081\n",
      "Train Epoch: 009 Batch: 00094/00094 | Loss: 477.6185 | CE: 0.2206 | KD: 1060.8842\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2308 | acc:93.8000\n",
      "[VAL Acc] Target: 93.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6070 | acc:50.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0801 | acc:58.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 58.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9949 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2720 | acc:54.0361\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5504 | acc:73.1054\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0105 | acc:60.3448\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 60.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7456 | acc:62.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.06%\n",
      "[VAL Acc] Avg 62.81%\n",
      "Train Epoch: 010 Batch: 00001/00094 | Loss: 477.4848 | CE: 0.2415 | KD: 1060.5406\n",
      "Train Epoch: 010 Batch: 00002/00094 | Loss: 477.5735 | CE: 0.2246 | KD: 1060.7754\n",
      "Train Epoch: 010 Batch: 00003/00094 | Loss: 477.4282 | CE: 0.2224 | KD: 1060.4574\n",
      "Train Epoch: 010 Batch: 00004/00094 | Loss: 477.5046 | CE: 0.2087 | KD: 1060.6576\n",
      "Train Epoch: 010 Batch: 00005/00094 | Loss: 477.7027 | CE: 0.2429 | KD: 1061.0217\n",
      "Train Epoch: 010 Batch: 00006/00094 | Loss: 477.5481 | CE: 0.2668 | KD: 1060.6251\n",
      "Train Epoch: 010 Batch: 00007/00094 | Loss: 477.5852 | CE: 0.1887 | KD: 1060.8810\n",
      "Train Epoch: 010 Batch: 00008/00094 | Loss: 477.4249 | CE: 0.1913 | KD: 1060.5190\n",
      "Train Epoch: 010 Batch: 00009/00094 | Loss: 477.6943 | CE: 0.2348 | KD: 1061.0212\n",
      "Train Epoch: 010 Batch: 00010/00094 | Loss: 477.4372 | CE: 0.1677 | KD: 1060.5989\n",
      "Train Epoch: 010 Batch: 00011/00094 | Loss: 477.5917 | CE: 0.3107 | KD: 1060.6244\n",
      "Train Epoch: 010 Batch: 00012/00094 | Loss: 477.3389 | CE: 0.1630 | KD: 1060.3909\n",
      "Train Epoch: 010 Batch: 00013/00094 | Loss: 477.7615 | CE: 0.3118 | KD: 1060.9994\n",
      "Train Epoch: 010 Batch: 00014/00094 | Loss: 477.5080 | CE: 0.2356 | KD: 1060.6052\n",
      "Train Epoch: 010 Batch: 00015/00094 | Loss: 477.4833 | CE: 0.2239 | KD: 1060.5764\n",
      "Train Epoch: 010 Batch: 00016/00094 | Loss: 477.5403 | CE: 0.2078 | KD: 1060.7389\n",
      "Train Epoch: 010 Batch: 00017/00094 | Loss: 477.5547 | CE: 0.2482 | KD: 1060.6810\n",
      "Train Epoch: 010 Batch: 00018/00094 | Loss: 477.4486 | CE: 0.2383 | KD: 1060.4673\n",
      "Train Epoch: 010 Batch: 00019/00094 | Loss: 477.5931 | CE: 0.2155 | KD: 1060.8392\n",
      "Train Epoch: 010 Batch: 00020/00094 | Loss: 477.4822 | CE: 0.1879 | KD: 1060.6542\n",
      "Train Epoch: 010 Batch: 00021/00094 | Loss: 477.4190 | CE: 0.1992 | KD: 1060.4885\n",
      "Train Epoch: 010 Batch: 00022/00094 | Loss: 477.3912 | CE: 0.1846 | KD: 1060.4594\n",
      "Train Epoch: 010 Batch: 00023/00094 | Loss: 477.5108 | CE: 0.2073 | KD: 1060.6744\n",
      "Train Epoch: 010 Batch: 00024/00094 | Loss: 477.5763 | CE: 0.2643 | KD: 1060.6932\n",
      "Train Epoch: 010 Batch: 00025/00094 | Loss: 477.4791 | CE: 0.1870 | KD: 1060.6492\n",
      "Train Epoch: 010 Batch: 00026/00094 | Loss: 477.5112 | CE: 0.2384 | KD: 1060.6062\n",
      "Train Epoch: 010 Batch: 00027/00094 | Loss: 477.5878 | CE: 0.2338 | KD: 1060.7867\n",
      "Train Epoch: 010 Batch: 00028/00094 | Loss: 477.4430 | CE: 0.1899 | KD: 1060.5625\n",
      "Train Epoch: 010 Batch: 00029/00094 | Loss: 477.7677 | CE: 0.4304 | KD: 1060.7494\n",
      "Train Epoch: 010 Batch: 00030/00094 | Loss: 477.5330 | CE: 0.1764 | KD: 1060.7924\n",
      "Train Epoch: 010 Batch: 00031/00094 | Loss: 477.6095 | CE: 0.2368 | KD: 1060.8284\n",
      "Train Epoch: 010 Batch: 00032/00094 | Loss: 477.4552 | CE: 0.2437 | KD: 1060.4700\n",
      "Train Epoch: 010 Batch: 00033/00094 | Loss: 477.6328 | CE: 0.2781 | KD: 1060.7882\n",
      "Train Epoch: 010 Batch: 00034/00094 | Loss: 477.5346 | CE: 0.1919 | KD: 1060.7615\n",
      "Train Epoch: 010 Batch: 00035/00094 | Loss: 477.5985 | CE: 0.2553 | KD: 1060.7627\n",
      "Train Epoch: 010 Batch: 00036/00094 | Loss: 477.6086 | CE: 0.2106 | KD: 1060.8846\n",
      "Train Epoch: 010 Batch: 00037/00094 | Loss: 477.5216 | CE: 0.2129 | KD: 1060.6862\n",
      "Train Epoch: 010 Batch: 00038/00094 | Loss: 477.3061 | CE: 0.1614 | KD: 1060.3215\n",
      "Train Epoch: 010 Batch: 00039/00094 | Loss: 477.5277 | CE: 0.2426 | KD: 1060.6338\n",
      "Train Epoch: 010 Batch: 00040/00094 | Loss: 477.4181 | CE: 0.2586 | KD: 1060.3544\n",
      "Train Epoch: 010 Batch: 00041/00094 | Loss: 477.6699 | CE: 0.2306 | KD: 1060.9763\n",
      "Train Epoch: 010 Batch: 00042/00094 | Loss: 477.6224 | CE: 0.1970 | KD: 1060.9454\n",
      "Train Epoch: 010 Batch: 00043/00094 | Loss: 477.4730 | CE: 0.2329 | KD: 1060.5334\n",
      "Train Epoch: 010 Batch: 00044/00094 | Loss: 477.4936 | CE: 0.1991 | KD: 1060.6543\n",
      "Train Epoch: 010 Batch: 00045/00094 | Loss: 477.4362 | CE: 0.2253 | KD: 1060.4686\n",
      "Train Epoch: 010 Batch: 00046/00094 | Loss: 477.4449 | CE: 0.2187 | KD: 1060.5026\n",
      "Train Epoch: 010 Batch: 00047/00094 | Loss: 477.6998 | CE: 0.2938 | KD: 1060.9021\n",
      "Train Epoch: 010 Batch: 00048/00094 | Loss: 477.5151 | CE: 0.2560 | KD: 1060.5759\n",
      "Train Epoch: 010 Batch: 00049/00094 | Loss: 477.8447 | CE: 0.2657 | KD: 1061.2866\n",
      "Train Epoch: 010 Batch: 00050/00094 | Loss: 477.6205 | CE: 0.2368 | KD: 1060.8527\n",
      "Train Epoch: 010 Batch: 00051/00094 | Loss: 477.7820 | CE: 0.3355 | KD: 1060.9922\n",
      "Train Epoch: 010 Batch: 00052/00094 | Loss: 477.4732 | CE: 0.2055 | KD: 1060.5950\n",
      "Train Epoch: 010 Batch: 00053/00094 | Loss: 477.5526 | CE: 0.1706 | KD: 1060.8490\n",
      "Train Epoch: 010 Batch: 00054/00094 | Loss: 477.3990 | CE: 0.2060 | KD: 1060.4290\n",
      "Train Epoch: 010 Batch: 00055/00094 | Loss: 477.5287 | CE: 0.2059 | KD: 1060.7174\n",
      "Train Epoch: 010 Batch: 00056/00094 | Loss: 477.5313 | CE: 0.2231 | KD: 1060.6848\n",
      "Train Epoch: 010 Batch: 00057/00094 | Loss: 477.5757 | CE: 0.2127 | KD: 1060.8066\n",
      "Train Epoch: 010 Batch: 00058/00094 | Loss: 477.7855 | CE: 0.3110 | KD: 1061.0544\n",
      "Train Epoch: 010 Batch: 00059/00094 | Loss: 477.4925 | CE: 0.1468 | KD: 1060.7682\n",
      "Train Epoch: 010 Batch: 00060/00094 | Loss: 477.4625 | CE: 0.2040 | KD: 1060.5746\n",
      "Train Epoch: 010 Batch: 00061/00094 | Loss: 477.5966 | CE: 0.2696 | KD: 1060.7268\n",
      "Train Epoch: 010 Batch: 00062/00094 | Loss: 477.6469 | CE: 0.3321 | KD: 1060.6995\n",
      "Train Epoch: 010 Batch: 00063/00094 | Loss: 477.7395 | CE: 0.2985 | KD: 1060.9801\n",
      "Train Epoch: 010 Batch: 00064/00094 | Loss: 477.5035 | CE: 0.2441 | KD: 1060.5765\n",
      "Train Epoch: 010 Batch: 00065/00094 | Loss: 477.6743 | CE: 0.2701 | KD: 1060.8981\n",
      "Train Epoch: 010 Batch: 00066/00094 | Loss: 477.5240 | CE: 0.1994 | KD: 1060.7216\n",
      "Train Epoch: 010 Batch: 00067/00094 | Loss: 477.5028 | CE: 0.3012 | KD: 1060.4480\n",
      "Train Epoch: 010 Batch: 00068/00094 | Loss: 477.4323 | CE: 0.2051 | KD: 1060.5050\n",
      "Train Epoch: 010 Batch: 00069/00094 | Loss: 477.7144 | CE: 0.2459 | KD: 1061.0411\n",
      "Train Epoch: 010 Batch: 00070/00094 | Loss: 477.6422 | CE: 0.2025 | KD: 1060.9772\n",
      "Train Epoch: 010 Batch: 00071/00094 | Loss: 477.5317 | CE: 0.1548 | KD: 1060.8376\n",
      "Train Epoch: 010 Batch: 00072/00094 | Loss: 477.5655 | CE: 0.2429 | KD: 1060.7169\n",
      "Train Epoch: 010 Batch: 00073/00094 | Loss: 477.8799 | CE: 0.2505 | KD: 1061.3988\n",
      "Train Epoch: 010 Batch: 00074/00094 | Loss: 477.6278 | CE: 0.2349 | KD: 1060.8732\n",
      "Train Epoch: 010 Batch: 00075/00094 | Loss: 477.5484 | CE: 0.1693 | KD: 1060.8425\n",
      "Train Epoch: 010 Batch: 00076/00094 | Loss: 477.4997 | CE: 0.1872 | KD: 1060.6945\n",
      "Train Epoch: 010 Batch: 00077/00094 | Loss: 477.5422 | CE: 0.2568 | KD: 1060.6343\n",
      "Train Epoch: 010 Batch: 00078/00094 | Loss: 477.5495 | CE: 0.2385 | KD: 1060.6913\n",
      "Train Epoch: 010 Batch: 00079/00094 | Loss: 477.5246 | CE: 0.2435 | KD: 1060.6248\n",
      "Train Epoch: 010 Batch: 00080/00094 | Loss: 477.5724 | CE: 0.3014 | KD: 1060.6022\n",
      "Train Epoch: 010 Batch: 00081/00094 | Loss: 477.6970 | CE: 0.1627 | KD: 1061.1874\n",
      "Train Epoch: 010 Batch: 00082/00094 | Loss: 477.5912 | CE: 0.2454 | KD: 1060.7683\n",
      "Train Epoch: 010 Batch: 00083/00094 | Loss: 477.4153 | CE: 0.2580 | KD: 1060.3495\n",
      "Train Epoch: 010 Batch: 00084/00094 | Loss: 477.5502 | CE: 0.1835 | KD: 1060.8149\n",
      "Train Epoch: 010 Batch: 00085/00094 | Loss: 477.7395 | CE: 0.3507 | KD: 1060.8640\n",
      "Train Epoch: 010 Batch: 00086/00094 | Loss: 477.3742 | CE: 0.1363 | KD: 1060.5288\n",
      "Train Epoch: 010 Batch: 00087/00094 | Loss: 477.5316 | CE: 0.2630 | KD: 1060.5968\n",
      "Train Epoch: 010 Batch: 00088/00094 | Loss: 477.5527 | CE: 0.2006 | KD: 1060.7826\n",
      "Train Epoch: 010 Batch: 00089/00094 | Loss: 477.4662 | CE: 0.1888 | KD: 1060.6166\n",
      "Train Epoch: 010 Batch: 00090/00094 | Loss: 477.3984 | CE: 0.1756 | KD: 1060.4952\n",
      "Train Epoch: 010 Batch: 00091/00094 | Loss: 477.6523 | CE: 0.2242 | KD: 1060.9514\n",
      "Train Epoch: 010 Batch: 00092/00094 | Loss: 477.6290 | CE: 0.2787 | KD: 1060.7784\n",
      "Train Epoch: 010 Batch: 00093/00094 | Loss: 477.7969 | CE: 0.2930 | KD: 1061.1198\n",
      "Train Epoch: 010 Batch: 00094/00094 | Loss: 477.4228 | CE: 0.1861 | KD: 1060.5260\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2240 | acc:94.0500\n",
      "[VAL Acc] Target: 94.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6067 | acc:50.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0230 | acc:59.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9685 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2448 | acc:54.3103\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5613 | acc:71.7190\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9871 | acc:61.6379\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7454 | acc:62.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.12%\n",
      "[VAL Acc] Avg 63.03%\n",
      "VAL Acc improve from 62.91% to 63.03%\n",
      "Save best model\n",
      "Train Epoch: 011 Batch: 00001/00094 | Loss: 477.4753 | CE: 0.1910 | KD: 1060.6318\n",
      "Train Epoch: 011 Batch: 00002/00094 | Loss: 477.7586 | CE: 0.2953 | KD: 1061.0297\n",
      "Train Epoch: 011 Batch: 00003/00094 | Loss: 477.5529 | CE: 0.2018 | KD: 1060.7804\n",
      "Train Epoch: 011 Batch: 00004/00094 | Loss: 477.4796 | CE: 0.1938 | KD: 1060.6353\n",
      "Train Epoch: 011 Batch: 00005/00094 | Loss: 477.6428 | CE: 0.2164 | KD: 1060.9474\n",
      "Train Epoch: 011 Batch: 00006/00094 | Loss: 477.5890 | CE: 0.1990 | KD: 1060.8668\n",
      "Train Epoch: 011 Batch: 00007/00094 | Loss: 477.5482 | CE: 0.2548 | KD: 1060.6522\n",
      "Train Epoch: 011 Batch: 00008/00094 | Loss: 477.5154 | CE: 0.2155 | KD: 1060.6666\n",
      "Train Epoch: 011 Batch: 00009/00094 | Loss: 477.5229 | CE: 0.2091 | KD: 1060.6973\n",
      "Train Epoch: 011 Batch: 00010/00094 | Loss: 477.6515 | CE: 0.2559 | KD: 1060.8793\n",
      "Train Epoch: 011 Batch: 00011/00094 | Loss: 477.4894 | CE: 0.1790 | KD: 1060.6899\n",
      "Train Epoch: 011 Batch: 00012/00094 | Loss: 477.4247 | CE: 0.1849 | KD: 1060.5330\n",
      "Train Epoch: 011 Batch: 00013/00094 | Loss: 477.4066 | CE: 0.2256 | KD: 1060.4023\n",
      "Train Epoch: 011 Batch: 00014/00094 | Loss: 477.4092 | CE: 0.1698 | KD: 1060.5321\n",
      "Train Epoch: 011 Batch: 00015/00094 | Loss: 477.6742 | CE: 0.3050 | KD: 1060.8203\n",
      "Train Epoch: 011 Batch: 00016/00094 | Loss: 477.7433 | CE: 0.2441 | KD: 1061.1093\n",
      "Train Epoch: 011 Batch: 00017/00094 | Loss: 477.4035 | CE: 0.1958 | KD: 1060.4615\n",
      "Train Epoch: 011 Batch: 00018/00094 | Loss: 477.5063 | CE: 0.2472 | KD: 1060.5756\n",
      "Train Epoch: 011 Batch: 00019/00094 | Loss: 477.4106 | CE: 0.2376 | KD: 1060.3844\n",
      "Train Epoch: 011 Batch: 00020/00094 | Loss: 477.4557 | CE: 0.1531 | KD: 1060.6725\n",
      "Train Epoch: 011 Batch: 00021/00094 | Loss: 477.5901 | CE: 0.3559 | KD: 1060.5204\n",
      "Train Epoch: 011 Batch: 00022/00094 | Loss: 477.3843 | CE: 0.1772 | KD: 1060.4602\n",
      "Train Epoch: 011 Batch: 00023/00094 | Loss: 477.6440 | CE: 0.3166 | KD: 1060.7277\n",
      "Train Epoch: 011 Batch: 00024/00094 | Loss: 477.5916 | CE: 0.2269 | KD: 1060.8103\n",
      "Train Epoch: 011 Batch: 00025/00094 | Loss: 477.7232 | CE: 0.2915 | KD: 1060.9595\n",
      "Train Epoch: 011 Batch: 00026/00094 | Loss: 477.6355 | CE: 0.2471 | KD: 1060.8630\n",
      "Train Epoch: 011 Batch: 00027/00094 | Loss: 477.4875 | CE: 0.1957 | KD: 1060.6484\n",
      "Train Epoch: 011 Batch: 00028/00094 | Loss: 477.5346 | CE: 0.2292 | KD: 1060.6786\n",
      "Train Epoch: 011 Batch: 00029/00094 | Loss: 477.6263 | CE: 0.2150 | KD: 1060.9142\n",
      "Train Epoch: 011 Batch: 00030/00094 | Loss: 477.5501 | CE: 0.2408 | KD: 1060.6873\n",
      "Train Epoch: 011 Batch: 00031/00094 | Loss: 477.4512 | CE: 0.2540 | KD: 1060.4382\n",
      "Train Epoch: 011 Batch: 00032/00094 | Loss: 477.6570 | CE: 0.2887 | KD: 1060.8184\n",
      "Train Epoch: 011 Batch: 00033/00094 | Loss: 477.5981 | CE: 0.2138 | KD: 1060.8540\n",
      "Train Epoch: 011 Batch: 00034/00094 | Loss: 477.3344 | CE: 0.1899 | KD: 1060.3213\n",
      "Train Epoch: 011 Batch: 00035/00094 | Loss: 477.5608 | CE: 0.1862 | KD: 1060.8324\n",
      "Train Epoch: 011 Batch: 00036/00094 | Loss: 477.4901 | CE: 0.1630 | KD: 1060.7271\n",
      "Train Epoch: 011 Batch: 00037/00094 | Loss: 477.4713 | CE: 0.2038 | KD: 1060.5945\n",
      "Train Epoch: 011 Batch: 00038/00094 | Loss: 477.4891 | CE: 0.1743 | KD: 1060.6995\n",
      "Train Epoch: 011 Batch: 00039/00094 | Loss: 477.4866 | CE: 0.2407 | KD: 1060.5463\n",
      "Train Epoch: 011 Batch: 00040/00094 | Loss: 477.6281 | CE: 0.2615 | KD: 1060.8148\n",
      "Train Epoch: 011 Batch: 00041/00094 | Loss: 477.4587 | CE: 0.2204 | KD: 1060.5295\n",
      "Train Epoch: 011 Batch: 00042/00094 | Loss: 477.4757 | CE: 0.2556 | KD: 1060.4891\n",
      "Train Epoch: 011 Batch: 00043/00094 | Loss: 477.4150 | CE: 0.1657 | KD: 1060.5541\n",
      "Train Epoch: 011 Batch: 00044/00094 | Loss: 477.5562 | CE: 0.1876 | KD: 1060.8192\n",
      "Train Epoch: 011 Batch: 00045/00094 | Loss: 477.4480 | CE: 0.1869 | KD: 1060.5803\n",
      "Train Epoch: 011 Batch: 00046/00094 | Loss: 477.6788 | CE: 0.2219 | KD: 1061.0155\n",
      "Train Epoch: 011 Batch: 00047/00094 | Loss: 477.5651 | CE: 0.2300 | KD: 1060.7445\n",
      "Train Epoch: 011 Batch: 00048/00094 | Loss: 477.5183 | CE: 0.1969 | KD: 1060.7142\n",
      "Train Epoch: 011 Batch: 00049/00094 | Loss: 477.7183 | CE: 0.3090 | KD: 1060.9094\n",
      "Train Epoch: 011 Batch: 00050/00094 | Loss: 477.6992 | CE: 0.2533 | KD: 1060.9908\n",
      "Train Epoch: 011 Batch: 00051/00094 | Loss: 477.6437 | CE: 0.1892 | KD: 1061.0103\n",
      "Train Epoch: 011 Batch: 00052/00094 | Loss: 477.6439 | CE: 0.3414 | KD: 1060.6722\n",
      "Train Epoch: 011 Batch: 00053/00094 | Loss: 477.8344 | CE: 0.2589 | KD: 1061.2789\n",
      "Train Epoch: 011 Batch: 00054/00094 | Loss: 477.4357 | CE: 0.1679 | KD: 1060.5951\n",
      "Train Epoch: 011 Batch: 00055/00094 | Loss: 477.5123 | CE: 0.2291 | KD: 1060.6294\n",
      "Train Epoch: 011 Batch: 00056/00094 | Loss: 477.7318 | CE: 0.2520 | KD: 1061.0662\n",
      "Train Epoch: 011 Batch: 00057/00094 | Loss: 477.5498 | CE: 0.1913 | KD: 1060.7966\n",
      "Train Epoch: 011 Batch: 00058/00094 | Loss: 477.5475 | CE: 0.2718 | KD: 1060.6127\n",
      "Train Epoch: 011 Batch: 00059/00094 | Loss: 477.6711 | CE: 0.3717 | KD: 1060.6652\n",
      "Train Epoch: 011 Batch: 00060/00094 | Loss: 477.4554 | CE: 0.1893 | KD: 1060.5913\n",
      "Train Epoch: 011 Batch: 00061/00094 | Loss: 477.3796 | CE: 0.2032 | KD: 1060.3921\n",
      "Train Epoch: 011 Batch: 00062/00094 | Loss: 477.5172 | CE: 0.1918 | KD: 1060.7231\n",
      "Train Epoch: 011 Batch: 00063/00094 | Loss: 477.6327 | CE: 0.2280 | KD: 1060.8994\n",
      "Train Epoch: 011 Batch: 00064/00094 | Loss: 477.4765 | CE: 0.1590 | KD: 1060.7057\n",
      "Train Epoch: 011 Batch: 00065/00094 | Loss: 477.3705 | CE: 0.1787 | KD: 1060.4260\n",
      "Train Epoch: 011 Batch: 00066/00094 | Loss: 477.6298 | CE: 0.2445 | KD: 1060.8563\n",
      "Train Epoch: 011 Batch: 00067/00094 | Loss: 477.2995 | CE: 0.1878 | KD: 1060.2484\n",
      "Train Epoch: 011 Batch: 00068/00094 | Loss: 477.4686 | CE: 0.1782 | KD: 1060.6455\n",
      "Train Epoch: 011 Batch: 00069/00094 | Loss: 477.5338 | CE: 0.1868 | KD: 1060.7711\n",
      "Train Epoch: 011 Batch: 00070/00094 | Loss: 477.6430 | CE: 0.2346 | KD: 1060.9076\n",
      "Train Epoch: 011 Batch: 00071/00094 | Loss: 477.4507 | CE: 0.1732 | KD: 1060.6166\n",
      "Train Epoch: 011 Batch: 00072/00094 | Loss: 477.4773 | CE: 0.2038 | KD: 1060.6078\n",
      "Train Epoch: 011 Batch: 00073/00094 | Loss: 477.6705 | CE: 0.2911 | KD: 1060.8431\n",
      "Train Epoch: 011 Batch: 00074/00094 | Loss: 477.5698 | CE: 0.2345 | KD: 1060.7450\n",
      "Train Epoch: 011 Batch: 00075/00094 | Loss: 477.4362 | CE: 0.1728 | KD: 1060.5854\n",
      "Train Epoch: 011 Batch: 00076/00094 | Loss: 477.7527 | CE: 0.2855 | KD: 1061.0382\n",
      "Train Epoch: 011 Batch: 00077/00094 | Loss: 477.5862 | CE: 0.3217 | KD: 1060.5879\n",
      "Train Epoch: 011 Batch: 00078/00094 | Loss: 477.5427 | CE: 0.2033 | KD: 1060.7542\n",
      "Train Epoch: 011 Batch: 00079/00094 | Loss: 477.7465 | CE: 0.2804 | KD: 1061.0358\n",
      "Train Epoch: 011 Batch: 00080/00094 | Loss: 477.5638 | CE: 0.2627 | KD: 1060.6689\n",
      "Train Epoch: 011 Batch: 00081/00094 | Loss: 477.5334 | CE: 0.2349 | KD: 1060.6636\n",
      "Train Epoch: 011 Batch: 00082/00094 | Loss: 477.6259 | CE: 0.2862 | KD: 1060.7550\n",
      "Train Epoch: 011 Batch: 00083/00094 | Loss: 477.4653 | CE: 0.1895 | KD: 1060.6130\n",
      "Train Epoch: 011 Batch: 00084/00094 | Loss: 477.5156 | CE: 0.1760 | KD: 1060.7546\n",
      "Train Epoch: 011 Batch: 00085/00094 | Loss: 477.5376 | CE: 0.1566 | KD: 1060.8467\n",
      "Train Epoch: 011 Batch: 00086/00094 | Loss: 477.5618 | CE: 0.2176 | KD: 1060.7650\n",
      "Train Epoch: 011 Batch: 00087/00094 | Loss: 477.7508 | CE: 0.2987 | KD: 1061.0048\n",
      "Train Epoch: 011 Batch: 00088/00094 | Loss: 477.6609 | CE: 0.2868 | KD: 1060.8314\n",
      "Train Epoch: 011 Batch: 00089/00094 | Loss: 477.4316 | CE: 0.1714 | KD: 1060.5784\n",
      "Train Epoch: 011 Batch: 00090/00094 | Loss: 477.6035 | CE: 0.2781 | KD: 1060.7233\n",
      "Train Epoch: 011 Batch: 00091/00094 | Loss: 477.7846 | CE: 0.3716 | KD: 1060.9177\n",
      "Train Epoch: 011 Batch: 00092/00094 | Loss: 477.4271 | CE: 0.1894 | KD: 1060.5282\n",
      "Train Epoch: 011 Batch: 00093/00094 | Loss: 477.6536 | CE: 0.2772 | KD: 1060.8364\n",
      "Train Epoch: 011 Batch: 00094/00094 | Loss: 477.5384 | CE: 0.2538 | KD: 1060.6326\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2437 | acc:93.0000\n",
      "[VAL Acc] Target: 93.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6113 | acc:50.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1215 | acc:57.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 57.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9845 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3124 | acc:53.4875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5598 | acc:70.0555\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0114 | acc:60.8542\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 60.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7787 | acc:59.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.81%\n",
      "[VAL Acc] Avg 61.85%\n",
      "Train Epoch: 012 Batch: 00001/00094 | Loss: 477.5925 | CE: 0.2543 | KD: 1060.7515\n",
      "Train Epoch: 012 Batch: 00002/00094 | Loss: 477.5830 | CE: 0.2393 | KD: 1060.7639\n",
      "Train Epoch: 012 Batch: 00003/00094 | Loss: 477.5300 | CE: 0.1791 | KD: 1060.7798\n",
      "Train Epoch: 012 Batch: 00004/00094 | Loss: 477.4825 | CE: 0.1579 | KD: 1060.7213\n",
      "Train Epoch: 012 Batch: 00005/00094 | Loss: 477.4072 | CE: 0.1739 | KD: 1060.5186\n",
      "Train Epoch: 012 Batch: 00006/00094 | Loss: 477.5691 | CE: 0.1773 | KD: 1060.8706\n",
      "Train Epoch: 012 Batch: 00007/00094 | Loss: 477.9289 | CE: 0.5305 | KD: 1060.8855\n",
      "Train Epoch: 012 Batch: 00008/00094 | Loss: 477.6562 | CE: 0.2305 | KD: 1060.9459\n",
      "Train Epoch: 012 Batch: 00009/00094 | Loss: 477.3754 | CE: 0.1828 | KD: 1060.4279\n",
      "Train Epoch: 012 Batch: 00010/00094 | Loss: 477.6191 | CE: 0.2571 | KD: 1060.8044\n",
      "Train Epoch: 012 Batch: 00011/00094 | Loss: 477.6082 | CE: 0.3170 | KD: 1060.6470\n",
      "Train Epoch: 012 Batch: 00012/00094 | Loss: 477.5371 | CE: 0.2016 | KD: 1060.7455\n",
      "Train Epoch: 012 Batch: 00013/00094 | Loss: 477.5080 | CE: 0.2457 | KD: 1060.5829\n",
      "Train Epoch: 012 Batch: 00014/00094 | Loss: 477.5599 | CE: 0.2005 | KD: 1060.7987\n",
      "Train Epoch: 012 Batch: 00015/00094 | Loss: 477.7043 | CE: 0.2520 | KD: 1061.0051\n",
      "Train Epoch: 012 Batch: 00016/00094 | Loss: 477.4927 | CE: 0.2213 | KD: 1060.6031\n",
      "Train Epoch: 012 Batch: 00017/00094 | Loss: 477.3713 | CE: 0.1854 | KD: 1060.4132\n",
      "Train Epoch: 012 Batch: 00018/00094 | Loss: 477.6547 | CE: 0.2796 | KD: 1060.8335\n",
      "Train Epoch: 012 Batch: 00019/00094 | Loss: 477.4351 | CE: 0.1731 | KD: 1060.5824\n",
      "Train Epoch: 012 Batch: 00020/00094 | Loss: 477.6187 | CE: 0.2389 | KD: 1060.8441\n",
      "Train Epoch: 012 Batch: 00021/00094 | Loss: 477.6284 | CE: 0.2214 | KD: 1060.9044\n",
      "Train Epoch: 012 Batch: 00022/00094 | Loss: 477.5316 | CE: 0.1965 | KD: 1060.7448\n",
      "Train Epoch: 012 Batch: 00023/00094 | Loss: 477.6070 | CE: 0.1797 | KD: 1060.9496\n",
      "Train Epoch: 012 Batch: 00024/00094 | Loss: 477.7186 | CE: 0.3394 | KD: 1060.8427\n",
      "Train Epoch: 012 Batch: 00025/00094 | Loss: 477.3440 | CE: 0.1583 | KD: 1060.4128\n",
      "Train Epoch: 012 Batch: 00026/00094 | Loss: 477.5707 | CE: 0.2511 | KD: 1060.7101\n",
      "Train Epoch: 012 Batch: 00027/00094 | Loss: 477.5934 | CE: 0.2305 | KD: 1060.8066\n",
      "Train Epoch: 012 Batch: 00028/00094 | Loss: 477.7287 | CE: 0.2233 | KD: 1061.1232\n",
      "Train Epoch: 012 Batch: 00029/00094 | Loss: 477.6346 | CE: 0.2646 | KD: 1060.8223\n",
      "Train Epoch: 012 Batch: 00030/00094 | Loss: 477.3619 | CE: 0.2343 | KD: 1060.2837\n",
      "Train Epoch: 012 Batch: 00031/00094 | Loss: 477.5800 | CE: 0.2854 | KD: 1060.6547\n",
      "Train Epoch: 012 Batch: 00032/00094 | Loss: 477.6467 | CE: 0.3029 | KD: 1060.7640\n",
      "Train Epoch: 012 Batch: 00033/00094 | Loss: 477.5619 | CE: 0.2018 | KD: 1060.8003\n",
      "Train Epoch: 012 Batch: 00034/00094 | Loss: 477.4460 | CE: 0.2070 | KD: 1060.5310\n",
      "Train Epoch: 012 Batch: 00035/00094 | Loss: 477.4929 | CE: 0.1731 | KD: 1060.7108\n",
      "Train Epoch: 012 Batch: 00036/00094 | Loss: 477.4694 | CE: 0.1848 | KD: 1060.6323\n",
      "Train Epoch: 012 Batch: 00037/00094 | Loss: 477.3521 | CE: 0.1913 | KD: 1060.3572\n",
      "Train Epoch: 012 Batch: 00038/00094 | Loss: 477.5030 | CE: 0.2573 | KD: 1060.5459\n",
      "Train Epoch: 012 Batch: 00039/00094 | Loss: 477.5284 | CE: 0.2204 | KD: 1060.6843\n",
      "Train Epoch: 012 Batch: 00040/00094 | Loss: 477.5592 | CE: 0.3024 | KD: 1060.5707\n",
      "Train Epoch: 012 Batch: 00041/00094 | Loss: 477.6783 | CE: 0.3604 | KD: 1060.7064\n",
      "Train Epoch: 012 Batch: 00042/00094 | Loss: 477.3989 | CE: 0.2421 | KD: 1060.3484\n",
      "Train Epoch: 012 Batch: 00043/00094 | Loss: 477.6583 | CE: 0.2062 | KD: 1061.0048\n",
      "Train Epoch: 012 Batch: 00044/00094 | Loss: 477.5623 | CE: 0.2323 | KD: 1060.7333\n",
      "Train Epoch: 012 Batch: 00045/00094 | Loss: 477.7440 | CE: 0.3915 | KD: 1060.7834\n",
      "Train Epoch: 012 Batch: 00046/00094 | Loss: 477.7442 | CE: 0.2841 | KD: 1061.0223\n",
      "Train Epoch: 012 Batch: 00047/00094 | Loss: 477.5558 | CE: 0.2363 | KD: 1060.7101\n",
      "Train Epoch: 012 Batch: 00048/00094 | Loss: 477.6127 | CE: 0.3401 | KD: 1060.6058\n",
      "Train Epoch: 012 Batch: 00049/00094 | Loss: 477.5186 | CE: 0.2662 | KD: 1060.5610\n",
      "Train Epoch: 012 Batch: 00050/00094 | Loss: 477.5998 | CE: 0.2434 | KD: 1060.7920\n",
      "Train Epoch: 012 Batch: 00051/00094 | Loss: 477.5550 | CE: 0.1773 | KD: 1060.8394\n",
      "Train Epoch: 012 Batch: 00052/00094 | Loss: 477.3314 | CE: 0.2326 | KD: 1060.2196\n",
      "Train Epoch: 012 Batch: 00053/00094 | Loss: 477.5617 | CE: 0.1845 | KD: 1060.8384\n",
      "Train Epoch: 012 Batch: 00054/00094 | Loss: 477.4915 | CE: 0.2880 | KD: 1060.4524\n",
      "Train Epoch: 012 Batch: 00055/00094 | Loss: 477.4557 | CE: 0.2405 | KD: 1060.4783\n",
      "Train Epoch: 012 Batch: 00056/00094 | Loss: 477.7122 | CE: 0.3101 | KD: 1060.8936\n",
      "Train Epoch: 012 Batch: 00057/00094 | Loss: 477.4058 | CE: 0.1946 | KD: 1060.4692\n",
      "Train Epoch: 012 Batch: 00058/00094 | Loss: 477.6104 | CE: 0.3051 | KD: 1060.6785\n",
      "Train Epoch: 012 Batch: 00059/00094 | Loss: 477.5628 | CE: 0.2549 | KD: 1060.6841\n",
      "Train Epoch: 012 Batch: 00060/00094 | Loss: 477.5088 | CE: 0.2565 | KD: 1060.5607\n",
      "Train Epoch: 012 Batch: 00061/00094 | Loss: 477.6035 | CE: 0.2127 | KD: 1060.8685\n",
      "Train Epoch: 012 Batch: 00062/00094 | Loss: 477.5348 | CE: 0.2189 | KD: 1060.7020\n",
      "Train Epoch: 012 Batch: 00063/00094 | Loss: 477.4956 | CE: 0.1757 | KD: 1060.7111\n",
      "Train Epoch: 012 Batch: 00064/00094 | Loss: 477.4630 | CE: 0.1687 | KD: 1060.6539\n",
      "Train Epoch: 012 Batch: 00065/00094 | Loss: 477.6650 | CE: 0.2841 | KD: 1060.8464\n",
      "Train Epoch: 012 Batch: 00066/00094 | Loss: 477.4669 | CE: 0.1819 | KD: 1060.6334\n",
      "Train Epoch: 012 Batch: 00067/00094 | Loss: 477.5445 | CE: 0.2215 | KD: 1060.7177\n",
      "Train Epoch: 012 Batch: 00068/00094 | Loss: 477.4263 | CE: 0.1624 | KD: 1060.5864\n",
      "Train Epoch: 012 Batch: 00069/00094 | Loss: 477.4511 | CE: 0.2266 | KD: 1060.4990\n",
      "Train Epoch: 012 Batch: 00070/00094 | Loss: 477.5488 | CE: 0.2195 | KD: 1060.7318\n",
      "Train Epoch: 012 Batch: 00071/00094 | Loss: 477.4926 | CE: 0.2887 | KD: 1060.4530\n",
      "Train Epoch: 012 Batch: 00072/00094 | Loss: 477.5875 | CE: 0.2913 | KD: 1060.6582\n",
      "Train Epoch: 012 Batch: 00073/00094 | Loss: 477.5950 | CE: 0.3105 | KD: 1060.6323\n",
      "Train Epoch: 012 Batch: 00074/00094 | Loss: 477.4075 | CE: 0.2211 | KD: 1060.4144\n",
      "Train Epoch: 012 Batch: 00075/00094 | Loss: 477.6671 | CE: 0.2190 | KD: 1060.9958\n",
      "Train Epoch: 012 Batch: 00076/00094 | Loss: 477.7133 | CE: 0.1988 | KD: 1061.1436\n",
      "Train Epoch: 012 Batch: 00077/00094 | Loss: 477.5073 | CE: 0.1998 | KD: 1060.6832\n",
      "Train Epoch: 012 Batch: 00078/00094 | Loss: 477.4188 | CE: 0.1817 | KD: 1060.5267\n",
      "Train Epoch: 012 Batch: 00079/00094 | Loss: 477.5460 | CE: 0.2085 | KD: 1060.7499\n",
      "Train Epoch: 012 Batch: 00080/00094 | Loss: 477.5055 | CE: 0.2459 | KD: 1060.5769\n",
      "Train Epoch: 012 Batch: 00081/00094 | Loss: 477.4291 | CE: 0.1611 | KD: 1060.5957\n",
      "Train Epoch: 012 Batch: 00082/00094 | Loss: 477.5498 | CE: 0.1933 | KD: 1060.7922\n",
      "Train Epoch: 012 Batch: 00083/00094 | Loss: 477.5002 | CE: 0.2124 | KD: 1060.6394\n",
      "Train Epoch: 012 Batch: 00084/00094 | Loss: 477.4022 | CE: 0.1797 | KD: 1060.4944\n",
      "Train Epoch: 012 Batch: 00085/00094 | Loss: 477.4390 | CE: 0.2019 | KD: 1060.5270\n",
      "Train Epoch: 012 Batch: 00086/00094 | Loss: 477.8085 | CE: 0.4267 | KD: 1060.8484\n",
      "Train Epoch: 012 Batch: 00087/00094 | Loss: 477.6006 | CE: 0.3026 | KD: 1060.6625\n",
      "Train Epoch: 012 Batch: 00088/00094 | Loss: 477.6742 | CE: 0.3661 | KD: 1060.6846\n",
      "Train Epoch: 012 Batch: 00089/00094 | Loss: 477.5196 | CE: 0.2153 | KD: 1060.6763\n",
      "Train Epoch: 012 Batch: 00090/00094 | Loss: 477.4917 | CE: 0.2609 | KD: 1060.5129\n",
      "Train Epoch: 012 Batch: 00091/00094 | Loss: 477.4183 | CE: 0.1899 | KD: 1060.5077\n",
      "Train Epoch: 012 Batch: 00092/00094 | Loss: 477.5566 | CE: 0.2419 | KD: 1060.6992\n",
      "Train Epoch: 012 Batch: 00093/00094 | Loss: 477.7628 | CE: 0.2634 | KD: 1061.1097\n",
      "Train Epoch: 012 Batch: 00094/00094 | Loss: 478.0221 | CE: 0.5751 | KD: 1060.9934\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2368 | acc:93.0000\n",
      "[VAL Acc] Target: 93.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7040 | acc:50.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1292 | acc:58.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 58.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0207 | acc:47.9008\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2954 | acc:54.3103\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5538 | acc:71.4418\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0437 | acc:60.5799\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 60.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7846 | acc:61.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.50%\n",
      "[VAL Acc] Avg 62.16%\n",
      "Train Epoch: 013 Batch: 00001/00094 | Loss: 477.5361 | CE: 0.2597 | KD: 1060.6144\n",
      "Train Epoch: 013 Batch: 00002/00094 | Loss: 477.5461 | CE: 0.2249 | KD: 1060.7139\n",
      "Train Epoch: 013 Batch: 00003/00094 | Loss: 477.5712 | CE: 0.1970 | KD: 1060.8317\n",
      "Train Epoch: 013 Batch: 00004/00094 | Loss: 477.6568 | CE: 0.3199 | KD: 1060.7487\n",
      "Train Epoch: 013 Batch: 00005/00094 | Loss: 477.5289 | CE: 0.2429 | KD: 1060.6355\n",
      "Train Epoch: 013 Batch: 00006/00094 | Loss: 477.5322 | CE: 0.2066 | KD: 1060.7235\n",
      "Train Epoch: 013 Batch: 00007/00094 | Loss: 477.6296 | CE: 0.2797 | KD: 1060.7776\n",
      "Train Epoch: 013 Batch: 00008/00094 | Loss: 477.7819 | CE: 0.2010 | KD: 1061.2909\n",
      "Train Epoch: 013 Batch: 00009/00094 | Loss: 477.5082 | CE: 0.2179 | KD: 1060.6451\n",
      "Train Epoch: 013 Batch: 00010/00094 | Loss: 477.5745 | CE: 0.2049 | KD: 1060.8214\n",
      "Train Epoch: 013 Batch: 00011/00094 | Loss: 477.5905 | CE: 0.2709 | KD: 1060.7103\n",
      "Train Epoch: 013 Batch: 00012/00094 | Loss: 477.4820 | CE: 0.2396 | KD: 1060.5387\n",
      "Train Epoch: 013 Batch: 00013/00094 | Loss: 477.5414 | CE: 0.2624 | KD: 1060.6199\n",
      "Train Epoch: 013 Batch: 00014/00094 | Loss: 477.4594 | CE: 0.1611 | KD: 1060.6631\n",
      "Train Epoch: 013 Batch: 00015/00094 | Loss: 477.4712 | CE: 0.2425 | KD: 1060.5083\n",
      "Train Epoch: 013 Batch: 00016/00094 | Loss: 477.2748 | CE: 0.1865 | KD: 1060.1963\n",
      "Train Epoch: 013 Batch: 00017/00094 | Loss: 477.4854 | CE: 0.2211 | KD: 1060.5874\n",
      "Train Epoch: 013 Batch: 00018/00094 | Loss: 477.5670 | CE: 0.2801 | KD: 1060.6377\n",
      "Train Epoch: 013 Batch: 00019/00094 | Loss: 477.5700 | CE: 0.2341 | KD: 1060.7466\n",
      "Train Epoch: 013 Batch: 00020/00094 | Loss: 477.5152 | CE: 0.1834 | KD: 1060.7373\n",
      "Train Epoch: 013 Batch: 00021/00094 | Loss: 477.6369 | CE: 0.1919 | KD: 1060.9888\n",
      "Train Epoch: 013 Batch: 00022/00094 | Loss: 477.3735 | CE: 0.2095 | KD: 1060.3645\n",
      "Train Epoch: 013 Batch: 00023/00094 | Loss: 477.6855 | CE: 0.3171 | KD: 1060.8186\n",
      "Train Epoch: 013 Batch: 00024/00094 | Loss: 477.4879 | CE: 0.2500 | KD: 1060.5288\n",
      "Train Epoch: 013 Batch: 00025/00094 | Loss: 477.5674 | CE: 0.2242 | KD: 1060.7627\n",
      "Train Epoch: 013 Batch: 00026/00094 | Loss: 477.5251 | CE: 0.2175 | KD: 1060.6835\n",
      "Train Epoch: 013 Batch: 00027/00094 | Loss: 477.5781 | CE: 0.2535 | KD: 1060.7213\n",
      "Train Epoch: 013 Batch: 00028/00094 | Loss: 477.6523 | CE: 0.3303 | KD: 1060.7155\n",
      "Train Epoch: 013 Batch: 00029/00094 | Loss: 477.7022 | CE: 0.3125 | KD: 1060.8660\n",
      "Train Epoch: 013 Batch: 00030/00094 | Loss: 477.4591 | CE: 0.2165 | KD: 1060.5392\n",
      "Train Epoch: 013 Batch: 00031/00094 | Loss: 477.3603 | CE: 0.2106 | KD: 1060.3328\n",
      "Train Epoch: 013 Batch: 00032/00094 | Loss: 477.3840 | CE: 0.1959 | KD: 1060.4178\n",
      "Train Epoch: 013 Batch: 00033/00094 | Loss: 477.6339 | CE: 0.3021 | KD: 1060.7373\n",
      "Train Epoch: 013 Batch: 00034/00094 | Loss: 477.6326 | CE: 0.2016 | KD: 1060.9576\n",
      "Train Epoch: 013 Batch: 00035/00094 | Loss: 477.6404 | CE: 0.1998 | KD: 1060.9792\n",
      "Train Epoch: 013 Batch: 00036/00094 | Loss: 477.6309 | CE: 0.2446 | KD: 1060.8584\n",
      "Train Epoch: 013 Batch: 00037/00094 | Loss: 477.5764 | CE: 0.2960 | KD: 1060.6230\n",
      "Train Epoch: 013 Batch: 00038/00094 | Loss: 477.6270 | CE: 0.2809 | KD: 1060.7692\n",
      "Train Epoch: 013 Batch: 00039/00094 | Loss: 477.4656 | CE: 0.2207 | KD: 1060.5442\n",
      "Train Epoch: 013 Batch: 00040/00094 | Loss: 477.6081 | CE: 0.2113 | KD: 1060.8817\n",
      "Train Epoch: 013 Batch: 00041/00094 | Loss: 477.5814 | CE: 0.2293 | KD: 1060.7826\n",
      "Train Epoch: 013 Batch: 00042/00094 | Loss: 477.6989 | CE: 0.2656 | KD: 1060.9629\n",
      "Train Epoch: 013 Batch: 00043/00094 | Loss: 477.5869 | CE: 0.2083 | KD: 1060.8414\n",
      "Train Epoch: 013 Batch: 00044/00094 | Loss: 477.4220 | CE: 0.2061 | KD: 1060.4799\n",
      "Train Epoch: 013 Batch: 00045/00094 | Loss: 477.5886 | CE: 0.2780 | KD: 1060.6903\n",
      "Train Epoch: 013 Batch: 00046/00094 | Loss: 477.4693 | CE: 0.2499 | KD: 1060.4875\n",
      "Train Epoch: 013 Batch: 00047/00094 | Loss: 477.7559 | CE: 0.2607 | KD: 1061.1003\n",
      "Train Epoch: 013 Batch: 00048/00094 | Loss: 477.7897 | CE: 0.2760 | KD: 1061.1415\n",
      "Train Epoch: 013 Batch: 00049/00094 | Loss: 477.5073 | CE: 0.1597 | KD: 1060.7725\n",
      "Train Epoch: 013 Batch: 00050/00094 | Loss: 477.4677 | CE: 0.1751 | KD: 1060.6503\n",
      "Train Epoch: 013 Batch: 00051/00094 | Loss: 477.6949 | CE: 0.3741 | KD: 1060.7128\n",
      "Train Epoch: 013 Batch: 00052/00094 | Loss: 477.4399 | CE: 0.2391 | KD: 1060.4463\n",
      "Train Epoch: 013 Batch: 00053/00094 | Loss: 477.5499 | CE: 0.2129 | KD: 1060.7490\n",
      "Train Epoch: 013 Batch: 00054/00094 | Loss: 477.6504 | CE: 0.2567 | KD: 1060.8749\n",
      "Train Epoch: 013 Batch: 00055/00094 | Loss: 477.6066 | CE: 0.2206 | KD: 1060.8579\n",
      "Train Epoch: 013 Batch: 00056/00094 | Loss: 477.5252 | CE: 0.2259 | KD: 1060.6652\n",
      "Train Epoch: 013 Batch: 00057/00094 | Loss: 477.4331 | CE: 0.1863 | KD: 1060.5485\n",
      "Train Epoch: 013 Batch: 00058/00094 | Loss: 477.4649 | CE: 0.2332 | KD: 1060.5148\n",
      "Train Epoch: 013 Batch: 00059/00094 | Loss: 477.6348 | CE: 0.2352 | KD: 1060.8879\n",
      "Train Epoch: 013 Batch: 00060/00094 | Loss: 477.6078 | CE: 0.2209 | KD: 1060.8596\n",
      "Train Epoch: 013 Batch: 00061/00094 | Loss: 477.5246 | CE: 0.2106 | KD: 1060.6976\n",
      "Train Epoch: 013 Batch: 00062/00094 | Loss: 477.5142 | CE: 0.2210 | KD: 1060.6515\n",
      "Train Epoch: 013 Batch: 00063/00094 | Loss: 477.3468 | CE: 0.1891 | KD: 1060.3506\n",
      "Train Epoch: 013 Batch: 00064/00094 | Loss: 477.6298 | CE: 0.2163 | KD: 1060.9189\n",
      "Train Epoch: 013 Batch: 00065/00094 | Loss: 477.4054 | CE: 0.1612 | KD: 1060.5426\n",
      "Train Epoch: 013 Batch: 00066/00094 | Loss: 477.8067 | CE: 0.2389 | KD: 1061.2616\n",
      "Train Epoch: 013 Batch: 00067/00094 | Loss: 477.4551 | CE: 0.2243 | KD: 1060.5129\n",
      "Train Epoch: 013 Batch: 00068/00094 | Loss: 477.5231 | CE: 0.2302 | KD: 1060.6508\n",
      "Train Epoch: 013 Batch: 00069/00094 | Loss: 477.6257 | CE: 0.2384 | KD: 1060.8606\n",
      "Train Epoch: 013 Batch: 00070/00094 | Loss: 477.4493 | CE: 0.2248 | KD: 1060.4989\n",
      "Train Epoch: 013 Batch: 00071/00094 | Loss: 477.4127 | CE: 0.1666 | KD: 1060.5470\n",
      "Train Epoch: 013 Batch: 00072/00094 | Loss: 477.4666 | CE: 0.1751 | KD: 1060.6477\n",
      "Train Epoch: 013 Batch: 00073/00094 | Loss: 477.3788 | CE: 0.1733 | KD: 1060.4568\n",
      "Train Epoch: 013 Batch: 00074/00094 | Loss: 477.6025 | CE: 0.2999 | KD: 1060.6724\n",
      "Train Epoch: 013 Batch: 00075/00094 | Loss: 477.4582 | CE: 0.1771 | KD: 1060.6246\n",
      "Train Epoch: 013 Batch: 00076/00094 | Loss: 477.3437 | CE: 0.1487 | KD: 1060.4335\n",
      "Train Epoch: 013 Batch: 00077/00094 | Loss: 477.6114 | CE: 0.2796 | KD: 1060.7374\n",
      "Train Epoch: 013 Batch: 00078/00094 | Loss: 477.6552 | CE: 0.2665 | KD: 1060.8638\n",
      "Train Epoch: 013 Batch: 00079/00094 | Loss: 477.4384 | CE: 0.1811 | KD: 1060.5719\n",
      "Train Epoch: 013 Batch: 00080/00094 | Loss: 477.3976 | CE: 0.1727 | KD: 1060.4998\n",
      "Train Epoch: 013 Batch: 00081/00094 | Loss: 477.6738 | CE: 0.2777 | KD: 1060.8802\n",
      "Train Epoch: 013 Batch: 00082/00094 | Loss: 477.5996 | CE: 0.3331 | KD: 1060.5924\n",
      "Train Epoch: 013 Batch: 00083/00094 | Loss: 477.5294 | CE: 0.2027 | KD: 1060.7261\n",
      "Train Epoch: 013 Batch: 00084/00094 | Loss: 477.7275 | CE: 0.3152 | KD: 1060.9164\n",
      "Train Epoch: 013 Batch: 00085/00094 | Loss: 477.6586 | CE: 0.2442 | KD: 1060.9208\n",
      "Train Epoch: 013 Batch: 00086/00094 | Loss: 477.6432 | CE: 0.2101 | KD: 1060.9625\n",
      "Train Epoch: 013 Batch: 00087/00094 | Loss: 477.7648 | CE: 0.3510 | KD: 1060.9196\n",
      "Train Epoch: 013 Batch: 00088/00094 | Loss: 477.6075 | CE: 0.2657 | KD: 1060.7596\n",
      "Train Epoch: 013 Batch: 00089/00094 | Loss: 477.6584 | CE: 0.2415 | KD: 1060.9263\n",
      "Train Epoch: 013 Batch: 00090/00094 | Loss: 477.3958 | CE: 0.1893 | KD: 1060.4589\n",
      "Train Epoch: 013 Batch: 00091/00094 | Loss: 477.7348 | CE: 0.3275 | KD: 1060.9053\n",
      "Train Epoch: 013 Batch: 00092/00094 | Loss: 477.4703 | CE: 0.2023 | KD: 1060.5956\n",
      "Train Epoch: 013 Batch: 00093/00094 | Loss: 477.6979 | CE: 0.2228 | KD: 1061.0558\n",
      "Train Epoch: 013 Batch: 00094/00094 | Loss: 477.5436 | CE: 0.2135 | KD: 1060.7336\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2278 | acc:94.4000\n",
      "[VAL Acc] Target: 94.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6427 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0386 | acc:57.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 57.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9936 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2792 | acc:53.6442\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5704 | acc:69.6858\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0383 | acc:59.9530\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 59.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7411 | acc:62.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.94%\n",
      "[VAL Acc] Avg 61.90%\n",
      "Train Epoch: 014 Batch: 00001/00094 | Loss: 477.5754 | CE: 0.2432 | KD: 1060.7383\n",
      "Train Epoch: 014 Batch: 00002/00094 | Loss: 477.6459 | CE: 0.2417 | KD: 1060.8982\n",
      "Train Epoch: 014 Batch: 00003/00094 | Loss: 477.6248 | CE: 0.3028 | KD: 1060.7157\n",
      "Train Epoch: 014 Batch: 00004/00094 | Loss: 477.6166 | CE: 0.2502 | KD: 1060.8141\n",
      "Train Epoch: 014 Batch: 00005/00094 | Loss: 477.6506 | CE: 0.2876 | KD: 1060.8068\n",
      "Train Epoch: 014 Batch: 00006/00094 | Loss: 477.4713 | CE: 0.2153 | KD: 1060.5687\n",
      "Train Epoch: 014 Batch: 00007/00094 | Loss: 477.6085 | CE: 0.2245 | KD: 1060.8533\n",
      "Train Epoch: 014 Batch: 00008/00094 | Loss: 477.5112 | CE: 0.1931 | KD: 1060.7069\n",
      "Train Epoch: 014 Batch: 00009/00094 | Loss: 477.4747 | CE: 0.1797 | KD: 1060.6556\n",
      "Train Epoch: 014 Batch: 00010/00094 | Loss: 477.4522 | CE: 0.2339 | KD: 1060.4851\n",
      "Train Epoch: 014 Batch: 00011/00094 | Loss: 477.7051 | CE: 0.3663 | KD: 1060.7528\n",
      "Train Epoch: 014 Batch: 00012/00094 | Loss: 477.4748 | CE: 0.1984 | KD: 1060.6141\n",
      "Train Epoch: 014 Batch: 00013/00094 | Loss: 477.7419 | CE: 0.2964 | KD: 1060.9901\n",
      "Train Epoch: 014 Batch: 00014/00094 | Loss: 477.4543 | CE: 0.1919 | KD: 1060.5833\n",
      "Train Epoch: 014 Batch: 00015/00094 | Loss: 477.4865 | CE: 0.2164 | KD: 1060.6001\n",
      "Train Epoch: 014 Batch: 00016/00094 | Loss: 477.5950 | CE: 0.2499 | KD: 1060.7670\n",
      "Train Epoch: 014 Batch: 00017/00094 | Loss: 477.5419 | CE: 0.2695 | KD: 1060.6055\n",
      "Train Epoch: 014 Batch: 00018/00094 | Loss: 477.4314 | CE: 0.2364 | KD: 1060.4333\n",
      "Train Epoch: 014 Batch: 00019/00094 | Loss: 477.4288 | CE: 0.1875 | KD: 1060.5363\n",
      "Train Epoch: 014 Batch: 00020/00094 | Loss: 477.4059 | CE: 0.1746 | KD: 1060.5140\n",
      "Train Epoch: 014 Batch: 00021/00094 | Loss: 477.6584 | CE: 0.2906 | KD: 1060.8171\n",
      "Train Epoch: 014 Batch: 00022/00094 | Loss: 477.4874 | CE: 0.2064 | KD: 1060.6245\n",
      "Train Epoch: 014 Batch: 00023/00094 | Loss: 477.6982 | CE: 0.3121 | KD: 1060.8580\n",
      "Train Epoch: 014 Batch: 00024/00094 | Loss: 477.5958 | CE: 0.2436 | KD: 1060.7827\n",
      "Train Epoch: 014 Batch: 00025/00094 | Loss: 477.5206 | CE: 0.1998 | KD: 1060.7129\n",
      "Train Epoch: 014 Batch: 00026/00094 | Loss: 477.5364 | CE: 0.1760 | KD: 1060.8009\n",
      "Train Epoch: 014 Batch: 00027/00094 | Loss: 477.4511 | CE: 0.1708 | KD: 1060.6230\n",
      "Train Epoch: 014 Batch: 00028/00094 | Loss: 477.4736 | CE: 0.2389 | KD: 1060.5216\n",
      "Train Epoch: 014 Batch: 00029/00094 | Loss: 477.6576 | CE: 0.2678 | KD: 1060.8661\n",
      "Train Epoch: 014 Batch: 00030/00094 | Loss: 477.5724 | CE: 0.2341 | KD: 1060.7517\n",
      "Train Epoch: 014 Batch: 00031/00094 | Loss: 477.5605 | CE: 0.2041 | KD: 1060.7919\n",
      "Train Epoch: 014 Batch: 00032/00094 | Loss: 477.7373 | CE: 0.2825 | KD: 1061.0109\n",
      "Train Epoch: 014 Batch: 00033/00094 | Loss: 477.5591 | CE: 0.2411 | KD: 1060.7067\n",
      "Train Epoch: 014 Batch: 00034/00094 | Loss: 477.4831 | CE: 0.2071 | KD: 1060.6133\n",
      "Train Epoch: 014 Batch: 00035/00094 | Loss: 477.4386 | CE: 0.2043 | KD: 1060.5206\n",
      "Train Epoch: 014 Batch: 00036/00094 | Loss: 477.5265 | CE: 0.2351 | KD: 1060.6476\n",
      "Train Epoch: 014 Batch: 00037/00094 | Loss: 477.5475 | CE: 0.2308 | KD: 1060.7040\n",
      "Train Epoch: 014 Batch: 00038/00094 | Loss: 477.4044 | CE: 0.1767 | KD: 1060.5059\n",
      "Train Epoch: 014 Batch: 00039/00094 | Loss: 477.6367 | CE: 0.3156 | KD: 1060.7137\n",
      "Train Epoch: 014 Batch: 00040/00094 | Loss: 477.5281 | CE: 0.3073 | KD: 1060.4906\n",
      "Train Epoch: 014 Batch: 00041/00094 | Loss: 477.5420 | CE: 0.2214 | KD: 1060.7125\n",
      "Train Epoch: 014 Batch: 00042/00094 | Loss: 477.3996 | CE: 0.2647 | KD: 1060.2997\n",
      "Train Epoch: 014 Batch: 00043/00094 | Loss: 477.3694 | CE: 0.1914 | KD: 1060.3958\n",
      "Train Epoch: 014 Batch: 00044/00094 | Loss: 477.5588 | CE: 0.1880 | KD: 1060.8239\n",
      "Train Epoch: 014 Batch: 00045/00094 | Loss: 477.4933 | CE: 0.1821 | KD: 1060.6917\n",
      "Train Epoch: 014 Batch: 00046/00094 | Loss: 477.5429 | CE: 0.1950 | KD: 1060.7731\n",
      "Train Epoch: 014 Batch: 00047/00094 | Loss: 477.6121 | CE: 0.2195 | KD: 1060.8726\n",
      "Train Epoch: 014 Batch: 00048/00094 | Loss: 477.6374 | CE: 0.2739 | KD: 1060.8079\n",
      "Train Epoch: 014 Batch: 00049/00094 | Loss: 477.5954 | CE: 0.2834 | KD: 1060.6932\n",
      "Train Epoch: 014 Batch: 00050/00094 | Loss: 477.4333 | CE: 0.1614 | KD: 1060.6044\n",
      "Train Epoch: 014 Batch: 00051/00094 | Loss: 477.5518 | CE: 0.2571 | KD: 1060.6550\n",
      "Train Epoch: 014 Batch: 00052/00094 | Loss: 477.5417 | CE: 0.2376 | KD: 1060.6759\n",
      "Train Epoch: 014 Batch: 00053/00094 | Loss: 477.5707 | CE: 0.2120 | KD: 1060.7971\n",
      "Train Epoch: 014 Batch: 00054/00094 | Loss: 477.6611 | CE: 0.2462 | KD: 1060.9221\n",
      "Train Epoch: 014 Batch: 00055/00094 | Loss: 477.4727 | CE: 0.2254 | KD: 1060.5494\n",
      "Train Epoch: 014 Batch: 00056/00094 | Loss: 477.4543 | CE: 0.2205 | KD: 1060.5194\n",
      "Train Epoch: 014 Batch: 00057/00094 | Loss: 477.4230 | CE: 0.1539 | KD: 1060.5979\n",
      "Train Epoch: 014 Batch: 00058/00094 | Loss: 477.5962 | CE: 0.2158 | KD: 1060.8452\n",
      "Train Epoch: 014 Batch: 00059/00094 | Loss: 477.6530 | CE: 0.2596 | KD: 1060.8743\n",
      "Train Epoch: 014 Batch: 00060/00094 | Loss: 477.4888 | CE: 0.1991 | KD: 1060.6438\n",
      "Train Epoch: 014 Batch: 00061/00094 | Loss: 477.6186 | CE: 0.2317 | KD: 1060.8599\n",
      "Train Epoch: 014 Batch: 00062/00094 | Loss: 477.5436 | CE: 0.1902 | KD: 1060.7855\n",
      "Train Epoch: 014 Batch: 00063/00094 | Loss: 477.5303 | CE: 0.1690 | KD: 1060.8030\n",
      "Train Epoch: 014 Batch: 00064/00094 | Loss: 477.4385 | CE: 0.2686 | KD: 1060.3777\n",
      "Train Epoch: 014 Batch: 00065/00094 | Loss: 477.7417 | CE: 0.1962 | KD: 1061.2122\n",
      "Train Epoch: 014 Batch: 00066/00094 | Loss: 477.5616 | CE: 0.2014 | KD: 1060.8004\n",
      "Train Epoch: 014 Batch: 00067/00094 | Loss: 477.4482 | CE: 0.2434 | KD: 1060.4550\n",
      "Train Epoch: 014 Batch: 00068/00094 | Loss: 477.6473 | CE: 0.2643 | KD: 1060.8511\n",
      "Train Epoch: 014 Batch: 00069/00094 | Loss: 477.6177 | CE: 0.2367 | KD: 1060.8468\n",
      "Train Epoch: 014 Batch: 00070/00094 | Loss: 477.5561 | CE: 0.3041 | KD: 1060.5602\n",
      "Train Epoch: 014 Batch: 00071/00094 | Loss: 477.5105 | CE: 0.1755 | KD: 1060.7445\n",
      "Train Epoch: 014 Batch: 00072/00094 | Loss: 477.4424 | CE: 0.1797 | KD: 1060.5837\n",
      "Train Epoch: 014 Batch: 00073/00094 | Loss: 477.5064 | CE: 0.2640 | KD: 1060.5387\n",
      "Train Epoch: 014 Batch: 00074/00094 | Loss: 477.5796 | CE: 0.2145 | KD: 1060.8112\n",
      "Train Epoch: 014 Batch: 00075/00094 | Loss: 477.4593 | CE: 0.2459 | KD: 1060.4741\n",
      "Train Epoch: 014 Batch: 00076/00094 | Loss: 477.5226 | CE: 0.2009 | KD: 1060.7150\n",
      "Train Epoch: 014 Batch: 00077/00094 | Loss: 477.5657 | CE: 0.2738 | KD: 1060.6487\n",
      "Train Epoch: 014 Batch: 00078/00094 | Loss: 477.4665 | CE: 0.1598 | KD: 1060.6814\n",
      "Train Epoch: 014 Batch: 00079/00094 | Loss: 477.4939 | CE: 0.2088 | KD: 1060.6337\n",
      "Train Epoch: 014 Batch: 00080/00094 | Loss: 477.4922 | CE: 0.1866 | KD: 1060.6792\n",
      "Train Epoch: 014 Batch: 00081/00094 | Loss: 477.5780 | CE: 0.2457 | KD: 1060.7384\n",
      "Train Epoch: 014 Batch: 00082/00094 | Loss: 477.5967 | CE: 0.1653 | KD: 1060.9587\n",
      "Train Epoch: 014 Batch: 00083/00094 | Loss: 477.6521 | CE: 0.2548 | KD: 1060.8828\n",
      "Train Epoch: 014 Batch: 00084/00094 | Loss: 477.7033 | CE: 0.2726 | KD: 1060.9572\n",
      "Train Epoch: 014 Batch: 00085/00094 | Loss: 477.5173 | CE: 0.1778 | KD: 1060.7545\n",
      "Train Epoch: 014 Batch: 00086/00094 | Loss: 477.5065 | CE: 0.2256 | KD: 1060.6243\n",
      "Train Epoch: 014 Batch: 00087/00094 | Loss: 477.5300 | CE: 0.2432 | KD: 1060.6375\n",
      "Train Epoch: 014 Batch: 00088/00094 | Loss: 477.7399 | CE: 0.2948 | KD: 1060.9891\n",
      "Train Epoch: 014 Batch: 00089/00094 | Loss: 477.6417 | CE: 0.3113 | KD: 1060.7343\n",
      "Train Epoch: 014 Batch: 00090/00094 | Loss: 477.7416 | CE: 0.2276 | KD: 1061.1423\n",
      "Train Epoch: 014 Batch: 00091/00094 | Loss: 477.5579 | CE: 0.2528 | KD: 1060.6781\n",
      "Train Epoch: 014 Batch: 00092/00094 | Loss: 477.4102 | CE: 0.2097 | KD: 1060.4458\n",
      "Train Epoch: 014 Batch: 00093/00094 | Loss: 477.6003 | CE: 0.2509 | KD: 1060.7764\n",
      "Train Epoch: 014 Batch: 00094/00094 | Loss: 477.5593 | CE: 0.1863 | KD: 1060.8289\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2255 | acc:94.1500\n",
      "[VAL Acc] Target: 94.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6026 | acc:50.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0528 | acc:57.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 57.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9620 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2638 | acc:53.8793\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5650 | acc:70.0555\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9810 | acc:61.9906\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7585 | acc:61.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.88%\n",
      "[VAL Acc] Avg 62.52%\n",
      "Train Epoch: 015 Batch: 00001/00094 | Loss: 429.9159 | CE: 0.2335 | KD: 1060.9442\n",
      "Train Epoch: 015 Batch: 00002/00094 | Loss: 429.8180 | CE: 0.2694 | KD: 1060.6139\n",
      "Train Epoch: 015 Batch: 00003/00094 | Loss: 429.9191 | CE: 0.1983 | KD: 1061.0388\n",
      "Train Epoch: 015 Batch: 00004/00094 | Loss: 429.6557 | CE: 0.1784 | KD: 1060.4377\n",
      "Train Epoch: 015 Batch: 00005/00094 | Loss: 429.7084 | CE: 0.1764 | KD: 1060.5729\n",
      "Train Epoch: 015 Batch: 00006/00094 | Loss: 429.9681 | CE: 0.2140 | KD: 1061.1212\n",
      "Train Epoch: 015 Batch: 00007/00094 | Loss: 429.7460 | CE: 0.1867 | KD: 1060.6403\n",
      "Train Epoch: 015 Batch: 00008/00094 | Loss: 429.7516 | CE: 0.2012 | KD: 1060.6183\n",
      "Train Epoch: 015 Batch: 00009/00094 | Loss: 429.8074 | CE: 0.2256 | KD: 1060.6958\n",
      "Train Epoch: 015 Batch: 00010/00094 | Loss: 429.9217 | CE: 0.2766 | KD: 1060.8522\n",
      "Train Epoch: 015 Batch: 00011/00094 | Loss: 429.7626 | CE: 0.1608 | KD: 1060.7451\n",
      "Train Epoch: 015 Batch: 00012/00094 | Loss: 429.8256 | CE: 0.1987 | KD: 1060.8071\n",
      "Train Epoch: 015 Batch: 00013/00094 | Loss: 429.8605 | CE: 0.2685 | KD: 1060.7212\n",
      "Train Epoch: 015 Batch: 00014/00094 | Loss: 429.7951 | CE: 0.1724 | KD: 1060.7966\n",
      "Train Epoch: 015 Batch: 00015/00094 | Loss: 429.8082 | CE: 0.2084 | KD: 1060.7402\n",
      "Train Epoch: 015 Batch: 00016/00094 | Loss: 429.8199 | CE: 0.1911 | KD: 1060.8119\n",
      "Train Epoch: 015 Batch: 00017/00094 | Loss: 429.9199 | CE: 0.2755 | KD: 1060.8503\n",
      "Train Epoch: 015 Batch: 00018/00094 | Loss: 429.9583 | CE: 0.2395 | KD: 1061.0343\n",
      "Train Epoch: 015 Batch: 00019/00094 | Loss: 430.0479 | CE: 0.2274 | KD: 1061.2852\n",
      "Train Epoch: 015 Batch: 00020/00094 | Loss: 429.7801 | CE: 0.2262 | KD: 1060.6267\n",
      "Train Epoch: 015 Batch: 00021/00094 | Loss: 429.8172 | CE: 0.1888 | KD: 1060.8109\n",
      "Train Epoch: 015 Batch: 00022/00094 | Loss: 429.7267 | CE: 0.1643 | KD: 1060.6478\n",
      "Train Epoch: 015 Batch: 00023/00094 | Loss: 429.9054 | CE: 0.1898 | KD: 1061.0261\n",
      "Train Epoch: 015 Batch: 00024/00094 | Loss: 429.7857 | CE: 0.2133 | KD: 1060.6726\n",
      "Train Epoch: 015 Batch: 00025/00094 | Loss: 429.7341 | CE: 0.1644 | KD: 1060.6658\n",
      "Train Epoch: 015 Batch: 00026/00094 | Loss: 429.7081 | CE: 0.1914 | KD: 1060.5349\n",
      "Train Epoch: 015 Batch: 00027/00094 | Loss: 429.7586 | CE: 0.2329 | KD: 1060.5571\n",
      "Train Epoch: 015 Batch: 00028/00094 | Loss: 430.0925 | CE: 0.3688 | KD: 1061.0463\n",
      "Train Epoch: 015 Batch: 00029/00094 | Loss: 429.8769 | CE: 0.2418 | KD: 1060.8273\n",
      "Train Epoch: 015 Batch: 00030/00094 | Loss: 429.8794 | CE: 0.2238 | KD: 1060.8781\n",
      "Train Epoch: 015 Batch: 00031/00094 | Loss: 429.7256 | CE: 0.1920 | KD: 1060.5768\n",
      "Train Epoch: 015 Batch: 00032/00094 | Loss: 429.8330 | CE: 0.2477 | KD: 1060.7046\n",
      "Train Epoch: 015 Batch: 00033/00094 | Loss: 429.9619 | CE: 0.2447 | KD: 1061.0302\n",
      "Train Epoch: 015 Batch: 00034/00094 | Loss: 429.9702 | CE: 0.2304 | KD: 1061.0859\n",
      "Train Epoch: 015 Batch: 00035/00094 | Loss: 429.6984 | CE: 0.2124 | KD: 1060.4590\n",
      "Train Epoch: 015 Batch: 00036/00094 | Loss: 429.8397 | CE: 0.1979 | KD: 1060.8439\n",
      "Train Epoch: 015 Batch: 00037/00094 | Loss: 429.7780 | CE: 0.2154 | KD: 1060.6484\n",
      "Train Epoch: 015 Batch: 00038/00094 | Loss: 429.8418 | CE: 0.2179 | KD: 1060.7997\n",
      "Train Epoch: 015 Batch: 00039/00094 | Loss: 429.7863 | CE: 0.2151 | KD: 1060.6696\n",
      "Train Epoch: 015 Batch: 00040/00094 | Loss: 429.8640 | CE: 0.1940 | KD: 1060.9135\n",
      "Train Epoch: 015 Batch: 00041/00094 | Loss: 429.7102 | CE: 0.1795 | KD: 1060.5697\n",
      "Train Epoch: 015 Batch: 00042/00094 | Loss: 429.7600 | CE: 0.1903 | KD: 1060.6659\n",
      "Train Epoch: 015 Batch: 00043/00094 | Loss: 429.6770 | CE: 0.1925 | KD: 1060.4556\n",
      "Train Epoch: 015 Batch: 00044/00094 | Loss: 429.7365 | CE: 0.2245 | KD: 1060.5232\n",
      "Train Epoch: 015 Batch: 00045/00094 | Loss: 429.7354 | CE: 0.2218 | KD: 1060.5276\n",
      "Train Epoch: 015 Batch: 00046/00094 | Loss: 429.7795 | CE: 0.2373 | KD: 1060.5979\n",
      "Train Epoch: 015 Batch: 00047/00094 | Loss: 429.6977 | CE: 0.1783 | KD: 1060.5416\n",
      "Train Epoch: 015 Batch: 00048/00094 | Loss: 429.9064 | CE: 0.2615 | KD: 1060.8514\n",
      "Train Epoch: 015 Batch: 00049/00094 | Loss: 429.6805 | CE: 0.2087 | KD: 1060.4242\n",
      "Train Epoch: 015 Batch: 00050/00094 | Loss: 429.9363 | CE: 0.4086 | KD: 1060.5623\n",
      "Train Epoch: 015 Batch: 00051/00094 | Loss: 429.7430 | CE: 0.1813 | KD: 1060.6462\n",
      "Train Epoch: 015 Batch: 00052/00094 | Loss: 429.9045 | CE: 0.2052 | KD: 1060.9858\n",
      "Train Epoch: 015 Batch: 00053/00094 | Loss: 429.7475 | CE: 0.1658 | KD: 1060.6956\n",
      "Train Epoch: 015 Batch: 00054/00094 | Loss: 429.9164 | CE: 0.1917 | KD: 1061.0485\n",
      "Train Epoch: 015 Batch: 00055/00094 | Loss: 429.7293 | CE: 0.2210 | KD: 1060.5143\n",
      "Train Epoch: 015 Batch: 00056/00094 | Loss: 429.8719 | CE: 0.2174 | KD: 1060.8755\n",
      "Train Epoch: 015 Batch: 00057/00094 | Loss: 429.8945 | CE: 0.2584 | KD: 1060.8298\n",
      "Train Epoch: 015 Batch: 00058/00094 | Loss: 429.8596 | CE: 0.2674 | KD: 1060.7216\n",
      "Train Epoch: 015 Batch: 00059/00094 | Loss: 429.6276 | CE: 0.1769 | KD: 1060.3719\n",
      "Train Epoch: 015 Batch: 00060/00094 | Loss: 429.7404 | CE: 0.2017 | KD: 1060.5895\n",
      "Train Epoch: 015 Batch: 00061/00094 | Loss: 429.9491 | CE: 0.2406 | KD: 1061.0085\n",
      "Train Epoch: 015 Batch: 00062/00094 | Loss: 429.7116 | CE: 0.2356 | KD: 1060.4347\n",
      "Train Epoch: 015 Batch: 00063/00094 | Loss: 429.9168 | CE: 0.2910 | KD: 1060.8043\n",
      "Train Epoch: 015 Batch: 00064/00094 | Loss: 429.6125 | CE: 0.1706 | KD: 1060.3503\n",
      "Train Epoch: 015 Batch: 00065/00094 | Loss: 429.9217 | CE: 0.2424 | KD: 1060.9365\n",
      "Train Epoch: 015 Batch: 00066/00094 | Loss: 429.9600 | CE: 0.3063 | KD: 1060.8734\n",
      "Train Epoch: 015 Batch: 00067/00094 | Loss: 429.7649 | CE: 0.1661 | KD: 1060.7379\n",
      "Train Epoch: 015 Batch: 00068/00094 | Loss: 429.7010 | CE: 0.2261 | KD: 1060.4319\n",
      "Train Epoch: 015 Batch: 00069/00094 | Loss: 429.9580 | CE: 0.2577 | KD: 1060.9884\n",
      "Train Epoch: 015 Batch: 00070/00094 | Loss: 429.9732 | CE: 0.2775 | KD: 1060.9769\n",
      "Train Epoch: 015 Batch: 00071/00094 | Loss: 429.9037 | CE: 0.2770 | KD: 1060.8066\n",
      "Train Epoch: 015 Batch: 00072/00094 | Loss: 429.9510 | CE: 0.2476 | KD: 1060.9961\n",
      "Train Epoch: 015 Batch: 00073/00094 | Loss: 429.8924 | CE: 0.2161 | KD: 1060.9291\n",
      "Train Epoch: 015 Batch: 00074/00094 | Loss: 429.9187 | CE: 0.2229 | KD: 1060.9774\n",
      "Train Epoch: 015 Batch: 00075/00094 | Loss: 429.7565 | CE: 0.2086 | KD: 1060.6119\n",
      "Train Epoch: 015 Batch: 00076/00094 | Loss: 429.8790 | CE: 0.2409 | KD: 1060.8347\n",
      "Train Epoch: 015 Batch: 00077/00094 | Loss: 429.6956 | CE: 0.1782 | KD: 1060.5369\n",
      "Train Epoch: 015 Batch: 00078/00094 | Loss: 429.8154 | CE: 0.2819 | KD: 1060.5765\n",
      "Train Epoch: 015 Batch: 00079/00094 | Loss: 429.8942 | CE: 0.2237 | KD: 1060.9148\n",
      "Train Epoch: 015 Batch: 00080/00094 | Loss: 429.6808 | CE: 0.1747 | KD: 1060.5090\n",
      "Train Epoch: 015 Batch: 00081/00094 | Loss: 429.7384 | CE: 0.1708 | KD: 1060.6608\n",
      "Train Epoch: 015 Batch: 00082/00094 | Loss: 429.7563 | CE: 0.1801 | KD: 1060.6819\n",
      "Train Epoch: 015 Batch: 00083/00094 | Loss: 429.9416 | CE: 0.3587 | KD: 1060.6986\n",
      "Train Epoch: 015 Batch: 00084/00094 | Loss: 429.8296 | CE: 0.2597 | KD: 1060.6665\n",
      "Train Epoch: 015 Batch: 00085/00094 | Loss: 429.7061 | CE: 0.1744 | KD: 1060.5721\n",
      "Train Epoch: 015 Batch: 00086/00094 | Loss: 429.7007 | CE: 0.1960 | KD: 1060.5052\n",
      "Train Epoch: 015 Batch: 00087/00094 | Loss: 429.7008 | CE: 0.2115 | KD: 1060.4673\n",
      "Train Epoch: 015 Batch: 00088/00094 | Loss: 429.8686 | CE: 0.1949 | KD: 1060.9227\n",
      "Train Epoch: 015 Batch: 00089/00094 | Loss: 429.7522 | CE: 0.1701 | KD: 1060.6964\n",
      "Train Epoch: 015 Batch: 00090/00094 | Loss: 429.6804 | CE: 0.1676 | KD: 1060.5255\n",
      "Train Epoch: 015 Batch: 00091/00094 | Loss: 429.9879 | CE: 0.2622 | KD: 1061.0513\n",
      "Train Epoch: 015 Batch: 00092/00094 | Loss: 429.9124 | CE: 0.2323 | KD: 1060.9385\n",
      "Train Epoch: 015 Batch: 00093/00094 | Loss: 429.7846 | CE: 0.2372 | KD: 1060.6107\n",
      "Train Epoch: 015 Batch: 00094/00094 | Loss: 429.8606 | CE: 0.1529 | KD: 1061.0066\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2153 | acc:94.4500\n",
      "[VAL Acc] Target: 94.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5568 | acc:50.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0047 | acc:58.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 58.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9553 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3184 | acc:52.1944\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5777 | acc:70.7948\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0146 | acc:59.7179\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 59.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7521 | acc:61.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.12%\n",
      "[VAL Acc] Avg 62.18%\n",
      "Train Epoch: 016 Batch: 00001/00094 | Loss: 429.8617 | CE: 0.2072 | KD: 1060.8754\n",
      "Train Epoch: 016 Batch: 00002/00094 | Loss: 429.9089 | CE: 0.3391 | KD: 1060.6664\n",
      "Train Epoch: 016 Batch: 00003/00094 | Loss: 429.7598 | CE: 0.1848 | KD: 1060.6790\n",
      "Train Epoch: 016 Batch: 00004/00094 | Loss: 429.7420 | CE: 0.2155 | KD: 1060.5594\n",
      "Train Epoch: 016 Batch: 00005/00094 | Loss: 429.6922 | CE: 0.1803 | KD: 1060.5232\n",
      "Train Epoch: 016 Batch: 00006/00094 | Loss: 429.9038 | CE: 0.2160 | KD: 1060.9576\n",
      "Train Epoch: 016 Batch: 00007/00094 | Loss: 429.7575 | CE: 0.2080 | KD: 1060.6160\n",
      "Train Epoch: 016 Batch: 00008/00094 | Loss: 429.8020 | CE: 0.2013 | KD: 1060.7426\n",
      "Train Epoch: 016 Batch: 00009/00094 | Loss: 429.6150 | CE: 0.1879 | KD: 1060.3137\n",
      "Train Epoch: 016 Batch: 00010/00094 | Loss: 429.7935 | CE: 0.2015 | KD: 1060.7208\n",
      "Train Epoch: 016 Batch: 00011/00094 | Loss: 429.7820 | CE: 0.1840 | KD: 1060.7360\n",
      "Train Epoch: 016 Batch: 00012/00094 | Loss: 429.8040 | CE: 0.1644 | KD: 1060.8385\n",
      "Train Epoch: 016 Batch: 00013/00094 | Loss: 429.8312 | CE: 0.2004 | KD: 1060.8167\n",
      "Train Epoch: 016 Batch: 00014/00094 | Loss: 429.6811 | CE: 0.2064 | KD: 1060.4312\n",
      "Train Epoch: 016 Batch: 00015/00094 | Loss: 429.7781 | CE: 0.2024 | KD: 1060.6809\n",
      "Train Epoch: 016 Batch: 00016/00094 | Loss: 429.7048 | CE: 0.1610 | KD: 1060.6019\n",
      "Train Epoch: 016 Batch: 00017/00094 | Loss: 429.8137 | CE: 0.2570 | KD: 1060.6337\n",
      "Train Epoch: 016 Batch: 00018/00094 | Loss: 429.8737 | CE: 0.2346 | KD: 1060.8373\n",
      "Train Epoch: 016 Batch: 00019/00094 | Loss: 429.8101 | CE: 0.2072 | KD: 1060.7478\n",
      "Train Epoch: 016 Batch: 00020/00094 | Loss: 429.9253 | CE: 0.2605 | KD: 1060.9006\n",
      "Train Epoch: 016 Batch: 00021/00094 | Loss: 429.9263 | CE: 0.2531 | KD: 1060.9214\n",
      "Train Epoch: 016 Batch: 00022/00094 | Loss: 429.6537 | CE: 0.1717 | KD: 1060.4492\n",
      "Train Epoch: 016 Batch: 00023/00094 | Loss: 429.7872 | CE: 0.1962 | KD: 1060.7185\n",
      "Train Epoch: 016 Batch: 00024/00094 | Loss: 429.7981 | CE: 0.1669 | KD: 1060.8179\n",
      "Train Epoch: 016 Batch: 00025/00094 | Loss: 429.7497 | CE: 0.1787 | KD: 1060.6692\n",
      "Train Epoch: 016 Batch: 00026/00094 | Loss: 429.7394 | CE: 0.1907 | KD: 1060.6141\n",
      "Train Epoch: 016 Batch: 00027/00094 | Loss: 429.7871 | CE: 0.2343 | KD: 1060.6240\n",
      "Train Epoch: 016 Batch: 00028/00094 | Loss: 429.8439 | CE: 0.2141 | KD: 1060.8142\n",
      "Train Epoch: 016 Batch: 00029/00094 | Loss: 429.8029 | CE: 0.2564 | KD: 1060.6085\n",
      "Train Epoch: 016 Batch: 00030/00094 | Loss: 429.6902 | CE: 0.1717 | KD: 1060.5397\n",
      "Train Epoch: 016 Batch: 00031/00094 | Loss: 429.7851 | CE: 0.2140 | KD: 1060.6693\n",
      "Train Epoch: 016 Batch: 00032/00094 | Loss: 429.8806 | CE: 0.2684 | KD: 1060.7709\n",
      "Train Epoch: 016 Batch: 00033/00094 | Loss: 429.8327 | CE: 0.2726 | KD: 1060.6423\n",
      "Train Epoch: 016 Batch: 00034/00094 | Loss: 429.7916 | CE: 0.2370 | KD: 1060.6285\n",
      "Train Epoch: 016 Batch: 00035/00094 | Loss: 429.9352 | CE: 0.2358 | KD: 1060.9863\n",
      "Train Epoch: 016 Batch: 00036/00094 | Loss: 429.6187 | CE: 0.1749 | KD: 1060.3552\n",
      "Train Epoch: 016 Batch: 00037/00094 | Loss: 429.9855 | CE: 0.3736 | KD: 1060.7703\n",
      "Train Epoch: 016 Batch: 00038/00094 | Loss: 429.6063 | CE: 0.1631 | KD: 1060.3535\n",
      "Train Epoch: 016 Batch: 00039/00094 | Loss: 429.7396 | CE: 0.1995 | KD: 1060.5928\n",
      "Train Epoch: 016 Batch: 00040/00094 | Loss: 429.7469 | CE: 0.2127 | KD: 1060.5785\n",
      "Train Epoch: 016 Batch: 00041/00094 | Loss: 429.9395 | CE: 0.2603 | KD: 1060.9364\n",
      "Train Epoch: 016 Batch: 00042/00094 | Loss: 429.8442 | CE: 0.1688 | KD: 1060.9270\n",
      "Train Epoch: 016 Batch: 00043/00094 | Loss: 429.7704 | CE: 0.1952 | KD: 1060.6794\n",
      "Train Epoch: 016 Batch: 00044/00094 | Loss: 429.7833 | CE: 0.1853 | KD: 1060.7356\n",
      "Train Epoch: 016 Batch: 00045/00094 | Loss: 429.6887 | CE: 0.1960 | KD: 1060.4757\n",
      "Train Epoch: 016 Batch: 00046/00094 | Loss: 429.8083 | CE: 0.2243 | KD: 1060.7010\n",
      "Train Epoch: 016 Batch: 00047/00094 | Loss: 429.7392 | CE: 0.2187 | KD: 1060.5444\n",
      "Train Epoch: 016 Batch: 00048/00094 | Loss: 429.7993 | CE: 0.2351 | KD: 1060.6522\n",
      "Train Epoch: 016 Batch: 00049/00094 | Loss: 429.6703 | CE: 0.2112 | KD: 1060.3928\n",
      "Train Epoch: 016 Batch: 00050/00094 | Loss: 429.7426 | CE: 0.1688 | KD: 1060.6761\n",
      "Train Epoch: 016 Batch: 00051/00094 | Loss: 429.8309 | CE: 0.2309 | KD: 1060.7407\n",
      "Train Epoch: 016 Batch: 00052/00094 | Loss: 429.8403 | CE: 0.2046 | KD: 1060.8289\n",
      "Train Epoch: 016 Batch: 00053/00094 | Loss: 429.7288 | CE: 0.2234 | KD: 1060.5071\n",
      "Train Epoch: 016 Batch: 00054/00094 | Loss: 429.8108 | CE: 0.2621 | KD: 1060.6141\n",
      "Train Epoch: 016 Batch: 00055/00094 | Loss: 429.6738 | CE: 0.2062 | KD: 1060.4139\n",
      "Train Epoch: 016 Batch: 00056/00094 | Loss: 429.7934 | CE: 0.1516 | KD: 1060.8440\n",
      "Train Epoch: 016 Batch: 00057/00094 | Loss: 429.8387 | CE: 0.1782 | KD: 1060.8901\n",
      "Train Epoch: 016 Batch: 00058/00094 | Loss: 429.7783 | CE: 0.1704 | KD: 1060.7603\n",
      "Train Epoch: 016 Batch: 00059/00094 | Loss: 429.7286 | CE: 0.2471 | KD: 1060.4481\n",
      "Train Epoch: 016 Batch: 00060/00094 | Loss: 429.9106 | CE: 0.1838 | KD: 1061.0537\n",
      "Train Epoch: 016 Batch: 00061/00094 | Loss: 429.6708 | CE: 0.1721 | KD: 1060.4906\n",
      "Train Epoch: 016 Batch: 00062/00094 | Loss: 429.7032 | CE: 0.1759 | KD: 1060.5615\n",
      "Train Epoch: 016 Batch: 00063/00094 | Loss: 429.8391 | CE: 0.2628 | KD: 1060.6823\n",
      "Train Epoch: 016 Batch: 00064/00094 | Loss: 429.9517 | CE: 0.2351 | KD: 1061.0284\n",
      "Train Epoch: 016 Batch: 00065/00094 | Loss: 429.7321 | CE: 0.1855 | KD: 1060.6089\n",
      "Train Epoch: 016 Batch: 00066/00094 | Loss: 429.6692 | CE: 0.1510 | KD: 1060.5387\n",
      "Train Epoch: 016 Batch: 00067/00094 | Loss: 429.6885 | CE: 0.1739 | KD: 1060.5299\n",
      "Train Epoch: 016 Batch: 00068/00094 | Loss: 429.6942 | CE: 0.2069 | KD: 1060.4625\n",
      "Train Epoch: 016 Batch: 00069/00094 | Loss: 429.9703 | CE: 0.2917 | KD: 1060.9349\n",
      "Train Epoch: 016 Batch: 00070/00094 | Loss: 429.9410 | CE: 0.3140 | KD: 1060.8074\n",
      "Train Epoch: 016 Batch: 00071/00094 | Loss: 429.7422 | CE: 0.1887 | KD: 1060.6261\n",
      "Train Epoch: 016 Batch: 00072/00094 | Loss: 429.8855 | CE: 0.1833 | KD: 1060.9930\n",
      "Train Epoch: 016 Batch: 00073/00094 | Loss: 429.8372 | CE: 0.2106 | KD: 1060.8065\n",
      "Train Epoch: 016 Batch: 00074/00094 | Loss: 429.8012 | CE: 0.2488 | KD: 1060.6233\n",
      "Train Epoch: 016 Batch: 00075/00094 | Loss: 429.7725 | CE: 0.2411 | KD: 1060.5712\n",
      "Train Epoch: 016 Batch: 00076/00094 | Loss: 429.8091 | CE: 0.1969 | KD: 1060.7710\n",
      "Train Epoch: 016 Batch: 00077/00094 | Loss: 429.8949 | CE: 0.2647 | KD: 1060.8152\n",
      "Train Epoch: 016 Batch: 00078/00094 | Loss: 430.0284 | CE: 0.3183 | KD: 1061.0125\n",
      "Train Epoch: 016 Batch: 00079/00094 | Loss: 429.7336 | CE: 0.2278 | KD: 1060.5082\n",
      "Train Epoch: 016 Batch: 00080/00094 | Loss: 429.6354 | CE: 0.1760 | KD: 1060.3937\n",
      "Train Epoch: 016 Batch: 00081/00094 | Loss: 429.8579 | CE: 0.1531 | KD: 1060.9994\n",
      "Train Epoch: 016 Batch: 00082/00094 | Loss: 429.9461 | CE: 0.2617 | KD: 1060.9491\n",
      "Train Epoch: 016 Batch: 00083/00094 | Loss: 429.7027 | CE: 0.2341 | KD: 1060.4161\n",
      "Train Epoch: 016 Batch: 00084/00094 | Loss: 429.9461 | CE: 0.2456 | KD: 1060.9889\n",
      "Train Epoch: 016 Batch: 00085/00094 | Loss: 429.9059 | CE: 0.2394 | KD: 1060.9049\n",
      "Train Epoch: 016 Batch: 00086/00094 | Loss: 429.9203 | CE: 0.2602 | KD: 1060.8892\n",
      "Train Epoch: 016 Batch: 00087/00094 | Loss: 429.7693 | CE: 0.2016 | KD: 1060.6609\n",
      "Train Epoch: 016 Batch: 00088/00094 | Loss: 429.7655 | CE: 0.2034 | KD: 1060.6472\n",
      "Train Epoch: 016 Batch: 00089/00094 | Loss: 429.9143 | CE: 0.2899 | KD: 1060.8010\n",
      "Train Epoch: 016 Batch: 00090/00094 | Loss: 429.8157 | CE: 0.2046 | KD: 1060.7682\n",
      "Train Epoch: 016 Batch: 00091/00094 | Loss: 429.7855 | CE: 0.2085 | KD: 1060.6838\n",
      "Train Epoch: 016 Batch: 00092/00094 | Loss: 429.8119 | CE: 0.2567 | KD: 1060.6302\n",
      "Train Epoch: 016 Batch: 00093/00094 | Loss: 429.9472 | CE: 0.2827 | KD: 1060.8999\n",
      "Train Epoch: 016 Batch: 00094/00094 | Loss: 429.8160 | CE: 0.2621 | KD: 1060.6271\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2075 | acc:94.3500\n",
      "[VAL Acc] Target: 94.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6020 | acc:50.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0532 | acc:58.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 58.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0118 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2730 | acc:53.0172\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5657 | acc:70.3327\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9725 | acc:61.9906\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7264 | acc:62.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.25%\n",
      "[VAL Acc] Avg 62.37%\n",
      "Train Epoch: 017 Batch: 00001/00094 | Loss: 429.9297 | CE: 0.2870 | KD: 1060.8462\n",
      "Train Epoch: 017 Batch: 00002/00094 | Loss: 430.1572 | CE: 0.5223 | KD: 1060.8270\n",
      "Train Epoch: 017 Batch: 00003/00094 | Loss: 429.7520 | CE: 0.1767 | KD: 1060.6797\n",
      "Train Epoch: 017 Batch: 00004/00094 | Loss: 429.8104 | CE: 0.2227 | KD: 1060.7103\n",
      "Train Epoch: 017 Batch: 00005/00094 | Loss: 429.6970 | CE: 0.1739 | KD: 1060.5508\n",
      "Train Epoch: 017 Batch: 00006/00094 | Loss: 429.7890 | CE: 0.1947 | KD: 1060.7267\n",
      "Train Epoch: 017 Batch: 00007/00094 | Loss: 430.0260 | CE: 0.2200 | KD: 1061.2495\n",
      "Train Epoch: 017 Batch: 00008/00094 | Loss: 429.7534 | CE: 0.1691 | KD: 1060.7019\n",
      "Train Epoch: 017 Batch: 00009/00094 | Loss: 429.9017 | CE: 0.4089 | KD: 1060.4761\n",
      "Train Epoch: 017 Batch: 00010/00094 | Loss: 429.7674 | CE: 0.2068 | KD: 1060.6436\n",
      "Train Epoch: 017 Batch: 00011/00094 | Loss: 429.8192 | CE: 0.1991 | KD: 1060.7904\n",
      "Train Epoch: 017 Batch: 00012/00094 | Loss: 429.8036 | CE: 0.1968 | KD: 1060.7574\n",
      "Train Epoch: 017 Batch: 00013/00094 | Loss: 429.8065 | CE: 0.2217 | KD: 1060.7034\n",
      "Train Epoch: 017 Batch: 00014/00094 | Loss: 429.7765 | CE: 0.1965 | KD: 1060.6912\n",
      "Train Epoch: 017 Batch: 00015/00094 | Loss: 429.8766 | CE: 0.1946 | KD: 1060.9430\n",
      "Train Epoch: 017 Batch: 00016/00094 | Loss: 429.9074 | CE: 0.2703 | KD: 1060.8322\n",
      "Train Epoch: 017 Batch: 00017/00094 | Loss: 429.7431 | CE: 0.1832 | KD: 1060.6418\n",
      "Train Epoch: 017 Batch: 00018/00094 | Loss: 429.8412 | CE: 0.1650 | KD: 1060.9288\n",
      "Train Epoch: 017 Batch: 00019/00094 | Loss: 429.7534 | CE: 0.2176 | KD: 1060.5823\n",
      "Train Epoch: 017 Batch: 00020/00094 | Loss: 429.6502 | CE: 0.1775 | KD: 1060.4264\n",
      "Train Epoch: 017 Batch: 00021/00094 | Loss: 429.7743 | CE: 0.2376 | KD: 1060.5844\n",
      "Train Epoch: 017 Batch: 00022/00094 | Loss: 429.9109 | CE: 0.1951 | KD: 1061.0266\n",
      "Train Epoch: 017 Batch: 00023/00094 | Loss: 429.7527 | CE: 0.1900 | KD: 1060.6488\n",
      "Train Epoch: 017 Batch: 00024/00094 | Loss: 429.7627 | CE: 0.1709 | KD: 1060.7205\n",
      "Train Epoch: 017 Batch: 00025/00094 | Loss: 429.8636 | CE: 0.2072 | KD: 1060.8798\n",
      "Train Epoch: 017 Batch: 00026/00094 | Loss: 429.8525 | CE: 0.2005 | KD: 1060.8691\n",
      "Train Epoch: 017 Batch: 00027/00094 | Loss: 429.7831 | CE: 0.1936 | KD: 1060.7147\n",
      "Train Epoch: 017 Batch: 00028/00094 | Loss: 429.8675 | CE: 0.2472 | KD: 1060.7909\n",
      "Train Epoch: 017 Batch: 00029/00094 | Loss: 429.6663 | CE: 0.2139 | KD: 1060.3763\n",
      "Train Epoch: 017 Batch: 00030/00094 | Loss: 429.8099 | CE: 0.2143 | KD: 1060.7299\n",
      "Train Epoch: 017 Batch: 00031/00094 | Loss: 429.8024 | CE: 0.2397 | KD: 1060.6487\n",
      "Train Epoch: 017 Batch: 00032/00094 | Loss: 429.8996 | CE: 0.1975 | KD: 1060.9928\n",
      "Train Epoch: 017 Batch: 00033/00094 | Loss: 429.7352 | CE: 0.2215 | KD: 1060.5277\n",
      "Train Epoch: 017 Batch: 00034/00094 | Loss: 429.8916 | CE: 0.2482 | KD: 1060.8479\n",
      "Train Epoch: 017 Batch: 00035/00094 | Loss: 429.8822 | CE: 0.2335 | KD: 1060.8610\n",
      "Train Epoch: 017 Batch: 00036/00094 | Loss: 429.8283 | CE: 0.2142 | KD: 1060.7755\n",
      "Train Epoch: 017 Batch: 00037/00094 | Loss: 429.8249 | CE: 0.2309 | KD: 1060.7261\n",
      "Train Epoch: 017 Batch: 00038/00094 | Loss: 429.8552 | CE: 0.2209 | KD: 1060.8254\n",
      "Train Epoch: 017 Batch: 00039/00094 | Loss: 429.8784 | CE: 0.1591 | KD: 1061.0353\n",
      "Train Epoch: 017 Batch: 00040/00094 | Loss: 429.6313 | CE: 0.1487 | KD: 1060.4509\n",
      "Train Epoch: 017 Batch: 00041/00094 | Loss: 429.6895 | CE: 0.2070 | KD: 1060.4507\n",
      "Train Epoch: 017 Batch: 00042/00094 | Loss: 429.9088 | CE: 0.2246 | KD: 1060.9487\n",
      "Train Epoch: 017 Batch: 00043/00094 | Loss: 429.7115 | CE: 0.1973 | KD: 1060.5289\n",
      "Train Epoch: 017 Batch: 00044/00094 | Loss: 429.8544 | CE: 0.2284 | KD: 1060.8049\n",
      "Train Epoch: 017 Batch: 00045/00094 | Loss: 429.8652 | CE: 0.2152 | KD: 1060.8640\n",
      "Train Epoch: 017 Batch: 00046/00094 | Loss: 429.8258 | CE: 0.1996 | KD: 1060.8053\n",
      "Train Epoch: 017 Batch: 00047/00094 | Loss: 429.7614 | CE: 0.2664 | KD: 1060.4817\n",
      "Train Epoch: 017 Batch: 00048/00094 | Loss: 429.8126 | CE: 0.1457 | KD: 1060.9059\n",
      "Train Epoch: 017 Batch: 00049/00094 | Loss: 429.6785 | CE: 0.1965 | KD: 1060.4493\n",
      "Train Epoch: 017 Batch: 00050/00094 | Loss: 429.7553 | CE: 0.1923 | KD: 1060.6494\n",
      "Train Epoch: 017 Batch: 00051/00094 | Loss: 429.8180 | CE: 0.2184 | KD: 1060.7396\n",
      "Train Epoch: 017 Batch: 00052/00094 | Loss: 429.7787 | CE: 0.1975 | KD: 1060.6942\n",
      "Train Epoch: 017 Batch: 00053/00094 | Loss: 429.6454 | CE: 0.1367 | KD: 1060.5154\n",
      "Train Epoch: 017 Batch: 00054/00094 | Loss: 429.8951 | CE: 0.2281 | KD: 1060.9064\n",
      "Train Epoch: 017 Batch: 00055/00094 | Loss: 430.0497 | CE: 0.3971 | KD: 1060.8706\n",
      "Train Epoch: 017 Batch: 00056/00094 | Loss: 429.8828 | CE: 0.1768 | KD: 1061.0027\n",
      "Train Epoch: 017 Batch: 00057/00094 | Loss: 429.6705 | CE: 0.1852 | KD: 1060.4574\n",
      "Train Epoch: 017 Batch: 00058/00094 | Loss: 430.0019 | CE: 0.2992 | KD: 1060.9943\n",
      "Train Epoch: 017 Batch: 00059/00094 | Loss: 429.8942 | CE: 0.2206 | KD: 1060.9225\n",
      "Train Epoch: 017 Batch: 00060/00094 | Loss: 429.8649 | CE: 0.2812 | KD: 1060.7004\n",
      "Train Epoch: 017 Batch: 00061/00094 | Loss: 429.7896 | CE: 0.2084 | KD: 1060.6945\n",
      "Train Epoch: 017 Batch: 00062/00094 | Loss: 429.8817 | CE: 0.2026 | KD: 1060.9363\n",
      "Train Epoch: 017 Batch: 00063/00094 | Loss: 429.6626 | CE: 0.1973 | KD: 1060.4082\n",
      "Train Epoch: 017 Batch: 00064/00094 | Loss: 429.7327 | CE: 0.1972 | KD: 1060.5813\n",
      "Train Epoch: 017 Batch: 00065/00094 | Loss: 429.9304 | CE: 0.2221 | KD: 1061.0081\n",
      "Train Epoch: 017 Batch: 00066/00094 | Loss: 429.7351 | CE: 0.1728 | KD: 1060.6477\n",
      "Train Epoch: 017 Batch: 00067/00094 | Loss: 429.8341 | CE: 0.1639 | KD: 1060.9141\n",
      "Train Epoch: 017 Batch: 00068/00094 | Loss: 429.8605 | CE: 0.2448 | KD: 1060.7794\n",
      "Train Epoch: 017 Batch: 00069/00094 | Loss: 429.8040 | CE: 0.2082 | KD: 1060.7302\n",
      "Train Epoch: 017 Batch: 00070/00094 | Loss: 429.6782 | CE: 0.2066 | KD: 1060.4236\n",
      "Train Epoch: 017 Batch: 00071/00094 | Loss: 429.7603 | CE: 0.2360 | KD: 1060.5537\n",
      "Train Epoch: 017 Batch: 00072/00094 | Loss: 429.8321 | CE: 0.1974 | KD: 1060.8264\n",
      "Train Epoch: 017 Batch: 00073/00094 | Loss: 429.9243 | CE: 0.3062 | KD: 1060.7854\n",
      "Train Epoch: 017 Batch: 00074/00094 | Loss: 429.9291 | CE: 0.2436 | KD: 1060.9520\n",
      "Train Epoch: 017 Batch: 00075/00094 | Loss: 430.0144 | CE: 0.1832 | KD: 1061.3115\n",
      "Train Epoch: 017 Batch: 00076/00094 | Loss: 429.8664 | CE: 0.2280 | KD: 1060.8354\n",
      "Train Epoch: 017 Batch: 00077/00094 | Loss: 429.7410 | CE: 0.1635 | KD: 1060.6851\n",
      "Train Epoch: 017 Batch: 00078/00094 | Loss: 429.8153 | CE: 0.2477 | KD: 1060.6608\n",
      "Train Epoch: 017 Batch: 00079/00094 | Loss: 429.8719 | CE: 0.2127 | KD: 1060.8868\n",
      "Train Epoch: 017 Batch: 00080/00094 | Loss: 429.6845 | CE: 0.1947 | KD: 1060.4688\n",
      "Train Epoch: 017 Batch: 00081/00094 | Loss: 429.8993 | CE: 0.2624 | KD: 1060.8317\n",
      "Train Epoch: 017 Batch: 00082/00094 | Loss: 429.8040 | CE: 0.1960 | KD: 1060.7605\n",
      "Train Epoch: 017 Batch: 00083/00094 | Loss: 429.7322 | CE: 0.1869 | KD: 1060.6057\n",
      "Train Epoch: 017 Batch: 00084/00094 | Loss: 429.7451 | CE: 0.1492 | KD: 1060.7308\n",
      "Train Epoch: 017 Batch: 00085/00094 | Loss: 429.8291 | CE: 0.2581 | KD: 1060.6691\n",
      "Train Epoch: 017 Batch: 00086/00094 | Loss: 429.7729 | CE: 0.2504 | KD: 1060.5496\n",
      "Train Epoch: 017 Batch: 00087/00094 | Loss: 429.7654 | CE: 0.1962 | KD: 1060.6646\n",
      "Train Epoch: 017 Batch: 00088/00094 | Loss: 429.9048 | CE: 0.2586 | KD: 1060.8547\n",
      "Train Epoch: 017 Batch: 00089/00094 | Loss: 429.7868 | CE: 0.1918 | KD: 1060.7284\n",
      "Train Epoch: 017 Batch: 00090/00094 | Loss: 429.7741 | CE: 0.1996 | KD: 1060.6779\n",
      "Train Epoch: 017 Batch: 00091/00094 | Loss: 429.7086 | CE: 0.1703 | KD: 1060.5883\n",
      "Train Epoch: 017 Batch: 00092/00094 | Loss: 429.7478 | CE: 0.1779 | KD: 1060.6665\n",
      "Train Epoch: 017 Batch: 00093/00094 | Loss: 429.6751 | CE: 0.1399 | KD: 1060.5807\n",
      "Train Epoch: 017 Batch: 00094/00094 | Loss: 430.2093 | CE: 0.4848 | KD: 1061.0481\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1993 | acc:96.0000\n",
      "[VAL Acc] Target: 96.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.4672 | acc:51.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9859 | acc:58.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 58.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9920 | acc:44.2748\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2204 | acc:53.5266\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5722 | acc:70.7948\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9675 | acc:60.5016\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 60.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7357 | acc:63.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 63.25%\n",
      "[VAL Acc] Avg 62.26%\n",
      "Train Epoch: 018 Batch: 00001/00094 | Loss: 429.8199 | CE: 0.2203 | KD: 1060.7397\n",
      "Train Epoch: 018 Batch: 00002/00094 | Loss: 429.7481 | CE: 0.2081 | KD: 1060.5928\n",
      "Train Epoch: 018 Batch: 00003/00094 | Loss: 429.8428 | CE: 0.1719 | KD: 1060.9158\n",
      "Train Epoch: 018 Batch: 00004/00094 | Loss: 429.9714 | CE: 0.2393 | KD: 1061.0669\n",
      "Train Epoch: 018 Batch: 00005/00094 | Loss: 429.8055 | CE: 0.1624 | KD: 1060.8472\n",
      "Train Epoch: 018 Batch: 00006/00094 | Loss: 429.7887 | CE: 0.1809 | KD: 1060.7599\n",
      "Train Epoch: 018 Batch: 00007/00094 | Loss: 429.7144 | CE: 0.1600 | KD: 1060.6282\n",
      "Train Epoch: 018 Batch: 00008/00094 | Loss: 429.9160 | CE: 0.2630 | KD: 1060.8715\n",
      "Train Epoch: 018 Batch: 00009/00094 | Loss: 429.8997 | CE: 0.2185 | KD: 1060.9413\n",
      "Train Epoch: 018 Batch: 00010/00094 | Loss: 429.7541 | CE: 0.1821 | KD: 1060.6716\n",
      "Train Epoch: 018 Batch: 00011/00094 | Loss: 429.9321 | CE: 0.2526 | KD: 1060.9371\n",
      "Train Epoch: 018 Batch: 00012/00094 | Loss: 429.7791 | CE: 0.1553 | KD: 1060.7996\n",
      "Train Epoch: 018 Batch: 00013/00094 | Loss: 429.8104 | CE: 0.2090 | KD: 1060.7444\n",
      "Train Epoch: 018 Batch: 00014/00094 | Loss: 429.8593 | CE: 0.2540 | KD: 1060.7539\n",
      "Train Epoch: 018 Batch: 00015/00094 | Loss: 429.7696 | CE: 0.1849 | KD: 1060.7029\n",
      "Train Epoch: 018 Batch: 00016/00094 | Loss: 429.8507 | CE: 0.2202 | KD: 1060.8162\n",
      "Train Epoch: 018 Batch: 00017/00094 | Loss: 429.6872 | CE: 0.1350 | KD: 1060.6226\n",
      "Train Epoch: 018 Batch: 00018/00094 | Loss: 429.7352 | CE: 0.1998 | KD: 1060.5813\n",
      "Train Epoch: 018 Batch: 00019/00094 | Loss: 429.9582 | CE: 0.1854 | KD: 1061.1674\n",
      "Train Epoch: 018 Batch: 00020/00094 | Loss: 430.0409 | CE: 0.3210 | KD: 1061.0367\n",
      "Train Epoch: 018 Batch: 00021/00094 | Loss: 429.9282 | CE: 0.2554 | KD: 1060.9204\n",
      "Train Epoch: 018 Batch: 00022/00094 | Loss: 429.7000 | CE: 0.1914 | KD: 1060.5151\n",
      "Train Epoch: 018 Batch: 00023/00094 | Loss: 429.7665 | CE: 0.2159 | KD: 1060.6189\n",
      "Train Epoch: 018 Batch: 00024/00094 | Loss: 429.8355 | CE: 0.1914 | KD: 1060.8499\n",
      "Train Epoch: 018 Batch: 00025/00094 | Loss: 429.8573 | CE: 0.2095 | KD: 1060.8588\n",
      "Train Epoch: 018 Batch: 00026/00094 | Loss: 429.9271 | CE: 0.2730 | KD: 1060.8743\n",
      "Train Epoch: 018 Batch: 00027/00094 | Loss: 429.7187 | CE: 0.2061 | KD: 1060.5250\n",
      "Train Epoch: 018 Batch: 00028/00094 | Loss: 429.8417 | CE: 0.2563 | KD: 1060.7046\n",
      "Train Epoch: 018 Batch: 00029/00094 | Loss: 429.7208 | CE: 0.1889 | KD: 1060.5725\n",
      "Train Epoch: 018 Batch: 00030/00094 | Loss: 429.9032 | CE: 0.2887 | KD: 1060.7765\n",
      "Train Epoch: 018 Batch: 00031/00094 | Loss: 430.0284 | CE: 0.2456 | KD: 1061.1919\n",
      "Train Epoch: 018 Batch: 00032/00094 | Loss: 429.8274 | CE: 0.1911 | KD: 1060.8304\n",
      "Train Epoch: 018 Batch: 00033/00094 | Loss: 429.7506 | CE: 0.1704 | KD: 1060.6919\n",
      "Train Epoch: 018 Batch: 00034/00094 | Loss: 429.8988 | CE: 0.3158 | KD: 1060.6987\n",
      "Train Epoch: 018 Batch: 00035/00094 | Loss: 429.8203 | CE: 0.1909 | KD: 1060.8132\n",
      "Train Epoch: 018 Batch: 00036/00094 | Loss: 429.8058 | CE: 0.2186 | KD: 1060.7094\n",
      "Train Epoch: 018 Batch: 00037/00094 | Loss: 429.7289 | CE: 0.2289 | KD: 1060.4939\n",
      "Train Epoch: 018 Batch: 00038/00094 | Loss: 429.7752 | CE: 0.2007 | KD: 1060.6780\n",
      "Train Epoch: 018 Batch: 00039/00094 | Loss: 429.9362 | CE: 0.3244 | KD: 1060.7699\n",
      "Train Epoch: 018 Batch: 00040/00094 | Loss: 429.7623 | CE: 0.1569 | KD: 1060.7539\n",
      "Train Epoch: 018 Batch: 00041/00094 | Loss: 429.7402 | CE: 0.2003 | KD: 1060.5924\n",
      "Train Epoch: 018 Batch: 00042/00094 | Loss: 430.0076 | CE: 0.2562 | KD: 1061.1145\n",
      "Train Epoch: 018 Batch: 00043/00094 | Loss: 429.7762 | CE: 0.2228 | KD: 1060.6256\n",
      "Train Epoch: 018 Batch: 00044/00094 | Loss: 429.8450 | CE: 0.2221 | KD: 1060.7972\n",
      "Train Epoch: 018 Batch: 00045/00094 | Loss: 429.8944 | CE: 0.2540 | KD: 1060.8406\n",
      "Train Epoch: 018 Batch: 00046/00094 | Loss: 429.9066 | CE: 0.2251 | KD: 1060.9421\n",
      "Train Epoch: 018 Batch: 00047/00094 | Loss: 429.8425 | CE: 0.2637 | KD: 1060.6885\n",
      "Train Epoch: 018 Batch: 00048/00094 | Loss: 429.7586 | CE: 0.1659 | KD: 1060.7227\n",
      "Train Epoch: 018 Batch: 00049/00094 | Loss: 429.8783 | CE: 0.2187 | KD: 1060.8879\n",
      "Train Epoch: 018 Batch: 00050/00094 | Loss: 429.7288 | CE: 0.1638 | KD: 1060.6544\n",
      "Train Epoch: 018 Batch: 00051/00094 | Loss: 429.9103 | CE: 0.2236 | KD: 1060.9548\n",
      "Train Epoch: 018 Batch: 00052/00094 | Loss: 429.6329 | CE: 0.1875 | KD: 1060.3591\n",
      "Train Epoch: 018 Batch: 00053/00094 | Loss: 429.8798 | CE: 0.1772 | KD: 1060.9941\n",
      "Train Epoch: 018 Batch: 00054/00094 | Loss: 429.8278 | CE: 0.1800 | KD: 1060.8586\n",
      "Train Epoch: 018 Batch: 00055/00094 | Loss: 430.0033 | CE: 0.2262 | KD: 1061.1780\n",
      "Train Epoch: 018 Batch: 00056/00094 | Loss: 429.7441 | CE: 0.2008 | KD: 1060.6008\n",
      "Train Epoch: 018 Batch: 00057/00094 | Loss: 429.8548 | CE: 0.2474 | KD: 1060.7589\n",
      "Train Epoch: 018 Batch: 00058/00094 | Loss: 429.8120 | CE: 0.1484 | KD: 1060.8977\n",
      "Train Epoch: 018 Batch: 00059/00094 | Loss: 429.8628 | CE: 0.1847 | KD: 1060.9336\n",
      "Train Epoch: 018 Batch: 00060/00094 | Loss: 429.7232 | CE: 0.1827 | KD: 1060.5940\n",
      "Train Epoch: 018 Batch: 00061/00094 | Loss: 429.6662 | CE: 0.1854 | KD: 1060.4464\n",
      "Train Epoch: 018 Batch: 00062/00094 | Loss: 429.8948 | CE: 0.2741 | KD: 1060.7919\n",
      "Train Epoch: 018 Batch: 00063/00094 | Loss: 429.8031 | CE: 0.2478 | KD: 1060.6304\n",
      "Train Epoch: 018 Batch: 00064/00094 | Loss: 429.6680 | CE: 0.1489 | KD: 1060.5409\n",
      "Train Epoch: 018 Batch: 00065/00094 | Loss: 429.7969 | CE: 0.1659 | KD: 1060.8171\n",
      "Train Epoch: 018 Batch: 00066/00094 | Loss: 429.8519 | CE: 0.2225 | KD: 1060.8132\n",
      "Train Epoch: 018 Batch: 00067/00094 | Loss: 429.9117 | CE: 0.2907 | KD: 1060.7926\n",
      "Train Epoch: 018 Batch: 00068/00094 | Loss: 429.7797 | CE: 0.2101 | KD: 1060.6658\n",
      "Train Epoch: 018 Batch: 00069/00094 | Loss: 429.7209 | CE: 0.1755 | KD: 1060.6058\n",
      "Train Epoch: 018 Batch: 00070/00094 | Loss: 429.9245 | CE: 0.2202 | KD: 1060.9984\n",
      "Train Epoch: 018 Batch: 00071/00094 | Loss: 429.9521 | CE: 0.2518 | KD: 1060.9884\n",
      "Train Epoch: 018 Batch: 00072/00094 | Loss: 429.8956 | CE: 0.1634 | KD: 1061.0673\n",
      "Train Epoch: 018 Batch: 00073/00094 | Loss: 429.8148 | CE: 0.1921 | KD: 1060.7968\n",
      "Train Epoch: 018 Batch: 00074/00094 | Loss: 429.8430 | CE: 0.1836 | KD: 1060.8875\n",
      "Train Epoch: 018 Batch: 00075/00094 | Loss: 429.6821 | CE: 0.2056 | KD: 1060.4358\n",
      "Train Epoch: 018 Batch: 00076/00094 | Loss: 429.6934 | CE: 0.1711 | KD: 1060.5490\n",
      "Train Epoch: 018 Batch: 00077/00094 | Loss: 429.8199 | CE: 0.1890 | KD: 1060.8171\n",
      "Train Epoch: 018 Batch: 00078/00094 | Loss: 429.7196 | CE: 0.1755 | KD: 1060.6027\n",
      "Train Epoch: 018 Batch: 00079/00094 | Loss: 429.9088 | CE: 0.1868 | KD: 1061.0421\n",
      "Train Epoch: 018 Batch: 00080/00094 | Loss: 429.8606 | CE: 0.2352 | KD: 1060.8035\n",
      "Train Epoch: 018 Batch: 00081/00094 | Loss: 429.7879 | CE: 0.2339 | KD: 1060.6273\n",
      "Train Epoch: 018 Batch: 00082/00094 | Loss: 429.9248 | CE: 0.2955 | KD: 1060.8132\n",
      "Train Epoch: 018 Batch: 00083/00094 | Loss: 429.9818 | CE: 0.3471 | KD: 1060.8262\n",
      "Train Epoch: 018 Batch: 00084/00094 | Loss: 429.8719 | CE: 0.2118 | KD: 1060.8892\n",
      "Train Epoch: 018 Batch: 00085/00094 | Loss: 429.9673 | CE: 0.3096 | KD: 1060.8832\n",
      "Train Epoch: 018 Batch: 00086/00094 | Loss: 429.8013 | CE: 0.2374 | KD: 1060.6515\n",
      "Train Epoch: 018 Batch: 00087/00094 | Loss: 429.8678 | CE: 0.2066 | KD: 1060.8918\n",
      "Train Epoch: 018 Batch: 00088/00094 | Loss: 429.7586 | CE: 0.2589 | KD: 1060.4929\n",
      "Train Epoch: 018 Batch: 00089/00094 | Loss: 429.7433 | CE: 0.1883 | KD: 1060.6295\n",
      "Train Epoch: 018 Batch: 00090/00094 | Loss: 429.8991 | CE: 0.2497 | KD: 1060.8628\n",
      "Train Epoch: 018 Batch: 00091/00094 | Loss: 429.8560 | CE: 0.2242 | KD: 1060.8195\n",
      "Train Epoch: 018 Batch: 00092/00094 | Loss: 429.7402 | CE: 0.1788 | KD: 1060.6455\n",
      "Train Epoch: 018 Batch: 00093/00094 | Loss: 429.8896 | CE: 0.2601 | KD: 1060.8136\n",
      "Train Epoch: 018 Batch: 00094/00094 | Loss: 429.7144 | CE: 0.1532 | KD: 1060.6451\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1920 | acc:95.3500\n",
      "[VAL Acc] Target: 95.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7619 | acc:49.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1524 | acc:54.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0560 | acc:45.2290\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5509 | acc:52.5470\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6309 | acc:66.5434\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.2828 | acc:56.9357\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 56.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7910 | acc:59.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.75%\n",
      "[VAL Acc] Avg 60.09%\n",
      "Train Epoch: 019 Batch: 00001/00094 | Loss: 386.9668 | CE: 0.2663 | KD: 1060.9066\n",
      "Train Epoch: 019 Batch: 00002/00094 | Loss: 386.9832 | CE: 0.2256 | KD: 1061.0635\n",
      "Train Epoch: 019 Batch: 00003/00094 | Loss: 387.0891 | CE: 0.2204 | KD: 1061.3683\n",
      "Train Epoch: 019 Batch: 00004/00094 | Loss: 386.6741 | CE: 0.1757 | KD: 1060.3524\n",
      "Train Epoch: 019 Batch: 00005/00094 | Loss: 387.0867 | CE: 0.3248 | KD: 1061.0753\n",
      "Train Epoch: 019 Batch: 00006/00094 | Loss: 386.8114 | CE: 0.1932 | KD: 1060.6809\n",
      "Train Epoch: 019 Batch: 00007/00094 | Loss: 386.9337 | CE: 0.2499 | KD: 1060.8611\n",
      "Train Epoch: 019 Batch: 00008/00094 | Loss: 386.8415 | CE: 0.2214 | KD: 1060.6860\n",
      "Train Epoch: 019 Batch: 00009/00094 | Loss: 386.8237 | CE: 0.1692 | KD: 1060.7805\n",
      "Train Epoch: 019 Batch: 00010/00094 | Loss: 386.8751 | CE: 0.2556 | KD: 1060.6844\n",
      "Train Epoch: 019 Batch: 00011/00094 | Loss: 386.9408 | CE: 0.2305 | KD: 1060.9337\n",
      "Train Epoch: 019 Batch: 00012/00094 | Loss: 386.9489 | CE: 0.1863 | KD: 1061.0770\n",
      "Train Epoch: 019 Batch: 00013/00094 | Loss: 386.6902 | CE: 0.1729 | KD: 1060.4041\n",
      "Train Epoch: 019 Batch: 00014/00094 | Loss: 386.8593 | CE: 0.1840 | KD: 1060.8374\n",
      "Train Epoch: 019 Batch: 00015/00094 | Loss: 386.8386 | CE: 0.1894 | KD: 1060.7659\n",
      "Train Epoch: 019 Batch: 00016/00094 | Loss: 386.8287 | CE: 0.1865 | KD: 1060.7468\n",
      "Train Epoch: 019 Batch: 00017/00094 | Loss: 386.8506 | CE: 0.2064 | KD: 1060.7523\n",
      "Train Epoch: 019 Batch: 00018/00094 | Loss: 386.7874 | CE: 0.1442 | KD: 1060.7494\n",
      "Train Epoch: 019 Batch: 00019/00094 | Loss: 386.8482 | CE: 0.2421 | KD: 1060.6477\n",
      "Train Epoch: 019 Batch: 00020/00094 | Loss: 386.8692 | CE: 0.1971 | KD: 1060.8289\n",
      "Train Epoch: 019 Batch: 00021/00094 | Loss: 386.8785 | CE: 0.2490 | KD: 1060.7119\n",
      "Train Epoch: 019 Batch: 00022/00094 | Loss: 386.7444 | CE: 0.2020 | KD: 1060.4730\n",
      "Train Epoch: 019 Batch: 00023/00094 | Loss: 386.8429 | CE: 0.1381 | KD: 1060.9186\n",
      "Train Epoch: 019 Batch: 00024/00094 | Loss: 386.8247 | CE: 0.1878 | KD: 1060.7323\n",
      "Train Epoch: 019 Batch: 00025/00094 | Loss: 386.9829 | CE: 0.3170 | KD: 1060.8120\n",
      "Train Epoch: 019 Batch: 00026/00094 | Loss: 386.9756 | CE: 0.2466 | KD: 1060.9849\n",
      "Train Epoch: 019 Batch: 00027/00094 | Loss: 386.8406 | CE: 0.2185 | KD: 1060.6915\n",
      "Train Epoch: 019 Batch: 00028/00094 | Loss: 386.7316 | CE: 0.2028 | KD: 1060.4357\n",
      "Train Epoch: 019 Batch: 00029/00094 | Loss: 386.6930 | CE: 0.1706 | KD: 1060.4182\n",
      "Train Epoch: 019 Batch: 00030/00094 | Loss: 386.7341 | CE: 0.1234 | KD: 1060.6606\n",
      "Train Epoch: 019 Batch: 00031/00094 | Loss: 386.7556 | CE: 0.1901 | KD: 1060.5366\n",
      "Train Epoch: 019 Batch: 00032/00094 | Loss: 386.8499 | CE: 0.2180 | KD: 1060.7186\n",
      "Train Epoch: 019 Batch: 00033/00094 | Loss: 386.8594 | CE: 0.1520 | KD: 1060.9257\n",
      "Train Epoch: 019 Batch: 00034/00094 | Loss: 386.7563 | CE: 0.1928 | KD: 1060.5310\n",
      "Train Epoch: 019 Batch: 00035/00094 | Loss: 386.9314 | CE: 0.2077 | KD: 1060.9705\n",
      "Train Epoch: 019 Batch: 00036/00094 | Loss: 386.7940 | CE: 0.1786 | KD: 1060.6735\n",
      "Train Epoch: 019 Batch: 00037/00094 | Loss: 386.8330 | CE: 0.2413 | KD: 1060.6083\n",
      "Train Epoch: 019 Batch: 00038/00094 | Loss: 386.8457 | CE: 0.2307 | KD: 1060.6722\n",
      "Train Epoch: 019 Batch: 00039/00094 | Loss: 386.8350 | CE: 0.1798 | KD: 1060.7825\n",
      "Train Epoch: 019 Batch: 00040/00094 | Loss: 386.7520 | CE: 0.1531 | KD: 1060.6283\n",
      "Train Epoch: 019 Batch: 00041/00094 | Loss: 386.7943 | CE: 0.1926 | KD: 1060.6357\n",
      "Train Epoch: 019 Batch: 00042/00094 | Loss: 386.8718 | CE: 0.2014 | KD: 1060.8242\n",
      "Train Epoch: 019 Batch: 00043/00094 | Loss: 386.8080 | CE: 0.2108 | KD: 1060.6235\n",
      "Train Epoch: 019 Batch: 00044/00094 | Loss: 386.8626 | CE: 0.1882 | KD: 1060.8351\n",
      "Train Epoch: 019 Batch: 00045/00094 | Loss: 386.7956 | CE: 0.1545 | KD: 1060.7438\n",
      "Train Epoch: 019 Batch: 00046/00094 | Loss: 386.8194 | CE: 0.1751 | KD: 1060.7526\n",
      "Train Epoch: 019 Batch: 00047/00094 | Loss: 386.9254 | CE: 0.2259 | KD: 1060.9039\n",
      "Train Epoch: 019 Batch: 00048/00094 | Loss: 386.7776 | CE: 0.1586 | KD: 1060.6833\n",
      "Train Epoch: 019 Batch: 00049/00094 | Loss: 386.9288 | CE: 0.1987 | KD: 1060.9882\n",
      "Train Epoch: 019 Batch: 00050/00094 | Loss: 386.6823 | CE: 0.1414 | KD: 1060.4691\n",
      "Train Epoch: 019 Batch: 00051/00094 | Loss: 386.7726 | CE: 0.1829 | KD: 1060.6027\n",
      "Train Epoch: 019 Batch: 00052/00094 | Loss: 386.8622 | CE: 0.1454 | KD: 1060.9514\n",
      "Train Epoch: 019 Batch: 00053/00094 | Loss: 386.8061 | CE: 0.2006 | KD: 1060.6461\n",
      "Train Epoch: 019 Batch: 00054/00094 | Loss: 386.8758 | CE: 0.2124 | KD: 1060.8049\n",
      "Train Epoch: 019 Batch: 00055/00094 | Loss: 386.8508 | CE: 0.2383 | KD: 1060.6655\n",
      "Train Epoch: 019 Batch: 00056/00094 | Loss: 386.7494 | CE: 0.1679 | KD: 1060.5804\n",
      "Train Epoch: 019 Batch: 00057/00094 | Loss: 386.8084 | CE: 0.1771 | KD: 1060.7172\n",
      "Train Epoch: 019 Batch: 00058/00094 | Loss: 386.8423 | CE: 0.1786 | KD: 1060.8058\n",
      "Train Epoch: 019 Batch: 00059/00094 | Loss: 386.7823 | CE: 0.1544 | KD: 1060.7075\n",
      "Train Epoch: 019 Batch: 00060/00094 | Loss: 386.8698 | CE: 0.1658 | KD: 1060.9165\n",
      "Train Epoch: 019 Batch: 00061/00094 | Loss: 386.8316 | CE: 0.1803 | KD: 1060.7716\n",
      "Train Epoch: 019 Batch: 00062/00094 | Loss: 387.0967 | CE: 0.3708 | KD: 1060.9764\n",
      "Train Epoch: 019 Batch: 00063/00094 | Loss: 386.8486 | CE: 0.1982 | KD: 1060.7693\n",
      "Train Epoch: 019 Batch: 00064/00094 | Loss: 387.2003 | CE: 0.2959 | KD: 1061.4663\n",
      "Train Epoch: 019 Batch: 00065/00094 | Loss: 386.8198 | CE: 0.2144 | KD: 1060.6460\n",
      "Train Epoch: 019 Batch: 00066/00094 | Loss: 386.8733 | CE: 0.1976 | KD: 1060.8387\n",
      "Train Epoch: 019 Batch: 00067/00094 | Loss: 386.9771 | CE: 0.2862 | KD: 1060.8802\n",
      "Train Epoch: 019 Batch: 00068/00094 | Loss: 386.8652 | CE: 0.2354 | KD: 1060.7126\n",
      "Train Epoch: 019 Batch: 00069/00094 | Loss: 386.9390 | CE: 0.3234 | KD: 1060.6738\n",
      "Train Epoch: 019 Batch: 00070/00094 | Loss: 386.9505 | CE: 0.2648 | KD: 1060.8661\n",
      "Train Epoch: 019 Batch: 00071/00094 | Loss: 386.8556 | CE: 0.1955 | KD: 1060.7959\n",
      "Train Epoch: 019 Batch: 00072/00094 | Loss: 386.8780 | CE: 0.1924 | KD: 1060.8657\n",
      "Train Epoch: 019 Batch: 00073/00094 | Loss: 387.0175 | CE: 0.2748 | KD: 1061.0227\n",
      "Train Epoch: 019 Batch: 00074/00094 | Loss: 386.9658 | CE: 0.2273 | KD: 1061.0111\n",
      "Train Epoch: 019 Batch: 00075/00094 | Loss: 386.8528 | CE: 0.1905 | KD: 1060.8020\n",
      "Train Epoch: 019 Batch: 00076/00094 | Loss: 386.8013 | CE: 0.1763 | KD: 1060.6996\n",
      "Train Epoch: 019 Batch: 00077/00094 | Loss: 386.7673 | CE: 0.1905 | KD: 1060.5674\n",
      "Train Epoch: 019 Batch: 00078/00094 | Loss: 386.7211 | CE: 0.1695 | KD: 1060.4983\n",
      "Train Epoch: 019 Batch: 00079/00094 | Loss: 386.7198 | CE: 0.1472 | KD: 1060.5560\n",
      "Train Epoch: 019 Batch: 00080/00094 | Loss: 386.7565 | CE: 0.1602 | KD: 1060.6210\n",
      "Train Epoch: 019 Batch: 00081/00094 | Loss: 386.9634 | CE: 0.2657 | KD: 1060.8992\n",
      "Train Epoch: 019 Batch: 00082/00094 | Loss: 386.6924 | CE: 0.1575 | KD: 1060.4524\n",
      "Train Epoch: 019 Batch: 00083/00094 | Loss: 386.7757 | CE: 0.1516 | KD: 1060.6970\n",
      "Train Epoch: 019 Batch: 00084/00094 | Loss: 386.8282 | CE: 0.1608 | KD: 1060.8159\n",
      "Train Epoch: 019 Batch: 00085/00094 | Loss: 386.8060 | CE: 0.2119 | KD: 1060.6150\n",
      "Train Epoch: 019 Batch: 00086/00094 | Loss: 386.9324 | CE: 0.2317 | KD: 1060.9073\n",
      "Train Epoch: 019 Batch: 00087/00094 | Loss: 386.8280 | CE: 0.1930 | KD: 1060.7271\n",
      "Train Epoch: 019 Batch: 00088/00094 | Loss: 386.9207 | CE: 0.2610 | KD: 1060.7948\n",
      "Train Epoch: 019 Batch: 00089/00094 | Loss: 386.7982 | CE: 0.1958 | KD: 1060.6375\n",
      "Train Epoch: 019 Batch: 00090/00094 | Loss: 386.8328 | CE: 0.2179 | KD: 1060.6719\n",
      "Train Epoch: 019 Batch: 00091/00094 | Loss: 386.8525 | CE: 0.1993 | KD: 1060.7771\n",
      "Train Epoch: 019 Batch: 00092/00094 | Loss: 386.8080 | CE: 0.1736 | KD: 1060.7255\n",
      "Train Epoch: 019 Batch: 00093/00094 | Loss: 386.7327 | CE: 0.1759 | KD: 1060.5125\n",
      "Train Epoch: 019 Batch: 00094/00094 | Loss: 386.8966 | CE: 0.2009 | KD: 1060.8938\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1912 | acc:95.2500\n",
      "[VAL Acc] Target: 95.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5407 | acc:50.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0136 | acc:57.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 57.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9913 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2790 | acc:53.4091\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6306 | acc:66.2662\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9590 | acc:61.4028\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8266 | acc:59.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.44%\n",
      "[VAL Acc] Avg 61.47%\n",
      "Train Epoch: 020 Batch: 00001/00094 | Loss: 387.0403 | CE: 0.2963 | KD: 1061.0259\n",
      "Train Epoch: 020 Batch: 00002/00094 | Loss: 386.8916 | CE: 0.2280 | KD: 1060.8054\n",
      "Train Epoch: 020 Batch: 00003/00094 | Loss: 387.0490 | CE: 0.1793 | KD: 1061.3708\n",
      "Train Epoch: 020 Batch: 00004/00094 | Loss: 386.9002 | CE: 0.2157 | KD: 1060.8628\n",
      "Train Epoch: 020 Batch: 00005/00094 | Loss: 386.9542 | CE: 0.2326 | KD: 1060.9647\n",
      "Train Epoch: 020 Batch: 00006/00094 | Loss: 386.9880 | CE: 0.2651 | KD: 1060.9683\n",
      "Train Epoch: 020 Batch: 00007/00094 | Loss: 386.7587 | CE: 0.1957 | KD: 1060.5294\n",
      "Train Epoch: 020 Batch: 00008/00094 | Loss: 386.7897 | CE: 0.1718 | KD: 1060.6802\n",
      "Train Epoch: 020 Batch: 00009/00094 | Loss: 386.9100 | CE: 0.2575 | KD: 1060.7750\n",
      "Train Epoch: 020 Batch: 00010/00094 | Loss: 386.8462 | CE: 0.1908 | KD: 1060.7830\n",
      "Train Epoch: 020 Batch: 00011/00094 | Loss: 386.8329 | CE: 0.1754 | KD: 1060.7889\n",
      "Train Epoch: 020 Batch: 00012/00094 | Loss: 386.8094 | CE: 0.1881 | KD: 1060.6893\n",
      "Train Epoch: 020 Batch: 00013/00094 | Loss: 386.9373 | CE: 0.1815 | KD: 1061.0586\n",
      "Train Epoch: 020 Batch: 00014/00094 | Loss: 386.8105 | CE: 0.1832 | KD: 1060.7058\n",
      "Train Epoch: 020 Batch: 00015/00094 | Loss: 386.9208 | CE: 0.2319 | KD: 1060.8748\n",
      "Train Epoch: 020 Batch: 00016/00094 | Loss: 386.9496 | CE: 0.1718 | KD: 1061.1188\n",
      "Train Epoch: 020 Batch: 00017/00094 | Loss: 386.7797 | CE: 0.1829 | KD: 1060.6222\n",
      "Train Epoch: 020 Batch: 00018/00094 | Loss: 386.8457 | CE: 0.2054 | KD: 1060.7417\n",
      "Train Epoch: 020 Batch: 00019/00094 | Loss: 386.7882 | CE: 0.1780 | KD: 1060.6593\n",
      "Train Epoch: 020 Batch: 00020/00094 | Loss: 386.7728 | CE: 0.1949 | KD: 1060.5702\n",
      "Train Epoch: 020 Batch: 00021/00094 | Loss: 386.7799 | CE: 0.1948 | KD: 1060.5903\n",
      "Train Epoch: 020 Batch: 00022/00094 | Loss: 386.7198 | CE: 0.1486 | KD: 1060.5521\n",
      "Train Epoch: 020 Batch: 00023/00094 | Loss: 386.8469 | CE: 0.2123 | KD: 1060.7258\n",
      "Train Epoch: 020 Batch: 00024/00094 | Loss: 386.7262 | CE: 0.1206 | KD: 1060.6465\n",
      "Train Epoch: 020 Batch: 00025/00094 | Loss: 386.8861 | CE: 0.1353 | KD: 1061.0447\n",
      "Train Epoch: 020 Batch: 00026/00094 | Loss: 386.9461 | CE: 0.2627 | KD: 1060.8599\n",
      "Train Epoch: 020 Batch: 00027/00094 | Loss: 386.7036 | CE: 0.1603 | KD: 1060.4756\n",
      "Train Epoch: 020 Batch: 00028/00094 | Loss: 386.8677 | CE: 0.1465 | KD: 1060.9635\n",
      "Train Epoch: 020 Batch: 00029/00094 | Loss: 386.9168 | CE: 0.1933 | KD: 1060.9700\n",
      "Train Epoch: 020 Batch: 00030/00094 | Loss: 386.8490 | CE: 0.1948 | KD: 1060.7798\n",
      "Train Epoch: 020 Batch: 00031/00094 | Loss: 386.7623 | CE: 0.1785 | KD: 1060.5865\n",
      "Train Epoch: 020 Batch: 00032/00094 | Loss: 386.7934 | CE: 0.1794 | KD: 1060.6693\n",
      "Train Epoch: 020 Batch: 00033/00094 | Loss: 386.8132 | CE: 0.1964 | KD: 1060.6771\n",
      "Train Epoch: 020 Batch: 00034/00094 | Loss: 386.9048 | CE: 0.1897 | KD: 1060.9468\n",
      "Train Epoch: 020 Batch: 00035/00094 | Loss: 386.8098 | CE: 0.1702 | KD: 1060.7396\n",
      "Train Epoch: 020 Batch: 00036/00094 | Loss: 386.9159 | CE: 0.1452 | KD: 1061.0995\n",
      "Train Epoch: 020 Batch: 00037/00094 | Loss: 386.7175 | CE: 0.1461 | KD: 1060.5525\n",
      "Train Epoch: 020 Batch: 00038/00094 | Loss: 386.7383 | CE: 0.1445 | KD: 1060.6140\n",
      "Train Epoch: 020 Batch: 00039/00094 | Loss: 386.8748 | CE: 0.1934 | KD: 1060.8544\n",
      "Train Epoch: 020 Batch: 00040/00094 | Loss: 386.7884 | CE: 0.2087 | KD: 1060.5753\n",
      "Train Epoch: 020 Batch: 00041/00094 | Loss: 386.7613 | CE: 0.1336 | KD: 1060.7070\n",
      "Train Epoch: 020 Batch: 00042/00094 | Loss: 386.8499 | CE: 0.1527 | KD: 1060.8978\n",
      "Train Epoch: 020 Batch: 00043/00094 | Loss: 386.8196 | CE: 0.1803 | KD: 1060.7390\n",
      "Train Epoch: 020 Batch: 00044/00094 | Loss: 386.9187 | CE: 0.2253 | KD: 1060.8873\n",
      "Train Epoch: 020 Batch: 00045/00094 | Loss: 386.9388 | CE: 0.2718 | KD: 1060.8146\n",
      "Train Epoch: 020 Batch: 00046/00094 | Loss: 386.7842 | CE: 0.1709 | KD: 1060.6674\n",
      "Train Epoch: 020 Batch: 00047/00094 | Loss: 386.8124 | CE: 0.1715 | KD: 1060.7434\n",
      "Train Epoch: 020 Batch: 00048/00094 | Loss: 386.7980 | CE: 0.1648 | KD: 1060.7220\n",
      "Train Epoch: 020 Batch: 00049/00094 | Loss: 386.7824 | CE: 0.1464 | KD: 1060.7300\n",
      "Train Epoch: 020 Batch: 00050/00094 | Loss: 386.9047 | CE: 0.2215 | KD: 1060.8593\n",
      "Train Epoch: 020 Batch: 00051/00094 | Loss: 386.8679 | CE: 0.2290 | KD: 1060.7378\n",
      "Train Epoch: 020 Batch: 00052/00094 | Loss: 386.8288 | CE: 0.1661 | KD: 1060.8032\n",
      "Train Epoch: 020 Batch: 00053/00094 | Loss: 386.9257 | CE: 0.2157 | KD: 1060.9329\n",
      "Train Epoch: 020 Batch: 00054/00094 | Loss: 386.7428 | CE: 0.1510 | KD: 1060.6086\n",
      "Train Epoch: 020 Batch: 00055/00094 | Loss: 386.8919 | CE: 0.2574 | KD: 1060.7256\n",
      "Train Epoch: 020 Batch: 00056/00094 | Loss: 387.0063 | CE: 0.1942 | KD: 1061.2130\n",
      "Train Epoch: 020 Batch: 00057/00094 | Loss: 386.7687 | CE: 0.1778 | KD: 1060.6062\n",
      "Train Epoch: 020 Batch: 00058/00094 | Loss: 386.7236 | CE: 0.1764 | KD: 1060.4862\n",
      "Train Epoch: 020 Batch: 00059/00094 | Loss: 386.7391 | CE: 0.1456 | KD: 1060.6133\n",
      "Train Epoch: 020 Batch: 00060/00094 | Loss: 386.7923 | CE: 0.1467 | KD: 1060.7560\n",
      "Train Epoch: 020 Batch: 00061/00094 | Loss: 386.7087 | CE: 0.1383 | KD: 1060.5500\n",
      "Train Epoch: 020 Batch: 00062/00094 | Loss: 386.7448 | CE: 0.1493 | KD: 1060.6188\n",
      "Train Epoch: 020 Batch: 00063/00094 | Loss: 386.7337 | CE: 0.2507 | KD: 1060.3102\n",
      "Train Epoch: 020 Batch: 00064/00094 | Loss: 386.8332 | CE: 0.1644 | KD: 1060.8197\n",
      "Train Epoch: 020 Batch: 00065/00094 | Loss: 386.7503 | CE: 0.1489 | KD: 1060.6348\n",
      "Train Epoch: 020 Batch: 00066/00094 | Loss: 386.8493 | CE: 0.1922 | KD: 1060.7877\n",
      "Train Epoch: 020 Batch: 00067/00094 | Loss: 386.8101 | CE: 0.1560 | KD: 1060.7794\n",
      "Train Epoch: 020 Batch: 00068/00094 | Loss: 386.8056 | CE: 0.1554 | KD: 1060.7688\n",
      "Train Epoch: 020 Batch: 00069/00094 | Loss: 387.0524 | CE: 0.2219 | KD: 1061.2633\n",
      "Train Epoch: 020 Batch: 00070/00094 | Loss: 386.9713 | CE: 0.2677 | KD: 1060.9154\n",
      "Train Epoch: 020 Batch: 00071/00094 | Loss: 386.8029 | CE: 0.1815 | KD: 1060.6898\n",
      "Train Epoch: 020 Batch: 00072/00094 | Loss: 386.8380 | CE: 0.1962 | KD: 1060.7457\n",
      "Train Epoch: 020 Batch: 00073/00094 | Loss: 386.7799 | CE: 0.1291 | KD: 1060.7706\n",
      "Train Epoch: 020 Batch: 00074/00094 | Loss: 386.7559 | CE: 0.1886 | KD: 1060.5413\n",
      "Train Epoch: 020 Batch: 00075/00094 | Loss: 387.0530 | CE: 0.3377 | KD: 1060.9474\n",
      "Train Epoch: 020 Batch: 00076/00094 | Loss: 386.8070 | CE: 0.1979 | KD: 1060.6560\n",
      "Train Epoch: 020 Batch: 00077/00094 | Loss: 386.7726 | CE: 0.1725 | KD: 1060.6316\n",
      "Train Epoch: 020 Batch: 00078/00094 | Loss: 386.6646 | CE: 0.1725 | KD: 1060.3352\n",
      "Train Epoch: 020 Batch: 00079/00094 | Loss: 386.7364 | CE: 0.2003 | KD: 1060.4558\n",
      "Train Epoch: 020 Batch: 00080/00094 | Loss: 386.9202 | CE: 0.2210 | KD: 1060.9034\n",
      "Train Epoch: 020 Batch: 00081/00094 | Loss: 386.7830 | CE: 0.1785 | KD: 1060.6434\n",
      "Train Epoch: 020 Batch: 00082/00094 | Loss: 386.9000 | CE: 0.1787 | KD: 1060.9636\n",
      "Train Epoch: 020 Batch: 00083/00094 | Loss: 386.7248 | CE: 0.1429 | KD: 1060.5814\n",
      "Train Epoch: 020 Batch: 00084/00094 | Loss: 386.6956 | CE: 0.1416 | KD: 1060.5048\n",
      "Train Epoch: 020 Batch: 00085/00094 | Loss: 386.7848 | CE: 0.1486 | KD: 1060.7303\n",
      "Train Epoch: 020 Batch: 00086/00094 | Loss: 386.8388 | CE: 0.1903 | KD: 1060.7639\n",
      "Train Epoch: 020 Batch: 00087/00094 | Loss: 386.7918 | CE: 0.1567 | KD: 1060.7274\n",
      "Train Epoch: 020 Batch: 00088/00094 | Loss: 386.8582 | CE: 0.2039 | KD: 1060.7802\n",
      "Train Epoch: 020 Batch: 00089/00094 | Loss: 387.0508 | CE: 0.2179 | KD: 1061.2701\n",
      "Train Epoch: 020 Batch: 00090/00094 | Loss: 386.9363 | CE: 0.2319 | KD: 1060.9177\n",
      "Train Epoch: 020 Batch: 00091/00094 | Loss: 386.6801 | CE: 0.1540 | KD: 1060.4283\n",
      "Train Epoch: 020 Batch: 00092/00094 | Loss: 386.7433 | CE: 0.1816 | KD: 1060.5262\n",
      "Train Epoch: 020 Batch: 00093/00094 | Loss: 386.8424 | CE: 0.2223 | KD: 1060.6863\n",
      "Train Epoch: 020 Batch: 00094/00094 | Loss: 386.9097 | CE: 0.2116 | KD: 1060.9000\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2237 | acc:93.6500\n",
      "[VAL Acc] Target: 93.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6172 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1122 | acc:56.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0307 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1428 | acc:56.9749\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.97%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5424 | acc:73.1978\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8716 | acc:65.9875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7358 | acc:63.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 63.56%\n",
      "[VAL Acc] Avg 63.18%\n",
      "VAL Acc improve from 63.03% to 63.18%\n",
      "Save best model\n",
      "Train Epoch: 021 Batch: 00001/00094 | Loss: 386.7643 | CE: 0.1583 | KD: 1060.6475\n",
      "Train Epoch: 021 Batch: 00002/00094 | Loss: 386.7519 | CE: 0.1990 | KD: 1060.5016\n",
      "Train Epoch: 021 Batch: 00003/00094 | Loss: 386.8142 | CE: 0.1503 | KD: 1060.8064\n",
      "Train Epoch: 021 Batch: 00004/00094 | Loss: 386.8335 | CE: 0.1621 | KD: 1060.8269\n",
      "Train Epoch: 021 Batch: 00005/00094 | Loss: 386.8674 | CE: 0.1967 | KD: 1060.8251\n",
      "Train Epoch: 021 Batch: 00006/00094 | Loss: 386.9292 | CE: 0.2089 | KD: 1060.9611\n",
      "Train Epoch: 021 Batch: 00007/00094 | Loss: 386.9230 | CE: 0.2601 | KD: 1060.8035\n",
      "Train Epoch: 021 Batch: 00008/00094 | Loss: 386.8401 | CE: 0.2023 | KD: 1060.7349\n",
      "Train Epoch: 021 Batch: 00009/00094 | Loss: 386.7752 | CE: 0.1713 | KD: 1060.6417\n",
      "Train Epoch: 021 Batch: 00010/00094 | Loss: 386.7660 | CE: 0.2300 | KD: 1060.4556\n",
      "Train Epoch: 021 Batch: 00011/00094 | Loss: 386.9237 | CE: 0.1727 | KD: 1061.0453\n",
      "Train Epoch: 021 Batch: 00012/00094 | Loss: 386.8838 | CE: 0.2156 | KD: 1060.8182\n",
      "Train Epoch: 021 Batch: 00013/00094 | Loss: 386.8905 | CE: 0.1495 | KD: 1061.0181\n",
      "Train Epoch: 021 Batch: 00014/00094 | Loss: 386.8120 | CE: 0.1647 | KD: 1060.7609\n",
      "Train Epoch: 021 Batch: 00015/00094 | Loss: 386.8263 | CE: 0.2161 | KD: 1060.6589\n",
      "Train Epoch: 021 Batch: 00016/00094 | Loss: 386.9294 | CE: 0.1781 | KD: 1061.0459\n",
      "Train Epoch: 021 Batch: 00017/00094 | Loss: 386.8416 | CE: 0.1786 | KD: 1060.8040\n",
      "Train Epoch: 021 Batch: 00018/00094 | Loss: 386.7721 | CE: 0.1480 | KD: 1060.6973\n",
      "Train Epoch: 021 Batch: 00019/00094 | Loss: 386.8742 | CE: 0.2078 | KD: 1060.8134\n",
      "Train Epoch: 021 Batch: 00020/00094 | Loss: 386.8392 | CE: 0.2008 | KD: 1060.7363\n",
      "Train Epoch: 021 Batch: 00021/00094 | Loss: 386.9090 | CE: 0.1770 | KD: 1060.9930\n",
      "Train Epoch: 021 Batch: 00022/00094 | Loss: 386.7511 | CE: 0.1822 | KD: 1060.5458\n",
      "Train Epoch: 021 Batch: 00023/00094 | Loss: 386.8658 | CE: 0.1967 | KD: 1060.8204\n",
      "Train Epoch: 021 Batch: 00024/00094 | Loss: 386.7773 | CE: 0.1943 | KD: 1060.5842\n",
      "Train Epoch: 021 Batch: 00025/00094 | Loss: 386.7605 | CE: 0.1526 | KD: 1060.6527\n",
      "Train Epoch: 021 Batch: 00026/00094 | Loss: 386.8492 | CE: 0.2252 | KD: 1060.6967\n",
      "Train Epoch: 021 Batch: 00027/00094 | Loss: 386.7699 | CE: 0.2274 | KD: 1060.4734\n",
      "Train Epoch: 021 Batch: 00028/00094 | Loss: 386.7825 | CE: 0.2346 | KD: 1060.4882\n",
      "Train Epoch: 021 Batch: 00029/00094 | Loss: 386.9075 | CE: 0.2360 | KD: 1060.8274\n",
      "Train Epoch: 021 Batch: 00030/00094 | Loss: 386.8406 | CE: 0.1387 | KD: 1060.9106\n",
      "Train Epoch: 021 Batch: 00031/00094 | Loss: 386.8727 | CE: 0.2029 | KD: 1060.8226\n",
      "Train Epoch: 021 Batch: 00032/00094 | Loss: 386.8819 | CE: 0.1473 | KD: 1061.0004\n",
      "Train Epoch: 021 Batch: 00033/00094 | Loss: 386.9406 | CE: 0.2087 | KD: 1060.9929\n",
      "Train Epoch: 021 Batch: 00034/00094 | Loss: 386.9488 | CE: 0.2180 | KD: 1060.9899\n",
      "Train Epoch: 021 Batch: 00035/00094 | Loss: 386.8322 | CE: 0.2115 | KD: 1060.6877\n",
      "Train Epoch: 021 Batch: 00036/00094 | Loss: 386.8008 | CE: 0.1652 | KD: 1060.7286\n",
      "Train Epoch: 021 Batch: 00037/00094 | Loss: 386.8404 | CE: 0.1956 | KD: 1060.7539\n",
      "Train Epoch: 021 Batch: 00038/00094 | Loss: 387.0592 | CE: 0.2811 | KD: 1061.1195\n",
      "Train Epoch: 021 Batch: 00039/00094 | Loss: 386.7971 | CE: 0.1858 | KD: 1060.6621\n",
      "Train Epoch: 021 Batch: 00040/00094 | Loss: 386.8011 | CE: 0.1837 | KD: 1060.6787\n",
      "Train Epoch: 021 Batch: 00041/00094 | Loss: 386.9931 | CE: 0.2023 | KD: 1061.1547\n",
      "Train Epoch: 021 Batch: 00042/00094 | Loss: 386.7401 | CE: 0.1418 | KD: 1060.6262\n",
      "Train Epoch: 021 Batch: 00043/00094 | Loss: 386.8764 | CE: 0.2754 | KD: 1060.6338\n",
      "Train Epoch: 021 Batch: 00044/00094 | Loss: 386.7695 | CE: 0.1959 | KD: 1060.5585\n",
      "Train Epoch: 021 Batch: 00045/00094 | Loss: 386.7731 | CE: 0.2140 | KD: 1060.5189\n",
      "Train Epoch: 021 Batch: 00046/00094 | Loss: 386.8026 | CE: 0.1539 | KD: 1060.7646\n",
      "Train Epoch: 021 Batch: 00047/00094 | Loss: 386.8230 | CE: 0.2173 | KD: 1060.6465\n",
      "Train Epoch: 021 Batch: 00048/00094 | Loss: 386.9370 | CE: 0.2041 | KD: 1060.9958\n",
      "Train Epoch: 021 Batch: 00049/00094 | Loss: 386.8239 | CE: 0.1758 | KD: 1060.7629\n",
      "Train Epoch: 021 Batch: 00050/00094 | Loss: 386.9063 | CE: 0.1864 | KD: 1060.9601\n",
      "Train Epoch: 021 Batch: 00051/00094 | Loss: 386.8916 | CE: 0.2160 | KD: 1060.8383\n",
      "Train Epoch: 021 Batch: 00052/00094 | Loss: 386.7424 | CE: 0.1562 | KD: 1060.5931\n",
      "Train Epoch: 021 Batch: 00053/00094 | Loss: 386.8725 | CE: 0.2152 | KD: 1060.7881\n",
      "Train Epoch: 021 Batch: 00054/00094 | Loss: 386.9110 | CE: 0.1723 | KD: 1061.0115\n",
      "Train Epoch: 021 Batch: 00055/00094 | Loss: 386.9216 | CE: 0.1697 | KD: 1061.0477\n",
      "Train Epoch: 021 Batch: 00056/00094 | Loss: 386.8554 | CE: 0.2067 | KD: 1060.7645\n",
      "Train Epoch: 021 Batch: 00057/00094 | Loss: 386.8303 | CE: 0.1890 | KD: 1060.7441\n",
      "Train Epoch: 021 Batch: 00058/00094 | Loss: 386.8332 | CE: 0.2119 | KD: 1060.6893\n",
      "Train Epoch: 021 Batch: 00059/00094 | Loss: 386.7377 | CE: 0.1557 | KD: 1060.5815\n",
      "Train Epoch: 021 Batch: 00060/00094 | Loss: 386.7683 | CE: 0.1612 | KD: 1060.6505\n",
      "Train Epoch: 021 Batch: 00061/00094 | Loss: 386.9177 | CE: 0.1989 | KD: 1060.9569\n",
      "Train Epoch: 021 Batch: 00062/00094 | Loss: 386.9531 | CE: 0.2506 | KD: 1060.9122\n",
      "Train Epoch: 021 Batch: 00063/00094 | Loss: 386.8680 | CE: 0.1813 | KD: 1060.8689\n",
      "Train Epoch: 021 Batch: 00064/00094 | Loss: 387.0353 | CE: 0.3003 | KD: 1061.0016\n",
      "Train Epoch: 021 Batch: 00065/00094 | Loss: 387.1951 | CE: 0.4139 | KD: 1061.1279\n",
      "Train Epoch: 021 Batch: 00066/00094 | Loss: 386.7539 | CE: 0.1596 | KD: 1060.6152\n",
      "Train Epoch: 021 Batch: 00067/00094 | Loss: 386.9861 | CE: 0.2184 | KD: 1061.0911\n",
      "Train Epoch: 021 Batch: 00068/00094 | Loss: 386.8812 | CE: 0.1986 | KD: 1060.8575\n",
      "Train Epoch: 021 Batch: 00069/00094 | Loss: 386.7873 | CE: 0.2165 | KD: 1060.5510\n",
      "Train Epoch: 021 Batch: 00070/00094 | Loss: 386.8249 | CE: 0.1783 | KD: 1060.7587\n",
      "Train Epoch: 021 Batch: 00071/00094 | Loss: 386.8040 | CE: 0.1742 | KD: 1060.7128\n",
      "Train Epoch: 021 Batch: 00072/00094 | Loss: 386.8376 | CE: 0.2288 | KD: 1060.6553\n",
      "Train Epoch: 021 Batch: 00073/00094 | Loss: 386.7056 | CE: 0.1930 | KD: 1060.3912\n",
      "Train Epoch: 021 Batch: 00074/00094 | Loss: 386.9005 | CE: 0.2396 | KD: 1060.7979\n",
      "Train Epoch: 021 Batch: 00075/00094 | Loss: 386.8553 | CE: 0.1914 | KD: 1060.8064\n",
      "Train Epoch: 021 Batch: 00076/00094 | Loss: 386.7160 | CE: 0.1608 | KD: 1060.5082\n",
      "Train Epoch: 021 Batch: 00077/00094 | Loss: 386.9004 | CE: 0.1997 | KD: 1060.9072\n",
      "Train Epoch: 021 Batch: 00078/00094 | Loss: 386.7767 | CE: 0.1749 | KD: 1060.6360\n",
      "Train Epoch: 021 Batch: 00079/00094 | Loss: 386.8209 | CE: 0.1608 | KD: 1060.7959\n",
      "Train Epoch: 021 Batch: 00080/00094 | Loss: 386.7867 | CE: 0.1624 | KD: 1060.6979\n",
      "Train Epoch: 021 Batch: 00081/00094 | Loss: 386.7098 | CE: 0.1739 | KD: 1060.4551\n",
      "Train Epoch: 021 Batch: 00082/00094 | Loss: 386.9787 | CE: 0.2260 | KD: 1061.0500\n",
      "Train Epoch: 021 Batch: 00083/00094 | Loss: 386.7940 | CE: 0.1688 | KD: 1060.7003\n",
      "Train Epoch: 021 Batch: 00084/00094 | Loss: 386.8819 | CE: 0.1940 | KD: 1060.8723\n",
      "Train Epoch: 021 Batch: 00085/00094 | Loss: 386.8925 | CE: 0.1617 | KD: 1060.9900\n",
      "Train Epoch: 021 Batch: 00086/00094 | Loss: 386.9381 | CE: 0.1852 | KD: 1061.0508\n",
      "Train Epoch: 021 Batch: 00087/00094 | Loss: 386.9521 | CE: 0.2422 | KD: 1060.9325\n",
      "Train Epoch: 021 Batch: 00088/00094 | Loss: 386.8069 | CE: 0.2213 | KD: 1060.5914\n",
      "Train Epoch: 021 Batch: 00089/00094 | Loss: 386.6989 | CE: 0.1772 | KD: 1060.4160\n",
      "Train Epoch: 021 Batch: 00090/00094 | Loss: 386.7894 | CE: 0.1511 | KD: 1060.7360\n",
      "Train Epoch: 021 Batch: 00091/00094 | Loss: 386.8460 | CE: 0.1620 | KD: 1060.8616\n",
      "Train Epoch: 021 Batch: 00092/00094 | Loss: 386.8231 | CE: 0.1848 | KD: 1060.7360\n",
      "Train Epoch: 021 Batch: 00093/00094 | Loss: 386.8178 | CE: 0.1831 | KD: 1060.7263\n",
      "Train Epoch: 021 Batch: 00094/00094 | Loss: 386.6739 | CE: 0.1386 | KD: 1060.4537\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1914 | acc:95.7500\n",
      "[VAL Acc] Target: 95.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5444 | acc:51.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0438 | acc:57.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 57.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9963 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2324 | acc:55.9169\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5228 | acc:73.5675\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9234 | acc:63.5188\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7597 | acc:61.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.94%\n",
      "[VAL Acc] Avg 63.52%\n",
      "VAL Acc improve from 63.18% to 63.52%\n",
      "Save best model\n",
      "Train Epoch: 022 Batch: 00001/00094 | Loss: 386.8404 | CE: 0.2148 | KD: 1060.7014\n",
      "Train Epoch: 022 Batch: 00002/00094 | Loss: 386.7668 | CE: 0.1725 | KD: 1060.6154\n",
      "Train Epoch: 022 Batch: 00003/00094 | Loss: 387.0020 | CE: 0.3696 | KD: 1060.7197\n",
      "Train Epoch: 022 Batch: 00004/00094 | Loss: 386.8336 | CE: 0.1745 | KD: 1060.7931\n",
      "Train Epoch: 022 Batch: 00005/00094 | Loss: 386.8803 | CE: 0.1507 | KD: 1060.9867\n",
      "Train Epoch: 022 Batch: 00006/00094 | Loss: 386.8160 | CE: 0.2312 | KD: 1060.5894\n",
      "Train Epoch: 022 Batch: 00007/00094 | Loss: 386.8824 | CE: 0.2490 | KD: 1060.7228\n",
      "Train Epoch: 022 Batch: 00008/00094 | Loss: 386.8264 | CE: 0.1926 | KD: 1060.7238\n",
      "Train Epoch: 022 Batch: 00009/00094 | Loss: 386.7827 | CE: 0.1394 | KD: 1060.7496\n",
      "Train Epoch: 022 Batch: 00010/00094 | Loss: 386.7628 | CE: 0.1996 | KD: 1060.5302\n",
      "Train Epoch: 022 Batch: 00011/00094 | Loss: 386.7389 | CE: 0.1385 | KD: 1060.6320\n",
      "Train Epoch: 022 Batch: 00012/00094 | Loss: 386.8849 | CE: 0.1798 | KD: 1060.9194\n",
      "Train Epoch: 022 Batch: 00013/00094 | Loss: 386.7644 | CE: 0.1485 | KD: 1060.6747\n",
      "Train Epoch: 022 Batch: 00014/00094 | Loss: 386.8549 | CE: 0.2138 | KD: 1060.7437\n",
      "Train Epoch: 022 Batch: 00015/00094 | Loss: 386.8867 | CE: 0.1725 | KD: 1060.9442\n",
      "Train Epoch: 022 Batch: 00016/00094 | Loss: 386.9208 | CE: 0.1879 | KD: 1060.9956\n",
      "Train Epoch: 022 Batch: 00017/00094 | Loss: 386.7570 | CE: 0.1753 | KD: 1060.5808\n",
      "Train Epoch: 022 Batch: 00018/00094 | Loss: 386.7415 | CE: 0.1746 | KD: 1060.5404\n",
      "Train Epoch: 022 Batch: 00019/00094 | Loss: 386.7861 | CE: 0.1564 | KD: 1060.7125\n",
      "Train Epoch: 022 Batch: 00020/00094 | Loss: 386.7852 | CE: 0.1651 | KD: 1060.6860\n",
      "Train Epoch: 022 Batch: 00021/00094 | Loss: 386.8109 | CE: 0.1656 | KD: 1060.7554\n",
      "Train Epoch: 022 Batch: 00022/00094 | Loss: 386.8852 | CE: 0.1591 | KD: 1060.9772\n",
      "Train Epoch: 022 Batch: 00023/00094 | Loss: 386.8229 | CE: 0.1690 | KD: 1060.7788\n",
      "Train Epoch: 022 Batch: 00024/00094 | Loss: 386.9286 | CE: 0.1625 | KD: 1061.0868\n",
      "Train Epoch: 022 Batch: 00025/00094 | Loss: 386.9393 | CE: 0.2426 | KD: 1060.8962\n",
      "Train Epoch: 022 Batch: 00026/00094 | Loss: 386.7545 | CE: 0.2129 | KD: 1060.4707\n",
      "Train Epoch: 022 Batch: 00027/00094 | Loss: 386.9410 | CE: 0.2051 | KD: 1061.0039\n",
      "Train Epoch: 022 Batch: 00028/00094 | Loss: 386.8153 | CE: 0.1700 | KD: 1060.7554\n",
      "Train Epoch: 022 Batch: 00029/00094 | Loss: 386.9579 | CE: 0.2019 | KD: 1061.0590\n",
      "Train Epoch: 022 Batch: 00030/00094 | Loss: 386.7471 | CE: 0.1810 | KD: 1060.5378\n",
      "Train Epoch: 022 Batch: 00031/00094 | Loss: 386.8551 | CE: 0.1825 | KD: 1060.8302\n",
      "Train Epoch: 022 Batch: 00032/00094 | Loss: 386.8099 | CE: 0.1479 | KD: 1060.8014\n",
      "Train Epoch: 022 Batch: 00033/00094 | Loss: 386.9474 | CE: 0.2297 | KD: 1060.9539\n",
      "Train Epoch: 022 Batch: 00034/00094 | Loss: 386.7268 | CE: 0.1510 | KD: 1060.5646\n",
      "Train Epoch: 022 Batch: 00035/00094 | Loss: 386.8035 | CE: 0.2100 | KD: 1060.6130\n",
      "Train Epoch: 022 Batch: 00036/00094 | Loss: 386.9875 | CE: 0.1962 | KD: 1061.1558\n",
      "Train Epoch: 022 Batch: 00037/00094 | Loss: 386.8974 | CE: 0.2206 | KD: 1060.8418\n",
      "Train Epoch: 022 Batch: 00038/00094 | Loss: 386.7871 | CE: 0.2271 | KD: 1060.5215\n",
      "Train Epoch: 022 Batch: 00039/00094 | Loss: 386.7721 | CE: 0.1643 | KD: 1060.6525\n",
      "Train Epoch: 022 Batch: 00040/00094 | Loss: 386.8694 | CE: 0.2275 | KD: 1060.7457\n",
      "Train Epoch: 022 Batch: 00041/00094 | Loss: 386.7383 | CE: 0.1417 | KD: 1060.6217\n",
      "Train Epoch: 022 Batch: 00042/00094 | Loss: 386.8237 | CE: 0.1401 | KD: 1060.8605\n",
      "Train Epoch: 022 Batch: 00043/00094 | Loss: 386.8194 | CE: 0.2020 | KD: 1060.6787\n",
      "Train Epoch: 022 Batch: 00044/00094 | Loss: 386.8331 | CE: 0.1335 | KD: 1060.9045\n",
      "Train Epoch: 022 Batch: 00045/00094 | Loss: 386.7393 | CE: 0.1636 | KD: 1060.5645\n",
      "Train Epoch: 022 Batch: 00046/00094 | Loss: 386.7414 | CE: 0.1787 | KD: 1060.5286\n",
      "Train Epoch: 022 Batch: 00047/00094 | Loss: 386.8004 | CE: 0.1914 | KD: 1060.6559\n",
      "Train Epoch: 022 Batch: 00048/00094 | Loss: 386.7580 | CE: 0.1181 | KD: 1060.7405\n",
      "Train Epoch: 022 Batch: 00049/00094 | Loss: 386.9221 | CE: 0.2840 | KD: 1060.7355\n",
      "Train Epoch: 022 Batch: 00050/00094 | Loss: 386.6717 | CE: 0.1116 | KD: 1060.5217\n",
      "Train Epoch: 022 Batch: 00051/00094 | Loss: 386.7773 | CE: 0.1931 | KD: 1060.5875\n",
      "Train Epoch: 022 Batch: 00052/00094 | Loss: 386.8337 | CE: 0.2129 | KD: 1060.6884\n",
      "Train Epoch: 022 Batch: 00053/00094 | Loss: 386.8870 | CE: 0.2244 | KD: 1060.8027\n",
      "Train Epoch: 022 Batch: 00054/00094 | Loss: 387.0078 | CE: 0.2577 | KD: 1061.0428\n",
      "Train Epoch: 022 Batch: 00055/00094 | Loss: 386.7598 | CE: 0.1486 | KD: 1060.6620\n",
      "Train Epoch: 022 Batch: 00056/00094 | Loss: 386.9134 | CE: 0.2169 | KD: 1060.8955\n",
      "Train Epoch: 022 Batch: 00057/00094 | Loss: 386.6728 | CE: 0.1696 | KD: 1060.3656\n",
      "Train Epoch: 022 Batch: 00058/00094 | Loss: 386.7770 | CE: 0.1618 | KD: 1060.6727\n",
      "Train Epoch: 022 Batch: 00059/00094 | Loss: 386.9379 | CE: 0.2763 | KD: 1060.8000\n",
      "Train Epoch: 022 Batch: 00060/00094 | Loss: 386.8483 | CE: 0.2176 | KD: 1060.7155\n",
      "Train Epoch: 022 Batch: 00061/00094 | Loss: 386.8768 | CE: 0.1827 | KD: 1060.8893\n",
      "Train Epoch: 022 Batch: 00062/00094 | Loss: 386.7988 | CE: 0.1872 | KD: 1060.6630\n",
      "Train Epoch: 022 Batch: 00063/00094 | Loss: 386.7920 | CE: 0.1870 | KD: 1060.6448\n",
      "Train Epoch: 022 Batch: 00064/00094 | Loss: 386.7087 | CE: 0.1744 | KD: 1060.4507\n",
      "Train Epoch: 022 Batch: 00065/00094 | Loss: 386.7882 | CE: 0.1549 | KD: 1060.7223\n",
      "Train Epoch: 022 Batch: 00066/00094 | Loss: 386.8806 | CE: 0.2265 | KD: 1060.7794\n",
      "Train Epoch: 022 Batch: 00067/00094 | Loss: 386.7269 | CE: 0.1721 | KD: 1060.5070\n",
      "Train Epoch: 022 Batch: 00068/00094 | Loss: 386.9109 | CE: 0.2301 | KD: 1060.8528\n",
      "Train Epoch: 022 Batch: 00069/00094 | Loss: 386.8573 | CE: 0.2260 | KD: 1060.7170\n",
      "Train Epoch: 022 Batch: 00070/00094 | Loss: 386.8098 | CE: 0.2306 | KD: 1060.5742\n",
      "Train Epoch: 022 Batch: 00071/00094 | Loss: 386.8837 | CE: 0.1776 | KD: 1060.9220\n",
      "Train Epoch: 022 Batch: 00072/00094 | Loss: 386.8700 | CE: 0.1918 | KD: 1060.8456\n",
      "Train Epoch: 022 Batch: 00073/00094 | Loss: 386.7883 | CE: 0.1520 | KD: 1060.7306\n",
      "Train Epoch: 022 Batch: 00074/00094 | Loss: 386.8223 | CE: 0.2315 | KD: 1060.6057\n",
      "Train Epoch: 022 Batch: 00075/00094 | Loss: 386.8931 | CE: 0.2005 | KD: 1060.8851\n",
      "Train Epoch: 022 Batch: 00076/00094 | Loss: 387.0312 | CE: 0.1839 | KD: 1061.3094\n",
      "Train Epoch: 022 Batch: 00077/00094 | Loss: 386.9764 | CE: 0.2840 | KD: 1060.8844\n",
      "Train Epoch: 022 Batch: 00078/00094 | Loss: 386.9181 | CE: 0.2395 | KD: 1060.8468\n",
      "Train Epoch: 022 Batch: 00079/00094 | Loss: 386.9023 | CE: 0.1479 | KD: 1061.0547\n",
      "Train Epoch: 022 Batch: 00080/00094 | Loss: 386.7176 | CE: 0.1561 | KD: 1060.5254\n",
      "Train Epoch: 022 Batch: 00081/00094 | Loss: 386.7328 | CE: 0.1712 | KD: 1060.5256\n",
      "Train Epoch: 022 Batch: 00082/00094 | Loss: 386.8969 | CE: 0.2036 | KD: 1060.8870\n",
      "Train Epoch: 022 Batch: 00083/00094 | Loss: 386.7885 | CE: 0.1530 | KD: 1060.7281\n",
      "Train Epoch: 022 Batch: 00084/00094 | Loss: 386.9651 | CE: 0.2391 | KD: 1060.9767\n",
      "Train Epoch: 022 Batch: 00085/00094 | Loss: 386.8678 | CE: 0.1911 | KD: 1060.8414\n",
      "Train Epoch: 022 Batch: 00086/00094 | Loss: 386.8070 | CE: 0.2299 | KD: 1060.5682\n",
      "Train Epoch: 022 Batch: 00087/00094 | Loss: 386.6022 | CE: 0.1272 | KD: 1060.2881\n",
      "Train Epoch: 022 Batch: 00088/00094 | Loss: 386.8292 | CE: 0.1727 | KD: 1060.7861\n",
      "Train Epoch: 022 Batch: 00089/00094 | Loss: 386.7245 | CE: 0.1401 | KD: 1060.5883\n",
      "Train Epoch: 022 Batch: 00090/00094 | Loss: 386.8208 | CE: 0.1658 | KD: 1060.7821\n",
      "Train Epoch: 022 Batch: 00091/00094 | Loss: 386.8620 | CE: 0.2138 | KD: 1060.7634\n",
      "Train Epoch: 022 Batch: 00092/00094 | Loss: 386.8145 | CE: 0.1758 | KD: 1060.7372\n",
      "Train Epoch: 022 Batch: 00093/00094 | Loss: 386.7993 | CE: 0.1356 | KD: 1060.8059\n",
      "Train Epoch: 022 Batch: 00094/00094 | Loss: 386.7215 | CE: 0.1413 | KD: 1060.5767\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1712 | acc:96.6000\n",
      "[VAL Acc] Target: 96.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6091 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1071 | acc:54.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0667 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2752 | acc:53.8793\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5787 | acc:70.1479\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9179 | acc:63.7147\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7497 | acc:61.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.50%\n",
      "[VAL Acc] Avg 61.93%\n",
      "Train Epoch: 023 Batch: 00001/00094 | Loss: 386.6781 | CE: 0.1245 | KD: 1060.5038\n",
      "Train Epoch: 023 Batch: 00002/00094 | Loss: 386.7410 | CE: 0.1609 | KD: 1060.5763\n",
      "Train Epoch: 023 Batch: 00003/00094 | Loss: 386.7774 | CE: 0.1391 | KD: 1060.7361\n",
      "Train Epoch: 023 Batch: 00004/00094 | Loss: 386.7104 | CE: 0.1173 | KD: 1060.6121\n",
      "Train Epoch: 023 Batch: 00005/00094 | Loss: 386.8297 | CE: 0.1685 | KD: 1060.7990\n",
      "Train Epoch: 023 Batch: 00006/00094 | Loss: 386.8111 | CE: 0.1825 | KD: 1060.7095\n",
      "Train Epoch: 023 Batch: 00007/00094 | Loss: 386.7884 | CE: 0.1600 | KD: 1060.7091\n",
      "Train Epoch: 023 Batch: 00008/00094 | Loss: 386.7625 | CE: 0.1133 | KD: 1060.7659\n",
      "Train Epoch: 023 Batch: 00009/00094 | Loss: 386.8132 | CE: 0.1788 | KD: 1060.7253\n",
      "Train Epoch: 023 Batch: 00010/00094 | Loss: 386.8434 | CE: 0.1571 | KD: 1060.8679\n",
      "Train Epoch: 023 Batch: 00011/00094 | Loss: 386.7115 | CE: 0.1745 | KD: 1060.4581\n",
      "Train Epoch: 023 Batch: 00012/00094 | Loss: 386.8299 | CE: 0.2016 | KD: 1060.7086\n",
      "Train Epoch: 023 Batch: 00013/00094 | Loss: 386.6609 | CE: 0.1139 | KD: 1060.4857\n",
      "Train Epoch: 023 Batch: 00014/00094 | Loss: 386.9262 | CE: 0.2658 | KD: 1060.7968\n",
      "Train Epoch: 023 Batch: 00015/00094 | Loss: 386.8388 | CE: 0.1461 | KD: 1060.8851\n",
      "Train Epoch: 023 Batch: 00016/00094 | Loss: 386.8634 | CE: 0.1573 | KD: 1060.9221\n",
      "Train Epoch: 023 Batch: 00017/00094 | Loss: 386.9317 | CE: 0.2456 | KD: 1060.8672\n",
      "Train Epoch: 023 Batch: 00018/00094 | Loss: 386.8066 | CE: 0.2235 | KD: 1060.5848\n",
      "Train Epoch: 023 Batch: 00019/00094 | Loss: 386.6366 | CE: 0.1357 | KD: 1060.3591\n",
      "Train Epoch: 023 Batch: 00020/00094 | Loss: 386.8174 | CE: 0.1658 | KD: 1060.7726\n",
      "Train Epoch: 023 Batch: 00021/00094 | Loss: 386.7519 | CE: 0.1706 | KD: 1060.5797\n",
      "Train Epoch: 023 Batch: 00022/00094 | Loss: 386.6779 | CE: 0.1250 | KD: 1060.5017\n",
      "Train Epoch: 023 Batch: 00023/00094 | Loss: 386.7420 | CE: 0.1458 | KD: 1060.6205\n",
      "Train Epoch: 023 Batch: 00024/00094 | Loss: 386.7783 | CE: 0.1741 | KD: 1060.6427\n",
      "Train Epoch: 023 Batch: 00025/00094 | Loss: 386.8424 | CE: 0.1298 | KD: 1060.9402\n",
      "Train Epoch: 023 Batch: 00026/00094 | Loss: 386.8699 | CE: 0.1966 | KD: 1060.8322\n",
      "Train Epoch: 023 Batch: 00027/00094 | Loss: 386.9224 | CE: 0.2499 | KD: 1060.8300\n",
      "Train Epoch: 023 Batch: 00028/00094 | Loss: 386.8025 | CE: 0.1354 | KD: 1060.8151\n",
      "Train Epoch: 023 Batch: 00029/00094 | Loss: 386.7451 | CE: 0.1458 | KD: 1060.6293\n",
      "Train Epoch: 023 Batch: 00030/00094 | Loss: 386.9464 | CE: 0.2874 | KD: 1060.7928\n",
      "Train Epoch: 023 Batch: 00031/00094 | Loss: 386.7856 | CE: 0.1591 | KD: 1060.7035\n",
      "Train Epoch: 023 Batch: 00032/00094 | Loss: 386.9401 | CE: 0.1710 | KD: 1061.0947\n",
      "Train Epoch: 023 Batch: 00033/00094 | Loss: 386.6602 | CE: 0.1475 | KD: 1060.3914\n",
      "Train Epoch: 023 Batch: 00034/00094 | Loss: 387.0716 | CE: 0.3295 | KD: 1061.0208\n",
      "Train Epoch: 023 Batch: 00035/00094 | Loss: 386.6864 | CE: 0.1220 | KD: 1060.5333\n",
      "Train Epoch: 023 Batch: 00036/00094 | Loss: 386.8113 | CE: 0.1794 | KD: 1060.7186\n",
      "Train Epoch: 023 Batch: 00037/00094 | Loss: 386.8679 | CE: 0.1828 | KD: 1060.8646\n",
      "Train Epoch: 023 Batch: 00038/00094 | Loss: 386.8040 | CE: 0.1512 | KD: 1060.7760\n",
      "Train Epoch: 023 Batch: 00039/00094 | Loss: 386.7155 | CE: 0.1661 | KD: 1060.4923\n",
      "Train Epoch: 023 Batch: 00040/00094 | Loss: 386.7574 | CE: 0.1558 | KD: 1060.6356\n",
      "Train Epoch: 023 Batch: 00041/00094 | Loss: 386.8072 | CE: 0.1739 | KD: 1060.7224\n",
      "Train Epoch: 023 Batch: 00042/00094 | Loss: 386.9185 | CE: 0.2699 | KD: 1060.7644\n",
      "Train Epoch: 023 Batch: 00043/00094 | Loss: 386.8647 | CE: 0.1868 | KD: 1060.8448\n",
      "Train Epoch: 023 Batch: 00044/00094 | Loss: 386.7992 | CE: 0.1587 | KD: 1060.7421\n",
      "Train Epoch: 023 Batch: 00045/00094 | Loss: 386.8992 | CE: 0.2416 | KD: 1060.7891\n",
      "Train Epoch: 023 Batch: 00046/00094 | Loss: 386.7572 | CE: 0.1633 | KD: 1060.6144\n",
      "Train Epoch: 023 Batch: 00047/00094 | Loss: 386.9146 | CE: 0.2708 | KD: 1060.7511\n",
      "Train Epoch: 023 Batch: 00048/00094 | Loss: 386.9175 | CE: 0.1689 | KD: 1061.0388\n",
      "Train Epoch: 023 Batch: 00049/00094 | Loss: 386.8141 | CE: 0.1524 | KD: 1060.8003\n",
      "Train Epoch: 023 Batch: 00050/00094 | Loss: 386.8452 | CE: 0.2284 | KD: 1060.6774\n",
      "Train Epoch: 023 Batch: 00051/00094 | Loss: 386.9124 | CE: 0.2489 | KD: 1060.8053\n",
      "Train Epoch: 023 Batch: 00052/00094 | Loss: 386.8272 | CE: 0.1543 | KD: 1060.8311\n",
      "Train Epoch: 023 Batch: 00053/00094 | Loss: 386.7931 | CE: 0.1918 | KD: 1060.6345\n",
      "Train Epoch: 023 Batch: 00054/00094 | Loss: 386.8033 | CE: 0.1430 | KD: 1060.7964\n",
      "Train Epoch: 023 Batch: 00055/00094 | Loss: 386.8430 | CE: 0.1580 | KD: 1060.8644\n",
      "Train Epoch: 023 Batch: 00056/00094 | Loss: 386.7252 | CE: 0.1790 | KD: 1060.4834\n",
      "Train Epoch: 023 Batch: 00057/00094 | Loss: 386.9516 | CE: 0.2606 | KD: 1060.8807\n",
      "Train Epoch: 023 Batch: 00058/00094 | Loss: 386.7113 | CE: 0.1560 | KD: 1060.5084\n",
      "Train Epoch: 023 Batch: 00059/00094 | Loss: 386.7544 | CE: 0.1298 | KD: 1060.6986\n",
      "Train Epoch: 023 Batch: 00060/00094 | Loss: 386.8444 | CE: 0.1753 | KD: 1060.8206\n",
      "Train Epoch: 023 Batch: 00061/00094 | Loss: 386.7887 | CE: 0.2102 | KD: 1060.5720\n",
      "Train Epoch: 023 Batch: 00062/00094 | Loss: 386.7284 | CE: 0.1591 | KD: 1060.5466\n",
      "Train Epoch: 023 Batch: 00063/00094 | Loss: 386.8542 | CE: 0.1773 | KD: 1060.8420\n",
      "Train Epoch: 023 Batch: 00064/00094 | Loss: 386.7425 | CE: 0.1680 | KD: 1060.5610\n",
      "Train Epoch: 023 Batch: 00065/00094 | Loss: 386.9031 | CE: 0.1536 | KD: 1061.0414\n",
      "Train Epoch: 023 Batch: 00066/00094 | Loss: 386.6807 | CE: 0.1248 | KD: 1060.5100\n",
      "Train Epoch: 023 Batch: 00067/00094 | Loss: 386.7850 | CE: 0.1403 | KD: 1060.7535\n",
      "Train Epoch: 023 Batch: 00068/00094 | Loss: 386.6897 | CE: 0.1449 | KD: 1060.4797\n",
      "Train Epoch: 023 Batch: 00069/00094 | Loss: 386.8443 | CE: 0.1539 | KD: 1060.8790\n",
      "Train Epoch: 023 Batch: 00070/00094 | Loss: 386.8343 | CE: 0.1812 | KD: 1060.7769\n",
      "Train Epoch: 023 Batch: 00071/00094 | Loss: 386.8046 | CE: 0.1782 | KD: 1060.7035\n",
      "Train Epoch: 023 Batch: 00072/00094 | Loss: 386.9873 | CE: 0.2927 | KD: 1060.8904\n",
      "Train Epoch: 023 Batch: 00073/00094 | Loss: 386.7634 | CE: 0.2064 | KD: 1060.5131\n",
      "Train Epoch: 023 Batch: 00074/00094 | Loss: 386.8312 | CE: 0.1852 | KD: 1060.7572\n",
      "Train Epoch: 023 Batch: 00075/00094 | Loss: 386.7522 | CE: 0.1950 | KD: 1060.5135\n",
      "Train Epoch: 023 Batch: 00076/00094 | Loss: 386.8584 | CE: 0.1364 | KD: 1060.9658\n",
      "Train Epoch: 023 Batch: 00077/00094 | Loss: 386.9648 | CE: 0.2415 | KD: 1060.9694\n",
      "Train Epoch: 023 Batch: 00078/00094 | Loss: 386.7907 | CE: 0.1790 | KD: 1060.6632\n",
      "Train Epoch: 023 Batch: 00079/00094 | Loss: 386.7072 | CE: 0.1311 | KD: 1060.5653\n",
      "Train Epoch: 023 Batch: 00080/00094 | Loss: 386.7825 | CE: 0.1743 | KD: 1060.6536\n",
      "Train Epoch: 023 Batch: 00081/00094 | Loss: 386.7000 | CE: 0.1401 | KD: 1060.5209\n",
      "Train Epoch: 023 Batch: 00082/00094 | Loss: 386.8565 | CE: 0.1393 | KD: 1060.9525\n",
      "Train Epoch: 023 Batch: 00083/00094 | Loss: 386.8141 | CE: 0.1699 | KD: 1060.7524\n",
      "Train Epoch: 023 Batch: 00084/00094 | Loss: 386.8350 | CE: 0.2114 | KD: 1060.6959\n",
      "Train Epoch: 023 Batch: 00085/00094 | Loss: 386.8653 | CE: 0.2266 | KD: 1060.7374\n",
      "Train Epoch: 023 Batch: 00086/00094 | Loss: 386.6862 | CE: 0.1123 | KD: 1060.5594\n",
      "Train Epoch: 023 Batch: 00087/00094 | Loss: 386.8378 | CE: 0.2218 | KD: 1060.6750\n",
      "Train Epoch: 023 Batch: 00088/00094 | Loss: 386.7692 | CE: 0.1420 | KD: 1060.7056\n",
      "Train Epoch: 023 Batch: 00089/00094 | Loss: 386.9017 | CE: 0.2092 | KD: 1060.8849\n",
      "Train Epoch: 023 Batch: 00090/00094 | Loss: 386.8612 | CE: 0.1673 | KD: 1060.8887\n",
      "Train Epoch: 023 Batch: 00091/00094 | Loss: 386.6848 | CE: 0.1457 | KD: 1060.4641\n",
      "Train Epoch: 023 Batch: 00092/00094 | Loss: 386.9133 | CE: 0.2248 | KD: 1060.8739\n",
      "Train Epoch: 023 Batch: 00093/00094 | Loss: 386.7656 | CE: 0.1862 | KD: 1060.5745\n",
      "Train Epoch: 023 Batch: 00094/00094 | Loss: 386.7841 | CE: 0.1657 | KD: 1060.6814\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1794 | acc:96.1000\n",
      "[VAL Acc] Target: 96.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6379 | acc:50.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1181 | acc:55.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0811 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3197 | acc:52.7821\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5104 | acc:75.4159\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9356 | acc:62.8527\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7762 | acc:62.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.19%\n",
      "[VAL Acc] Avg 62.91%\n",
      "Train Epoch: 024 Batch: 00001/00094 | Loss: 386.8778 | CE: 0.1996 | KD: 1060.8455\n",
      "Train Epoch: 024 Batch: 00002/00094 | Loss: 386.6871 | CE: 0.1443 | KD: 1060.4742\n",
      "Train Epoch: 024 Batch: 00003/00094 | Loss: 386.8226 | CE: 0.2210 | KD: 1060.6356\n",
      "Train Epoch: 024 Batch: 00004/00094 | Loss: 386.7729 | CE: 0.1435 | KD: 1060.7117\n",
      "Train Epoch: 024 Batch: 00005/00094 | Loss: 386.7793 | CE: 0.1271 | KD: 1060.7740\n",
      "Train Epoch: 024 Batch: 00006/00094 | Loss: 386.9093 | CE: 0.1655 | KD: 1061.0256\n",
      "Train Epoch: 024 Batch: 00007/00094 | Loss: 386.8429 | CE: 0.2246 | KD: 1060.6812\n",
      "Train Epoch: 024 Batch: 00008/00094 | Loss: 386.8866 | CE: 0.1308 | KD: 1061.0583\n",
      "Train Epoch: 024 Batch: 00009/00094 | Loss: 386.7094 | CE: 0.1601 | KD: 1060.4919\n",
      "Train Epoch: 024 Batch: 00010/00094 | Loss: 386.9744 | CE: 0.2407 | KD: 1060.9978\n",
      "Train Epoch: 024 Batch: 00011/00094 | Loss: 386.8521 | CE: 0.1986 | KD: 1060.7777\n",
      "Train Epoch: 024 Batch: 00012/00094 | Loss: 386.9418 | CE: 0.2019 | KD: 1061.0149\n",
      "Train Epoch: 024 Batch: 00013/00094 | Loss: 386.7993 | CE: 0.1556 | KD: 1060.7511\n",
      "Train Epoch: 024 Batch: 00014/00094 | Loss: 386.8239 | CE: 0.1844 | KD: 1060.7393\n",
      "Train Epoch: 024 Batch: 00015/00094 | Loss: 386.8093 | CE: 0.1413 | KD: 1060.8176\n",
      "Train Epoch: 024 Batch: 00016/00094 | Loss: 386.8310 | CE: 0.1816 | KD: 1060.7664\n",
      "Train Epoch: 024 Batch: 00017/00094 | Loss: 386.8644 | CE: 0.1461 | KD: 1060.9556\n",
      "Train Epoch: 024 Batch: 00018/00094 | Loss: 386.8772 | CE: 0.1725 | KD: 1060.9183\n",
      "Train Epoch: 024 Batch: 00019/00094 | Loss: 386.8181 | CE: 0.1379 | KD: 1060.8512\n",
      "Train Epoch: 024 Batch: 00020/00094 | Loss: 386.8875 | CE: 0.2100 | KD: 1060.8436\n",
      "Train Epoch: 024 Batch: 00021/00094 | Loss: 386.7108 | CE: 0.1497 | KD: 1060.5242\n",
      "Train Epoch: 024 Batch: 00022/00094 | Loss: 386.9155 | CE: 0.2500 | KD: 1060.8109\n",
      "Train Epoch: 024 Batch: 00023/00094 | Loss: 386.7154 | CE: 0.1622 | KD: 1060.5027\n",
      "Train Epoch: 024 Batch: 00024/00094 | Loss: 386.8713 | CE: 0.1398 | KD: 1060.9916\n",
      "Train Epoch: 024 Batch: 00025/00094 | Loss: 386.8474 | CE: 0.1827 | KD: 1060.8085\n",
      "Train Epoch: 024 Batch: 00026/00094 | Loss: 386.8456 | CE: 0.1357 | KD: 1060.9325\n",
      "Train Epoch: 024 Batch: 00027/00094 | Loss: 386.8318 | CE: 0.1878 | KD: 1060.7516\n",
      "Train Epoch: 024 Batch: 00028/00094 | Loss: 386.9420 | CE: 0.2509 | KD: 1060.8809\n",
      "Train Epoch: 024 Batch: 00029/00094 | Loss: 386.9259 | CE: 0.1717 | KD: 1061.0543\n",
      "Train Epoch: 024 Batch: 00030/00094 | Loss: 386.8288 | CE: 0.1782 | KD: 1060.7699\n",
      "Train Epoch: 024 Batch: 00031/00094 | Loss: 386.9052 | CE: 0.1777 | KD: 1060.9807\n",
      "Train Epoch: 024 Batch: 00032/00094 | Loss: 386.7162 | CE: 0.1588 | KD: 1060.5142\n",
      "Train Epoch: 024 Batch: 00033/00094 | Loss: 386.9084 | CE: 0.1962 | KD: 1060.9388\n",
      "Train Epoch: 024 Batch: 00034/00094 | Loss: 386.9771 | CE: 0.2183 | KD: 1061.0669\n",
      "Train Epoch: 024 Batch: 00035/00094 | Loss: 386.7128 | CE: 0.1886 | KD: 1060.4231\n",
      "Train Epoch: 024 Batch: 00036/00094 | Loss: 386.7028 | CE: 0.1386 | KD: 1060.5330\n",
      "Train Epoch: 024 Batch: 00037/00094 | Loss: 386.9668 | CE: 0.2483 | KD: 1060.9562\n",
      "Train Epoch: 024 Batch: 00038/00094 | Loss: 386.7743 | CE: 0.1369 | KD: 1060.7336\n",
      "Train Epoch: 024 Batch: 00039/00094 | Loss: 386.9138 | CE: 0.2413 | KD: 1060.8298\n",
      "Train Epoch: 024 Batch: 00040/00094 | Loss: 386.7289 | CE: 0.1867 | KD: 1060.4724\n",
      "Train Epoch: 024 Batch: 00041/00094 | Loss: 386.7578 | CE: 0.1701 | KD: 1060.5972\n",
      "Train Epoch: 024 Batch: 00042/00094 | Loss: 386.7471 | CE: 0.2190 | KD: 1060.4338\n",
      "Train Epoch: 024 Batch: 00043/00094 | Loss: 386.8216 | CE: 0.1490 | KD: 1060.8303\n",
      "Train Epoch: 024 Batch: 00044/00094 | Loss: 386.7193 | CE: 0.1437 | KD: 1060.5641\n",
      "Train Epoch: 024 Batch: 00045/00094 | Loss: 386.8994 | CE: 0.2054 | KD: 1060.8890\n",
      "Train Epoch: 024 Batch: 00046/00094 | Loss: 386.7312 | CE: 0.1306 | KD: 1060.6324\n",
      "Train Epoch: 024 Batch: 00047/00094 | Loss: 386.8080 | CE: 0.1560 | KD: 1060.7738\n",
      "Train Epoch: 024 Batch: 00048/00094 | Loss: 386.8792 | CE: 0.1460 | KD: 1060.9965\n",
      "Train Epoch: 024 Batch: 00049/00094 | Loss: 386.8020 | CE: 0.1581 | KD: 1060.7516\n",
      "Train Epoch: 024 Batch: 00050/00094 | Loss: 386.6573 | CE: 0.1311 | KD: 1060.4286\n",
      "Train Epoch: 024 Batch: 00051/00094 | Loss: 386.9044 | CE: 0.2187 | KD: 1060.8662\n",
      "Train Epoch: 024 Batch: 00052/00094 | Loss: 387.0114 | CE: 0.2084 | KD: 1061.1880\n",
      "Train Epoch: 024 Batch: 00053/00094 | Loss: 386.7603 | CE: 0.1654 | KD: 1060.6172\n",
      "Train Epoch: 024 Batch: 00054/00094 | Loss: 387.0218 | CE: 0.1649 | KD: 1061.3361\n",
      "Train Epoch: 024 Batch: 00055/00094 | Loss: 386.8434 | CE: 0.1680 | KD: 1060.8380\n",
      "Train Epoch: 024 Batch: 00056/00094 | Loss: 386.8023 | CE: 0.1384 | KD: 1060.8063\n",
      "Train Epoch: 024 Batch: 00057/00094 | Loss: 386.7659 | CE: 0.1413 | KD: 1060.6986\n",
      "Train Epoch: 024 Batch: 00058/00094 | Loss: 386.9573 | CE: 0.2101 | KD: 1061.0349\n",
      "Train Epoch: 024 Batch: 00059/00094 | Loss: 386.7060 | CE: 0.1635 | KD: 1060.4733\n",
      "Train Epoch: 024 Batch: 00060/00094 | Loss: 386.8917 | CE: 0.2854 | KD: 1060.6482\n",
      "Train Epoch: 024 Batch: 00061/00094 | Loss: 386.7329 | CE: 0.1523 | KD: 1060.5779\n",
      "Train Epoch: 024 Batch: 00062/00094 | Loss: 386.8400 | CE: 0.1886 | KD: 1060.7722\n",
      "Train Epoch: 024 Batch: 00063/00094 | Loss: 386.7603 | CE: 0.1598 | KD: 1060.6326\n",
      "Train Epoch: 024 Batch: 00064/00094 | Loss: 386.8303 | CE: 0.2315 | KD: 1060.6278\n",
      "Train Epoch: 024 Batch: 00065/00094 | Loss: 386.7564 | CE: 0.1540 | KD: 1060.6376\n",
      "Train Epoch: 024 Batch: 00066/00094 | Loss: 386.8125 | CE: 0.1650 | KD: 1060.7615\n",
      "Train Epoch: 024 Batch: 00067/00094 | Loss: 386.9172 | CE: 0.1898 | KD: 1060.9806\n",
      "Train Epoch: 024 Batch: 00068/00094 | Loss: 386.7352 | CE: 0.1505 | KD: 1060.5892\n",
      "Train Epoch: 024 Batch: 00069/00094 | Loss: 386.7482 | CE: 0.1580 | KD: 1060.6042\n",
      "Train Epoch: 024 Batch: 00070/00094 | Loss: 386.7523 | CE: 0.1680 | KD: 1060.5881\n",
      "Train Epoch: 024 Batch: 00071/00094 | Loss: 386.7593 | CE: 0.1402 | KD: 1060.6833\n",
      "Train Epoch: 024 Batch: 00072/00094 | Loss: 386.7478 | CE: 0.1408 | KD: 1060.6503\n",
      "Train Epoch: 024 Batch: 00073/00094 | Loss: 386.9413 | CE: 0.2716 | KD: 1060.8220\n",
      "Train Epoch: 024 Batch: 00074/00094 | Loss: 386.7376 | CE: 0.1811 | KD: 1060.5117\n",
      "Train Epoch: 024 Batch: 00075/00094 | Loss: 386.7852 | CE: 0.1507 | KD: 1060.7255\n",
      "Train Epoch: 024 Batch: 00076/00094 | Loss: 386.8001 | CE: 0.2279 | KD: 1060.5548\n",
      "Train Epoch: 024 Batch: 00077/00094 | Loss: 386.7186 | CE: 0.1274 | KD: 1060.6068\n",
      "Train Epoch: 024 Batch: 00078/00094 | Loss: 386.7691 | CE: 0.1279 | KD: 1060.7440\n",
      "Train Epoch: 024 Batch: 00079/00094 | Loss: 386.7758 | CE: 0.1515 | KD: 1060.6976\n",
      "Train Epoch: 024 Batch: 00080/00094 | Loss: 386.7898 | CE: 0.1547 | KD: 1060.7274\n",
      "Train Epoch: 024 Batch: 00081/00094 | Loss: 386.8025 | CE: 0.1652 | KD: 1060.7333\n",
      "Train Epoch: 024 Batch: 00082/00094 | Loss: 386.7836 | CE: 0.1459 | KD: 1060.7346\n",
      "Train Epoch: 024 Batch: 00083/00094 | Loss: 386.9637 | CE: 0.2445 | KD: 1060.9580\n",
      "Train Epoch: 024 Batch: 00084/00094 | Loss: 386.9221 | CE: 0.2750 | KD: 1060.7603\n",
      "Train Epoch: 024 Batch: 00085/00094 | Loss: 386.8274 | CE: 0.1330 | KD: 1060.8901\n",
      "Train Epoch: 024 Batch: 00086/00094 | Loss: 386.7744 | CE: 0.1830 | KD: 1060.6075\n",
      "Train Epoch: 024 Batch: 00087/00094 | Loss: 386.8943 | CE: 0.1425 | KD: 1061.0474\n",
      "Train Epoch: 024 Batch: 00088/00094 | Loss: 387.0341 | CE: 0.2375 | KD: 1061.1705\n",
      "Train Epoch: 024 Batch: 00089/00094 | Loss: 386.9974 | CE: 0.1782 | KD: 1061.2322\n",
      "Train Epoch: 024 Batch: 00090/00094 | Loss: 386.7825 | CE: 0.1379 | KD: 1060.7535\n",
      "Train Epoch: 024 Batch: 00091/00094 | Loss: 386.9307 | CE: 0.1634 | KD: 1061.0900\n",
      "Train Epoch: 024 Batch: 00092/00094 | Loss: 386.8355 | CE: 0.1539 | KD: 1060.8550\n",
      "Train Epoch: 024 Batch: 00093/00094 | Loss: 386.7187 | CE: 0.1140 | KD: 1060.6439\n",
      "Train Epoch: 024 Batch: 00094/00094 | Loss: 387.1816 | CE: 0.2694 | KD: 1061.4875\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1775 | acc:96.4500\n",
      "[VAL Acc] Target: 96.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6825 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1121 | acc:56.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0227 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3443 | acc:53.1740\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.17%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5416 | acc:72.7357\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0320 | acc:61.7555\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7671 | acc:61.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.44%\n",
      "[VAL Acc] Avg 62.73%\n",
      "Train Epoch: 025 Batch: 00001/00094 | Loss: 386.8285 | CE: 0.1800 | KD: 1060.7640\n",
      "Train Epoch: 025 Batch: 00002/00094 | Loss: 386.7514 | CE: 0.1646 | KD: 1060.5950\n",
      "Train Epoch: 025 Batch: 00003/00094 | Loss: 386.7736 | CE: 0.1523 | KD: 1060.6896\n",
      "Train Epoch: 025 Batch: 00004/00094 | Loss: 386.7027 | CE: 0.1341 | KD: 1060.5448\n",
      "Train Epoch: 025 Batch: 00005/00094 | Loss: 386.9015 | CE: 0.1791 | KD: 1060.9668\n",
      "Train Epoch: 025 Batch: 00006/00094 | Loss: 386.7562 | CE: 0.1294 | KD: 1060.7046\n",
      "Train Epoch: 025 Batch: 00007/00094 | Loss: 386.8885 | CE: 0.1752 | KD: 1060.9419\n",
      "Train Epoch: 025 Batch: 00008/00094 | Loss: 386.8494 | CE: 0.1912 | KD: 1060.7909\n",
      "Train Epoch: 025 Batch: 00009/00094 | Loss: 387.0577 | CE: 0.3319 | KD: 1060.9763\n",
      "Train Epoch: 025 Batch: 00010/00094 | Loss: 386.8306 | CE: 0.2179 | KD: 1060.6658\n",
      "Train Epoch: 025 Batch: 00011/00094 | Loss: 386.8160 | CE: 0.1682 | KD: 1060.7621\n",
      "Train Epoch: 025 Batch: 00012/00094 | Loss: 386.8289 | CE: 0.1751 | KD: 1060.7787\n",
      "Train Epoch: 025 Batch: 00013/00094 | Loss: 386.8243 | CE: 0.1629 | KD: 1060.7997\n",
      "Train Epoch: 025 Batch: 00014/00094 | Loss: 386.7878 | CE: 0.1475 | KD: 1060.7415\n",
      "Train Epoch: 025 Batch: 00015/00094 | Loss: 386.6926 | CE: 0.1316 | KD: 1060.5240\n",
      "Train Epoch: 025 Batch: 00016/00094 | Loss: 386.8630 | CE: 0.1258 | KD: 1061.0073\n",
      "Train Epoch: 025 Batch: 00017/00094 | Loss: 386.8585 | CE: 0.1770 | KD: 1060.8545\n",
      "Train Epoch: 025 Batch: 00018/00094 | Loss: 386.9190 | CE: 0.1703 | KD: 1061.0389\n",
      "Train Epoch: 025 Batch: 00019/00094 | Loss: 386.8277 | CE: 0.1861 | KD: 1060.7451\n",
      "Train Epoch: 025 Batch: 00020/00094 | Loss: 386.7776 | CE: 0.1913 | KD: 1060.5933\n",
      "Train Epoch: 025 Batch: 00021/00094 | Loss: 386.9071 | CE: 0.1809 | KD: 1060.9772\n",
      "Train Epoch: 025 Batch: 00022/00094 | Loss: 386.7271 | CE: 0.1510 | KD: 1060.5654\n",
      "Train Epoch: 025 Batch: 00023/00094 | Loss: 386.8055 | CE: 0.1117 | KD: 1060.8883\n",
      "Train Epoch: 025 Batch: 00024/00094 | Loss: 386.8383 | CE: 0.1613 | KD: 1060.8422\n",
      "Train Epoch: 025 Batch: 00025/00094 | Loss: 386.8026 | CE: 0.1621 | KD: 1060.7422\n",
      "Train Epoch: 025 Batch: 00026/00094 | Loss: 386.7507 | CE: 0.2019 | KD: 1060.4905\n",
      "Train Epoch: 025 Batch: 00027/00094 | Loss: 386.9174 | CE: 0.1477 | KD: 1061.0966\n",
      "Train Epoch: 025 Batch: 00028/00094 | Loss: 386.7682 | CE: 0.1724 | KD: 1060.6193\n",
      "Train Epoch: 025 Batch: 00029/00094 | Loss: 386.8633 | CE: 0.1735 | KD: 1060.8773\n",
      "Train Epoch: 025 Batch: 00030/00094 | Loss: 386.7173 | CE: 0.1352 | KD: 1060.5820\n",
      "Train Epoch: 025 Batch: 00031/00094 | Loss: 386.8147 | CE: 0.1549 | KD: 1060.7950\n",
      "Train Epoch: 025 Batch: 00032/00094 | Loss: 386.8112 | CE: 0.1640 | KD: 1060.7605\n",
      "Train Epoch: 025 Batch: 00033/00094 | Loss: 386.7513 | CE: 0.1946 | KD: 1060.5123\n",
      "Train Epoch: 025 Batch: 00034/00094 | Loss: 386.8870 | CE: 0.1853 | KD: 1060.9103\n",
      "Train Epoch: 025 Batch: 00035/00094 | Loss: 386.7348 | CE: 0.1771 | KD: 1060.5148\n",
      "Train Epoch: 025 Batch: 00036/00094 | Loss: 386.8008 | CE: 0.1938 | KD: 1060.6504\n",
      "Train Epoch: 025 Batch: 00037/00094 | Loss: 386.8282 | CE: 0.1581 | KD: 1060.8235\n",
      "Train Epoch: 025 Batch: 00038/00094 | Loss: 386.9088 | CE: 0.2451 | KD: 1060.8059\n",
      "Train Epoch: 025 Batch: 00039/00094 | Loss: 386.9883 | CE: 0.2304 | KD: 1061.0642\n",
      "Train Epoch: 025 Batch: 00040/00094 | Loss: 386.8229 | CE: 0.1690 | KD: 1060.7789\n",
      "Train Epoch: 025 Batch: 00041/00094 | Loss: 386.9345 | CE: 0.2533 | KD: 1060.8539\n",
      "Train Epoch: 025 Batch: 00042/00094 | Loss: 386.8477 | CE: 0.1837 | KD: 1060.8065\n",
      "Train Epoch: 025 Batch: 00043/00094 | Loss: 386.8370 | CE: 0.1905 | KD: 1060.7585\n",
      "Train Epoch: 025 Batch: 00044/00094 | Loss: 386.7325 | CE: 0.1283 | KD: 1060.6423\n",
      "Train Epoch: 025 Batch: 00045/00094 | Loss: 386.8150 | CE: 0.1623 | KD: 1060.7756\n",
      "Train Epoch: 025 Batch: 00046/00094 | Loss: 386.8721 | CE: 0.1849 | KD: 1060.8702\n",
      "Train Epoch: 025 Batch: 00047/00094 | Loss: 386.7744 | CE: 0.1903 | KD: 1060.5874\n",
      "Train Epoch: 025 Batch: 00048/00094 | Loss: 386.8543 | CE: 0.1872 | KD: 1060.8153\n",
      "Train Epoch: 025 Batch: 00049/00094 | Loss: 386.7140 | CE: 0.1550 | KD: 1060.5186\n",
      "Train Epoch: 025 Batch: 00050/00094 | Loss: 386.7103 | CE: 0.1396 | KD: 1060.5507\n",
      "Train Epoch: 025 Batch: 00051/00094 | Loss: 386.8535 | CE: 0.2291 | KD: 1060.6981\n",
      "Train Epoch: 025 Batch: 00052/00094 | Loss: 386.6772 | CE: 0.1471 | KD: 1060.4392\n",
      "Train Epoch: 025 Batch: 00053/00094 | Loss: 386.8585 | CE: 0.2365 | KD: 1060.6915\n",
      "Train Epoch: 025 Batch: 00054/00094 | Loss: 386.8504 | CE: 0.1423 | KD: 1060.9279\n",
      "Train Epoch: 025 Batch: 00055/00094 | Loss: 386.8969 | CE: 0.1826 | KD: 1060.9446\n",
      "Train Epoch: 025 Batch: 00056/00094 | Loss: 386.9021 | CE: 0.2657 | KD: 1060.7308\n",
      "Train Epoch: 025 Batch: 00057/00094 | Loss: 386.8587 | CE: 0.1534 | KD: 1060.9199\n",
      "Train Epoch: 025 Batch: 00058/00094 | Loss: 386.7433 | CE: 0.1764 | KD: 1060.5404\n",
      "Train Epoch: 025 Batch: 00059/00094 | Loss: 386.8460 | CE: 0.2081 | KD: 1060.7351\n",
      "Train Epoch: 025 Batch: 00060/00094 | Loss: 386.8131 | CE: 0.1881 | KD: 1060.6997\n",
      "Train Epoch: 025 Batch: 00061/00094 | Loss: 386.8836 | CE: 0.1371 | KD: 1061.0330\n",
      "Train Epoch: 025 Batch: 00062/00094 | Loss: 386.8020 | CE: 0.1571 | KD: 1060.7543\n",
      "Train Epoch: 025 Batch: 00063/00094 | Loss: 386.7843 | CE: 0.1896 | KD: 1060.6166\n",
      "Train Epoch: 025 Batch: 00064/00094 | Loss: 386.8020 | CE: 0.1503 | KD: 1060.7728\n",
      "Train Epoch: 025 Batch: 00065/00094 | Loss: 386.8979 | CE: 0.1969 | KD: 1060.9082\n",
      "Train Epoch: 025 Batch: 00066/00094 | Loss: 386.8542 | CE: 0.1735 | KD: 1060.8524\n",
      "Train Epoch: 025 Batch: 00067/00094 | Loss: 386.8764 | CE: 0.1998 | KD: 1060.8412\n",
      "Train Epoch: 025 Batch: 00068/00094 | Loss: 386.8630 | CE: 0.1773 | KD: 1060.8662\n",
      "Train Epoch: 025 Batch: 00069/00094 | Loss: 386.9240 | CE: 0.1354 | KD: 1061.1486\n",
      "Train Epoch: 025 Batch: 00070/00094 | Loss: 386.8368 | CE: 0.1580 | KD: 1060.8472\n",
      "Train Epoch: 025 Batch: 00071/00094 | Loss: 386.9801 | CE: 0.2659 | KD: 1060.9442\n",
      "Train Epoch: 025 Batch: 00072/00094 | Loss: 386.8017 | CE: 0.1876 | KD: 1060.6697\n",
      "Train Epoch: 025 Batch: 00073/00094 | Loss: 386.7686 | CE: 0.1710 | KD: 1060.6244\n",
      "Train Epoch: 025 Batch: 00074/00094 | Loss: 386.8500 | CE: 0.1498 | KD: 1060.9061\n",
      "Train Epoch: 025 Batch: 00075/00094 | Loss: 386.8463 | CE: 0.2011 | KD: 1060.7549\n",
      "Train Epoch: 025 Batch: 00076/00094 | Loss: 386.8722 | CE: 0.1507 | KD: 1060.9644\n",
      "Train Epoch: 025 Batch: 00077/00094 | Loss: 387.0156 | CE: 0.2829 | KD: 1060.9952\n",
      "Train Epoch: 025 Batch: 00078/00094 | Loss: 386.9612 | CE: 0.2869 | KD: 1060.8348\n",
      "Train Epoch: 025 Batch: 00079/00094 | Loss: 386.7638 | CE: 0.1599 | KD: 1060.6420\n",
      "Train Epoch: 025 Batch: 00080/00094 | Loss: 386.8235 | CE: 0.1713 | KD: 1060.7743\n",
      "Train Epoch: 025 Batch: 00081/00094 | Loss: 386.9550 | CE: 0.2308 | KD: 1060.9718\n",
      "Train Epoch: 025 Batch: 00082/00094 | Loss: 386.7990 | CE: 0.1478 | KD: 1060.7716\n",
      "Train Epoch: 025 Batch: 00083/00094 | Loss: 386.8309 | CE: 0.1436 | KD: 1060.8706\n",
      "Train Epoch: 025 Batch: 00084/00094 | Loss: 386.7136 | CE: 0.1366 | KD: 1060.5681\n",
      "Train Epoch: 025 Batch: 00085/00094 | Loss: 386.9287 | CE: 0.1951 | KD: 1060.9976\n",
      "Train Epoch: 025 Batch: 00086/00094 | Loss: 386.8767 | CE: 0.1572 | KD: 1060.9587\n",
      "Train Epoch: 025 Batch: 00087/00094 | Loss: 386.7033 | CE: 0.1875 | KD: 1060.4001\n",
      "Train Epoch: 025 Batch: 00088/00094 | Loss: 386.7784 | CE: 0.1456 | KD: 1060.7212\n",
      "Train Epoch: 025 Batch: 00089/00094 | Loss: 386.8337 | CE: 0.2149 | KD: 1060.6826\n",
      "Train Epoch: 025 Batch: 00090/00094 | Loss: 386.8722 | CE: 0.1767 | KD: 1060.8931\n",
      "Train Epoch: 025 Batch: 00091/00094 | Loss: 386.7928 | CE: 0.1749 | KD: 1060.6803\n",
      "Train Epoch: 025 Batch: 00092/00094 | Loss: 386.8264 | CE: 0.1354 | KD: 1060.8807\n",
      "Train Epoch: 025 Batch: 00093/00094 | Loss: 386.7649 | CE: 0.1411 | KD: 1060.6964\n",
      "Train Epoch: 025 Batch: 00094/00094 | Loss: 386.7142 | CE: 0.1897 | KD: 1060.4240\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1664 | acc:96.9000\n",
      "[VAL Acc] Target: 96.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6669 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0629 | acc:56.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0348 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2236 | acc:54.7806\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5739 | acc:69.8706\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8490 | acc:65.7524\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7883 | acc:60.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.69%\n",
      "[VAL Acc] Avg 62.87%\n",
      "Train Epoch: 026 Batch: 00001/00094 | Loss: 348.0499 | CE: 0.1285 | KD: 1060.5743\n",
      "Train Epoch: 026 Batch: 00002/00094 | Loss: 348.0072 | CE: 0.1282 | KD: 1060.4449\n",
      "Train Epoch: 026 Batch: 00003/00094 | Loss: 348.1550 | CE: 0.1820 | KD: 1060.7317\n",
      "Train Epoch: 026 Batch: 00004/00094 | Loss: 348.0834 | CE: 0.1462 | KD: 1060.6227\n",
      "Train Epoch: 026 Batch: 00005/00094 | Loss: 348.2220 | CE: 0.1509 | KD: 1061.0308\n",
      "Train Epoch: 026 Batch: 00006/00094 | Loss: 348.0741 | CE: 0.1212 | KD: 1060.6704\n",
      "Train Epoch: 026 Batch: 00007/00094 | Loss: 348.0374 | CE: 0.1207 | KD: 1060.5601\n",
      "Train Epoch: 026 Batch: 00008/00094 | Loss: 348.3034 | CE: 0.1855 | KD: 1061.1731\n",
      "Train Epoch: 026 Batch: 00009/00094 | Loss: 348.2078 | CE: 0.1822 | KD: 1060.8921\n",
      "Train Epoch: 026 Batch: 00010/00094 | Loss: 348.1447 | CE: 0.1460 | KD: 1060.8101\n",
      "Train Epoch: 026 Batch: 00011/00094 | Loss: 348.2230 | CE: 0.2518 | KD: 1060.7260\n",
      "Train Epoch: 026 Batch: 00012/00094 | Loss: 348.0626 | CE: 0.1859 | KD: 1060.4380\n",
      "Train Epoch: 026 Batch: 00013/00094 | Loss: 348.3156 | CE: 0.2694 | KD: 1060.9546\n",
      "Train Epoch: 026 Batch: 00014/00094 | Loss: 348.0981 | CE: 0.1642 | KD: 1060.6127\n",
      "Train Epoch: 026 Batch: 00015/00094 | Loss: 348.1266 | CE: 0.1730 | KD: 1060.6725\n",
      "Train Epoch: 026 Batch: 00016/00094 | Loss: 348.1172 | CE: 0.1784 | KD: 1060.6274\n",
      "Train Epoch: 026 Batch: 00017/00094 | Loss: 348.1220 | CE: 0.1446 | KD: 1060.7451\n",
      "Train Epoch: 026 Batch: 00018/00094 | Loss: 348.2202 | CE: 0.1539 | KD: 1061.0160\n",
      "Train Epoch: 026 Batch: 00019/00094 | Loss: 348.0734 | CE: 0.1369 | KD: 1060.6202\n",
      "Train Epoch: 026 Batch: 00020/00094 | Loss: 348.1050 | CE: 0.1507 | KD: 1060.6747\n",
      "Train Epoch: 026 Batch: 00021/00094 | Loss: 348.1826 | CE: 0.1557 | KD: 1060.8960\n",
      "Train Epoch: 026 Batch: 00022/00094 | Loss: 348.1796 | CE: 0.1551 | KD: 1060.8885\n",
      "Train Epoch: 026 Batch: 00023/00094 | Loss: 348.2850 | CE: 0.2422 | KD: 1060.9443\n",
      "Train Epoch: 026 Batch: 00024/00094 | Loss: 348.0034 | CE: 0.1364 | KD: 1060.4086\n",
      "Train Epoch: 026 Batch: 00025/00094 | Loss: 348.0872 | CE: 0.1319 | KD: 1060.6775\n",
      "Train Epoch: 026 Batch: 00026/00094 | Loss: 348.1032 | CE: 0.1710 | KD: 1060.6074\n",
      "Train Epoch: 026 Batch: 00027/00094 | Loss: 348.0865 | CE: 0.1748 | KD: 1060.5449\n",
      "Train Epoch: 026 Batch: 00028/00094 | Loss: 348.2818 | CE: 0.1809 | KD: 1061.1216\n",
      "Train Epoch: 026 Batch: 00029/00094 | Loss: 348.1412 | CE: 0.1518 | KD: 1060.7817\n",
      "Train Epoch: 026 Batch: 00030/00094 | Loss: 348.1001 | CE: 0.1329 | KD: 1060.7139\n",
      "Train Epoch: 026 Batch: 00031/00094 | Loss: 348.1127 | CE: 0.1333 | KD: 1060.7512\n",
      "Train Epoch: 026 Batch: 00032/00094 | Loss: 348.2511 | CE: 0.2120 | KD: 1060.9330\n",
      "Train Epoch: 026 Batch: 00033/00094 | Loss: 348.1230 | CE: 0.1374 | KD: 1060.7701\n",
      "Train Epoch: 026 Batch: 00034/00094 | Loss: 348.0241 | CE: 0.1517 | KD: 1060.4249\n",
      "Train Epoch: 026 Batch: 00035/00094 | Loss: 348.1663 | CE: 0.1721 | KD: 1060.7961\n",
      "Train Epoch: 026 Batch: 00036/00094 | Loss: 348.2205 | CE: 0.1483 | KD: 1061.0339\n",
      "Train Epoch: 026 Batch: 00037/00094 | Loss: 348.1767 | CE: 0.1356 | KD: 1060.9392\n",
      "Train Epoch: 026 Batch: 00038/00094 | Loss: 348.1270 | CE: 0.1298 | KD: 1060.8053\n",
      "Train Epoch: 026 Batch: 00039/00094 | Loss: 348.2660 | CE: 0.1983 | KD: 1061.0204\n",
      "Train Epoch: 026 Batch: 00040/00094 | Loss: 348.1706 | CE: 0.1459 | KD: 1060.8892\n",
      "Train Epoch: 026 Batch: 00041/00094 | Loss: 348.0987 | CE: 0.1263 | KD: 1060.7297\n",
      "Train Epoch: 026 Batch: 00042/00094 | Loss: 348.1211 | CE: 0.1403 | KD: 1060.7554\n",
      "Train Epoch: 026 Batch: 00043/00094 | Loss: 348.1393 | CE: 0.1134 | KD: 1060.8928\n",
      "Train Epoch: 026 Batch: 00044/00094 | Loss: 348.1354 | CE: 0.1995 | KD: 1060.6185\n",
      "Train Epoch: 026 Batch: 00045/00094 | Loss: 348.0266 | CE: 0.1629 | KD: 1060.3986\n",
      "Train Epoch: 026 Batch: 00046/00094 | Loss: 348.0904 | CE: 0.1443 | KD: 1060.6494\n",
      "Train Epoch: 026 Batch: 00047/00094 | Loss: 348.0809 | CE: 0.1668 | KD: 1060.5519\n",
      "Train Epoch: 026 Batch: 00048/00094 | Loss: 348.1179 | CE: 0.1550 | KD: 1060.7007\n",
      "Train Epoch: 026 Batch: 00049/00094 | Loss: 348.1732 | CE: 0.1352 | KD: 1060.9297\n",
      "Train Epoch: 026 Batch: 00050/00094 | Loss: 348.0602 | CE: 0.1284 | KD: 1060.6062\n",
      "Train Epoch: 026 Batch: 00051/00094 | Loss: 348.1453 | CE: 0.1789 | KD: 1060.7115\n",
      "Train Epoch: 026 Batch: 00052/00094 | Loss: 348.4124 | CE: 0.2515 | KD: 1061.3042\n",
      "Train Epoch: 026 Batch: 00053/00094 | Loss: 348.1570 | CE: 0.1897 | KD: 1060.7142\n",
      "Train Epoch: 026 Batch: 00054/00094 | Loss: 348.0727 | CE: 0.1186 | KD: 1060.6741\n",
      "Train Epoch: 026 Batch: 00055/00094 | Loss: 348.0861 | CE: 0.1413 | KD: 1060.6454\n",
      "Train Epoch: 026 Batch: 00056/00094 | Loss: 348.1744 | CE: 0.1602 | KD: 1060.8574\n",
      "Train Epoch: 026 Batch: 00057/00094 | Loss: 348.2044 | CE: 0.1821 | KD: 1060.8818\n",
      "Train Epoch: 026 Batch: 00058/00094 | Loss: 348.2609 | CE: 0.2677 | KD: 1060.7931\n",
      "Train Epoch: 026 Batch: 00059/00094 | Loss: 348.1497 | CE: 0.1482 | KD: 1060.8187\n",
      "Train Epoch: 026 Batch: 00060/00094 | Loss: 348.2965 | CE: 0.1816 | KD: 1061.1643\n",
      "Train Epoch: 026 Batch: 00061/00094 | Loss: 348.1084 | CE: 0.1480 | KD: 1060.6934\n",
      "Train Epoch: 026 Batch: 00062/00094 | Loss: 348.1490 | CE: 0.1640 | KD: 1060.7684\n",
      "Train Epoch: 026 Batch: 00063/00094 | Loss: 348.0927 | CE: 0.1363 | KD: 1060.6809\n",
      "Train Epoch: 026 Batch: 00064/00094 | Loss: 348.1267 | CE: 0.1464 | KD: 1060.7540\n",
      "Train Epoch: 026 Batch: 00065/00094 | Loss: 348.0862 | CE: 0.1680 | KD: 1060.5645\n",
      "Train Epoch: 026 Batch: 00066/00094 | Loss: 348.0668 | CE: 0.1151 | KD: 1060.6667\n",
      "Train Epoch: 026 Batch: 00067/00094 | Loss: 348.0782 | CE: 0.1677 | KD: 1060.5411\n",
      "Train Epoch: 026 Batch: 00068/00094 | Loss: 348.1804 | CE: 0.2359 | KD: 1060.6445\n",
      "Train Epoch: 026 Batch: 00069/00094 | Loss: 348.0632 | CE: 0.1281 | KD: 1060.6162\n",
      "Train Epoch: 026 Batch: 00070/00094 | Loss: 348.1097 | CE: 0.1518 | KD: 1060.6855\n",
      "Train Epoch: 026 Batch: 00071/00094 | Loss: 348.1624 | CE: 0.1751 | KD: 1060.7750\n",
      "Train Epoch: 026 Batch: 00072/00094 | Loss: 348.2165 | CE: 0.1927 | KD: 1060.8866\n",
      "Train Epoch: 026 Batch: 00073/00094 | Loss: 348.0816 | CE: 0.1626 | KD: 1060.5670\n",
      "Train Epoch: 026 Batch: 00074/00094 | Loss: 348.1329 | CE: 0.1526 | KD: 1060.7540\n",
      "Train Epoch: 026 Batch: 00075/00094 | Loss: 348.2291 | CE: 0.1675 | KD: 1061.0015\n",
      "Train Epoch: 026 Batch: 00076/00094 | Loss: 348.2541 | CE: 0.1507 | KD: 1061.1290\n",
      "Train Epoch: 026 Batch: 00077/00094 | Loss: 348.1139 | CE: 0.1351 | KD: 1060.7495\n",
      "Train Epoch: 026 Batch: 00078/00094 | Loss: 348.2483 | CE: 0.2172 | KD: 1060.9089\n",
      "Train Epoch: 026 Batch: 00079/00094 | Loss: 348.1782 | CE: 0.1341 | KD: 1060.9481\n",
      "Train Epoch: 026 Batch: 00080/00094 | Loss: 348.0717 | CE: 0.1440 | KD: 1060.5935\n",
      "Train Epoch: 026 Batch: 00081/00094 | Loss: 348.3306 | CE: 0.2295 | KD: 1061.1221\n",
      "Train Epoch: 026 Batch: 00082/00094 | Loss: 348.2631 | CE: 0.1717 | KD: 1061.0925\n",
      "Train Epoch: 026 Batch: 00083/00094 | Loss: 348.0992 | CE: 0.1729 | KD: 1060.5892\n",
      "Train Epoch: 026 Batch: 00084/00094 | Loss: 348.1529 | CE: 0.1662 | KD: 1060.7733\n",
      "Train Epoch: 026 Batch: 00085/00094 | Loss: 348.2349 | CE: 0.1488 | KD: 1061.0764\n",
      "Train Epoch: 026 Batch: 00086/00094 | Loss: 348.3358 | CE: 0.1512 | KD: 1061.3768\n",
      "Train Epoch: 026 Batch: 00087/00094 | Loss: 348.2082 | CE: 0.1324 | KD: 1061.0448\n",
      "Train Epoch: 026 Batch: 00088/00094 | Loss: 348.1194 | CE: 0.1712 | KD: 1060.6560\n",
      "Train Epoch: 026 Batch: 00089/00094 | Loss: 348.0429 | CE: 0.1541 | KD: 1060.4750\n",
      "Train Epoch: 026 Batch: 00090/00094 | Loss: 348.1235 | CE: 0.1217 | KD: 1060.8192\n",
      "Train Epoch: 026 Batch: 00091/00094 | Loss: 348.1109 | CE: 0.1253 | KD: 1060.7699\n",
      "Train Epoch: 026 Batch: 00092/00094 | Loss: 348.1018 | CE: 0.1566 | KD: 1060.6469\n",
      "Train Epoch: 026 Batch: 00093/00094 | Loss: 348.2934 | CE: 0.2093 | KD: 1061.0702\n",
      "Train Epoch: 026 Batch: 00094/00094 | Loss: 348.1895 | CE: 0.2496 | KD: 1060.6309\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1596 | acc:97.0500\n",
      "[VAL Acc] Target: 97.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5817 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0265 | acc:57.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 57.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0510 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2594 | acc:53.8401\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5902 | acc:68.9464\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9181 | acc:63.4404\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7762 | acc:60.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.94%\n",
      "[VAL Acc] Avg 62.11%\n",
      "Train Epoch: 027 Batch: 00001/00094 | Loss: 348.1280 | CE: 0.1577 | KD: 1060.7236\n",
      "Train Epoch: 027 Batch: 00002/00094 | Loss: 348.2198 | CE: 0.2068 | KD: 1060.8534\n",
      "Train Epoch: 027 Batch: 00003/00094 | Loss: 347.9792 | CE: 0.1195 | KD: 1060.3862\n",
      "Train Epoch: 027 Batch: 00004/00094 | Loss: 348.0502 | CE: 0.1730 | KD: 1060.4397\n",
      "Train Epoch: 027 Batch: 00005/00094 | Loss: 348.0600 | CE: 0.1370 | KD: 1060.5792\n",
      "Train Epoch: 027 Batch: 00006/00094 | Loss: 348.2523 | CE: 0.1854 | KD: 1061.0181\n",
      "Train Epoch: 027 Batch: 00007/00094 | Loss: 348.1601 | CE: 0.2138 | KD: 1060.6504\n",
      "Train Epoch: 027 Batch: 00008/00094 | Loss: 348.1564 | CE: 0.1448 | KD: 1060.8492\n",
      "Train Epoch: 027 Batch: 00009/00094 | Loss: 348.1730 | CE: 0.1746 | KD: 1060.8090\n",
      "Train Epoch: 027 Batch: 00010/00094 | Loss: 347.9391 | CE: 0.1274 | KD: 1060.2397\n",
      "Train Epoch: 027 Batch: 00011/00094 | Loss: 348.2211 | CE: 0.2554 | KD: 1060.7095\n",
      "Train Epoch: 027 Batch: 00012/00094 | Loss: 348.0868 | CE: 0.1298 | KD: 1060.6830\n",
      "Train Epoch: 027 Batch: 00013/00094 | Loss: 348.0649 | CE: 0.1050 | KD: 1060.6917\n",
      "Train Epoch: 027 Batch: 00014/00094 | Loss: 348.0541 | CE: 0.1428 | KD: 1060.5436\n",
      "Train Epoch: 027 Batch: 00015/00094 | Loss: 348.2717 | CE: 0.2007 | KD: 1061.0303\n",
      "Train Epoch: 027 Batch: 00016/00094 | Loss: 348.2876 | CE: 0.2444 | KD: 1060.9457\n",
      "Train Epoch: 027 Batch: 00017/00094 | Loss: 348.1833 | CE: 0.1314 | KD: 1060.9719\n",
      "Train Epoch: 027 Batch: 00018/00094 | Loss: 348.1852 | CE: 0.1727 | KD: 1060.8522\n",
      "Train Epoch: 027 Batch: 00019/00094 | Loss: 348.1309 | CE: 0.1715 | KD: 1060.6902\n",
      "Train Epoch: 027 Batch: 00020/00094 | Loss: 348.0742 | CE: 0.1536 | KD: 1060.5719\n",
      "Train Epoch: 027 Batch: 00021/00094 | Loss: 348.2594 | CE: 0.1767 | KD: 1061.0662\n",
      "Train Epoch: 027 Batch: 00022/00094 | Loss: 348.1859 | CE: 0.1740 | KD: 1060.8503\n",
      "Train Epoch: 027 Batch: 00023/00094 | Loss: 348.3192 | CE: 0.1992 | KD: 1061.1797\n",
      "Train Epoch: 027 Batch: 00024/00094 | Loss: 348.2093 | CE: 0.1934 | KD: 1060.8624\n",
      "Train Epoch: 027 Batch: 00025/00094 | Loss: 348.1361 | CE: 0.1592 | KD: 1060.7437\n",
      "Train Epoch: 027 Batch: 00026/00094 | Loss: 348.1236 | CE: 0.1584 | KD: 1060.7079\n",
      "Train Epoch: 027 Batch: 00027/00094 | Loss: 348.1475 | CE: 0.1703 | KD: 1060.7441\n",
      "Train Epoch: 027 Batch: 00028/00094 | Loss: 348.0685 | CE: 0.1472 | KD: 1060.5740\n",
      "Train Epoch: 027 Batch: 00029/00094 | Loss: 348.0825 | CE: 0.1456 | KD: 1060.6213\n",
      "Train Epoch: 027 Batch: 00030/00094 | Loss: 348.0848 | CE: 0.1395 | KD: 1060.6473\n",
      "Train Epoch: 027 Batch: 00031/00094 | Loss: 348.2000 | CE: 0.2482 | KD: 1060.6669\n",
      "Train Epoch: 027 Batch: 00032/00094 | Loss: 348.1352 | CE: 0.1254 | KD: 1060.8439\n",
      "Train Epoch: 027 Batch: 00033/00094 | Loss: 348.1597 | CE: 0.1709 | KD: 1060.7800\n",
      "Train Epoch: 027 Batch: 00034/00094 | Loss: 348.1443 | CE: 0.1379 | KD: 1060.8333\n",
      "Train Epoch: 027 Batch: 00035/00094 | Loss: 348.1426 | CE: 0.1273 | KD: 1060.8606\n",
      "Train Epoch: 027 Batch: 00036/00094 | Loss: 348.0640 | CE: 0.1713 | KD: 1060.4868\n",
      "Train Epoch: 027 Batch: 00037/00094 | Loss: 348.0944 | CE: 0.1528 | KD: 1060.6359\n",
      "Train Epoch: 027 Batch: 00038/00094 | Loss: 348.0353 | CE: 0.1427 | KD: 1060.4866\n",
      "Train Epoch: 027 Batch: 00039/00094 | Loss: 348.1129 | CE: 0.1502 | KD: 1060.7001\n",
      "Train Epoch: 027 Batch: 00040/00094 | Loss: 348.1386 | CE: 0.1556 | KD: 1060.7622\n",
      "Train Epoch: 027 Batch: 00041/00094 | Loss: 348.2764 | CE: 0.2117 | KD: 1061.0111\n",
      "Train Epoch: 027 Batch: 00042/00094 | Loss: 348.1755 | CE: 0.2002 | KD: 1060.7388\n",
      "Train Epoch: 027 Batch: 00043/00094 | Loss: 348.1703 | CE: 0.2394 | KD: 1060.6034\n",
      "Train Epoch: 027 Batch: 00044/00094 | Loss: 348.2283 | CE: 0.1697 | KD: 1060.9926\n",
      "Train Epoch: 027 Batch: 00045/00094 | Loss: 348.1906 | CE: 0.1956 | KD: 1060.7985\n",
      "Train Epoch: 027 Batch: 00046/00094 | Loss: 348.0809 | CE: 0.1578 | KD: 1060.5793\n",
      "Train Epoch: 027 Batch: 00047/00094 | Loss: 348.1313 | CE: 0.1597 | KD: 1060.7273\n",
      "Train Epoch: 027 Batch: 00048/00094 | Loss: 348.1597 | CE: 0.1647 | KD: 1060.7985\n",
      "Train Epoch: 027 Batch: 00049/00094 | Loss: 348.1160 | CE: 0.2275 | KD: 1060.4739\n",
      "Train Epoch: 027 Batch: 00050/00094 | Loss: 348.2184 | CE: 0.2730 | KD: 1060.6475\n",
      "Train Epoch: 027 Batch: 00051/00094 | Loss: 348.1901 | CE: 0.1458 | KD: 1060.9490\n",
      "Train Epoch: 027 Batch: 00052/00094 | Loss: 348.0327 | CE: 0.1308 | KD: 1060.5149\n",
      "Train Epoch: 027 Batch: 00053/00094 | Loss: 348.1793 | CE: 0.1774 | KD: 1060.8196\n",
      "Train Epoch: 027 Batch: 00054/00094 | Loss: 348.1984 | CE: 0.1785 | KD: 1060.8744\n",
      "Train Epoch: 027 Batch: 00055/00094 | Loss: 348.1347 | CE: 0.1639 | KD: 1060.7250\n",
      "Train Epoch: 027 Batch: 00056/00094 | Loss: 348.0751 | CE: 0.1248 | KD: 1060.6624\n",
      "Train Epoch: 027 Batch: 00057/00094 | Loss: 348.0172 | CE: 0.1182 | KD: 1060.5061\n",
      "Train Epoch: 027 Batch: 00058/00094 | Loss: 348.2376 | CE: 0.1557 | KD: 1061.0636\n",
      "Train Epoch: 027 Batch: 00059/00094 | Loss: 348.0590 | CE: 0.1647 | KD: 1060.4917\n",
      "Train Epoch: 027 Batch: 00060/00094 | Loss: 348.2609 | CE: 0.2192 | KD: 1060.9410\n",
      "Train Epoch: 027 Batch: 00061/00094 | Loss: 348.1267 | CE: 0.1726 | KD: 1060.6741\n",
      "Train Epoch: 027 Batch: 00062/00094 | Loss: 348.2732 | CE: 0.2698 | KD: 1060.8246\n",
      "Train Epoch: 027 Batch: 00063/00094 | Loss: 348.0821 | CE: 0.1343 | KD: 1060.6549\n",
      "Train Epoch: 027 Batch: 00064/00094 | Loss: 348.1674 | CE: 0.1690 | KD: 1060.8088\n",
      "Train Epoch: 027 Batch: 00065/00094 | Loss: 348.1749 | CE: 0.1674 | KD: 1060.8365\n",
      "Train Epoch: 027 Batch: 00066/00094 | Loss: 348.1817 | CE: 0.1669 | KD: 1060.8589\n",
      "Train Epoch: 027 Batch: 00067/00094 | Loss: 348.2505 | CE: 0.2361 | KD: 1060.8579\n",
      "Train Epoch: 027 Batch: 00068/00094 | Loss: 348.1413 | CE: 0.1258 | KD: 1060.8612\n",
      "Train Epoch: 027 Batch: 00069/00094 | Loss: 348.2417 | CE: 0.1892 | KD: 1060.9740\n",
      "Train Epoch: 027 Batch: 00070/00094 | Loss: 348.0978 | CE: 0.1325 | KD: 1060.7081\n",
      "Train Epoch: 027 Batch: 00071/00094 | Loss: 348.1864 | CE: 0.1454 | KD: 1060.9390\n",
      "Train Epoch: 027 Batch: 00072/00094 | Loss: 348.2672 | CE: 0.2228 | KD: 1060.9493\n",
      "Train Epoch: 027 Batch: 00073/00094 | Loss: 348.2592 | CE: 0.1948 | KD: 1061.0104\n",
      "Train Epoch: 027 Batch: 00074/00094 | Loss: 348.2083 | CE: 0.1466 | KD: 1061.0018\n",
      "Train Epoch: 027 Batch: 00075/00094 | Loss: 348.1342 | CE: 0.1845 | KD: 1060.6606\n",
      "Train Epoch: 027 Batch: 00076/00094 | Loss: 348.2064 | CE: 0.1749 | KD: 1060.9102\n",
      "Train Epoch: 027 Batch: 00077/00094 | Loss: 348.1181 | CE: 0.1870 | KD: 1060.6038\n",
      "Train Epoch: 027 Batch: 00078/00094 | Loss: 348.1690 | CE: 0.2214 | KD: 1060.6541\n",
      "Train Epoch: 027 Batch: 00079/00094 | Loss: 348.1690 | CE: 0.1545 | KD: 1060.8580\n",
      "Train Epoch: 027 Batch: 00080/00094 | Loss: 348.3341 | CE: 0.1521 | KD: 1061.3687\n",
      "Train Epoch: 027 Batch: 00081/00094 | Loss: 348.2141 | CE: 0.1603 | KD: 1060.9777\n",
      "Train Epoch: 027 Batch: 00082/00094 | Loss: 348.1659 | CE: 0.1952 | KD: 1060.7246\n",
      "Train Epoch: 027 Batch: 00083/00094 | Loss: 348.1214 | CE: 0.1237 | KD: 1060.8070\n",
      "Train Epoch: 027 Batch: 00084/00094 | Loss: 348.0955 | CE: 0.1451 | KD: 1060.6628\n",
      "Train Epoch: 027 Batch: 00085/00094 | Loss: 348.0757 | CE: 0.1379 | KD: 1060.6241\n",
      "Train Epoch: 027 Batch: 00086/00094 | Loss: 348.2613 | CE: 0.1808 | KD: 1061.0593\n",
      "Train Epoch: 027 Batch: 00087/00094 | Loss: 348.1783 | CE: 0.1781 | KD: 1060.8147\n",
      "Train Epoch: 027 Batch: 00088/00094 | Loss: 348.0714 | CE: 0.1380 | KD: 1060.6110\n",
      "Train Epoch: 027 Batch: 00089/00094 | Loss: 348.2788 | CE: 0.2052 | KD: 1061.0382\n",
      "Train Epoch: 027 Batch: 00090/00094 | Loss: 348.1032 | CE: 0.1779 | KD: 1060.5863\n",
      "Train Epoch: 027 Batch: 00091/00094 | Loss: 348.1240 | CE: 0.1774 | KD: 1060.6511\n",
      "Train Epoch: 027 Batch: 00092/00094 | Loss: 348.0324 | CE: 0.1097 | KD: 1060.5785\n",
      "Train Epoch: 027 Batch: 00093/00094 | Loss: 348.1456 | CE: 0.1689 | KD: 1060.7432\n",
      "Train Epoch: 027 Batch: 00094/00094 | Loss: 348.2011 | CE: 0.1805 | KD: 1060.8768\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1543 | acc:97.6500\n",
      "[VAL Acc] Target: 97.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6069 | acc:50.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0953 | acc:55.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0974 | acc:46.5649\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2880 | acc:53.5266\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5340 | acc:74.0296\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8882 | acc:63.9890\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7784 | acc:62.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.38%\n",
      "[VAL Acc] Avg 63.03%\n",
      "Train Epoch: 028 Batch: 00001/00094 | Loss: 348.1576 | CE: 0.1769 | KD: 1060.7550\n",
      "Train Epoch: 028 Batch: 00002/00094 | Loss: 348.2967 | CE: 0.2221 | KD: 1061.0415\n",
      "Train Epoch: 028 Batch: 00003/00094 | Loss: 348.0818 | CE: 0.1312 | KD: 1060.6635\n",
      "Train Epoch: 028 Batch: 00004/00094 | Loss: 348.2105 | CE: 0.1594 | KD: 1060.9698\n",
      "Train Epoch: 028 Batch: 00005/00094 | Loss: 348.0640 | CE: 0.1186 | KD: 1060.6473\n",
      "Train Epoch: 028 Batch: 00006/00094 | Loss: 348.0843 | CE: 0.1769 | KD: 1060.5317\n",
      "Train Epoch: 028 Batch: 00007/00094 | Loss: 348.0359 | CE: 0.1329 | KD: 1060.5184\n",
      "Train Epoch: 028 Batch: 00008/00094 | Loss: 348.1629 | CE: 0.1797 | KD: 1060.7628\n",
      "Train Epoch: 028 Batch: 00009/00094 | Loss: 348.1092 | CE: 0.1967 | KD: 1060.5471\n",
      "Train Epoch: 028 Batch: 00010/00094 | Loss: 348.3186 | CE: 0.2364 | KD: 1061.0645\n",
      "Train Epoch: 028 Batch: 00011/00094 | Loss: 348.0003 | CE: 0.1212 | KD: 1060.4456\n",
      "Train Epoch: 028 Batch: 00012/00094 | Loss: 348.2268 | CE: 0.1594 | KD: 1061.0197\n",
      "Train Epoch: 028 Batch: 00013/00094 | Loss: 348.0485 | CE: 0.1418 | KD: 1060.5298\n",
      "Train Epoch: 028 Batch: 00014/00094 | Loss: 348.1422 | CE: 0.1486 | KD: 1060.7946\n",
      "Train Epoch: 028 Batch: 00015/00094 | Loss: 348.1563 | CE: 0.1450 | KD: 1060.8484\n",
      "Train Epoch: 028 Batch: 00016/00094 | Loss: 348.2100 | CE: 0.1616 | KD: 1060.9615\n",
      "Train Epoch: 028 Batch: 00017/00094 | Loss: 348.2383 | CE: 0.2100 | KD: 1060.9001\n",
      "Train Epoch: 028 Batch: 00018/00094 | Loss: 348.2396 | CE: 0.1555 | KD: 1061.0704\n",
      "Train Epoch: 028 Batch: 00019/00094 | Loss: 348.1970 | CE: 0.1936 | KD: 1060.8243\n",
      "Train Epoch: 028 Batch: 00020/00094 | Loss: 348.3344 | CE: 0.2690 | KD: 1061.0131\n",
      "Train Epoch: 028 Batch: 00021/00094 | Loss: 348.1102 | CE: 0.1622 | KD: 1060.6555\n",
      "Train Epoch: 028 Batch: 00022/00094 | Loss: 348.1321 | CE: 0.1751 | KD: 1060.6830\n",
      "Train Epoch: 028 Batch: 00023/00094 | Loss: 348.2376 | CE: 0.2000 | KD: 1060.9286\n",
      "Train Epoch: 028 Batch: 00024/00094 | Loss: 348.0788 | CE: 0.1618 | KD: 1060.5609\n",
      "Train Epoch: 028 Batch: 00025/00094 | Loss: 348.1306 | CE: 0.1487 | KD: 1060.7589\n",
      "Train Epoch: 028 Batch: 00026/00094 | Loss: 348.2031 | CE: 0.2100 | KD: 1060.7930\n",
      "Train Epoch: 028 Batch: 00027/00094 | Loss: 348.3149 | CE: 0.2279 | KD: 1061.0790\n",
      "Train Epoch: 028 Batch: 00028/00094 | Loss: 348.1597 | CE: 0.1541 | KD: 1060.8309\n",
      "Train Epoch: 028 Batch: 00029/00094 | Loss: 348.1712 | CE: 0.1529 | KD: 1060.8699\n",
      "Train Epoch: 028 Batch: 00030/00094 | Loss: 348.1070 | CE: 0.1576 | KD: 1060.6597\n",
      "Train Epoch: 028 Batch: 00031/00094 | Loss: 348.3519 | CE: 0.2598 | KD: 1061.0946\n",
      "Train Epoch: 028 Batch: 00032/00094 | Loss: 348.2339 | CE: 0.2798 | KD: 1060.6741\n",
      "Train Epoch: 028 Batch: 00033/00094 | Loss: 348.2409 | CE: 0.1563 | KD: 1061.0720\n",
      "Train Epoch: 028 Batch: 00034/00094 | Loss: 348.1188 | CE: 0.1605 | KD: 1060.6868\n",
      "Train Epoch: 028 Batch: 00035/00094 | Loss: 348.0038 | CE: 0.1224 | KD: 1060.4525\n",
      "Train Epoch: 028 Batch: 00036/00094 | Loss: 348.2000 | CE: 0.1717 | KD: 1060.8999\n",
      "Train Epoch: 028 Batch: 00037/00094 | Loss: 348.2943 | CE: 0.2151 | KD: 1061.0551\n",
      "Train Epoch: 028 Batch: 00038/00094 | Loss: 348.0949 | CE: 0.1156 | KD: 1060.7509\n",
      "Train Epoch: 028 Batch: 00039/00094 | Loss: 348.2056 | CE: 0.1740 | KD: 1060.9105\n",
      "Train Epoch: 028 Batch: 00040/00094 | Loss: 348.0075 | CE: 0.1430 | KD: 1060.4009\n",
      "Train Epoch: 028 Batch: 00041/00094 | Loss: 348.0928 | CE: 0.1989 | KD: 1060.4906\n",
      "Train Epoch: 028 Batch: 00042/00094 | Loss: 348.2465 | CE: 0.1339 | KD: 1061.1571\n",
      "Train Epoch: 028 Batch: 00043/00094 | Loss: 348.1706 | CE: 0.1393 | KD: 1060.9093\n",
      "Train Epoch: 028 Batch: 00044/00094 | Loss: 348.0631 | CE: 0.1367 | KD: 1060.5896\n",
      "Train Epoch: 028 Batch: 00045/00094 | Loss: 348.0926 | CE: 0.1759 | KD: 1060.5599\n",
      "Train Epoch: 028 Batch: 00046/00094 | Loss: 348.1214 | CE: 0.1544 | KD: 1060.7135\n",
      "Train Epoch: 028 Batch: 00047/00094 | Loss: 348.2007 | CE: 0.1472 | KD: 1060.9772\n",
      "Train Epoch: 028 Batch: 00048/00094 | Loss: 348.0680 | CE: 0.1242 | KD: 1060.6426\n",
      "Train Epoch: 028 Batch: 00049/00094 | Loss: 348.1344 | CE: 0.1583 | KD: 1060.7410\n",
      "Train Epoch: 028 Batch: 00050/00094 | Loss: 348.2012 | CE: 0.1649 | KD: 1060.9246\n",
      "Train Epoch: 028 Batch: 00051/00094 | Loss: 348.1571 | CE: 0.1266 | KD: 1060.9070\n",
      "Train Epoch: 028 Batch: 00052/00094 | Loss: 348.0968 | CE: 0.1318 | KD: 1060.7070\n",
      "Train Epoch: 028 Batch: 00053/00094 | Loss: 348.1930 | CE: 0.1246 | KD: 1061.0225\n",
      "Train Epoch: 028 Batch: 00054/00094 | Loss: 348.2240 | CE: 0.1964 | KD: 1060.8981\n",
      "Train Epoch: 028 Batch: 00055/00094 | Loss: 348.0183 | CE: 0.1336 | KD: 1060.4625\n",
      "Train Epoch: 028 Batch: 00056/00094 | Loss: 348.3137 | CE: 0.2069 | KD: 1061.1395\n",
      "Train Epoch: 028 Batch: 00057/00094 | Loss: 348.1806 | CE: 0.1777 | KD: 1060.8228\n",
      "Train Epoch: 028 Batch: 00058/00094 | Loss: 348.2047 | CE: 0.1163 | KD: 1061.0833\n",
      "Train Epoch: 028 Batch: 00059/00094 | Loss: 348.0792 | CE: 0.1488 | KD: 1060.6019\n",
      "Train Epoch: 028 Batch: 00060/00094 | Loss: 348.1019 | CE: 0.1851 | KD: 1060.5605\n",
      "Train Epoch: 028 Batch: 00061/00094 | Loss: 348.0812 | CE: 0.1345 | KD: 1060.6517\n",
      "Train Epoch: 028 Batch: 00062/00094 | Loss: 348.1607 | CE: 0.1526 | KD: 1060.8386\n",
      "Train Epoch: 028 Batch: 00063/00094 | Loss: 348.1174 | CE: 0.1583 | KD: 1060.6891\n",
      "Train Epoch: 028 Batch: 00064/00094 | Loss: 348.1233 | CE: 0.1955 | KD: 1060.5939\n",
      "Train Epoch: 028 Batch: 00065/00094 | Loss: 348.0583 | CE: 0.1322 | KD: 1060.5885\n",
      "Train Epoch: 028 Batch: 00066/00094 | Loss: 348.0993 | CE: 0.1449 | KD: 1060.6750\n",
      "Train Epoch: 028 Batch: 00067/00094 | Loss: 348.1450 | CE: 0.1296 | KD: 1060.8610\n",
      "Train Epoch: 028 Batch: 00068/00094 | Loss: 348.1686 | CE: 0.1392 | KD: 1060.9036\n",
      "Train Epoch: 028 Batch: 00069/00094 | Loss: 348.1058 | CE: 0.2263 | KD: 1060.4467\n",
      "Train Epoch: 028 Batch: 00070/00094 | Loss: 348.1882 | CE: 0.2180 | KD: 1060.7230\n",
      "Train Epoch: 028 Batch: 00071/00094 | Loss: 348.1717 | CE: 0.1682 | KD: 1060.8246\n",
      "Train Epoch: 028 Batch: 00072/00094 | Loss: 348.1657 | CE: 0.1608 | KD: 1060.8289\n",
      "Train Epoch: 028 Batch: 00073/00094 | Loss: 348.0866 | CE: 0.1166 | KD: 1060.7228\n",
      "Train Epoch: 028 Batch: 00074/00094 | Loss: 348.1660 | CE: 0.1647 | KD: 1060.8179\n",
      "Train Epoch: 028 Batch: 00075/00094 | Loss: 348.0915 | CE: 0.1080 | KD: 1060.7637\n",
      "Train Epoch: 028 Batch: 00076/00094 | Loss: 348.1864 | CE: 0.1589 | KD: 1060.8978\n",
      "Train Epoch: 028 Batch: 00077/00094 | Loss: 348.2496 | CE: 0.1676 | KD: 1061.0640\n",
      "Train Epoch: 028 Batch: 00078/00094 | Loss: 348.1223 | CE: 0.1811 | KD: 1060.6346\n",
      "Train Epoch: 028 Batch: 00079/00094 | Loss: 348.2188 | CE: 0.2005 | KD: 1060.8698\n",
      "Train Epoch: 028 Batch: 00080/00094 | Loss: 348.0843 | CE: 0.1203 | KD: 1060.7043\n",
      "Train Epoch: 028 Batch: 00081/00094 | Loss: 348.3294 | CE: 0.2470 | KD: 1061.0652\n",
      "Train Epoch: 028 Batch: 00082/00094 | Loss: 347.9925 | CE: 0.1173 | KD: 1060.4335\n",
      "Train Epoch: 028 Batch: 00083/00094 | Loss: 348.2870 | CE: 0.2065 | KD: 1061.0593\n",
      "Train Epoch: 028 Batch: 00084/00094 | Loss: 348.1804 | CE: 0.1594 | KD: 1060.8781\n",
      "Train Epoch: 028 Batch: 00085/00094 | Loss: 348.3119 | CE: 0.1502 | KD: 1061.3068\n",
      "Train Epoch: 028 Batch: 00086/00094 | Loss: 348.1660 | CE: 0.1496 | KD: 1060.8643\n",
      "Train Epoch: 028 Batch: 00087/00094 | Loss: 348.2341 | CE: 0.2235 | KD: 1060.8461\n",
      "Train Epoch: 028 Batch: 00088/00094 | Loss: 348.1511 | CE: 0.1391 | KD: 1060.8505\n",
      "Train Epoch: 028 Batch: 00089/00094 | Loss: 348.1628 | CE: 0.1466 | KD: 1060.8633\n",
      "Train Epoch: 028 Batch: 00090/00094 | Loss: 348.0767 | CE: 0.1308 | KD: 1060.6490\n",
      "Train Epoch: 028 Batch: 00091/00094 | Loss: 348.1226 | CE: 0.1354 | KD: 1060.7749\n",
      "Train Epoch: 028 Batch: 00092/00094 | Loss: 348.2716 | CE: 0.2196 | KD: 1060.9725\n",
      "Train Epoch: 028 Batch: 00093/00094 | Loss: 348.0972 | CE: 0.1507 | KD: 1060.6509\n",
      "Train Epoch: 028 Batch: 00094/00094 | Loss: 348.1726 | CE: 0.2334 | KD: 1060.6288\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1481 | acc:97.7500\n",
      "[VAL Acc] Target: 97.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5467 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0632 | acc:55.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0462 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2403 | acc:54.0361\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6058 | acc:68.8540\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9018 | acc:64.1850\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7449 | acc:62.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.12%\n",
      "[VAL Acc] Avg 62.41%\n",
      "Train Epoch: 029 Batch: 00001/00094 | Loss: 348.2520 | CE: 0.1988 | KD: 1060.9762\n",
      "Train Epoch: 029 Batch: 00002/00094 | Loss: 348.2381 | CE: 0.1889 | KD: 1060.9639\n",
      "Train Epoch: 029 Batch: 00003/00094 | Loss: 348.1486 | CE: 0.1402 | KD: 1060.8396\n",
      "Train Epoch: 029 Batch: 00004/00094 | Loss: 348.0060 | CE: 0.1167 | KD: 1060.4766\n",
      "Train Epoch: 029 Batch: 00005/00094 | Loss: 348.1543 | CE: 0.1677 | KD: 1060.7731\n",
      "Train Epoch: 029 Batch: 00006/00094 | Loss: 348.0106 | CE: 0.1424 | KD: 1060.4122\n",
      "Train Epoch: 029 Batch: 00007/00094 | Loss: 348.0487 | CE: 0.1301 | KD: 1060.5658\n",
      "Train Epoch: 029 Batch: 00008/00094 | Loss: 348.1632 | CE: 0.1905 | KD: 1060.7308\n",
      "Train Epoch: 029 Batch: 00009/00094 | Loss: 348.2355 | CE: 0.1584 | KD: 1061.0490\n",
      "Train Epoch: 029 Batch: 00010/00094 | Loss: 348.1071 | CE: 0.1498 | KD: 1060.6840\n",
      "Train Epoch: 029 Batch: 00011/00094 | Loss: 348.2498 | CE: 0.1845 | KD: 1061.0129\n",
      "Train Epoch: 029 Batch: 00012/00094 | Loss: 348.2675 | CE: 0.2854 | KD: 1060.7594\n",
      "Train Epoch: 029 Batch: 00013/00094 | Loss: 348.1646 | CE: 0.1610 | KD: 1060.8248\n",
      "Train Epoch: 029 Batch: 00014/00094 | Loss: 348.1027 | CE: 0.1461 | KD: 1060.6815\n",
      "Train Epoch: 029 Batch: 00015/00094 | Loss: 348.2776 | CE: 0.2065 | KD: 1061.0306\n",
      "Train Epoch: 029 Batch: 00016/00094 | Loss: 348.0751 | CE: 0.1600 | KD: 1060.5552\n",
      "Train Epoch: 029 Batch: 00017/00094 | Loss: 348.1689 | CE: 0.1345 | KD: 1060.9191\n",
      "Train Epoch: 029 Batch: 00018/00094 | Loss: 348.2591 | CE: 0.2386 | KD: 1060.8763\n",
      "Train Epoch: 029 Batch: 00019/00094 | Loss: 348.1862 | CE: 0.2115 | KD: 1060.7367\n",
      "Train Epoch: 029 Batch: 00020/00094 | Loss: 348.0506 | CE: 0.1251 | KD: 1060.5869\n",
      "Train Epoch: 029 Batch: 00021/00094 | Loss: 347.9618 | CE: 0.1174 | KD: 1060.3394\n",
      "Train Epoch: 029 Batch: 00022/00094 | Loss: 348.2480 | CE: 0.1735 | KD: 1061.0411\n",
      "Train Epoch: 029 Batch: 00023/00094 | Loss: 348.0442 | CE: 0.1399 | KD: 1060.5222\n",
      "Train Epoch: 029 Batch: 00024/00094 | Loss: 348.2895 | CE: 0.1839 | KD: 1061.1356\n",
      "Train Epoch: 029 Batch: 00025/00094 | Loss: 348.1953 | CE: 0.1880 | KD: 1060.8362\n",
      "Train Epoch: 029 Batch: 00026/00094 | Loss: 348.3008 | CE: 0.1672 | KD: 1061.2212\n",
      "Train Epoch: 029 Batch: 00027/00094 | Loss: 348.0968 | CE: 0.1592 | KD: 1060.6238\n",
      "Train Epoch: 029 Batch: 00028/00094 | Loss: 348.0311 | CE: 0.1270 | KD: 1060.5216\n",
      "Train Epoch: 029 Batch: 00029/00094 | Loss: 348.1028 | CE: 0.1323 | KD: 1060.7241\n",
      "Train Epoch: 029 Batch: 00030/00094 | Loss: 348.0367 | CE: 0.1199 | KD: 1060.5604\n",
      "Train Epoch: 029 Batch: 00031/00094 | Loss: 348.2213 | CE: 0.1667 | KD: 1060.9803\n",
      "Train Epoch: 029 Batch: 00032/00094 | Loss: 348.2916 | CE: 0.1899 | KD: 1061.1240\n",
      "Train Epoch: 029 Batch: 00033/00094 | Loss: 348.2201 | CE: 0.1685 | KD: 1060.9712\n",
      "Train Epoch: 029 Batch: 00034/00094 | Loss: 348.0826 | CE: 0.1472 | KD: 1060.6171\n",
      "Train Epoch: 029 Batch: 00035/00094 | Loss: 348.0375 | CE: 0.1396 | KD: 1060.5029\n",
      "Train Epoch: 029 Batch: 00036/00094 | Loss: 348.1974 | CE: 0.2003 | KD: 1060.8054\n",
      "Train Epoch: 029 Batch: 00037/00094 | Loss: 348.1137 | CE: 0.1182 | KD: 1060.8003\n",
      "Train Epoch: 029 Batch: 00038/00094 | Loss: 348.1463 | CE: 0.1864 | KD: 1060.6919\n",
      "Train Epoch: 029 Batch: 00039/00094 | Loss: 348.1149 | CE: 0.1462 | KD: 1060.7186\n",
      "Train Epoch: 029 Batch: 00040/00094 | Loss: 348.0567 | CE: 0.1278 | KD: 1060.5970\n",
      "Train Epoch: 029 Batch: 00041/00094 | Loss: 348.0736 | CE: 0.1365 | KD: 1060.6222\n",
      "Train Epoch: 029 Batch: 00042/00094 | Loss: 348.0727 | CE: 0.1642 | KD: 1060.5349\n",
      "Train Epoch: 029 Batch: 00043/00094 | Loss: 348.1316 | CE: 0.1622 | KD: 1060.7208\n",
      "Train Epoch: 029 Batch: 00044/00094 | Loss: 348.2419 | CE: 0.1373 | KD: 1061.1328\n",
      "Train Epoch: 029 Batch: 00045/00094 | Loss: 348.0924 | CE: 0.2043 | KD: 1060.4728\n",
      "Train Epoch: 029 Batch: 00046/00094 | Loss: 348.1535 | CE: 0.1468 | KD: 1060.8344\n",
      "Train Epoch: 029 Batch: 00047/00094 | Loss: 348.1176 | CE: 0.2150 | KD: 1060.5172\n",
      "Train Epoch: 029 Batch: 00048/00094 | Loss: 348.1630 | CE: 0.1739 | KD: 1060.7805\n",
      "Train Epoch: 029 Batch: 00049/00094 | Loss: 348.1436 | CE: 0.1305 | KD: 1060.8536\n",
      "Train Epoch: 029 Batch: 00050/00094 | Loss: 348.2582 | CE: 0.1649 | KD: 1061.0983\n",
      "Train Epoch: 029 Batch: 00051/00094 | Loss: 348.2542 | CE: 0.1275 | KD: 1061.2002\n",
      "Train Epoch: 029 Batch: 00052/00094 | Loss: 348.1400 | CE: 0.1432 | KD: 1060.8042\n",
      "Train Epoch: 029 Batch: 00053/00094 | Loss: 348.1440 | CE: 0.1597 | KD: 1060.7659\n",
      "Train Epoch: 029 Batch: 00054/00094 | Loss: 348.0576 | CE: 0.1585 | KD: 1060.5062\n",
      "Train Epoch: 029 Batch: 00055/00094 | Loss: 348.1395 | CE: 0.1213 | KD: 1060.8694\n",
      "Train Epoch: 029 Batch: 00056/00094 | Loss: 348.0856 | CE: 0.1321 | KD: 1060.6722\n",
      "Train Epoch: 029 Batch: 00057/00094 | Loss: 348.2317 | CE: 0.1455 | KD: 1061.0767\n",
      "Train Epoch: 029 Batch: 00058/00094 | Loss: 348.1460 | CE: 0.1264 | KD: 1060.8737\n",
      "Train Epoch: 029 Batch: 00059/00094 | Loss: 348.2466 | CE: 0.1973 | KD: 1060.9641\n",
      "Train Epoch: 029 Batch: 00060/00094 | Loss: 348.0934 | CE: 0.1309 | KD: 1060.6997\n",
      "Train Epoch: 029 Batch: 00061/00094 | Loss: 348.0760 | CE: 0.1567 | KD: 1060.5677\n",
      "Train Epoch: 029 Batch: 00062/00094 | Loss: 348.2884 | CE: 0.2015 | KD: 1061.0790\n",
      "Train Epoch: 029 Batch: 00063/00094 | Loss: 348.0100 | CE: 0.1678 | KD: 1060.3330\n",
      "Train Epoch: 029 Batch: 00064/00094 | Loss: 348.1134 | CE: 0.1564 | KD: 1060.6829\n",
      "Train Epoch: 029 Batch: 00065/00094 | Loss: 347.9963 | CE: 0.1402 | KD: 1060.3751\n",
      "Train Epoch: 029 Batch: 00066/00094 | Loss: 348.1423 | CE: 0.1250 | KD: 1060.8665\n",
      "Train Epoch: 029 Batch: 00067/00094 | Loss: 348.1840 | CE: 0.1693 | KD: 1060.8589\n",
      "Train Epoch: 029 Batch: 00068/00094 | Loss: 348.1204 | CE: 0.1662 | KD: 1060.6744\n",
      "Train Epoch: 029 Batch: 00069/00094 | Loss: 348.0132 | CE: 0.1495 | KD: 1060.3983\n",
      "Train Epoch: 029 Batch: 00070/00094 | Loss: 348.2086 | CE: 0.1898 | KD: 1060.8713\n",
      "Train Epoch: 029 Batch: 00071/00094 | Loss: 348.1158 | CE: 0.1464 | KD: 1060.7207\n",
      "Train Epoch: 029 Batch: 00072/00094 | Loss: 348.0718 | CE: 0.1411 | KD: 1060.6028\n",
      "Train Epoch: 029 Batch: 00073/00094 | Loss: 348.3111 | CE: 0.2488 | KD: 1061.0038\n",
      "Train Epoch: 029 Batch: 00074/00094 | Loss: 348.2136 | CE: 0.1799 | KD: 1060.9167\n",
      "Train Epoch: 029 Batch: 00075/00094 | Loss: 348.1350 | CE: 0.1740 | KD: 1060.6949\n",
      "Train Epoch: 029 Batch: 00076/00094 | Loss: 348.2439 | CE: 0.2152 | KD: 1060.9015\n",
      "Train Epoch: 029 Batch: 00077/00094 | Loss: 348.0129 | CE: 0.1153 | KD: 1060.5020\n",
      "Train Epoch: 029 Batch: 00078/00094 | Loss: 348.0520 | CE: 0.1309 | KD: 1060.5736\n",
      "Train Epoch: 029 Batch: 00079/00094 | Loss: 348.1124 | CE: 0.1529 | KD: 1060.6903\n",
      "Train Epoch: 029 Batch: 00080/00094 | Loss: 348.0708 | CE: 0.1412 | KD: 1060.5994\n",
      "Train Epoch: 029 Batch: 00081/00094 | Loss: 348.1341 | CE: 0.1428 | KD: 1060.7872\n",
      "Train Epoch: 029 Batch: 00082/00094 | Loss: 348.1364 | CE: 0.2150 | KD: 1060.5741\n",
      "Train Epoch: 029 Batch: 00083/00094 | Loss: 348.1233 | CE: 0.1803 | KD: 1060.6401\n",
      "Train Epoch: 029 Batch: 00084/00094 | Loss: 348.1392 | CE: 0.1523 | KD: 1060.7739\n",
      "Train Epoch: 029 Batch: 00085/00094 | Loss: 348.1201 | CE: 0.1288 | KD: 1060.7874\n",
      "Train Epoch: 029 Batch: 00086/00094 | Loss: 348.3033 | CE: 0.1553 | KD: 1061.2649\n",
      "Train Epoch: 029 Batch: 00087/00094 | Loss: 348.1069 | CE: 0.1548 | KD: 1060.6677\n",
      "Train Epoch: 029 Batch: 00088/00094 | Loss: 348.2569 | CE: 0.2715 | KD: 1060.7697\n",
      "Train Epoch: 029 Batch: 00089/00094 | Loss: 348.0431 | CE: 0.1547 | KD: 1060.4738\n",
      "Train Epoch: 029 Batch: 00090/00094 | Loss: 348.2552 | CE: 0.1385 | KD: 1061.1696\n",
      "Train Epoch: 029 Batch: 00091/00094 | Loss: 348.2661 | CE: 0.2049 | KD: 1061.0006\n",
      "Train Epoch: 029 Batch: 00092/00094 | Loss: 348.2074 | CE: 0.1412 | KD: 1061.0156\n",
      "Train Epoch: 029 Batch: 00093/00094 | Loss: 348.1863 | CE: 0.1319 | KD: 1060.9799\n",
      "Train Epoch: 029 Batch: 00094/00094 | Loss: 348.0414 | CE: 0.1039 | KD: 1060.6235\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1549 | acc:96.8000\n",
      "[VAL Acc] Target: 96.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6182 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1422 | acc:52.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0502 | acc:46.7557\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3080 | acc:53.9185\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5749 | acc:70.8872\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9328 | acc:64.8119\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7756 | acc:61.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.38%\n",
      "[VAL Acc] Avg 62.13%\n",
      "Train Epoch: 030 Batch: 00001/00094 | Loss: 313.4547 | CE: 0.2268 | KD: 1060.9082\n",
      "Train Epoch: 030 Batch: 00002/00094 | Loss: 313.3115 | CE: 0.1727 | KD: 1060.6066\n",
      "Train Epoch: 030 Batch: 00003/00094 | Loss: 313.3087 | CE: 0.1319 | KD: 1060.7351\n",
      "Train Epoch: 030 Batch: 00004/00094 | Loss: 313.3079 | CE: 0.1510 | KD: 1060.6678\n",
      "Train Epoch: 030 Batch: 00005/00094 | Loss: 313.2927 | CE: 0.1325 | KD: 1060.6791\n",
      "Train Epoch: 030 Batch: 00006/00094 | Loss: 313.4158 | CE: 0.1539 | KD: 1061.0234\n",
      "Train Epoch: 030 Batch: 00007/00094 | Loss: 313.4424 | CE: 0.2167 | KD: 1060.9010\n",
      "Train Epoch: 030 Batch: 00008/00094 | Loss: 313.5291 | CE: 0.2441 | KD: 1061.1018\n",
      "Train Epoch: 030 Batch: 00009/00094 | Loss: 313.4200 | CE: 0.1581 | KD: 1061.0234\n",
      "Train Epoch: 030 Batch: 00010/00094 | Loss: 313.4098 | CE: 0.1868 | KD: 1060.8917\n",
      "Train Epoch: 030 Batch: 00011/00094 | Loss: 313.2903 | CE: 0.1427 | KD: 1060.6365\n",
      "Train Epoch: 030 Batch: 00012/00094 | Loss: 313.3027 | CE: 0.1510 | KD: 1060.6504\n",
      "Train Epoch: 030 Batch: 00013/00094 | Loss: 313.3408 | CE: 0.1632 | KD: 1060.7379\n",
      "Train Epoch: 030 Batch: 00014/00094 | Loss: 313.2512 | CE: 0.1393 | KD: 1060.5155\n",
      "Train Epoch: 030 Batch: 00015/00094 | Loss: 313.3396 | CE: 0.1590 | KD: 1060.7482\n",
      "Train Epoch: 030 Batch: 00016/00094 | Loss: 313.4225 | CE: 0.1721 | KD: 1060.9846\n",
      "Train Epoch: 030 Batch: 00017/00094 | Loss: 313.2581 | CE: 0.1236 | KD: 1060.5920\n",
      "Train Epoch: 030 Batch: 00018/00094 | Loss: 313.4146 | CE: 0.1826 | KD: 1060.9224\n",
      "Train Epoch: 030 Batch: 00019/00094 | Loss: 313.3201 | CE: 0.1219 | KD: 1060.8077\n",
      "Train Epoch: 030 Batch: 00020/00094 | Loss: 313.3199 | CE: 0.2065 | KD: 1060.5208\n",
      "Train Epoch: 030 Batch: 00021/00094 | Loss: 313.3256 | CE: 0.1320 | KD: 1060.7921\n",
      "Train Epoch: 030 Batch: 00022/00094 | Loss: 313.3078 | CE: 0.2095 | KD: 1060.4692\n",
      "Train Epoch: 030 Batch: 00023/00094 | Loss: 313.4578 | CE: 0.1973 | KD: 1061.0190\n",
      "Train Epoch: 030 Batch: 00024/00094 | Loss: 313.3249 | CE: 0.1589 | KD: 1060.6989\n",
      "Train Epoch: 030 Batch: 00025/00094 | Loss: 313.2631 | CE: 0.1666 | KD: 1060.4633\n",
      "Train Epoch: 030 Batch: 00026/00094 | Loss: 313.6294 | CE: 0.3131 | KD: 1061.2076\n",
      "Train Epoch: 030 Batch: 00027/00094 | Loss: 313.2549 | CE: 0.1721 | KD: 1060.4167\n",
      "Train Epoch: 030 Batch: 00028/00094 | Loss: 313.3592 | CE: 0.1612 | KD: 1060.8070\n",
      "Train Epoch: 030 Batch: 00029/00094 | Loss: 313.3087 | CE: 0.1605 | KD: 1060.6385\n",
      "Train Epoch: 030 Batch: 00030/00094 | Loss: 313.2644 | CE: 0.1245 | KD: 1060.6104\n",
      "Train Epoch: 030 Batch: 00031/00094 | Loss: 313.3027 | CE: 0.2060 | KD: 1060.4641\n",
      "Train Epoch: 030 Batch: 00032/00094 | Loss: 313.4488 | CE: 0.2036 | KD: 1060.9669\n",
      "Train Epoch: 030 Batch: 00033/00094 | Loss: 313.3131 | CE: 0.1663 | KD: 1060.6337\n",
      "Train Epoch: 030 Batch: 00034/00094 | Loss: 313.4231 | CE: 0.1811 | KD: 1060.9563\n",
      "Train Epoch: 030 Batch: 00035/00094 | Loss: 313.3242 | CE: 0.1829 | KD: 1060.6151\n",
      "Train Epoch: 030 Batch: 00036/00094 | Loss: 313.4818 | CE: 0.2332 | KD: 1060.9783\n",
      "Train Epoch: 030 Batch: 00037/00094 | Loss: 313.3181 | CE: 0.1390 | KD: 1060.7433\n",
      "Train Epoch: 030 Batch: 00038/00094 | Loss: 313.2333 | CE: 0.1308 | KD: 1060.4835\n",
      "Train Epoch: 030 Batch: 00039/00094 | Loss: 313.4629 | CE: 0.2306 | KD: 1060.9233\n",
      "Train Epoch: 030 Batch: 00040/00094 | Loss: 313.2799 | CE: 0.1859 | KD: 1060.4548\n",
      "Train Epoch: 030 Batch: 00041/00094 | Loss: 313.3988 | CE: 0.1694 | KD: 1060.9133\n",
      "Train Epoch: 030 Batch: 00042/00094 | Loss: 313.3355 | CE: 0.1255 | KD: 1060.8480\n",
      "Train Epoch: 030 Batch: 00043/00094 | Loss: 313.3731 | CE: 0.1747 | KD: 1060.8086\n",
      "Train Epoch: 030 Batch: 00044/00094 | Loss: 313.2327 | CE: 0.1324 | KD: 1060.4763\n",
      "Train Epoch: 030 Batch: 00045/00094 | Loss: 313.2808 | CE: 0.1770 | KD: 1060.4879\n",
      "Train Epoch: 030 Batch: 00046/00094 | Loss: 313.3481 | CE: 0.1955 | KD: 1060.6534\n",
      "Train Epoch: 030 Batch: 00047/00094 | Loss: 313.2719 | CE: 0.1218 | KD: 1060.6449\n",
      "Train Epoch: 030 Batch: 00048/00094 | Loss: 313.2961 | CE: 0.1622 | KD: 1060.5900\n",
      "Train Epoch: 030 Batch: 00049/00094 | Loss: 313.3387 | CE: 0.1269 | KD: 1060.8540\n",
      "Train Epoch: 030 Batch: 00050/00094 | Loss: 313.3503 | CE: 0.1165 | KD: 1060.9286\n",
      "Train Epoch: 030 Batch: 00051/00094 | Loss: 313.4825 | CE: 0.2039 | KD: 1061.0804\n",
      "Train Epoch: 030 Batch: 00052/00094 | Loss: 313.2522 | CE: 0.1497 | KD: 1060.4838\n",
      "Train Epoch: 030 Batch: 00053/00094 | Loss: 313.4016 | CE: 0.1802 | KD: 1060.8864\n",
      "Train Epoch: 030 Batch: 00054/00094 | Loss: 313.4329 | CE: 0.1696 | KD: 1061.0286\n",
      "Train Epoch: 030 Batch: 00055/00094 | Loss: 313.5050 | CE: 0.2742 | KD: 1060.9183\n",
      "Train Epoch: 030 Batch: 00056/00094 | Loss: 313.2837 | CE: 0.1097 | KD: 1060.7260\n",
      "Train Epoch: 030 Batch: 00057/00094 | Loss: 313.2888 | CE: 0.1495 | KD: 1060.6084\n",
      "Train Epoch: 030 Batch: 00058/00094 | Loss: 313.3628 | CE: 0.1241 | KD: 1060.9451\n",
      "Train Epoch: 030 Batch: 00059/00094 | Loss: 313.2989 | CE: 0.1845 | KD: 1060.5239\n",
      "Train Epoch: 030 Batch: 00060/00094 | Loss: 313.3417 | CE: 0.1639 | KD: 1060.7389\n",
      "Train Epoch: 030 Batch: 00061/00094 | Loss: 313.5465 | CE: 0.2185 | KD: 1061.2474\n",
      "Train Epoch: 030 Batch: 00062/00094 | Loss: 313.2601 | CE: 0.1147 | KD: 1060.6288\n",
      "Train Epoch: 030 Batch: 00063/00094 | Loss: 313.2816 | CE: 0.1145 | KD: 1060.7024\n",
      "Train Epoch: 030 Batch: 00064/00094 | Loss: 313.3269 | CE: 0.1425 | KD: 1060.7612\n",
      "Train Epoch: 030 Batch: 00065/00094 | Loss: 313.3531 | CE: 0.1463 | KD: 1060.8370\n",
      "Train Epoch: 030 Batch: 00066/00094 | Loss: 313.2712 | CE: 0.1106 | KD: 1060.6805\n",
      "Train Epoch: 030 Batch: 00067/00094 | Loss: 313.2272 | CE: 0.1206 | KD: 1060.4976\n",
      "Train Epoch: 030 Batch: 00068/00094 | Loss: 313.2721 | CE: 0.1542 | KD: 1060.5356\n",
      "Train Epoch: 030 Batch: 00069/00094 | Loss: 313.4196 | CE: 0.2060 | KD: 1060.8601\n",
      "Train Epoch: 030 Batch: 00070/00094 | Loss: 313.2334 | CE: 0.1637 | KD: 1060.3726\n",
      "Train Epoch: 030 Batch: 00071/00094 | Loss: 313.3510 | CE: 0.1340 | KD: 1060.8715\n",
      "Train Epoch: 030 Batch: 00072/00094 | Loss: 313.2375 | CE: 0.1359 | KD: 1060.4806\n",
      "Train Epoch: 030 Batch: 00073/00094 | Loss: 313.4300 | CE: 0.1383 | KD: 1061.1245\n",
      "Train Epoch: 030 Batch: 00074/00094 | Loss: 313.2548 | CE: 0.1475 | KD: 1060.5000\n",
      "Train Epoch: 030 Batch: 00075/00094 | Loss: 313.3303 | CE: 0.1514 | KD: 1060.7427\n",
      "Train Epoch: 030 Batch: 00076/00094 | Loss: 313.3318 | CE: 0.1451 | KD: 1060.7689\n",
      "Train Epoch: 030 Batch: 00077/00094 | Loss: 313.2140 | CE: 0.1150 | KD: 1060.4720\n",
      "Train Epoch: 030 Batch: 00078/00094 | Loss: 313.3876 | CE: 0.1595 | KD: 1060.9093\n",
      "Train Epoch: 030 Batch: 00079/00094 | Loss: 313.3868 | CE: 0.2074 | KD: 1060.7443\n",
      "Train Epoch: 030 Batch: 00080/00094 | Loss: 313.3744 | CE: 0.2344 | KD: 1060.6106\n",
      "Train Epoch: 030 Batch: 00081/00094 | Loss: 313.3037 | CE: 0.1339 | KD: 1060.7117\n",
      "Train Epoch: 030 Batch: 00082/00094 | Loss: 313.3044 | CE: 0.1149 | KD: 1060.7782\n",
      "Train Epoch: 030 Batch: 00083/00094 | Loss: 313.4519 | CE: 0.1294 | KD: 1061.2288\n",
      "Train Epoch: 030 Batch: 00084/00094 | Loss: 313.3606 | CE: 0.1268 | KD: 1060.9286\n",
      "Train Epoch: 030 Batch: 00085/00094 | Loss: 313.4688 | CE: 0.2105 | KD: 1061.0112\n",
      "Train Epoch: 030 Batch: 00086/00094 | Loss: 313.2709 | CE: 0.1374 | KD: 1060.5887\n",
      "Train Epoch: 030 Batch: 00087/00094 | Loss: 313.3354 | CE: 0.1561 | KD: 1060.7435\n",
      "Train Epoch: 030 Batch: 00088/00094 | Loss: 313.3477 | CE: 0.1883 | KD: 1060.6763\n",
      "Train Epoch: 030 Batch: 00089/00094 | Loss: 313.4128 | CE: 0.1943 | KD: 1060.8765\n",
      "Train Epoch: 030 Batch: 00090/00094 | Loss: 313.3223 | CE: 0.1451 | KD: 1060.7367\n",
      "Train Epoch: 030 Batch: 00091/00094 | Loss: 313.2507 | CE: 0.1331 | KD: 1060.5348\n",
      "Train Epoch: 030 Batch: 00092/00094 | Loss: 313.3351 | CE: 0.1572 | KD: 1060.7393\n",
      "Train Epoch: 030 Batch: 00093/00094 | Loss: 313.3055 | CE: 0.1309 | KD: 1060.7279\n",
      "Train Epoch: 030 Batch: 00094/00094 | Loss: 313.2209 | CE: 0.1388 | KD: 1060.4146\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1417 | acc:97.7000\n",
      "[VAL Acc] Target: 97.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6313 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1080 | acc:54.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0681 | acc:43.8931\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3044 | acc:54.2712\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5825 | acc:70.3327\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9306 | acc:65.2429\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7533 | acc:61.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.31%\n",
      "[VAL Acc] Avg 62.09%\n",
      "Train Epoch: 031 Batch: 00001/00094 | Loss: 313.2370 | CE: 0.1680 | KD: 1060.3701\n",
      "Train Epoch: 031 Batch: 00002/00094 | Loss: 313.2773 | CE: 0.1554 | KD: 1060.5492\n",
      "Train Epoch: 031 Batch: 00003/00094 | Loss: 313.2187 | CE: 0.1045 | KD: 1060.5234\n",
      "Train Epoch: 031 Batch: 00004/00094 | Loss: 313.3349 | CE: 0.1645 | KD: 1060.7136\n",
      "Train Epoch: 031 Batch: 00005/00094 | Loss: 313.4017 | CE: 0.1630 | KD: 1060.9448\n",
      "Train Epoch: 031 Batch: 00006/00094 | Loss: 313.3380 | CE: 0.1560 | KD: 1060.7528\n",
      "Train Epoch: 031 Batch: 00007/00094 | Loss: 313.2661 | CE: 0.1429 | KD: 1060.5537\n",
      "Train Epoch: 031 Batch: 00008/00094 | Loss: 313.3343 | CE: 0.1213 | KD: 1060.8580\n",
      "Train Epoch: 031 Batch: 00009/00094 | Loss: 313.3929 | CE: 0.1448 | KD: 1060.9767\n",
      "Train Epoch: 031 Batch: 00010/00094 | Loss: 313.2689 | CE: 0.1245 | KD: 1060.6254\n",
      "Train Epoch: 031 Batch: 00011/00094 | Loss: 313.2889 | CE: 0.1550 | KD: 1060.5902\n",
      "Train Epoch: 031 Batch: 00012/00094 | Loss: 313.4409 | CE: 0.1778 | KD: 1061.0276\n",
      "Train Epoch: 031 Batch: 00013/00094 | Loss: 313.4462 | CE: 0.2094 | KD: 1060.9385\n",
      "Train Epoch: 031 Batch: 00014/00094 | Loss: 313.4106 | CE: 0.2159 | KD: 1060.7958\n",
      "Train Epoch: 031 Batch: 00015/00094 | Loss: 313.2726 | CE: 0.1375 | KD: 1060.5944\n",
      "Train Epoch: 031 Batch: 00016/00094 | Loss: 313.2418 | CE: 0.1292 | KD: 1060.5179\n",
      "Train Epoch: 031 Batch: 00017/00094 | Loss: 313.3268 | CE: 0.1432 | KD: 1060.7582\n",
      "Train Epoch: 031 Batch: 00018/00094 | Loss: 313.3494 | CE: 0.1807 | KD: 1060.7081\n",
      "Train Epoch: 031 Batch: 00019/00094 | Loss: 313.3941 | CE: 0.1701 | KD: 1060.8951\n",
      "Train Epoch: 031 Batch: 00020/00094 | Loss: 313.3173 | CE: 0.1278 | KD: 1060.7784\n",
      "Train Epoch: 031 Batch: 00021/00094 | Loss: 313.3305 | CE: 0.1634 | KD: 1060.7024\n",
      "Train Epoch: 031 Batch: 00022/00094 | Loss: 313.5128 | CE: 0.3146 | KD: 1060.8076\n",
      "Train Epoch: 031 Batch: 00023/00094 | Loss: 313.4017 | CE: 0.1868 | KD: 1060.8647\n",
      "Train Epoch: 031 Batch: 00024/00094 | Loss: 313.4578 | CE: 0.1532 | KD: 1061.1680\n",
      "Train Epoch: 031 Batch: 00025/00094 | Loss: 313.1741 | CE: 0.1225 | KD: 1060.3113\n",
      "Train Epoch: 031 Batch: 00026/00094 | Loss: 313.3638 | CE: 0.1223 | KD: 1060.9545\n",
      "Train Epoch: 031 Batch: 00027/00094 | Loss: 313.4898 | CE: 0.2072 | KD: 1061.0939\n",
      "Train Epoch: 031 Batch: 00028/00094 | Loss: 313.4341 | CE: 0.1671 | KD: 1061.0410\n",
      "Train Epoch: 031 Batch: 00029/00094 | Loss: 313.4162 | CE: 0.1931 | KD: 1060.8923\n",
      "Train Epoch: 031 Batch: 00030/00094 | Loss: 313.4021 | CE: 0.1642 | KD: 1060.9420\n",
      "Train Epoch: 031 Batch: 00031/00094 | Loss: 313.4337 | CE: 0.2116 | KD: 1060.8885\n",
      "Train Epoch: 031 Batch: 00032/00094 | Loss: 313.2286 | CE: 0.1032 | KD: 1060.5614\n",
      "Train Epoch: 031 Batch: 00033/00094 | Loss: 313.4133 | CE: 0.2394 | KD: 1060.7255\n",
      "Train Epoch: 031 Batch: 00034/00094 | Loss: 313.3506 | CE: 0.1243 | KD: 1060.9031\n",
      "Train Epoch: 031 Batch: 00035/00094 | Loss: 313.3682 | CE: 0.1719 | KD: 1060.8011\n",
      "Train Epoch: 031 Batch: 00036/00094 | Loss: 313.3833 | CE: 0.2479 | KD: 1060.5951\n",
      "Train Epoch: 031 Batch: 00037/00094 | Loss: 313.3300 | CE: 0.1583 | KD: 1060.7183\n",
      "Train Epoch: 031 Batch: 00038/00094 | Loss: 313.4264 | CE: 0.1639 | KD: 1061.0255\n",
      "Train Epoch: 031 Batch: 00039/00094 | Loss: 313.3394 | CE: 0.1580 | KD: 1060.7511\n",
      "Train Epoch: 031 Batch: 00040/00094 | Loss: 313.3421 | CE: 0.1856 | KD: 1060.6667\n",
      "Train Epoch: 031 Batch: 00041/00094 | Loss: 313.3437 | CE: 0.1895 | KD: 1060.6587\n",
      "Train Epoch: 031 Batch: 00042/00094 | Loss: 313.3636 | CE: 0.1531 | KD: 1060.8496\n",
      "Train Epoch: 031 Batch: 00043/00094 | Loss: 313.5121 | CE: 0.1547 | KD: 1061.3472\n",
      "Train Epoch: 031 Batch: 00044/00094 | Loss: 313.3296 | CE: 0.1277 | KD: 1060.8204\n",
      "Train Epoch: 031 Batch: 00045/00094 | Loss: 313.2570 | CE: 0.1665 | KD: 1060.4432\n",
      "Train Epoch: 031 Batch: 00046/00094 | Loss: 313.3496 | CE: 0.1485 | KD: 1060.8176\n",
      "Train Epoch: 031 Batch: 00047/00094 | Loss: 313.2682 | CE: 0.1579 | KD: 1060.5101\n",
      "Train Epoch: 031 Batch: 00048/00094 | Loss: 313.2048 | CE: 0.1216 | KD: 1060.4181\n",
      "Train Epoch: 031 Batch: 00049/00094 | Loss: 313.5044 | CE: 0.1694 | KD: 1061.2710\n",
      "Train Epoch: 031 Batch: 00050/00094 | Loss: 313.3185 | CE: 0.1463 | KD: 1060.7197\n",
      "Train Epoch: 031 Batch: 00051/00094 | Loss: 313.3139 | CE: 0.1177 | KD: 1060.8010\n",
      "Train Epoch: 031 Batch: 00052/00094 | Loss: 313.4384 | CE: 0.1897 | KD: 1060.9786\n",
      "Train Epoch: 031 Batch: 00053/00094 | Loss: 313.2747 | CE: 0.1318 | KD: 1060.6207\n",
      "Train Epoch: 031 Batch: 00054/00094 | Loss: 313.2959 | CE: 0.1731 | KD: 1060.5525\n",
      "Train Epoch: 031 Batch: 00055/00094 | Loss: 313.3505 | CE: 0.1302 | KD: 1060.8827\n",
      "Train Epoch: 031 Batch: 00056/00094 | Loss: 313.3397 | CE: 0.1366 | KD: 1060.8245\n",
      "Train Epoch: 031 Batch: 00057/00094 | Loss: 313.3081 | CE: 0.1744 | KD: 1060.5894\n",
      "Train Epoch: 031 Batch: 00058/00094 | Loss: 313.3446 | CE: 0.1454 | KD: 1060.8113\n",
      "Train Epoch: 031 Batch: 00059/00094 | Loss: 313.3687 | CE: 0.1576 | KD: 1060.8517\n",
      "Train Epoch: 031 Batch: 00060/00094 | Loss: 313.3824 | CE: 0.1661 | KD: 1060.8689\n",
      "Train Epoch: 031 Batch: 00061/00094 | Loss: 313.4184 | CE: 0.1325 | KD: 1061.1049\n",
      "Train Epoch: 031 Batch: 00062/00094 | Loss: 313.3391 | CE: 0.1833 | KD: 1060.6642\n",
      "Train Epoch: 031 Batch: 00063/00094 | Loss: 313.2497 | CE: 0.1236 | KD: 1060.5635\n",
      "Train Epoch: 031 Batch: 00064/00094 | Loss: 313.2901 | CE: 0.1634 | KD: 1060.5657\n",
      "Train Epoch: 031 Batch: 00065/00094 | Loss: 313.2155 | CE: 0.1050 | KD: 1060.5109\n",
      "Train Epoch: 031 Batch: 00066/00094 | Loss: 313.3079 | CE: 0.1503 | KD: 1060.6703\n",
      "Train Epoch: 031 Batch: 00067/00094 | Loss: 313.4696 | CE: 0.2027 | KD: 1061.0405\n",
      "Train Epoch: 031 Batch: 00068/00094 | Loss: 313.4525 | CE: 0.2388 | KD: 1060.8605\n",
      "Train Epoch: 031 Batch: 00069/00094 | Loss: 313.3468 | CE: 0.1894 | KD: 1060.6697\n",
      "Train Epoch: 031 Batch: 00070/00094 | Loss: 313.3614 | CE: 0.1386 | KD: 1060.8911\n",
      "Train Epoch: 031 Batch: 00071/00094 | Loss: 313.4060 | CE: 0.2107 | KD: 1060.7981\n",
      "Train Epoch: 031 Batch: 00072/00094 | Loss: 313.3645 | CE: 0.1319 | KD: 1060.9244\n",
      "Train Epoch: 031 Batch: 00073/00094 | Loss: 313.2682 | CE: 0.1317 | KD: 1060.5985\n",
      "Train Epoch: 031 Batch: 00074/00094 | Loss: 313.4205 | CE: 0.1743 | KD: 1060.9705\n",
      "Train Epoch: 031 Batch: 00075/00094 | Loss: 313.2734 | CE: 0.1477 | KD: 1060.5624\n",
      "Train Epoch: 031 Batch: 00076/00094 | Loss: 313.3554 | CE: 0.1983 | KD: 1060.6686\n",
      "Train Epoch: 031 Batch: 00077/00094 | Loss: 313.3162 | CE: 0.1291 | KD: 1060.7701\n",
      "Train Epoch: 031 Batch: 00078/00094 | Loss: 313.3497 | CE: 0.1775 | KD: 1060.7197\n",
      "Train Epoch: 031 Batch: 00079/00094 | Loss: 313.3217 | CE: 0.1485 | KD: 1060.7233\n",
      "Train Epoch: 031 Batch: 00080/00094 | Loss: 313.3047 | CE: 0.1442 | KD: 1060.6803\n",
      "Train Epoch: 031 Batch: 00081/00094 | Loss: 313.2871 | CE: 0.1598 | KD: 1060.5677\n",
      "Train Epoch: 031 Batch: 00082/00094 | Loss: 313.3900 | CE: 0.1089 | KD: 1061.0886\n",
      "Train Epoch: 031 Batch: 00083/00094 | Loss: 313.4079 | CE: 0.1839 | KD: 1060.8951\n",
      "Train Epoch: 031 Batch: 00084/00094 | Loss: 313.3488 | CE: 0.1546 | KD: 1060.7943\n",
      "Train Epoch: 031 Batch: 00085/00094 | Loss: 313.3349 | CE: 0.1546 | KD: 1060.7469\n",
      "Train Epoch: 031 Batch: 00086/00094 | Loss: 313.3694 | CE: 0.1930 | KD: 1060.7340\n",
      "Train Epoch: 031 Batch: 00087/00094 | Loss: 313.3654 | CE: 0.1463 | KD: 1060.8789\n",
      "Train Epoch: 031 Batch: 00088/00094 | Loss: 313.3173 | CE: 0.1376 | KD: 1060.7450\n",
      "Train Epoch: 031 Batch: 00089/00094 | Loss: 313.3550 | CE: 0.1894 | KD: 1060.6975\n",
      "Train Epoch: 031 Batch: 00090/00094 | Loss: 313.4066 | CE: 0.2788 | KD: 1060.5693\n",
      "Train Epoch: 031 Batch: 00091/00094 | Loss: 313.3845 | CE: 0.1610 | KD: 1060.8934\n",
      "Train Epoch: 031 Batch: 00092/00094 | Loss: 313.3628 | CE: 0.1883 | KD: 1060.7274\n",
      "Train Epoch: 031 Batch: 00093/00094 | Loss: 313.3610 | CE: 0.1287 | KD: 1060.9235\n",
      "Train Epoch: 031 Batch: 00094/00094 | Loss: 313.4812 | CE: 0.2595 | KD: 1060.8876\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1509 | acc:97.4500\n",
      "[VAL Acc] Target: 97.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6822 | acc:49.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1512 | acc:53.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0712 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3276 | acc:54.1144\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5769 | acc:70.1479\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9957 | acc:63.5580\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7899 | acc:60.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.06%\n",
      "[VAL Acc] Avg 62.07%\n",
      "Train Epoch: 032 Batch: 00001/00094 | Loss: 313.4358 | CE: 0.2237 | KD: 1060.8549\n",
      "Train Epoch: 032 Batch: 00002/00094 | Loss: 313.3093 | CE: 0.1379 | KD: 1060.7170\n",
      "Train Epoch: 032 Batch: 00003/00094 | Loss: 313.3770 | CE: 0.2104 | KD: 1060.7008\n",
      "Train Epoch: 032 Batch: 00004/00094 | Loss: 313.4340 | CE: 0.2220 | KD: 1060.8547\n",
      "Train Epoch: 032 Batch: 00005/00094 | Loss: 313.4858 | CE: 0.2336 | KD: 1060.9908\n",
      "Train Epoch: 032 Batch: 00006/00094 | Loss: 313.3680 | CE: 0.1323 | KD: 1060.9348\n",
      "Train Epoch: 032 Batch: 00007/00094 | Loss: 313.3097 | CE: 0.1178 | KD: 1060.7863\n",
      "Train Epoch: 032 Batch: 00008/00094 | Loss: 313.2986 | CE: 0.1329 | KD: 1060.6975\n",
      "Train Epoch: 032 Batch: 00009/00094 | Loss: 313.2478 | CE: 0.1522 | KD: 1060.4603\n",
      "Train Epoch: 032 Batch: 00010/00094 | Loss: 313.3772 | CE: 0.1875 | KD: 1060.7791\n",
      "Train Epoch: 032 Batch: 00011/00094 | Loss: 313.3955 | CE: 0.1698 | KD: 1060.9012\n",
      "Train Epoch: 032 Batch: 00012/00094 | Loss: 313.2664 | CE: 0.1046 | KD: 1060.6846\n",
      "Train Epoch: 032 Batch: 00013/00094 | Loss: 313.3259 | CE: 0.1358 | KD: 1060.7803\n",
      "Train Epoch: 032 Batch: 00014/00094 | Loss: 313.2625 | CE: 0.1286 | KD: 1060.5900\n",
      "Train Epoch: 032 Batch: 00015/00094 | Loss: 313.2900 | CE: 0.1820 | KD: 1060.5026\n",
      "Train Epoch: 032 Batch: 00016/00094 | Loss: 313.2818 | CE: 0.1277 | KD: 1060.6584\n",
      "Train Epoch: 032 Batch: 00017/00094 | Loss: 313.2629 | CE: 0.1530 | KD: 1060.5087\n",
      "Train Epoch: 032 Batch: 00018/00094 | Loss: 313.4723 | CE: 0.2075 | KD: 1061.0334\n",
      "Train Epoch: 032 Batch: 00019/00094 | Loss: 313.2624 | CE: 0.1494 | KD: 1060.5193\n",
      "Train Epoch: 032 Batch: 00020/00094 | Loss: 313.1902 | CE: 0.1489 | KD: 1060.2764\n",
      "Train Epoch: 032 Batch: 00021/00094 | Loss: 313.4032 | CE: 0.1391 | KD: 1061.0310\n",
      "Train Epoch: 032 Batch: 00022/00094 | Loss: 313.4138 | CE: 0.1861 | KD: 1060.9077\n",
      "Train Epoch: 032 Batch: 00023/00094 | Loss: 313.3668 | CE: 0.1181 | KD: 1060.9788\n",
      "Train Epoch: 032 Batch: 00024/00094 | Loss: 313.3622 | CE: 0.1847 | KD: 1060.7377\n",
      "Train Epoch: 032 Batch: 00025/00094 | Loss: 313.3982 | CE: 0.1998 | KD: 1060.8085\n",
      "Train Epoch: 032 Batch: 00026/00094 | Loss: 313.4781 | CE: 0.2043 | KD: 1061.0640\n",
      "Train Epoch: 032 Batch: 00027/00094 | Loss: 313.5740 | CE: 0.2124 | KD: 1061.3613\n",
      "Train Epoch: 032 Batch: 00028/00094 | Loss: 313.3993 | CE: 0.1690 | KD: 1060.9163\n",
      "Train Epoch: 032 Batch: 00029/00094 | Loss: 313.3792 | CE: 0.1905 | KD: 1060.7758\n",
      "Train Epoch: 032 Batch: 00030/00094 | Loss: 313.2525 | CE: 0.1151 | KD: 1060.6022\n",
      "Train Epoch: 032 Batch: 00031/00094 | Loss: 313.3197 | CE: 0.1479 | KD: 1060.7183\n",
      "Train Epoch: 032 Batch: 00032/00094 | Loss: 313.3966 | CE: 0.1605 | KD: 1060.9362\n",
      "Train Epoch: 032 Batch: 00033/00094 | Loss: 313.3596 | CE: 0.1229 | KD: 1060.9384\n",
      "Train Epoch: 032 Batch: 00034/00094 | Loss: 313.4043 | CE: 0.1503 | KD: 1060.9966\n",
      "Train Epoch: 032 Batch: 00035/00094 | Loss: 313.3506 | CE: 0.1206 | KD: 1060.9156\n",
      "Train Epoch: 032 Batch: 00036/00094 | Loss: 313.3187 | CE: 0.1723 | KD: 1060.6323\n",
      "Train Epoch: 032 Batch: 00037/00094 | Loss: 313.3818 | CE: 0.1933 | KD: 1060.7751\n",
      "Train Epoch: 032 Batch: 00038/00094 | Loss: 313.2549 | CE: 0.1267 | KD: 1060.5709\n",
      "Train Epoch: 032 Batch: 00039/00094 | Loss: 313.2848 | CE: 0.1436 | KD: 1060.6147\n",
      "Train Epoch: 032 Batch: 00040/00094 | Loss: 313.4291 | CE: 0.1716 | KD: 1061.0089\n",
      "Train Epoch: 032 Batch: 00041/00094 | Loss: 313.3197 | CE: 0.1097 | KD: 1060.8480\n",
      "Train Epoch: 032 Batch: 00042/00094 | Loss: 313.3600 | CE: 0.1500 | KD: 1060.8478\n",
      "Train Epoch: 032 Batch: 00043/00094 | Loss: 313.2466 | CE: 0.1371 | KD: 1060.5077\n",
      "Train Epoch: 032 Batch: 00044/00094 | Loss: 313.4521 | CE: 0.2275 | KD: 1060.8970\n",
      "Train Epoch: 032 Batch: 00045/00094 | Loss: 313.4225 | CE: 0.1799 | KD: 1060.9584\n",
      "Train Epoch: 032 Batch: 00046/00094 | Loss: 313.3051 | CE: 0.1461 | KD: 1060.6753\n",
      "Train Epoch: 032 Batch: 00047/00094 | Loss: 313.4599 | CE: 0.2001 | KD: 1061.0162\n",
      "Train Epoch: 032 Batch: 00048/00094 | Loss: 313.4316 | CE: 0.1756 | KD: 1061.0035\n",
      "Train Epoch: 032 Batch: 00049/00094 | Loss: 313.3708 | CE: 0.1478 | KD: 1060.8921\n",
      "Train Epoch: 032 Batch: 00050/00094 | Loss: 313.3134 | CE: 0.1552 | KD: 1060.6725\n",
      "Train Epoch: 032 Batch: 00051/00094 | Loss: 313.3645 | CE: 0.1999 | KD: 1060.6942\n",
      "Train Epoch: 032 Batch: 00052/00094 | Loss: 313.3320 | CE: 0.1256 | KD: 1060.8354\n",
      "Train Epoch: 032 Batch: 00053/00094 | Loss: 313.3103 | CE: 0.1281 | KD: 1060.7538\n",
      "Train Epoch: 032 Batch: 00054/00094 | Loss: 313.3697 | CE: 0.1283 | KD: 1060.9540\n",
      "Train Epoch: 032 Batch: 00055/00094 | Loss: 313.3531 | CE: 0.1746 | KD: 1060.7411\n",
      "Train Epoch: 032 Batch: 00056/00094 | Loss: 313.1992 | CE: 0.1092 | KD: 1060.4410\n",
      "Train Epoch: 032 Batch: 00057/00094 | Loss: 313.3607 | CE: 0.1883 | KD: 1060.7206\n",
      "Train Epoch: 032 Batch: 00058/00094 | Loss: 313.2775 | CE: 0.1155 | KD: 1060.6854\n",
      "Train Epoch: 032 Batch: 00059/00094 | Loss: 313.3478 | CE: 0.1519 | KD: 1060.8000\n",
      "Train Epoch: 032 Batch: 00060/00094 | Loss: 313.3525 | CE: 0.1377 | KD: 1060.8640\n",
      "Train Epoch: 032 Batch: 00061/00094 | Loss: 313.3132 | CE: 0.1667 | KD: 1060.6327\n",
      "Train Epoch: 032 Batch: 00062/00094 | Loss: 313.3271 | CE: 0.1152 | KD: 1060.8541\n",
      "Train Epoch: 032 Batch: 00063/00094 | Loss: 313.3064 | CE: 0.1316 | KD: 1060.7284\n",
      "Train Epoch: 032 Batch: 00064/00094 | Loss: 313.3722 | CE: 0.1692 | KD: 1060.8239\n",
      "Train Epoch: 032 Batch: 00065/00094 | Loss: 313.3246 | CE: 0.1995 | KD: 1060.5603\n",
      "Train Epoch: 032 Batch: 00066/00094 | Loss: 313.2674 | CE: 0.1277 | KD: 1060.6095\n",
      "Train Epoch: 032 Batch: 00067/00094 | Loss: 313.4594 | CE: 0.2525 | KD: 1060.8373\n",
      "Train Epoch: 032 Batch: 00068/00094 | Loss: 313.2581 | CE: 0.1590 | KD: 1060.4722\n",
      "Train Epoch: 032 Batch: 00069/00094 | Loss: 313.3649 | CE: 0.1285 | KD: 1060.9371\n",
      "Train Epoch: 032 Batch: 00070/00094 | Loss: 313.3863 | CE: 0.1927 | KD: 1060.7925\n",
      "Train Epoch: 032 Batch: 00071/00094 | Loss: 313.2190 | CE: 0.1445 | KD: 1060.3889\n",
      "Train Epoch: 032 Batch: 00072/00094 | Loss: 313.2956 | CE: 0.1338 | KD: 1060.6848\n",
      "Train Epoch: 032 Batch: 00073/00094 | Loss: 313.3279 | CE: 0.1844 | KD: 1060.6227\n",
      "Train Epoch: 032 Batch: 00074/00094 | Loss: 313.2752 | CE: 0.1054 | KD: 1060.7114\n",
      "Train Epoch: 032 Batch: 00075/00094 | Loss: 313.3681 | CE: 0.1576 | KD: 1060.8495\n",
      "Train Epoch: 032 Batch: 00076/00094 | Loss: 313.2577 | CE: 0.1500 | KD: 1060.5012\n",
      "Train Epoch: 032 Batch: 00077/00094 | Loss: 313.4514 | CE: 0.1969 | KD: 1060.9987\n",
      "Train Epoch: 032 Batch: 00078/00094 | Loss: 313.3023 | CE: 0.1650 | KD: 1060.6018\n",
      "Train Epoch: 032 Batch: 00079/00094 | Loss: 313.3469 | CE: 0.1943 | KD: 1060.6534\n",
      "Train Epoch: 032 Batch: 00080/00094 | Loss: 313.3358 | CE: 0.1666 | KD: 1060.7097\n",
      "Train Epoch: 032 Batch: 00081/00094 | Loss: 313.2503 | CE: 0.1721 | KD: 1060.4015\n",
      "Train Epoch: 032 Batch: 00082/00094 | Loss: 313.3184 | CE: 0.1532 | KD: 1060.6960\n",
      "Train Epoch: 032 Batch: 00083/00094 | Loss: 313.3330 | CE: 0.1786 | KD: 1060.6597\n",
      "Train Epoch: 032 Batch: 00084/00094 | Loss: 313.3616 | CE: 0.1389 | KD: 1060.8906\n",
      "Train Epoch: 032 Batch: 00085/00094 | Loss: 313.4061 | CE: 0.2019 | KD: 1060.8280\n",
      "Train Epoch: 032 Batch: 00086/00094 | Loss: 313.2693 | CE: 0.1121 | KD: 1060.6691\n",
      "Train Epoch: 032 Batch: 00087/00094 | Loss: 313.4056 | CE: 0.1727 | KD: 1060.9255\n",
      "Train Epoch: 032 Batch: 00088/00094 | Loss: 313.2108 | CE: 0.1541 | KD: 1060.3284\n",
      "Train Epoch: 032 Batch: 00089/00094 | Loss: 313.3755 | CE: 0.1647 | KD: 1060.8507\n",
      "Train Epoch: 032 Batch: 00090/00094 | Loss: 313.2829 | CE: 0.1698 | KD: 1060.5195\n",
      "Train Epoch: 032 Batch: 00091/00094 | Loss: 313.3938 | CE: 0.1645 | KD: 1060.9133\n",
      "Train Epoch: 032 Batch: 00092/00094 | Loss: 313.3970 | CE: 0.2046 | KD: 1060.7883\n",
      "Train Epoch: 032 Batch: 00093/00094 | Loss: 313.3423 | CE: 0.1371 | KD: 1060.8314\n",
      "Train Epoch: 032 Batch: 00094/00094 | Loss: 313.3615 | CE: 0.2259 | KD: 1060.5957\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1451 | acc:97.2000\n",
      "[VAL Acc] Target: 97.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.4968 | acc:49.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0494 | acc:56.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0239 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1621 | acc:57.0925\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6072 | acc:68.9464\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8330 | acc:65.5564\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7488 | acc:62.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.50%\n",
      "[VAL Acc] Avg 63.04%\n",
      "Train Epoch: 033 Batch: 00001/00094 | Loss: 313.4882 | CE: 0.1737 | KD: 1061.2018\n",
      "Train Epoch: 033 Batch: 00002/00094 | Loss: 313.4411 | CE: 0.2257 | KD: 1060.8662\n",
      "Train Epoch: 033 Batch: 00003/00094 | Loss: 313.2815 | CE: 0.1464 | KD: 1060.5940\n",
      "Train Epoch: 033 Batch: 00004/00094 | Loss: 313.2714 | CE: 0.1491 | KD: 1060.5507\n",
      "Train Epoch: 033 Batch: 00005/00094 | Loss: 313.6575 | CE: 0.3436 | KD: 1061.2001\n",
      "Train Epoch: 033 Batch: 00006/00094 | Loss: 313.3447 | CE: 0.1199 | KD: 1060.8979\n",
      "Train Epoch: 033 Batch: 00007/00094 | Loss: 313.3810 | CE: 0.1436 | KD: 1060.9404\n",
      "Train Epoch: 033 Batch: 00008/00094 | Loss: 313.4036 | CE: 0.1647 | KD: 1060.9457\n",
      "Train Epoch: 033 Batch: 00009/00094 | Loss: 313.2675 | CE: 0.1644 | KD: 1060.4856\n",
      "Train Epoch: 033 Batch: 00010/00094 | Loss: 313.2607 | CE: 0.1299 | KD: 1060.5797\n",
      "Train Epoch: 033 Batch: 00011/00094 | Loss: 313.3631 | CE: 0.1517 | KD: 1060.8527\n",
      "Train Epoch: 033 Batch: 00012/00094 | Loss: 313.3426 | CE: 0.1363 | KD: 1060.8352\n",
      "Train Epoch: 033 Batch: 00013/00094 | Loss: 313.2939 | CE: 0.1807 | KD: 1060.5201\n",
      "Train Epoch: 033 Batch: 00014/00094 | Loss: 313.3220 | CE: 0.1825 | KD: 1060.6089\n",
      "Train Epoch: 033 Batch: 00015/00094 | Loss: 313.3822 | CE: 0.1272 | KD: 1061.0004\n",
      "Train Epoch: 033 Batch: 00016/00094 | Loss: 313.3786 | CE: 0.1849 | KD: 1060.7928\n",
      "Train Epoch: 033 Batch: 00017/00094 | Loss: 313.3629 | CE: 0.1497 | KD: 1060.8588\n",
      "Train Epoch: 033 Batch: 00018/00094 | Loss: 313.3341 | CE: 0.1550 | KD: 1060.7430\n",
      "Train Epoch: 033 Batch: 00019/00094 | Loss: 313.3373 | CE: 0.1582 | KD: 1060.7432\n",
      "Train Epoch: 033 Batch: 00020/00094 | Loss: 313.3622 | CE: 0.1550 | KD: 1060.8385\n",
      "Train Epoch: 033 Batch: 00021/00094 | Loss: 313.4461 | CE: 0.1940 | KD: 1060.9907\n",
      "Train Epoch: 033 Batch: 00022/00094 | Loss: 313.2389 | CE: 0.1503 | KD: 1060.4365\n",
      "Train Epoch: 033 Batch: 00023/00094 | Loss: 313.2665 | CE: 0.1387 | KD: 1060.5693\n",
      "Train Epoch: 033 Batch: 00024/00094 | Loss: 313.3482 | CE: 0.1537 | KD: 1060.7955\n",
      "Train Epoch: 033 Batch: 00025/00094 | Loss: 313.3262 | CE: 0.1594 | KD: 1060.7013\n",
      "Train Epoch: 033 Batch: 00026/00094 | Loss: 313.3639 | CE: 0.1350 | KD: 1060.9115\n",
      "Train Epoch: 033 Batch: 00027/00094 | Loss: 313.4342 | CE: 0.1488 | KD: 1061.1030\n",
      "Train Epoch: 033 Batch: 00028/00094 | Loss: 313.3891 | CE: 0.1316 | KD: 1061.0089\n",
      "Train Epoch: 033 Batch: 00029/00094 | Loss: 313.2464 | CE: 0.1501 | KD: 1060.4628\n",
      "Train Epoch: 033 Batch: 00030/00094 | Loss: 313.3828 | CE: 0.1932 | KD: 1060.7784\n",
      "Train Epoch: 033 Batch: 00031/00094 | Loss: 313.3618 | CE: 0.1202 | KD: 1060.9547\n",
      "Train Epoch: 033 Batch: 00032/00094 | Loss: 313.3803 | CE: 0.1830 | KD: 1060.8046\n",
      "Train Epoch: 033 Batch: 00033/00094 | Loss: 313.2780 | CE: 0.1070 | KD: 1060.7158\n",
      "Train Epoch: 033 Batch: 00034/00094 | Loss: 313.3896 | CE: 0.2151 | KD: 1060.7275\n",
      "Train Epoch: 033 Batch: 00035/00094 | Loss: 313.4774 | CE: 0.1362 | KD: 1061.2921\n",
      "Train Epoch: 033 Batch: 00036/00094 | Loss: 313.4117 | CE: 0.1363 | KD: 1061.0691\n",
      "Train Epoch: 033 Batch: 00037/00094 | Loss: 313.4357 | CE: 0.1715 | KD: 1061.0315\n",
      "Train Epoch: 033 Batch: 00038/00094 | Loss: 313.3948 | CE: 0.1876 | KD: 1060.8384\n",
      "Train Epoch: 033 Batch: 00039/00094 | Loss: 313.4335 | CE: 0.1618 | KD: 1061.0566\n",
      "Train Epoch: 033 Batch: 00040/00094 | Loss: 313.2551 | CE: 0.1449 | KD: 1060.5096\n",
      "Train Epoch: 033 Batch: 00041/00094 | Loss: 313.3949 | CE: 0.2085 | KD: 1060.7678\n",
      "Train Epoch: 033 Batch: 00042/00094 | Loss: 313.3411 | CE: 0.1422 | KD: 1060.8102\n",
      "Train Epoch: 033 Batch: 00043/00094 | Loss: 313.3528 | CE: 0.1326 | KD: 1060.8822\n",
      "Train Epoch: 033 Batch: 00044/00094 | Loss: 313.5210 | CE: 0.2411 | KD: 1061.0846\n",
      "Train Epoch: 033 Batch: 00045/00094 | Loss: 313.3023 | CE: 0.1326 | KD: 1060.7115\n",
      "Train Epoch: 033 Batch: 00046/00094 | Loss: 313.2905 | CE: 0.1422 | KD: 1060.6387\n",
      "Train Epoch: 033 Batch: 00047/00094 | Loss: 313.3704 | CE: 0.2242 | KD: 1060.6317\n",
      "Train Epoch: 033 Batch: 00048/00094 | Loss: 313.3978 | CE: 0.2223 | KD: 1060.7308\n",
      "Train Epoch: 033 Batch: 00049/00094 | Loss: 313.2541 | CE: 0.1342 | KD: 1060.5427\n",
      "Train Epoch: 033 Batch: 00050/00094 | Loss: 313.4577 | CE: 0.1470 | KD: 1061.1890\n",
      "Train Epoch: 033 Batch: 00051/00094 | Loss: 313.3943 | CE: 0.1816 | KD: 1060.8571\n",
      "Train Epoch: 033 Batch: 00052/00094 | Loss: 313.3711 | CE: 0.1938 | KD: 1060.7368\n",
      "Train Epoch: 033 Batch: 00053/00094 | Loss: 313.2959 | CE: 0.1461 | KD: 1060.6438\n",
      "Train Epoch: 033 Batch: 00054/00094 | Loss: 313.2861 | CE: 0.1516 | KD: 1060.5922\n",
      "Train Epoch: 033 Batch: 00055/00094 | Loss: 313.3616 | CE: 0.2067 | KD: 1060.6613\n",
      "Train Epoch: 033 Batch: 00056/00094 | Loss: 313.3090 | CE: 0.1398 | KD: 1060.7094\n",
      "Train Epoch: 033 Batch: 00057/00094 | Loss: 313.2527 | CE: 0.1203 | KD: 1060.5851\n",
      "Train Epoch: 033 Batch: 00058/00094 | Loss: 313.2863 | CE: 0.1415 | KD: 1060.6273\n",
      "Train Epoch: 033 Batch: 00059/00094 | Loss: 313.2471 | CE: 0.1214 | KD: 1060.5624\n",
      "Train Epoch: 033 Batch: 00060/00094 | Loss: 313.4071 | CE: 0.1305 | KD: 1061.0734\n",
      "Train Epoch: 033 Batch: 00061/00094 | Loss: 313.3326 | CE: 0.1386 | KD: 1060.7933\n",
      "Train Epoch: 033 Batch: 00062/00094 | Loss: 313.3694 | CE: 0.1123 | KD: 1061.0071\n",
      "Train Epoch: 033 Batch: 00063/00094 | Loss: 313.3473 | CE: 0.1699 | KD: 1060.7373\n",
      "Train Epoch: 033 Batch: 00064/00094 | Loss: 313.3938 | CE: 0.1583 | KD: 1060.9343\n",
      "Train Epoch: 033 Batch: 00065/00094 | Loss: 313.3398 | CE: 0.1655 | KD: 1060.7269\n",
      "Train Epoch: 033 Batch: 00066/00094 | Loss: 313.2884 | CE: 0.1381 | KD: 1060.6455\n",
      "Train Epoch: 033 Batch: 00067/00094 | Loss: 313.3921 | CE: 0.1185 | KD: 1061.0631\n",
      "Train Epoch: 033 Batch: 00068/00094 | Loss: 313.3959 | CE: 0.1801 | KD: 1060.8673\n",
      "Train Epoch: 033 Batch: 00069/00094 | Loss: 313.3371 | CE: 0.1805 | KD: 1060.6669\n",
      "Train Epoch: 033 Batch: 00070/00094 | Loss: 313.4976 | CE: 0.1524 | KD: 1061.3057\n",
      "Train Epoch: 033 Batch: 00071/00094 | Loss: 313.2950 | CE: 0.1761 | KD: 1060.5391\n",
      "Train Epoch: 033 Batch: 00072/00094 | Loss: 313.3302 | CE: 0.1189 | KD: 1060.8523\n",
      "Train Epoch: 033 Batch: 00073/00094 | Loss: 313.3506 | CE: 0.1332 | KD: 1060.8726\n",
      "Train Epoch: 033 Batch: 00074/00094 | Loss: 313.2513 | CE: 0.1331 | KD: 1060.5370\n",
      "Train Epoch: 033 Batch: 00075/00094 | Loss: 313.2858 | CE: 0.1810 | KD: 1060.4913\n",
      "Train Epoch: 033 Batch: 00076/00094 | Loss: 313.2879 | CE: 0.1670 | KD: 1060.5460\n",
      "Train Epoch: 033 Batch: 00077/00094 | Loss: 313.3883 | CE: 0.1246 | KD: 1061.0297\n",
      "Train Epoch: 033 Batch: 00078/00094 | Loss: 313.2436 | CE: 0.1212 | KD: 1060.5510\n",
      "Train Epoch: 033 Batch: 00079/00094 | Loss: 313.2703 | CE: 0.1433 | KD: 1060.5668\n",
      "Train Epoch: 033 Batch: 00080/00094 | Loss: 313.3370 | CE: 0.1336 | KD: 1060.8256\n",
      "Train Epoch: 033 Batch: 00081/00094 | Loss: 313.2965 | CE: 0.1880 | KD: 1060.5042\n",
      "Train Epoch: 033 Batch: 00082/00094 | Loss: 313.2579 | CE: 0.1612 | KD: 1060.4639\n",
      "Train Epoch: 033 Batch: 00083/00094 | Loss: 313.2863 | CE: 0.1282 | KD: 1060.6718\n",
      "Train Epoch: 033 Batch: 00084/00094 | Loss: 313.2643 | CE: 0.1594 | KD: 1060.4918\n",
      "Train Epoch: 033 Batch: 00085/00094 | Loss: 313.2423 | CE: 0.1432 | KD: 1060.4723\n",
      "Train Epoch: 033 Batch: 00086/00094 | Loss: 313.3533 | CE: 0.1625 | KD: 1060.7830\n",
      "Train Epoch: 033 Batch: 00087/00094 | Loss: 313.2705 | CE: 0.1528 | KD: 1060.5354\n",
      "Train Epoch: 033 Batch: 00088/00094 | Loss: 313.4518 | CE: 0.1601 | KD: 1061.1245\n",
      "Train Epoch: 033 Batch: 00089/00094 | Loss: 313.4214 | CE: 0.1143 | KD: 1061.1768\n",
      "Train Epoch: 033 Batch: 00090/00094 | Loss: 313.4747 | CE: 0.1213 | KD: 1061.3335\n",
      "Train Epoch: 033 Batch: 00091/00094 | Loss: 313.3420 | CE: 0.1381 | KD: 1060.8270\n",
      "Train Epoch: 033 Batch: 00092/00094 | Loss: 313.4763 | CE: 0.2216 | KD: 1060.9994\n",
      "Train Epoch: 033 Batch: 00093/00094 | Loss: 313.2285 | CE: 0.1340 | KD: 1060.4564\n",
      "Train Epoch: 033 Batch: 00094/00094 | Loss: 313.3668 | CE: 0.1534 | KD: 1060.8591\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1490 | acc:97.6000\n",
      "[VAL Acc] Target: 97.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6017 | acc:50.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0955 | acc:55.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0645 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3420 | acc:53.2132\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.21%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5719 | acc:71.0721\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9379 | acc:64.0282\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7708 | acc:61.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.19%\n",
      "[VAL Acc] Avg 62.16%\n",
      "Train Epoch: 034 Batch: 00001/00094 | Loss: 282.0499 | CE: 0.2077 | KD: 1060.6719\n",
      "Train Epoch: 034 Batch: 00002/00094 | Loss: 282.0104 | CE: 0.1186 | KD: 1060.8582\n",
      "Train Epoch: 034 Batch: 00003/00094 | Loss: 282.2302 | CE: 0.1959 | KD: 1061.3945\n",
      "Train Epoch: 034 Batch: 00004/00094 | Loss: 281.9825 | CE: 0.1537 | KD: 1060.6216\n",
      "Train Epoch: 034 Batch: 00005/00094 | Loss: 282.2893 | CE: 0.3032 | KD: 1061.2131\n",
      "Train Epoch: 034 Batch: 00006/00094 | Loss: 282.0480 | CE: 0.1545 | KD: 1060.8647\n",
      "Train Epoch: 034 Batch: 00007/00094 | Loss: 281.9720 | CE: 0.1484 | KD: 1060.6018\n",
      "Train Epoch: 034 Batch: 00008/00094 | Loss: 281.9013 | CE: 0.1279 | KD: 1060.4126\n",
      "Train Epoch: 034 Batch: 00009/00094 | Loss: 282.1076 | CE: 0.1857 | KD: 1060.9719\n",
      "Train Epoch: 034 Batch: 00010/00094 | Loss: 282.0865 | CE: 0.2657 | KD: 1060.5912\n",
      "Train Epoch: 034 Batch: 00011/00094 | Loss: 281.9124 | CE: 0.1262 | KD: 1060.4606\n",
      "Train Epoch: 034 Batch: 00012/00094 | Loss: 282.1052 | CE: 0.1404 | KD: 1061.1332\n",
      "Train Epoch: 034 Batch: 00013/00094 | Loss: 282.0261 | CE: 0.1326 | KD: 1060.8647\n",
      "Train Epoch: 034 Batch: 00014/00094 | Loss: 282.0537 | CE: 0.1501 | KD: 1060.9030\n",
      "Train Epoch: 034 Batch: 00015/00094 | Loss: 282.0038 | CE: 0.1404 | KD: 1060.7513\n",
      "Train Epoch: 034 Batch: 00016/00094 | Loss: 282.1817 | CE: 0.2000 | KD: 1061.1968\n",
      "Train Epoch: 034 Batch: 00017/00094 | Loss: 282.0657 | CE: 0.1392 | KD: 1060.9891\n",
      "Train Epoch: 034 Batch: 00018/00094 | Loss: 281.9812 | CE: 0.1088 | KD: 1060.7856\n",
      "Train Epoch: 034 Batch: 00019/00094 | Loss: 282.0476 | CE: 0.2454 | KD: 1060.5214\n",
      "Train Epoch: 034 Batch: 00020/00094 | Loss: 282.0090 | CE: 0.1617 | KD: 1060.6909\n",
      "Train Epoch: 034 Batch: 00021/00094 | Loss: 281.9282 | CE: 0.1365 | KD: 1060.4818\n",
      "Train Epoch: 034 Batch: 00022/00094 | Loss: 281.9778 | CE: 0.1633 | KD: 1060.5673\n",
      "Train Epoch: 034 Batch: 00023/00094 | Loss: 282.0223 | CE: 0.1376 | KD: 1060.8319\n",
      "Train Epoch: 034 Batch: 00024/00094 | Loss: 282.0280 | CE: 0.1316 | KD: 1060.8755\n",
      "Train Epoch: 034 Batch: 00025/00094 | Loss: 282.0555 | CE: 0.1401 | KD: 1060.9473\n",
      "Train Epoch: 034 Batch: 00026/00094 | Loss: 281.9539 | CE: 0.1075 | KD: 1060.6874\n",
      "Train Epoch: 034 Batch: 00027/00094 | Loss: 281.9343 | CE: 0.1127 | KD: 1060.5942\n",
      "Train Epoch: 034 Batch: 00028/00094 | Loss: 282.0235 | CE: 0.1405 | KD: 1060.8253\n",
      "Train Epoch: 034 Batch: 00029/00094 | Loss: 282.0241 | CE: 0.1226 | KD: 1060.8951\n",
      "Train Epoch: 034 Batch: 00030/00094 | Loss: 281.9677 | CE: 0.1376 | KD: 1060.6260\n",
      "Train Epoch: 034 Batch: 00031/00094 | Loss: 282.0660 | CE: 0.1558 | KD: 1060.9275\n",
      "Train Epoch: 034 Batch: 00032/00094 | Loss: 282.0885 | CE: 0.2198 | KD: 1060.7717\n",
      "Train Epoch: 034 Batch: 00033/00094 | Loss: 281.9253 | CE: 0.1315 | KD: 1060.4894\n",
      "Train Epoch: 034 Batch: 00034/00094 | Loss: 282.0291 | CE: 0.1439 | KD: 1060.8337\n",
      "Train Epoch: 034 Batch: 00035/00094 | Loss: 282.0554 | CE: 0.1526 | KD: 1060.8995\n",
      "Train Epoch: 034 Batch: 00036/00094 | Loss: 282.1317 | CE: 0.1493 | KD: 1061.1993\n",
      "Train Epoch: 034 Batch: 00037/00094 | Loss: 282.0200 | CE: 0.1450 | KD: 1060.7950\n",
      "Train Epoch: 034 Batch: 00038/00094 | Loss: 281.9995 | CE: 0.1533 | KD: 1060.6868\n",
      "Train Epoch: 034 Batch: 00039/00094 | Loss: 282.0078 | CE: 0.1386 | KD: 1060.7732\n",
      "Train Epoch: 034 Batch: 00040/00094 | Loss: 282.2670 | CE: 0.3000 | KD: 1061.1412\n",
      "Train Epoch: 034 Batch: 00041/00094 | Loss: 281.9610 | CE: 0.1294 | KD: 1060.6317\n",
      "Train Epoch: 034 Batch: 00042/00094 | Loss: 281.9324 | CE: 0.1235 | KD: 1060.5464\n",
      "Train Epoch: 034 Batch: 00043/00094 | Loss: 281.8834 | CE: 0.1100 | KD: 1060.4128\n",
      "Train Epoch: 034 Batch: 00044/00094 | Loss: 282.0441 | CE: 0.1533 | KD: 1060.8547\n",
      "Train Epoch: 034 Batch: 00045/00094 | Loss: 282.1357 | CE: 0.1485 | KD: 1061.2173\n",
      "Train Epoch: 034 Batch: 00046/00094 | Loss: 282.0678 | CE: 0.1764 | KD: 1060.8568\n",
      "Train Epoch: 034 Batch: 00047/00094 | Loss: 282.0185 | CE: 0.1448 | KD: 1060.7903\n",
      "Train Epoch: 034 Batch: 00048/00094 | Loss: 281.9799 | CE: 0.1618 | KD: 1060.5811\n",
      "Train Epoch: 034 Batch: 00049/00094 | Loss: 281.9510 | CE: 0.1262 | KD: 1060.6063\n",
      "Train Epoch: 034 Batch: 00050/00094 | Loss: 282.1060 | CE: 0.1733 | KD: 1061.0126\n",
      "Train Epoch: 034 Batch: 00051/00094 | Loss: 281.9467 | CE: 0.1333 | KD: 1060.5635\n",
      "Train Epoch: 034 Batch: 00052/00094 | Loss: 281.9688 | CE: 0.1753 | KD: 1060.4885\n",
      "Train Epoch: 034 Batch: 00053/00094 | Loss: 281.9996 | CE: 0.1226 | KD: 1060.8027\n",
      "Train Epoch: 034 Batch: 00054/00094 | Loss: 281.9823 | CE: 0.1382 | KD: 1060.6787\n",
      "Train Epoch: 034 Batch: 00055/00094 | Loss: 281.9172 | CE: 0.1171 | KD: 1060.5132\n",
      "Train Epoch: 034 Batch: 00056/00094 | Loss: 282.0792 | CE: 0.1310 | KD: 1061.0707\n",
      "Train Epoch: 034 Batch: 00057/00094 | Loss: 281.9953 | CE: 0.1213 | KD: 1060.7911\n",
      "Train Epoch: 034 Batch: 00058/00094 | Loss: 282.1703 | CE: 0.2634 | KD: 1060.9154\n",
      "Train Epoch: 034 Batch: 00059/00094 | Loss: 282.0945 | CE: 0.1853 | KD: 1060.9237\n",
      "Train Epoch: 034 Batch: 00060/00094 | Loss: 282.0318 | CE: 0.1521 | KD: 1060.8131\n",
      "Train Epoch: 034 Batch: 00061/00094 | Loss: 282.0784 | CE: 0.1715 | KD: 1060.9153\n",
      "Train Epoch: 034 Batch: 00062/00094 | Loss: 282.0745 | CE: 0.1295 | KD: 1061.0586\n",
      "Train Epoch: 034 Batch: 00063/00094 | Loss: 281.9718 | CE: 0.1771 | KD: 1060.4929\n",
      "Train Epoch: 034 Batch: 00064/00094 | Loss: 282.1202 | CE: 0.1948 | KD: 1060.9849\n",
      "Train Epoch: 034 Batch: 00065/00094 | Loss: 282.0458 | CE: 0.1342 | KD: 1060.9329\n",
      "Train Epoch: 034 Batch: 00066/00094 | Loss: 281.9025 | CE: 0.1012 | KD: 1060.5177\n",
      "Train Epoch: 034 Batch: 00067/00094 | Loss: 282.0332 | CE: 0.1145 | KD: 1060.9596\n",
      "Train Epoch: 034 Batch: 00068/00094 | Loss: 282.0233 | CE: 0.1179 | KD: 1060.9095\n",
      "Train Epoch: 034 Batch: 00069/00094 | Loss: 282.0728 | CE: 0.1433 | KD: 1061.0001\n",
      "Train Epoch: 034 Batch: 00070/00094 | Loss: 281.9525 | CE: 0.1199 | KD: 1060.6354\n",
      "Train Epoch: 034 Batch: 00071/00094 | Loss: 281.9837 | CE: 0.1227 | KD: 1060.7427\n",
      "Train Epoch: 034 Batch: 00072/00094 | Loss: 281.9497 | CE: 0.1132 | KD: 1060.6501\n",
      "Train Epoch: 034 Batch: 00073/00094 | Loss: 282.1638 | CE: 0.1747 | KD: 1061.2246\n",
      "Train Epoch: 034 Batch: 00074/00094 | Loss: 282.0528 | CE: 0.1501 | KD: 1060.8993\n",
      "Train Epoch: 034 Batch: 00075/00094 | Loss: 281.9935 | CE: 0.1338 | KD: 1060.7374\n",
      "Train Epoch: 034 Batch: 00076/00094 | Loss: 282.1050 | CE: 0.1629 | KD: 1061.0476\n",
      "Train Epoch: 034 Batch: 00077/00094 | Loss: 282.1044 | CE: 0.2062 | KD: 1060.8826\n",
      "Train Epoch: 034 Batch: 00078/00094 | Loss: 281.9889 | CE: 0.1150 | KD: 1060.7908\n",
      "Train Epoch: 034 Batch: 00079/00094 | Loss: 281.9429 | CE: 0.1175 | KD: 1060.6084\n",
      "Train Epoch: 034 Batch: 00080/00094 | Loss: 282.0189 | CE: 0.1328 | KD: 1060.8369\n",
      "Train Epoch: 034 Batch: 00081/00094 | Loss: 282.0130 | CE: 0.1110 | KD: 1060.8967\n",
      "Train Epoch: 034 Batch: 00082/00094 | Loss: 282.0270 | CE: 0.1500 | KD: 1060.8027\n",
      "Train Epoch: 034 Batch: 00083/00094 | Loss: 282.1307 | CE: 0.1507 | KD: 1061.1906\n",
      "Train Epoch: 034 Batch: 00084/00094 | Loss: 281.9806 | CE: 0.1622 | KD: 1060.5822\n",
      "Train Epoch: 034 Batch: 00085/00094 | Loss: 281.9643 | CE: 0.1203 | KD: 1060.6786\n",
      "Train Epoch: 034 Batch: 00086/00094 | Loss: 282.0343 | CE: 0.1680 | KD: 1060.7625\n",
      "Train Epoch: 034 Batch: 00087/00094 | Loss: 281.9857 | CE: 0.1067 | KD: 1060.8102\n",
      "Train Epoch: 034 Batch: 00088/00094 | Loss: 281.9539 | CE: 0.1185 | KD: 1060.6460\n",
      "Train Epoch: 034 Batch: 00089/00094 | Loss: 281.9539 | CE: 0.1283 | KD: 1060.6095\n",
      "Train Epoch: 034 Batch: 00090/00094 | Loss: 281.9023 | CE: 0.1186 | KD: 1060.4517\n",
      "Train Epoch: 034 Batch: 00091/00094 | Loss: 282.0277 | CE: 0.1520 | KD: 1060.7979\n",
      "Train Epoch: 034 Batch: 00092/00094 | Loss: 282.0235 | CE: 0.2125 | KD: 1060.5542\n",
      "Train Epoch: 034 Batch: 00093/00094 | Loss: 281.9568 | CE: 0.1217 | KD: 1060.6451\n",
      "Train Epoch: 034 Batch: 00094/00094 | Loss: 282.1581 | CE: 0.2615 | KD: 1060.8763\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1476 | acc:97.3500\n",
      "[VAL Acc] Target: 97.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5983 | acc:50.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0704 | acc:56.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0749 | acc:46.1832\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2554 | acc:53.9577\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5658 | acc:71.9963\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9310 | acc:64.3417\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7865 | acc:61.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.00%\n",
      "[VAL Acc] Avg 62.65%\n",
      "Train Epoch: 035 Batch: 00001/00094 | Loss: 282.0476 | CE: 0.1714 | KD: 1060.7996\n",
      "Train Epoch: 035 Batch: 00002/00094 | Loss: 282.0081 | CE: 0.1338 | KD: 1060.7926\n",
      "Train Epoch: 035 Batch: 00003/00094 | Loss: 282.0923 | CE: 0.2597 | KD: 1060.6355\n",
      "Train Epoch: 035 Batch: 00004/00094 | Loss: 282.1564 | CE: 0.1350 | KD: 1061.3463\n",
      "Train Epoch: 035 Batch: 00005/00094 | Loss: 282.1672 | CE: 0.2464 | KD: 1060.9674\n",
      "Train Epoch: 035 Batch: 00006/00094 | Loss: 281.9887 | CE: 0.1497 | KD: 1060.6598\n",
      "Train Epoch: 035 Batch: 00007/00094 | Loss: 282.0345 | CE: 0.1802 | KD: 1060.7173\n",
      "Train Epoch: 035 Batch: 00008/00094 | Loss: 281.9988 | CE: 0.1115 | KD: 1060.8414\n",
      "Train Epoch: 035 Batch: 00009/00094 | Loss: 282.0526 | CE: 0.1267 | KD: 1060.9869\n",
      "Train Epoch: 035 Batch: 00010/00094 | Loss: 281.9417 | CE: 0.1243 | KD: 1060.5782\n",
      "Train Epoch: 035 Batch: 00011/00094 | Loss: 281.8999 | CE: 0.1197 | KD: 1060.4385\n",
      "Train Epoch: 035 Batch: 00012/00094 | Loss: 282.0218 | CE: 0.1259 | KD: 1060.8740\n",
      "Train Epoch: 035 Batch: 00013/00094 | Loss: 281.9641 | CE: 0.1041 | KD: 1060.7386\n",
      "Train Epoch: 035 Batch: 00014/00094 | Loss: 282.0215 | CE: 0.1676 | KD: 1060.7155\n",
      "Train Epoch: 035 Batch: 00015/00094 | Loss: 282.0401 | CE: 0.1359 | KD: 1060.9050\n",
      "Train Epoch: 035 Batch: 00016/00094 | Loss: 281.9689 | CE: 0.1326 | KD: 1060.6493\n",
      "Train Epoch: 035 Batch: 00017/00094 | Loss: 281.9921 | CE: 0.1496 | KD: 1060.6727\n",
      "Train Epoch: 035 Batch: 00018/00094 | Loss: 282.1345 | CE: 0.1287 | KD: 1061.2872\n",
      "Train Epoch: 035 Batch: 00019/00094 | Loss: 282.0562 | CE: 0.2058 | KD: 1060.7025\n",
      "Train Epoch: 035 Batch: 00020/00094 | Loss: 282.3015 | CE: 0.3338 | KD: 1061.1442\n",
      "Train Epoch: 035 Batch: 00021/00094 | Loss: 282.0214 | CE: 0.1376 | KD: 1060.8280\n",
      "Train Epoch: 035 Batch: 00022/00094 | Loss: 282.0971 | CE: 0.1814 | KD: 1060.9485\n",
      "Train Epoch: 035 Batch: 00023/00094 | Loss: 281.9647 | CE: 0.1497 | KD: 1060.5692\n",
      "Train Epoch: 035 Batch: 00024/00094 | Loss: 282.1266 | CE: 0.2082 | KD: 1060.9586\n",
      "Train Epoch: 035 Batch: 00025/00094 | Loss: 282.0240 | CE: 0.1253 | KD: 1060.8844\n",
      "Train Epoch: 035 Batch: 00026/00094 | Loss: 282.0548 | CE: 0.1722 | KD: 1060.8239\n",
      "Train Epoch: 035 Batch: 00027/00094 | Loss: 282.0273 | CE: 0.1529 | KD: 1060.7927\n",
      "Train Epoch: 035 Batch: 00028/00094 | Loss: 281.9673 | CE: 0.1055 | KD: 1060.7454\n",
      "Train Epoch: 035 Batch: 00029/00094 | Loss: 282.0098 | CE: 0.1219 | KD: 1060.8440\n",
      "Train Epoch: 035 Batch: 00030/00094 | Loss: 282.0723 | CE: 0.1395 | KD: 1061.0128\n",
      "Train Epoch: 035 Batch: 00031/00094 | Loss: 282.0227 | CE: 0.1620 | KD: 1060.7412\n",
      "Train Epoch: 035 Batch: 00032/00094 | Loss: 282.0692 | CE: 0.1562 | KD: 1060.9380\n",
      "Train Epoch: 035 Batch: 00033/00094 | Loss: 281.9849 | CE: 0.1892 | KD: 1060.4968\n",
      "Train Epoch: 035 Batch: 00034/00094 | Loss: 282.0594 | CE: 0.1420 | KD: 1060.9548\n",
      "Train Epoch: 035 Batch: 00035/00094 | Loss: 281.9960 | CE: 0.1274 | KD: 1060.7712\n",
      "Train Epoch: 035 Batch: 00036/00094 | Loss: 281.9609 | CE: 0.1189 | KD: 1060.6710\n",
      "Train Epoch: 035 Batch: 00037/00094 | Loss: 282.0824 | CE: 0.1851 | KD: 1060.8789\n",
      "Train Epoch: 035 Batch: 00038/00094 | Loss: 282.0837 | CE: 0.1739 | KD: 1060.9263\n",
      "Train Epoch: 035 Batch: 00039/00094 | Loss: 282.0076 | CE: 0.1374 | KD: 1060.7771\n",
      "Train Epoch: 035 Batch: 00040/00094 | Loss: 281.9685 | CE: 0.2008 | KD: 1060.3912\n",
      "Train Epoch: 035 Batch: 00041/00094 | Loss: 281.9559 | CE: 0.1185 | KD: 1060.6539\n",
      "Train Epoch: 035 Batch: 00042/00094 | Loss: 282.0711 | CE: 0.1451 | KD: 1060.9871\n",
      "Train Epoch: 035 Batch: 00043/00094 | Loss: 282.0863 | CE: 0.1216 | KD: 1061.1328\n",
      "Train Epoch: 035 Batch: 00044/00094 | Loss: 282.0852 | CE: 0.1724 | KD: 1060.9374\n",
      "Train Epoch: 035 Batch: 00045/00094 | Loss: 282.0976 | CE: 0.1413 | KD: 1061.1012\n",
      "Train Epoch: 035 Batch: 00046/00094 | Loss: 282.0240 | CE: 0.1171 | KD: 1060.9152\n",
      "Train Epoch: 035 Batch: 00047/00094 | Loss: 282.0016 | CE: 0.0991 | KD: 1060.8984\n",
      "Train Epoch: 035 Batch: 00048/00094 | Loss: 281.9937 | CE: 0.1286 | KD: 1060.7577\n",
      "Train Epoch: 035 Batch: 00049/00094 | Loss: 282.0108 | CE: 0.1427 | KD: 1060.7693\n",
      "Train Epoch: 035 Batch: 00050/00094 | Loss: 281.9430 | CE: 0.1086 | KD: 1060.6422\n",
      "Train Epoch: 035 Batch: 00051/00094 | Loss: 282.0596 | CE: 0.1531 | KD: 1060.9138\n",
      "Train Epoch: 035 Batch: 00052/00094 | Loss: 281.9958 | CE: 0.1083 | KD: 1060.8420\n",
      "Train Epoch: 035 Batch: 00053/00094 | Loss: 282.0947 | CE: 0.1390 | KD: 1061.0988\n",
      "Train Epoch: 035 Batch: 00054/00094 | Loss: 281.9259 | CE: 0.1187 | KD: 1060.5403\n",
      "Train Epoch: 035 Batch: 00055/00094 | Loss: 281.9566 | CE: 0.1456 | KD: 1060.5543\n",
      "Train Epoch: 035 Batch: 00056/00094 | Loss: 282.2270 | CE: 0.3018 | KD: 1060.9841\n",
      "Train Epoch: 035 Batch: 00057/00094 | Loss: 281.9486 | CE: 0.1047 | KD: 1060.6780\n",
      "Train Epoch: 035 Batch: 00058/00094 | Loss: 282.1018 | CE: 0.1545 | KD: 1061.0673\n",
      "Train Epoch: 035 Batch: 00059/00094 | Loss: 282.0442 | CE: 0.1209 | KD: 1060.9769\n",
      "Train Epoch: 035 Batch: 00060/00094 | Loss: 282.1603 | CE: 0.2272 | KD: 1061.0138\n",
      "Train Epoch: 035 Batch: 00061/00094 | Loss: 281.9373 | CE: 0.1407 | KD: 1060.5002\n",
      "Train Epoch: 035 Batch: 00062/00094 | Loss: 282.0431 | CE: 0.1580 | KD: 1060.8331\n",
      "Train Epoch: 035 Batch: 00063/00094 | Loss: 282.0539 | CE: 0.1577 | KD: 1060.8749\n",
      "Train Epoch: 035 Batch: 00064/00094 | Loss: 282.0326 | CE: 0.1423 | KD: 1060.8529\n",
      "Train Epoch: 035 Batch: 00065/00094 | Loss: 281.9665 | CE: 0.1356 | KD: 1060.6290\n",
      "Train Epoch: 035 Batch: 00066/00094 | Loss: 282.1429 | CE: 0.1419 | KD: 1061.2692\n",
      "Train Epoch: 035 Batch: 00067/00094 | Loss: 282.0793 | CE: 0.1685 | KD: 1060.9301\n",
      "Train Epoch: 035 Batch: 00068/00094 | Loss: 282.0146 | CE: 0.1583 | KD: 1060.7249\n",
      "Train Epoch: 035 Batch: 00069/00094 | Loss: 282.0638 | CE: 0.1491 | KD: 1060.9447\n",
      "Train Epoch: 035 Batch: 00070/00094 | Loss: 282.0661 | CE: 0.1222 | KD: 1061.0544\n",
      "Train Epoch: 035 Batch: 00071/00094 | Loss: 282.0266 | CE: 0.1901 | KD: 1060.6505\n",
      "Train Epoch: 035 Batch: 00072/00094 | Loss: 282.1069 | CE: 0.2141 | KD: 1060.8622\n",
      "Train Epoch: 035 Batch: 00073/00094 | Loss: 282.1621 | CE: 0.2444 | KD: 1060.9559\n",
      "Train Epoch: 035 Batch: 00074/00094 | Loss: 281.9310 | CE: 0.1159 | KD: 1060.5697\n",
      "Train Epoch: 035 Batch: 00075/00094 | Loss: 282.0406 | CE: 0.1256 | KD: 1060.9454\n",
      "Train Epoch: 035 Batch: 00076/00094 | Loss: 282.0200 | CE: 0.1384 | KD: 1060.8197\n",
      "Train Epoch: 035 Batch: 00077/00094 | Loss: 281.9684 | CE: 0.1398 | KD: 1060.6204\n",
      "Train Epoch: 035 Batch: 00078/00094 | Loss: 281.9645 | CE: 0.1211 | KD: 1060.6761\n",
      "Train Epoch: 035 Batch: 00079/00094 | Loss: 282.0195 | CE: 0.1066 | KD: 1060.9376\n",
      "Train Epoch: 035 Batch: 00080/00094 | Loss: 281.9466 | CE: 0.1279 | KD: 1060.5834\n",
      "Train Epoch: 035 Batch: 00081/00094 | Loss: 281.9724 | CE: 0.1076 | KD: 1060.7566\n",
      "Train Epoch: 035 Batch: 00082/00094 | Loss: 282.0435 | CE: 0.1255 | KD: 1060.9568\n",
      "Train Epoch: 035 Batch: 00083/00094 | Loss: 282.0271 | CE: 0.1416 | KD: 1060.8345\n",
      "Train Epoch: 035 Batch: 00084/00094 | Loss: 282.0206 | CE: 0.1942 | KD: 1060.6124\n",
      "Train Epoch: 035 Batch: 00085/00094 | Loss: 282.1536 | CE: 0.1720 | KD: 1061.1964\n",
      "Train Epoch: 035 Batch: 00086/00094 | Loss: 281.9278 | CE: 0.1201 | KD: 1060.5417\n",
      "Train Epoch: 035 Batch: 00087/00094 | Loss: 282.0659 | CE: 0.1310 | KD: 1061.0205\n",
      "Train Epoch: 035 Batch: 00088/00094 | Loss: 282.1279 | CE: 0.1890 | KD: 1061.0359\n",
      "Train Epoch: 035 Batch: 00089/00094 | Loss: 282.1799 | CE: 0.1556 | KD: 1061.3568\n",
      "Train Epoch: 035 Batch: 00090/00094 | Loss: 281.9514 | CE: 0.1690 | KD: 1060.4463\n",
      "Train Epoch: 035 Batch: 00091/00094 | Loss: 281.9734 | CE: 0.1061 | KD: 1060.7660\n",
      "Train Epoch: 035 Batch: 00092/00094 | Loss: 282.0536 | CE: 0.1433 | KD: 1060.9279\n",
      "Train Epoch: 035 Batch: 00093/00094 | Loss: 281.9250 | CE: 0.1388 | KD: 1060.4612\n",
      "Train Epoch: 035 Batch: 00094/00094 | Loss: 282.1385 | CE: 0.1524 | KD: 1061.2135\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1436 | acc:97.2500\n",
      "[VAL Acc] Target: 97.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5812 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1205 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0612 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2682 | acc:53.8793\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5991 | acc:69.5009\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8781 | acc:64.9295\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.93%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7659 | acc:60.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.81%\n",
      "[VAL Acc] Avg 62.17%\n",
      "Train Epoch: 036 Batch: 00001/00094 | Loss: 281.9762 | CE: 0.1288 | KD: 1060.6913\n",
      "Train Epoch: 036 Batch: 00002/00094 | Loss: 282.0765 | CE: 0.1525 | KD: 1060.9796\n",
      "Train Epoch: 036 Batch: 00003/00094 | Loss: 282.0606 | CE: 0.1718 | KD: 1060.8469\n",
      "Train Epoch: 036 Batch: 00004/00094 | Loss: 282.1451 | CE: 0.2306 | KD: 1060.9440\n",
      "Train Epoch: 036 Batch: 00005/00094 | Loss: 282.1099 | CE: 0.2572 | KD: 1060.7112\n",
      "Train Epoch: 036 Batch: 00006/00094 | Loss: 282.0004 | CE: 0.1291 | KD: 1060.7811\n",
      "Train Epoch: 036 Batch: 00007/00094 | Loss: 281.9767 | CE: 0.1254 | KD: 1060.7059\n",
      "Train Epoch: 036 Batch: 00008/00094 | Loss: 281.9791 | CE: 0.1170 | KD: 1060.7466\n",
      "Train Epoch: 036 Batch: 00009/00094 | Loss: 282.1380 | CE: 0.1694 | KD: 1061.1475\n",
      "Train Epoch: 036 Batch: 00010/00094 | Loss: 282.1171 | CE: 0.2179 | KD: 1060.8861\n",
      "Train Epoch: 036 Batch: 00011/00094 | Loss: 282.1259 | CE: 0.1780 | KD: 1061.0695\n",
      "Train Epoch: 036 Batch: 00012/00094 | Loss: 281.9183 | CE: 0.1323 | KD: 1060.4601\n",
      "Train Epoch: 036 Batch: 00013/00094 | Loss: 281.9881 | CE: 0.1994 | KD: 1060.4706\n",
      "Train Epoch: 036 Batch: 00014/00094 | Loss: 281.9834 | CE: 0.1464 | KD: 1060.6521\n",
      "Train Epoch: 036 Batch: 00015/00094 | Loss: 281.9908 | CE: 0.1753 | KD: 1060.5713\n",
      "Train Epoch: 036 Batch: 00016/00094 | Loss: 282.1009 | CE: 0.1807 | KD: 1060.9651\n",
      "Train Epoch: 036 Batch: 00017/00094 | Loss: 282.1638 | CE: 0.1641 | KD: 1061.2644\n",
      "Train Epoch: 036 Batch: 00018/00094 | Loss: 281.8891 | CE: 0.1286 | KD: 1060.3644\n",
      "Train Epoch: 036 Batch: 00019/00094 | Loss: 282.0571 | CE: 0.1321 | KD: 1060.9833\n",
      "Train Epoch: 036 Batch: 00020/00094 | Loss: 282.0367 | CE: 0.1798 | KD: 1060.7272\n",
      "Train Epoch: 036 Batch: 00021/00094 | Loss: 282.0344 | CE: 0.1411 | KD: 1060.8638\n",
      "Train Epoch: 036 Batch: 00022/00094 | Loss: 282.0500 | CE: 0.1473 | KD: 1060.8995\n",
      "Train Epoch: 036 Batch: 00023/00094 | Loss: 282.1516 | CE: 0.1382 | KD: 1061.3159\n",
      "Train Epoch: 036 Batch: 00024/00094 | Loss: 282.1098 | CE: 0.2507 | KD: 1060.7352\n",
      "Train Epoch: 036 Batch: 00025/00094 | Loss: 281.9904 | CE: 0.1203 | KD: 1060.7767\n",
      "Train Epoch: 036 Batch: 00026/00094 | Loss: 282.1086 | CE: 0.2536 | KD: 1060.7197\n",
      "Train Epoch: 036 Batch: 00027/00094 | Loss: 282.0452 | CE: 0.1775 | KD: 1060.7678\n",
      "Train Epoch: 036 Batch: 00028/00094 | Loss: 282.0549 | CE: 0.1159 | KD: 1061.0360\n",
      "Train Epoch: 036 Batch: 00029/00094 | Loss: 282.0962 | CE: 0.1782 | KD: 1060.9572\n",
      "Train Epoch: 036 Batch: 00030/00094 | Loss: 282.0770 | CE: 0.1365 | KD: 1061.0416\n",
      "Train Epoch: 036 Batch: 00031/00094 | Loss: 281.9708 | CE: 0.1383 | KD: 1060.6351\n",
      "Train Epoch: 036 Batch: 00032/00094 | Loss: 281.9769 | CE: 0.1342 | KD: 1060.6738\n",
      "Train Epoch: 036 Batch: 00033/00094 | Loss: 281.9812 | CE: 0.1266 | KD: 1060.7184\n",
      "Train Epoch: 036 Batch: 00034/00094 | Loss: 282.0119 | CE: 0.1373 | KD: 1060.7935\n",
      "Train Epoch: 036 Batch: 00035/00094 | Loss: 282.0251 | CE: 0.2148 | KD: 1060.5520\n",
      "Train Epoch: 036 Batch: 00036/00094 | Loss: 282.0613 | CE: 0.1244 | KD: 1061.0283\n",
      "Train Epoch: 036 Batch: 00037/00094 | Loss: 281.9895 | CE: 0.1421 | KD: 1060.6914\n",
      "Train Epoch: 036 Batch: 00038/00094 | Loss: 282.0204 | CE: 0.1765 | KD: 1060.6781\n",
      "Train Epoch: 036 Batch: 00039/00094 | Loss: 282.0031 | CE: 0.1143 | KD: 1060.8474\n",
      "Train Epoch: 036 Batch: 00040/00094 | Loss: 281.9628 | CE: 0.1215 | KD: 1060.6682\n",
      "Train Epoch: 036 Batch: 00041/00094 | Loss: 281.9485 | CE: 0.1360 | KD: 1060.5603\n",
      "Train Epoch: 036 Batch: 00042/00094 | Loss: 281.9637 | CE: 0.0957 | KD: 1060.7686\n",
      "Train Epoch: 036 Batch: 00043/00094 | Loss: 281.9938 | CE: 0.1213 | KD: 1060.7856\n",
      "Train Epoch: 036 Batch: 00044/00094 | Loss: 281.9320 | CE: 0.1305 | KD: 1060.5186\n",
      "Train Epoch: 036 Batch: 00045/00094 | Loss: 282.1066 | CE: 0.1290 | KD: 1061.1813\n",
      "Train Epoch: 036 Batch: 00046/00094 | Loss: 282.0567 | CE: 0.1172 | KD: 1061.0381\n",
      "Train Epoch: 036 Batch: 00047/00094 | Loss: 281.9640 | CE: 0.1223 | KD: 1060.6696\n",
      "Train Epoch: 036 Batch: 00048/00094 | Loss: 282.0910 | CE: 0.1629 | KD: 1060.9952\n",
      "Train Epoch: 036 Batch: 00049/00094 | Loss: 282.0121 | CE: 0.1303 | KD: 1060.8209\n",
      "Train Epoch: 036 Batch: 00050/00094 | Loss: 281.9931 | CE: 0.1166 | KD: 1060.8010\n",
      "Train Epoch: 036 Batch: 00051/00094 | Loss: 281.9092 | CE: 0.0982 | KD: 1060.5544\n",
      "Train Epoch: 036 Batch: 00052/00094 | Loss: 282.0237 | CE: 0.1612 | KD: 1060.7480\n",
      "Train Epoch: 036 Batch: 00053/00094 | Loss: 282.0818 | CE: 0.1848 | KD: 1060.8781\n",
      "Train Epoch: 036 Batch: 00054/00094 | Loss: 282.0332 | CE: 0.1084 | KD: 1060.9824\n",
      "Train Epoch: 036 Batch: 00055/00094 | Loss: 281.9958 | CE: 0.1585 | KD: 1060.6534\n",
      "Train Epoch: 036 Batch: 00056/00094 | Loss: 281.9201 | CE: 0.1093 | KD: 1060.5536\n",
      "Train Epoch: 036 Batch: 00057/00094 | Loss: 282.0520 | CE: 0.2155 | KD: 1060.6505\n",
      "Train Epoch: 036 Batch: 00058/00094 | Loss: 282.0004 | CE: 0.0955 | KD: 1060.9076\n",
      "Train Epoch: 036 Batch: 00059/00094 | Loss: 282.1191 | CE: 0.1627 | KD: 1061.1012\n",
      "Train Epoch: 036 Batch: 00060/00094 | Loss: 282.0403 | CE: 0.1675 | KD: 1060.7866\n",
      "Train Epoch: 036 Batch: 00061/00094 | Loss: 282.0364 | CE: 0.1337 | KD: 1060.8994\n",
      "Train Epoch: 036 Batch: 00062/00094 | Loss: 281.9648 | CE: 0.1502 | KD: 1060.5677\n",
      "Train Epoch: 036 Batch: 00063/00094 | Loss: 282.0024 | CE: 0.1165 | KD: 1060.8364\n",
      "Train Epoch: 036 Batch: 00064/00094 | Loss: 281.9088 | CE: 0.1216 | KD: 1060.4651\n",
      "Train Epoch: 036 Batch: 00065/00094 | Loss: 282.1058 | CE: 0.1578 | KD: 1061.0701\n",
      "Train Epoch: 036 Batch: 00066/00094 | Loss: 282.0574 | CE: 0.1118 | KD: 1061.0605\n",
      "Train Epoch: 036 Batch: 00067/00094 | Loss: 282.0596 | CE: 0.1580 | KD: 1060.8954\n",
      "Train Epoch: 036 Batch: 00068/00094 | Loss: 282.2149 | CE: 0.2832 | KD: 1061.0085\n",
      "Train Epoch: 036 Batch: 00069/00094 | Loss: 282.1541 | CE: 0.1963 | KD: 1061.1064\n",
      "Train Epoch: 036 Batch: 00070/00094 | Loss: 282.0139 | CE: 0.1395 | KD: 1060.7932\n",
      "Train Epoch: 036 Batch: 00071/00094 | Loss: 282.0440 | CE: 0.1513 | KD: 1060.8616\n",
      "Train Epoch: 036 Batch: 00072/00094 | Loss: 282.1411 | CE: 0.1367 | KD: 1061.2821\n",
      "Train Epoch: 036 Batch: 00073/00094 | Loss: 281.9804 | CE: 0.1663 | KD: 1060.5662\n",
      "Train Epoch: 036 Batch: 00074/00094 | Loss: 282.0966 | CE: 0.1309 | KD: 1061.1366\n",
      "Train Epoch: 036 Batch: 00075/00094 | Loss: 282.2041 | CE: 0.2374 | KD: 1061.1401\n",
      "Train Epoch: 036 Batch: 00076/00094 | Loss: 281.9958 | CE: 0.1329 | KD: 1060.7496\n",
      "Train Epoch: 036 Batch: 00077/00094 | Loss: 282.1311 | CE: 0.1731 | KD: 1061.1073\n",
      "Train Epoch: 036 Batch: 00078/00094 | Loss: 282.1160 | CE: 0.1407 | KD: 1061.1725\n",
      "Train Epoch: 036 Batch: 00079/00094 | Loss: 282.0582 | CE: 0.1922 | KD: 1060.7614\n",
      "Train Epoch: 036 Batch: 00080/00094 | Loss: 282.0989 | CE: 0.2200 | KD: 1060.8099\n",
      "Train Epoch: 036 Batch: 00081/00094 | Loss: 281.9926 | CE: 0.1316 | KD: 1060.7427\n",
      "Train Epoch: 036 Batch: 00082/00094 | Loss: 282.0247 | CE: 0.1049 | KD: 1060.9640\n",
      "Train Epoch: 036 Batch: 00083/00094 | Loss: 282.0104 | CE: 0.1335 | KD: 1060.8024\n",
      "Train Epoch: 036 Batch: 00084/00094 | Loss: 282.0380 | CE: 0.1419 | KD: 1060.8744\n",
      "Train Epoch: 036 Batch: 00085/00094 | Loss: 282.1649 | CE: 0.2020 | KD: 1061.1261\n",
      "Train Epoch: 036 Batch: 00086/00094 | Loss: 282.0098 | CE: 0.1361 | KD: 1060.7902\n",
      "Train Epoch: 036 Batch: 00087/00094 | Loss: 282.1041 | CE: 0.2051 | KD: 1060.8855\n",
      "Train Epoch: 036 Batch: 00088/00094 | Loss: 282.0161 | CE: 0.1489 | KD: 1060.7655\n",
      "Train Epoch: 036 Batch: 00089/00094 | Loss: 281.9705 | CE: 0.1058 | KD: 1060.7562\n",
      "Train Epoch: 036 Batch: 00090/00094 | Loss: 282.0252 | CE: 0.1706 | KD: 1060.7181\n",
      "Train Epoch: 036 Batch: 00091/00094 | Loss: 282.0252 | CE: 0.1202 | KD: 1060.9082\n",
      "Train Epoch: 036 Batch: 00092/00094 | Loss: 282.0542 | CE: 0.1900 | KD: 1060.7542\n",
      "Train Epoch: 036 Batch: 00093/00094 | Loss: 282.1255 | CE: 0.2003 | KD: 1060.9841\n",
      "Train Epoch: 036 Batch: 00094/00094 | Loss: 282.1394 | CE: 0.2250 | KD: 1060.9434\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1416 | acc:97.4500\n",
      "[VAL Acc] Target: 97.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6260 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1082 | acc:55.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0754 | acc:48.2824\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3408 | acc:52.8997\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5333 | acc:74.3993\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9910 | acc:62.5392\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8038 | acc:61.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.81%\n",
      "[VAL Acc] Avg 62.85%\n",
      "Train Epoch: 037 Batch: 00001/00094 | Loss: 282.0133 | CE: 0.1918 | KD: 1060.5939\n",
      "Train Epoch: 037 Batch: 00002/00094 | Loss: 282.0706 | CE: 0.1355 | KD: 1061.0217\n",
      "Train Epoch: 037 Batch: 00003/00094 | Loss: 282.0470 | CE: 0.1655 | KD: 1060.8197\n",
      "Train Epoch: 037 Batch: 00004/00094 | Loss: 281.9691 | CE: 0.1469 | KD: 1060.5964\n",
      "Train Epoch: 037 Batch: 00005/00094 | Loss: 282.0627 | CE: 0.1515 | KD: 1060.9313\n",
      "Train Epoch: 037 Batch: 00006/00094 | Loss: 282.1207 | CE: 0.1369 | KD: 1061.2048\n",
      "Train Epoch: 037 Batch: 00007/00094 | Loss: 282.0227 | CE: 0.1495 | KD: 1060.7882\n",
      "Train Epoch: 037 Batch: 00008/00094 | Loss: 281.9966 | CE: 0.1489 | KD: 1060.6926\n",
      "Train Epoch: 037 Batch: 00009/00094 | Loss: 282.1511 | CE: 0.1673 | KD: 1061.2047\n",
      "Train Epoch: 037 Batch: 00010/00094 | Loss: 282.0439 | CE: 0.1412 | KD: 1060.8995\n",
      "Train Epoch: 037 Batch: 00011/00094 | Loss: 282.1841 | CE: 0.2081 | KD: 1061.1752\n",
      "Train Epoch: 037 Batch: 00012/00094 | Loss: 282.0677 | CE: 0.1101 | KD: 1061.1061\n",
      "Train Epoch: 037 Batch: 00013/00094 | Loss: 282.0107 | CE: 0.1563 | KD: 1060.7178\n",
      "Train Epoch: 037 Batch: 00014/00094 | Loss: 281.9631 | CE: 0.1302 | KD: 1060.6367\n",
      "Train Epoch: 037 Batch: 00015/00094 | Loss: 281.9204 | CE: 0.0951 | KD: 1060.6082\n",
      "Train Epoch: 037 Batch: 00016/00094 | Loss: 282.0700 | CE: 0.1514 | KD: 1060.9592\n",
      "Train Epoch: 037 Batch: 00017/00094 | Loss: 281.8658 | CE: 0.0992 | KD: 1060.3870\n",
      "Train Epoch: 037 Batch: 00018/00094 | Loss: 282.0805 | CE: 0.1153 | KD: 1061.1348\n",
      "Train Epoch: 037 Batch: 00019/00094 | Loss: 281.9846 | CE: 0.1925 | KD: 1060.4829\n",
      "Train Epoch: 037 Batch: 00020/00094 | Loss: 282.0348 | CE: 0.1254 | KD: 1060.9243\n",
      "Train Epoch: 037 Batch: 00021/00094 | Loss: 281.9601 | CE: 0.1251 | KD: 1060.6444\n",
      "Train Epoch: 037 Batch: 00022/00094 | Loss: 282.0125 | CE: 0.1751 | KD: 1060.6534\n",
      "Train Epoch: 037 Batch: 00023/00094 | Loss: 281.9675 | CE: 0.0967 | KD: 1060.7793\n",
      "Train Epoch: 037 Batch: 00024/00094 | Loss: 281.9569 | CE: 0.1478 | KD: 1060.5469\n",
      "Train Epoch: 037 Batch: 00025/00094 | Loss: 282.0900 | CE: 0.1839 | KD: 1060.9120\n",
      "Train Epoch: 037 Batch: 00026/00094 | Loss: 282.0121 | CE: 0.1167 | KD: 1060.8717\n",
      "Train Epoch: 037 Batch: 00027/00094 | Loss: 282.0440 | CE: 0.1957 | KD: 1060.6947\n",
      "Train Epoch: 037 Batch: 00028/00094 | Loss: 281.9793 | CE: 0.1416 | KD: 1060.6549\n",
      "Train Epoch: 037 Batch: 00029/00094 | Loss: 282.0333 | CE: 0.1074 | KD: 1060.9866\n",
      "Train Epoch: 037 Batch: 00030/00094 | Loss: 282.0722 | CE: 0.2090 | KD: 1060.7509\n",
      "Train Epoch: 037 Batch: 00031/00094 | Loss: 282.0937 | CE: 0.1365 | KD: 1061.1045\n",
      "Train Epoch: 037 Batch: 00032/00094 | Loss: 282.0839 | CE: 0.2064 | KD: 1060.8048\n",
      "Train Epoch: 037 Batch: 00033/00094 | Loss: 282.0044 | CE: 0.1361 | KD: 1060.7701\n",
      "Train Epoch: 037 Batch: 00034/00094 | Loss: 282.0587 | CE: 0.2152 | KD: 1060.6765\n",
      "Train Epoch: 037 Batch: 00035/00094 | Loss: 281.9470 | CE: 0.1246 | KD: 1060.5973\n",
      "Train Epoch: 037 Batch: 00036/00094 | Loss: 282.0349 | CE: 0.1870 | KD: 1060.6931\n",
      "Train Epoch: 037 Batch: 00037/00094 | Loss: 281.9821 | CE: 0.1231 | KD: 1060.7346\n",
      "Train Epoch: 037 Batch: 00038/00094 | Loss: 282.0972 | CE: 0.1927 | KD: 1060.9061\n",
      "Train Epoch: 037 Batch: 00039/00094 | Loss: 281.9963 | CE: 0.1554 | KD: 1060.6667\n",
      "Train Epoch: 037 Batch: 00040/00094 | Loss: 282.0395 | CE: 0.1977 | KD: 1060.6699\n",
      "Train Epoch: 037 Batch: 00041/00094 | Loss: 282.1759 | CE: 0.1455 | KD: 1061.3800\n",
      "Train Epoch: 037 Batch: 00042/00094 | Loss: 282.2033 | CE: 0.2627 | KD: 1061.0420\n",
      "Train Epoch: 037 Batch: 00043/00094 | Loss: 281.9771 | CE: 0.1220 | KD: 1060.7205\n",
      "Train Epoch: 037 Batch: 00044/00094 | Loss: 281.9714 | CE: 0.1321 | KD: 1060.6605\n",
      "Train Epoch: 037 Batch: 00045/00094 | Loss: 282.0583 | CE: 0.2021 | KD: 1060.7244\n",
      "Train Epoch: 037 Batch: 00046/00094 | Loss: 281.9893 | CE: 0.1661 | KD: 1060.6001\n",
      "Train Epoch: 037 Batch: 00047/00094 | Loss: 281.9543 | CE: 0.1293 | KD: 1060.6069\n",
      "Train Epoch: 037 Batch: 00048/00094 | Loss: 281.9918 | CE: 0.1952 | KD: 1060.5002\n",
      "Train Epoch: 037 Batch: 00049/00094 | Loss: 282.1060 | CE: 0.1599 | KD: 1061.0626\n",
      "Train Epoch: 037 Batch: 00050/00094 | Loss: 282.0897 | CE: 0.1615 | KD: 1060.9955\n",
      "Train Epoch: 037 Batch: 00051/00094 | Loss: 282.0675 | CE: 0.1527 | KD: 1060.9451\n",
      "Train Epoch: 037 Batch: 00052/00094 | Loss: 282.0270 | CE: 0.1376 | KD: 1060.8494\n",
      "Train Epoch: 037 Batch: 00053/00094 | Loss: 282.0327 | CE: 0.1174 | KD: 1060.9470\n",
      "Train Epoch: 037 Batch: 00054/00094 | Loss: 282.1008 | CE: 0.1781 | KD: 1060.9746\n",
      "Train Epoch: 037 Batch: 00055/00094 | Loss: 282.0574 | CE: 0.1241 | KD: 1061.0145\n",
      "Train Epoch: 037 Batch: 00056/00094 | Loss: 281.9340 | CE: 0.1105 | KD: 1060.6014\n",
      "Train Epoch: 037 Batch: 00057/00094 | Loss: 282.0355 | CE: 0.1675 | KD: 1060.7689\n",
      "Train Epoch: 037 Batch: 00058/00094 | Loss: 281.9761 | CE: 0.1047 | KD: 1060.7816\n",
      "Train Epoch: 037 Batch: 00059/00094 | Loss: 281.9725 | CE: 0.1031 | KD: 1060.7739\n",
      "Train Epoch: 037 Batch: 00060/00094 | Loss: 282.0971 | CE: 0.1809 | KD: 1060.9501\n",
      "Train Epoch: 037 Batch: 00061/00094 | Loss: 281.9886 | CE: 0.1811 | KD: 1060.5414\n",
      "Train Epoch: 037 Batch: 00062/00094 | Loss: 282.0536 | CE: 0.1291 | KD: 1060.9818\n",
      "Train Epoch: 037 Batch: 00063/00094 | Loss: 281.9906 | CE: 0.1552 | KD: 1060.6461\n",
      "Train Epoch: 037 Batch: 00064/00094 | Loss: 282.0835 | CE: 0.1705 | KD: 1060.9382\n",
      "Train Epoch: 037 Batch: 00065/00094 | Loss: 281.9897 | CE: 0.1397 | KD: 1060.7010\n",
      "Train Epoch: 037 Batch: 00066/00094 | Loss: 282.0014 | CE: 0.1750 | KD: 1060.6119\n",
      "Train Epoch: 037 Batch: 00067/00094 | Loss: 282.1057 | CE: 0.1489 | KD: 1061.1030\n",
      "Train Epoch: 037 Batch: 00068/00094 | Loss: 281.9733 | CE: 0.1270 | KD: 1060.6869\n",
      "Train Epoch: 037 Batch: 00069/00094 | Loss: 281.9187 | CE: 0.1650 | KD: 1060.3385\n",
      "Train Epoch: 037 Batch: 00070/00094 | Loss: 282.1723 | CE: 0.1725 | KD: 1061.2651\n",
      "Train Epoch: 037 Batch: 00071/00094 | Loss: 282.0723 | CE: 0.1940 | KD: 1060.8075\n",
      "Train Epoch: 037 Batch: 00072/00094 | Loss: 281.9561 | CE: 0.1188 | KD: 1060.6533\n",
      "Train Epoch: 037 Batch: 00073/00094 | Loss: 282.0291 | CE: 0.1115 | KD: 1060.9552\n",
      "Train Epoch: 037 Batch: 00074/00094 | Loss: 281.9652 | CE: 0.1539 | KD: 1060.5551\n",
      "Train Epoch: 037 Batch: 00075/00094 | Loss: 282.0517 | CE: 0.1459 | KD: 1060.9109\n",
      "Train Epoch: 037 Batch: 00076/00094 | Loss: 281.9895 | CE: 0.1331 | KD: 1060.7252\n",
      "Train Epoch: 037 Batch: 00077/00094 | Loss: 282.0474 | CE: 0.2053 | KD: 1060.6713\n",
      "Train Epoch: 037 Batch: 00078/00094 | Loss: 282.0292 | CE: 0.1145 | KD: 1060.9445\n",
      "Train Epoch: 037 Batch: 00079/00094 | Loss: 282.2485 | CE: 0.2473 | KD: 1061.2699\n",
      "Train Epoch: 037 Batch: 00080/00094 | Loss: 282.0222 | CE: 0.1276 | KD: 1060.8690\n",
      "Train Epoch: 037 Batch: 00081/00094 | Loss: 282.1428 | CE: 0.1559 | KD: 1061.2163\n",
      "Train Epoch: 037 Batch: 00082/00094 | Loss: 282.1728 | CE: 0.2111 | KD: 1061.1213\n",
      "Train Epoch: 037 Batch: 00083/00094 | Loss: 281.9358 | CE: 0.1259 | KD: 1060.5500\n",
      "Train Epoch: 037 Batch: 00084/00094 | Loss: 281.9134 | CE: 0.1393 | KD: 1060.4155\n",
      "Train Epoch: 037 Batch: 00085/00094 | Loss: 281.9895 | CE: 0.1137 | KD: 1060.7981\n",
      "Train Epoch: 037 Batch: 00086/00094 | Loss: 282.0852 | CE: 0.2570 | KD: 1060.6193\n",
      "Train Epoch: 037 Batch: 00087/00094 | Loss: 282.2040 | CE: 0.1962 | KD: 1061.2947\n",
      "Train Epoch: 037 Batch: 00088/00094 | Loss: 282.0309 | CE: 0.1412 | KD: 1060.8506\n",
      "Train Epoch: 037 Batch: 00089/00094 | Loss: 281.8995 | CE: 0.0924 | KD: 1060.5394\n",
      "Train Epoch: 037 Batch: 00090/00094 | Loss: 282.0772 | CE: 0.1619 | KD: 1060.9467\n",
      "Train Epoch: 037 Batch: 00091/00094 | Loss: 282.1091 | CE: 0.1874 | KD: 1060.9709\n",
      "Train Epoch: 037 Batch: 00092/00094 | Loss: 281.9273 | CE: 0.1249 | KD: 1060.5220\n",
      "Train Epoch: 037 Batch: 00093/00094 | Loss: 281.9634 | CE: 0.1462 | KD: 1060.5778\n",
      "Train Epoch: 037 Batch: 00094/00094 | Loss: 282.1618 | CE: 0.1678 | KD: 1061.2428\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1364 | acc:97.8000\n",
      "[VAL Acc] Target: 97.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6331 | acc:50.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1256 | acc:55.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0525 | acc:45.8015\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3418 | acc:53.4875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6341 | acc:66.7283\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.73%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9393 | acc:63.5580\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7669 | acc:61.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.25%\n",
      "[VAL Acc] Avg 61.75%\n",
      "Train Epoch: 038 Batch: 00001/00094 | Loss: 253.7994 | CE: 0.1030 | KD: 1060.8325\n",
      "Train Epoch: 038 Batch: 00002/00094 | Loss: 253.8042 | CE: 0.1316 | KD: 1060.7325\n",
      "Train Epoch: 038 Batch: 00003/00094 | Loss: 253.8677 | CE: 0.1860 | KD: 1060.7708\n",
      "Train Epoch: 038 Batch: 00004/00094 | Loss: 253.9097 | CE: 0.2367 | KD: 1060.7344\n",
      "Train Epoch: 038 Batch: 00005/00094 | Loss: 253.9244 | CE: 0.1757 | KD: 1061.0513\n",
      "Train Epoch: 038 Batch: 00006/00094 | Loss: 253.7344 | CE: 0.1435 | KD: 1060.3910\n",
      "Train Epoch: 038 Batch: 00007/00094 | Loss: 253.8627 | CE: 0.1663 | KD: 1060.8323\n",
      "Train Epoch: 038 Batch: 00008/00094 | Loss: 253.8865 | CE: 0.1751 | KD: 1060.8950\n",
      "Train Epoch: 038 Batch: 00009/00094 | Loss: 253.8740 | CE: 0.1610 | KD: 1060.9017\n",
      "Train Epoch: 038 Batch: 00010/00094 | Loss: 253.8368 | CE: 0.1605 | KD: 1060.7485\n",
      "Train Epoch: 038 Batch: 00011/00094 | Loss: 253.8656 | CE: 0.1394 | KD: 1060.9572\n",
      "Train Epoch: 038 Batch: 00012/00094 | Loss: 253.9457 | CE: 0.2019 | KD: 1061.0306\n",
      "Train Epoch: 038 Batch: 00013/00094 | Loss: 253.7365 | CE: 0.1144 | KD: 1060.5220\n",
      "Train Epoch: 038 Batch: 00014/00094 | Loss: 253.8566 | CE: 0.1307 | KD: 1060.9554\n",
      "Train Epoch: 038 Batch: 00015/00094 | Loss: 253.8496 | CE: 0.1473 | KD: 1060.8567\n",
      "Train Epoch: 038 Batch: 00016/00094 | Loss: 253.9343 | CE: 0.1605 | KD: 1061.1559\n",
      "Train Epoch: 038 Batch: 00017/00094 | Loss: 253.8690 | CE: 0.1601 | KD: 1060.8846\n",
      "Train Epoch: 038 Batch: 00018/00094 | Loss: 253.8862 | CE: 0.1531 | KD: 1060.9857\n",
      "Train Epoch: 038 Batch: 00019/00094 | Loss: 253.8013 | CE: 0.1114 | KD: 1060.8052\n",
      "Train Epoch: 038 Batch: 00020/00094 | Loss: 253.7754 | CE: 0.1192 | KD: 1060.6642\n",
      "Train Epoch: 038 Batch: 00021/00094 | Loss: 253.8273 | CE: 0.1355 | KD: 1060.8129\n",
      "Train Epoch: 038 Batch: 00022/00094 | Loss: 253.9225 | CE: 0.1493 | KD: 1061.1537\n",
      "Train Epoch: 038 Batch: 00023/00094 | Loss: 253.9419 | CE: 0.2390 | KD: 1060.8595\n",
      "Train Epoch: 038 Batch: 00024/00094 | Loss: 253.8146 | CE: 0.1271 | KD: 1060.7952\n",
      "Train Epoch: 038 Batch: 00025/00094 | Loss: 253.8724 | CE: 0.1804 | KD: 1060.8138\n",
      "Train Epoch: 038 Batch: 00026/00094 | Loss: 253.8155 | CE: 0.1386 | KD: 1060.7506\n",
      "Train Epoch: 038 Batch: 00027/00094 | Loss: 253.7505 | CE: 0.1237 | KD: 1060.5414\n",
      "Train Epoch: 038 Batch: 00028/00094 | Loss: 253.9659 | CE: 0.1488 | KD: 1061.3373\n",
      "Train Epoch: 038 Batch: 00029/00094 | Loss: 253.8261 | CE: 0.1113 | KD: 1060.9094\n",
      "Train Epoch: 038 Batch: 00030/00094 | Loss: 253.7841 | CE: 0.1304 | KD: 1060.6538\n",
      "Train Epoch: 038 Batch: 00031/00094 | Loss: 253.8382 | CE: 0.1481 | KD: 1060.8060\n",
      "Train Epoch: 038 Batch: 00032/00094 | Loss: 253.9645 | CE: 0.1918 | KD: 1061.1512\n",
      "Train Epoch: 038 Batch: 00033/00094 | Loss: 253.9127 | CE: 0.1432 | KD: 1061.1381\n",
      "Train Epoch: 038 Batch: 00034/00094 | Loss: 253.8504 | CE: 0.1333 | KD: 1060.9187\n",
      "Train Epoch: 038 Batch: 00035/00094 | Loss: 253.8499 | CE: 0.1286 | KD: 1060.9364\n",
      "Train Epoch: 038 Batch: 00036/00094 | Loss: 253.7467 | CE: 0.1025 | KD: 1060.6136\n",
      "Train Epoch: 038 Batch: 00037/00094 | Loss: 253.8568 | CE: 0.1401 | KD: 1060.9171\n",
      "Train Epoch: 038 Batch: 00038/00094 | Loss: 253.8030 | CE: 0.1401 | KD: 1060.6921\n",
      "Train Epoch: 038 Batch: 00039/00094 | Loss: 253.7562 | CE: 0.1371 | KD: 1060.5093\n",
      "Train Epoch: 038 Batch: 00040/00094 | Loss: 253.7630 | CE: 0.1254 | KD: 1060.5865\n",
      "Train Epoch: 038 Batch: 00041/00094 | Loss: 253.8284 | CE: 0.1786 | KD: 1060.6372\n",
      "Train Epoch: 038 Batch: 00042/00094 | Loss: 253.7773 | CE: 0.1344 | KD: 1060.6083\n",
      "Train Epoch: 038 Batch: 00043/00094 | Loss: 253.7799 | CE: 0.1060 | KD: 1060.7380\n",
      "Train Epoch: 038 Batch: 00044/00094 | Loss: 253.7403 | CE: 0.1215 | KD: 1060.5081\n",
      "Train Epoch: 038 Batch: 00045/00094 | Loss: 253.7366 | CE: 0.1176 | KD: 1060.5085\n",
      "Train Epoch: 038 Batch: 00046/00094 | Loss: 253.8573 | CE: 0.1415 | KD: 1060.9133\n",
      "Train Epoch: 038 Batch: 00047/00094 | Loss: 253.8573 | CE: 0.1543 | KD: 1060.8596\n",
      "Train Epoch: 038 Batch: 00048/00094 | Loss: 253.7596 | CE: 0.1461 | KD: 1060.4855\n",
      "Train Epoch: 038 Batch: 00049/00094 | Loss: 253.7845 | CE: 0.1376 | KD: 1060.6256\n",
      "Train Epoch: 038 Batch: 00050/00094 | Loss: 253.8513 | CE: 0.1178 | KD: 1060.9874\n",
      "Train Epoch: 038 Batch: 00051/00094 | Loss: 253.8284 | CE: 0.1404 | KD: 1060.7970\n",
      "Train Epoch: 038 Batch: 00052/00094 | Loss: 253.8226 | CE: 0.1346 | KD: 1060.7971\n",
      "Train Epoch: 038 Batch: 00053/00094 | Loss: 253.8677 | CE: 0.1383 | KD: 1060.9701\n",
      "Train Epoch: 038 Batch: 00054/00094 | Loss: 253.7320 | CE: 0.1167 | KD: 1060.4932\n",
      "Train Epoch: 038 Batch: 00055/00094 | Loss: 253.9405 | CE: 0.1758 | KD: 1061.1178\n",
      "Train Epoch: 038 Batch: 00056/00094 | Loss: 253.7882 | CE: 0.1256 | KD: 1060.6909\n",
      "Train Epoch: 038 Batch: 00057/00094 | Loss: 253.9386 | CE: 0.1606 | KD: 1061.1735\n",
      "Train Epoch: 038 Batch: 00058/00094 | Loss: 253.7836 | CE: 0.1345 | KD: 1060.6344\n",
      "Train Epoch: 038 Batch: 00059/00094 | Loss: 253.7604 | CE: 0.1087 | KD: 1060.6456\n",
      "Train Epoch: 038 Batch: 00060/00094 | Loss: 253.9812 | CE: 0.2846 | KD: 1060.8330\n",
      "Train Epoch: 038 Batch: 00061/00094 | Loss: 253.8443 | CE: 0.1442 | KD: 1060.8479\n",
      "Train Epoch: 038 Batch: 00062/00094 | Loss: 253.8826 | CE: 0.1574 | KD: 1060.9525\n",
      "Train Epoch: 038 Batch: 00063/00094 | Loss: 253.8024 | CE: 0.1029 | KD: 1060.8450\n",
      "Train Epoch: 038 Batch: 00064/00094 | Loss: 253.8462 | CE: 0.1274 | KD: 1060.9260\n",
      "Train Epoch: 038 Batch: 00065/00094 | Loss: 253.8651 | CE: 0.1356 | KD: 1060.9711\n",
      "Train Epoch: 038 Batch: 00066/00094 | Loss: 253.7849 | CE: 0.1080 | KD: 1060.7509\n",
      "Train Epoch: 038 Batch: 00067/00094 | Loss: 253.9314 | CE: 0.1820 | KD: 1061.0538\n",
      "Train Epoch: 038 Batch: 00068/00094 | Loss: 253.9196 | CE: 0.1927 | KD: 1060.9596\n",
      "Train Epoch: 038 Batch: 00069/00094 | Loss: 253.8130 | CE: 0.1517 | KD: 1060.6857\n",
      "Train Epoch: 038 Batch: 00070/00094 | Loss: 253.9132 | CE: 0.1364 | KD: 1061.1683\n",
      "Train Epoch: 038 Batch: 00071/00094 | Loss: 253.7974 | CE: 0.1119 | KD: 1060.7865\n",
      "Train Epoch: 038 Batch: 00072/00094 | Loss: 253.8302 | CE: 0.1166 | KD: 1060.9039\n",
      "Train Epoch: 038 Batch: 00073/00094 | Loss: 253.9310 | CE: 0.2424 | KD: 1060.7997\n",
      "Train Epoch: 038 Batch: 00074/00094 | Loss: 253.8475 | CE: 0.1786 | KD: 1060.7173\n",
      "Train Epoch: 038 Batch: 00075/00094 | Loss: 253.8014 | CE: 0.0997 | KD: 1060.8545\n",
      "Train Epoch: 038 Batch: 00076/00094 | Loss: 253.8661 | CE: 0.1506 | KD: 1060.9122\n",
      "Train Epoch: 038 Batch: 00077/00094 | Loss: 253.7789 | CE: 0.1403 | KD: 1060.5908\n",
      "Train Epoch: 038 Batch: 00078/00094 | Loss: 253.8177 | CE: 0.1315 | KD: 1060.7898\n",
      "Train Epoch: 038 Batch: 00079/00094 | Loss: 253.7917 | CE: 0.1209 | KD: 1060.7253\n",
      "Train Epoch: 038 Batch: 00080/00094 | Loss: 253.9001 | CE: 0.1644 | KD: 1060.9967\n",
      "Train Epoch: 038 Batch: 00081/00094 | Loss: 253.8358 | CE: 0.1524 | KD: 1060.7777\n",
      "Train Epoch: 038 Batch: 00082/00094 | Loss: 253.8115 | CE: 0.1802 | KD: 1060.5602\n",
      "Train Epoch: 038 Batch: 00083/00094 | Loss: 253.9011 | CE: 0.1390 | KD: 1061.1068\n",
      "Train Epoch: 038 Batch: 00084/00094 | Loss: 253.7666 | CE: 0.1050 | KD: 1060.6866\n",
      "Train Epoch: 038 Batch: 00085/00094 | Loss: 253.8566 | CE: 0.1434 | KD: 1060.9027\n",
      "Train Epoch: 038 Batch: 00086/00094 | Loss: 253.7942 | CE: 0.0897 | KD: 1060.8658\n",
      "Train Epoch: 038 Batch: 00087/00094 | Loss: 253.9017 | CE: 0.1299 | KD: 1061.1477\n",
      "Train Epoch: 038 Batch: 00088/00094 | Loss: 253.8434 | CE: 0.1279 | KD: 1060.9119\n",
      "Train Epoch: 038 Batch: 00089/00094 | Loss: 253.9345 | CE: 0.1187 | KD: 1061.3317\n",
      "Train Epoch: 038 Batch: 00090/00094 | Loss: 253.8072 | CE: 0.0869 | KD: 1060.9319\n",
      "Train Epoch: 038 Batch: 00091/00094 | Loss: 253.8290 | CE: 0.1556 | KD: 1060.7361\n",
      "Train Epoch: 038 Batch: 00092/00094 | Loss: 253.7901 | CE: 0.1275 | KD: 1060.6912\n",
      "Train Epoch: 038 Batch: 00093/00094 | Loss: 253.8401 | CE: 0.1418 | KD: 1060.8401\n",
      "Train Epoch: 038 Batch: 00094/00094 | Loss: 253.8364 | CE: 0.1684 | KD: 1060.7139\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1321 | acc:97.8500\n",
      "[VAL Acc] Target: 97.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7557 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1917 | acc:53.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1167 | acc:46.5649\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5307 | acc:52.0376\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5432 | acc:73.3826\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0923 | acc:61.3245\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.32%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8106 | acc:61.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.38%\n",
      "[VAL Acc] Avg 62.05%\n",
      "Train Epoch: 039 Batch: 00001/00094 | Loss: 253.8545 | CE: 0.1119 | KD: 1061.0255\n",
      "Train Epoch: 039 Batch: 00002/00094 | Loss: 253.8713 | CE: 0.1402 | KD: 1060.9774\n",
      "Train Epoch: 039 Batch: 00003/00094 | Loss: 253.8550 | CE: 0.1711 | KD: 1060.7799\n",
      "Train Epoch: 039 Batch: 00004/00094 | Loss: 253.7371 | CE: 0.1063 | KD: 1060.5580\n",
      "Train Epoch: 039 Batch: 00005/00094 | Loss: 253.7773 | CE: 0.0934 | KD: 1060.7799\n",
      "Train Epoch: 039 Batch: 00006/00094 | Loss: 253.8836 | CE: 0.1870 | KD: 1060.8333\n",
      "Train Epoch: 039 Batch: 00007/00094 | Loss: 253.8642 | CE: 0.1992 | KD: 1060.7009\n",
      "Train Epoch: 039 Batch: 00008/00094 | Loss: 253.7929 | CE: 0.1042 | KD: 1060.7999\n",
      "Train Epoch: 039 Batch: 00009/00094 | Loss: 253.8663 | CE: 0.1334 | KD: 1060.9849\n",
      "Train Epoch: 039 Batch: 00010/00094 | Loss: 253.9145 | CE: 0.1742 | KD: 1061.0156\n",
      "Train Epoch: 039 Batch: 00011/00094 | Loss: 253.7994 | CE: 0.1423 | KD: 1060.6677\n",
      "Train Epoch: 039 Batch: 00012/00094 | Loss: 253.9828 | CE: 0.1676 | KD: 1061.3291\n",
      "Train Epoch: 039 Batch: 00013/00094 | Loss: 253.9031 | CE: 0.2000 | KD: 1060.8602\n",
      "Train Epoch: 039 Batch: 00014/00094 | Loss: 253.8025 | CE: 0.1239 | KD: 1060.7579\n",
      "Train Epoch: 039 Batch: 00015/00094 | Loss: 253.8087 | CE: 0.1431 | KD: 1060.7034\n",
      "Train Epoch: 039 Batch: 00016/00094 | Loss: 253.7810 | CE: 0.1466 | KD: 1060.5730\n",
      "Train Epoch: 039 Batch: 00017/00094 | Loss: 253.8067 | CE: 0.0992 | KD: 1060.8789\n",
      "Train Epoch: 039 Batch: 00018/00094 | Loss: 253.8946 | CE: 0.1809 | KD: 1060.9044\n",
      "Train Epoch: 039 Batch: 00019/00094 | Loss: 253.7159 | CE: 0.0816 | KD: 1060.5728\n",
      "Train Epoch: 039 Batch: 00020/00094 | Loss: 253.8040 | CE: 0.1330 | KD: 1060.7258\n",
      "Train Epoch: 039 Batch: 00021/00094 | Loss: 253.7594 | CE: 0.1262 | KD: 1060.5679\n",
      "Train Epoch: 039 Batch: 00022/00094 | Loss: 253.7457 | CE: 0.1124 | KD: 1060.5688\n",
      "Train Epoch: 039 Batch: 00023/00094 | Loss: 253.8113 | CE: 0.1651 | KD: 1060.6226\n",
      "Train Epoch: 039 Batch: 00024/00094 | Loss: 253.7984 | CE: 0.1255 | KD: 1060.7339\n",
      "Train Epoch: 039 Batch: 00025/00094 | Loss: 253.7831 | CE: 0.1454 | KD: 1060.5870\n",
      "Train Epoch: 039 Batch: 00026/00094 | Loss: 253.8652 | CE: 0.1355 | KD: 1060.9716\n",
      "Train Epoch: 039 Batch: 00027/00094 | Loss: 253.8707 | CE: 0.1604 | KD: 1060.8905\n",
      "Train Epoch: 039 Batch: 00028/00094 | Loss: 253.8481 | CE: 0.1163 | KD: 1060.9802\n",
      "Train Epoch: 039 Batch: 00029/00094 | Loss: 253.7824 | CE: 0.1399 | KD: 1060.6066\n",
      "Train Epoch: 039 Batch: 00030/00094 | Loss: 253.7992 | CE: 0.1246 | KD: 1060.7411\n",
      "Train Epoch: 039 Batch: 00031/00094 | Loss: 253.7442 | CE: 0.1146 | KD: 1060.5531\n",
      "Train Epoch: 039 Batch: 00032/00094 | Loss: 253.7726 | CE: 0.1103 | KD: 1060.6896\n",
      "Train Epoch: 039 Batch: 00033/00094 | Loss: 253.9600 | CE: 0.1952 | KD: 1061.1183\n",
      "Train Epoch: 039 Batch: 00034/00094 | Loss: 253.8029 | CE: 0.1613 | KD: 1060.6031\n",
      "Train Epoch: 039 Batch: 00035/00094 | Loss: 253.7584 | CE: 0.1563 | KD: 1060.4380\n",
      "Train Epoch: 039 Batch: 00036/00094 | Loss: 253.8804 | CE: 0.1407 | KD: 1061.0134\n",
      "Train Epoch: 039 Batch: 00037/00094 | Loss: 253.8659 | CE: 0.1968 | KD: 1060.7184\n",
      "Train Epoch: 039 Batch: 00038/00094 | Loss: 253.8524 | CE: 0.1358 | KD: 1060.9167\n",
      "Train Epoch: 039 Batch: 00039/00094 | Loss: 253.9594 | CE: 0.1700 | KD: 1061.2212\n",
      "Train Epoch: 039 Batch: 00040/00094 | Loss: 253.8618 | CE: 0.1486 | KD: 1060.9025\n",
      "Train Epoch: 039 Batch: 00041/00094 | Loss: 253.7964 | CE: 0.1149 | KD: 1060.7701\n",
      "Train Epoch: 039 Batch: 00042/00094 | Loss: 253.8901 | CE: 0.1771 | KD: 1060.9016\n",
      "Train Epoch: 039 Batch: 00043/00094 | Loss: 253.8750 | CE: 0.1199 | KD: 1061.0778\n",
      "Train Epoch: 039 Batch: 00044/00094 | Loss: 253.9460 | CE: 0.1335 | KD: 1061.3180\n",
      "Train Epoch: 039 Batch: 00045/00094 | Loss: 253.9393 | CE: 0.1721 | KD: 1061.1281\n",
      "Train Epoch: 039 Batch: 00046/00094 | Loss: 253.8540 | CE: 0.1268 | KD: 1060.9613\n",
      "Train Epoch: 039 Batch: 00047/00094 | Loss: 253.8585 | CE: 0.1322 | KD: 1060.9574\n",
      "Train Epoch: 039 Batch: 00048/00094 | Loss: 253.8241 | CE: 0.1668 | KD: 1060.6689\n",
      "Train Epoch: 039 Batch: 00049/00094 | Loss: 253.7838 | CE: 0.1133 | KD: 1060.7240\n",
      "Train Epoch: 039 Batch: 00050/00094 | Loss: 253.7333 | CE: 0.1065 | KD: 1060.5415\n",
      "Train Epoch: 039 Batch: 00051/00094 | Loss: 253.9478 | CE: 0.2114 | KD: 1060.9995\n",
      "Train Epoch: 039 Batch: 00052/00094 | Loss: 253.7719 | CE: 0.1002 | KD: 1060.7286\n",
      "Train Epoch: 039 Batch: 00053/00094 | Loss: 253.7196 | CE: 0.0913 | KD: 1060.5471\n",
      "Train Epoch: 039 Batch: 00054/00094 | Loss: 253.8180 | CE: 0.1591 | KD: 1060.6757\n",
      "Train Epoch: 039 Batch: 00055/00094 | Loss: 253.8594 | CE: 0.1304 | KD: 1060.9683\n",
      "Train Epoch: 039 Batch: 00056/00094 | Loss: 253.9013 | CE: 0.1541 | KD: 1061.0449\n",
      "Train Epoch: 039 Batch: 00057/00094 | Loss: 253.8640 | CE: 0.1423 | KD: 1060.9381\n",
      "Train Epoch: 039 Batch: 00058/00094 | Loss: 253.8413 | CE: 0.1161 | KD: 1060.9529\n",
      "Train Epoch: 039 Batch: 00059/00094 | Loss: 253.8976 | CE: 0.1606 | KD: 1061.0022\n",
      "Train Epoch: 039 Batch: 00060/00094 | Loss: 253.7631 | CE: 0.0980 | KD: 1060.7015\n",
      "Train Epoch: 039 Batch: 00061/00094 | Loss: 253.8049 | CE: 0.1383 | KD: 1060.7078\n",
      "Train Epoch: 039 Batch: 00062/00094 | Loss: 253.9021 | CE: 0.2091 | KD: 1060.8180\n",
      "Train Epoch: 039 Batch: 00063/00094 | Loss: 253.9612 | CE: 0.1159 | KD: 1061.4547\n",
      "Train Epoch: 039 Batch: 00064/00094 | Loss: 253.7825 | CE: 0.1553 | KD: 1060.5432\n",
      "Train Epoch: 039 Batch: 00065/00094 | Loss: 253.7518 | CE: 0.0986 | KD: 1060.6516\n",
      "Train Epoch: 039 Batch: 00066/00094 | Loss: 253.8692 | CE: 0.1099 | KD: 1061.0951\n",
      "Train Epoch: 039 Batch: 00067/00094 | Loss: 253.7738 | CE: 0.1110 | KD: 1060.6921\n",
      "Train Epoch: 039 Batch: 00068/00094 | Loss: 253.7731 | CE: 0.0969 | KD: 1060.7480\n",
      "Train Epoch: 039 Batch: 00069/00094 | Loss: 253.9569 | CE: 0.1723 | KD: 1061.2009\n",
      "Train Epoch: 039 Batch: 00070/00094 | Loss: 253.8595 | CE: 0.1063 | KD: 1061.0699\n",
      "Train Epoch: 039 Batch: 00071/00094 | Loss: 253.8813 | CE: 0.1306 | KD: 1061.0594\n",
      "Train Epoch: 039 Batch: 00072/00094 | Loss: 253.8172 | CE: 0.1055 | KD: 1060.8962\n",
      "Train Epoch: 039 Batch: 00073/00094 | Loss: 253.7907 | CE: 0.1270 | KD: 1060.6956\n",
      "Train Epoch: 039 Batch: 00074/00094 | Loss: 253.7429 | CE: 0.0904 | KD: 1060.6490\n",
      "Train Epoch: 039 Batch: 00075/00094 | Loss: 254.0609 | CE: 0.2411 | KD: 1061.3484\n",
      "Train Epoch: 039 Batch: 00076/00094 | Loss: 253.7461 | CE: 0.0951 | KD: 1060.6427\n",
      "Train Epoch: 039 Batch: 00077/00094 | Loss: 253.7931 | CE: 0.1375 | KD: 1060.6617\n",
      "Train Epoch: 039 Batch: 00078/00094 | Loss: 253.8693 | CE: 0.1414 | KD: 1060.9641\n",
      "Train Epoch: 039 Batch: 00079/00094 | Loss: 253.9213 | CE: 0.1213 | KD: 1061.2655\n",
      "Train Epoch: 039 Batch: 00080/00094 | Loss: 253.8957 | CE: 0.1725 | KD: 1060.9443\n",
      "Train Epoch: 039 Batch: 00081/00094 | Loss: 253.9049 | CE: 0.1579 | KD: 1061.0438\n",
      "Train Epoch: 039 Batch: 00082/00094 | Loss: 253.9396 | CE: 0.1659 | KD: 1061.1556\n",
      "Train Epoch: 039 Batch: 00083/00094 | Loss: 253.8828 | CE: 0.1621 | KD: 1060.9338\n",
      "Train Epoch: 039 Batch: 00084/00094 | Loss: 253.9073 | CE: 0.1418 | KD: 1061.1211\n",
      "Train Epoch: 039 Batch: 00085/00094 | Loss: 253.7166 | CE: 0.1143 | KD: 1060.4390\n",
      "Train Epoch: 039 Batch: 00086/00094 | Loss: 253.7959 | CE: 0.0962 | KD: 1060.8462\n",
      "Train Epoch: 039 Batch: 00087/00094 | Loss: 253.7410 | CE: 0.0960 | KD: 1060.6173\n",
      "Train Epoch: 039 Batch: 00088/00094 | Loss: 253.9681 | CE: 0.1733 | KD: 1061.2437\n",
      "Train Epoch: 039 Batch: 00089/00094 | Loss: 253.8664 | CE: 0.1215 | KD: 1061.0352\n",
      "Train Epoch: 039 Batch: 00090/00094 | Loss: 253.7477 | CE: 0.0981 | KD: 1060.6367\n",
      "Train Epoch: 039 Batch: 00091/00094 | Loss: 253.7630 | CE: 0.1343 | KD: 1060.5490\n",
      "Train Epoch: 039 Batch: 00092/00094 | Loss: 253.8608 | CE: 0.1554 | KD: 1060.8701\n",
      "Train Epoch: 039 Batch: 00093/00094 | Loss: 253.7873 | CE: 0.1114 | KD: 1060.7466\n",
      "Train Epoch: 039 Batch: 00094/00094 | Loss: 253.9100 | CE: 0.2014 | KD: 1060.8834\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1385 | acc:97.3500\n",
      "[VAL Acc] Target: 97.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8490 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2249 | acc:53.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2068 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4983 | acc:53.0172\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6999 | acc:64.7874\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 64.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0968 | acc:61.5987\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8154 | acc:59.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.62%\n",
      "[VAL Acc] Avg 60.66%\n",
      "Train Epoch: 040 Batch: 00001/00094 | Loss: 253.8402 | CE: 0.1904 | KD: 1060.6375\n",
      "Train Epoch: 040 Batch: 00002/00094 | Loss: 253.8825 | CE: 0.1811 | KD: 1060.8533\n",
      "Train Epoch: 040 Batch: 00003/00094 | Loss: 253.8028 | CE: 0.1350 | KD: 1060.7129\n",
      "Train Epoch: 040 Batch: 00004/00094 | Loss: 253.9381 | CE: 0.1779 | KD: 1061.0992\n",
      "Train Epoch: 040 Batch: 00005/00094 | Loss: 253.8499 | CE: 0.1334 | KD: 1060.9164\n",
      "Train Epoch: 040 Batch: 00006/00094 | Loss: 253.8492 | CE: 0.1105 | KD: 1061.0089\n",
      "Train Epoch: 040 Batch: 00007/00094 | Loss: 253.8932 | CE: 0.1223 | KD: 1061.1438\n",
      "Train Epoch: 040 Batch: 00008/00094 | Loss: 253.7721 | CE: 0.1332 | KD: 1060.5917\n",
      "Train Epoch: 040 Batch: 00009/00094 | Loss: 253.8179 | CE: 0.0939 | KD: 1060.9475\n",
      "Train Epoch: 040 Batch: 00010/00094 | Loss: 253.8418 | CE: 0.1403 | KD: 1060.8535\n",
      "Train Epoch: 040 Batch: 00011/00094 | Loss: 253.9606 | CE: 0.1752 | KD: 1061.2047\n",
      "Train Epoch: 040 Batch: 00012/00094 | Loss: 253.7308 | CE: 0.0893 | KD: 1060.6028\n",
      "Train Epoch: 040 Batch: 00013/00094 | Loss: 253.7994 | CE: 0.1398 | KD: 1060.6785\n",
      "Train Epoch: 040 Batch: 00014/00094 | Loss: 253.8646 | CE: 0.1088 | KD: 1061.0808\n",
      "Train Epoch: 040 Batch: 00015/00094 | Loss: 253.9535 | CE: 0.1811 | KD: 1061.1501\n",
      "Train Epoch: 040 Batch: 00016/00094 | Loss: 253.8461 | CE: 0.1546 | KD: 1060.8116\n",
      "Train Epoch: 040 Batch: 00017/00094 | Loss: 253.9098 | CE: 0.1281 | KD: 1061.1888\n",
      "Train Epoch: 040 Batch: 00018/00094 | Loss: 253.8033 | CE: 0.1353 | KD: 1060.7136\n",
      "Train Epoch: 040 Batch: 00019/00094 | Loss: 253.9067 | CE: 0.1131 | KD: 1061.2386\n",
      "Train Epoch: 040 Batch: 00020/00094 | Loss: 253.8983 | CE: 0.1527 | KD: 1061.0380\n",
      "Train Epoch: 040 Batch: 00021/00094 | Loss: 253.8138 | CE: 0.1441 | KD: 1060.7207\n",
      "Train Epoch: 040 Batch: 00022/00094 | Loss: 253.8678 | CE: 0.1493 | KD: 1060.9248\n",
      "Train Epoch: 040 Batch: 00023/00094 | Loss: 253.9920 | CE: 0.2621 | KD: 1060.9725\n",
      "Train Epoch: 040 Batch: 00024/00094 | Loss: 253.8573 | CE: 0.1131 | KD: 1061.0322\n",
      "Train Epoch: 040 Batch: 00025/00094 | Loss: 253.8287 | CE: 0.1048 | KD: 1060.9469\n",
      "Train Epoch: 040 Batch: 00026/00094 | Loss: 253.8174 | CE: 0.1534 | KD: 1060.6967\n",
      "Train Epoch: 040 Batch: 00027/00094 | Loss: 253.8624 | CE: 0.1087 | KD: 1061.0719\n",
      "Train Epoch: 040 Batch: 00028/00094 | Loss: 253.8652 | CE: 0.1568 | KD: 1060.8826\n",
      "Train Epoch: 040 Batch: 00029/00094 | Loss: 253.8343 | CE: 0.1141 | KD: 1060.9315\n",
      "Train Epoch: 040 Batch: 00030/00094 | Loss: 253.8393 | CE: 0.0951 | KD: 1061.0321\n",
      "Train Epoch: 040 Batch: 00031/00094 | Loss: 253.9170 | CE: 0.1647 | KD: 1061.0660\n",
      "Train Epoch: 040 Batch: 00032/00094 | Loss: 253.8088 | CE: 0.1239 | KD: 1060.7843\n",
      "Train Epoch: 040 Batch: 00033/00094 | Loss: 253.8807 | CE: 0.1239 | KD: 1061.0850\n",
      "Train Epoch: 040 Batch: 00034/00094 | Loss: 253.8646 | CE: 0.1649 | KD: 1060.8459\n",
      "Train Epoch: 040 Batch: 00035/00094 | Loss: 253.8433 | CE: 0.1057 | KD: 1061.0046\n",
      "Train Epoch: 040 Batch: 00036/00094 | Loss: 253.8548 | CE: 0.1536 | KD: 1060.8523\n",
      "Train Epoch: 040 Batch: 00037/00094 | Loss: 253.8803 | CE: 0.1496 | KD: 1060.9758\n",
      "Train Epoch: 040 Batch: 00038/00094 | Loss: 253.8174 | CE: 0.1068 | KD: 1060.8917\n",
      "Train Epoch: 040 Batch: 00039/00094 | Loss: 253.7461 | CE: 0.1394 | KD: 1060.4570\n",
      "Train Epoch: 040 Batch: 00040/00094 | Loss: 253.8342 | CE: 0.1577 | KD: 1060.7490\n",
      "Train Epoch: 040 Batch: 00041/00094 | Loss: 253.8194 | CE: 0.0883 | KD: 1060.9777\n",
      "Train Epoch: 040 Batch: 00042/00094 | Loss: 253.8001 | CE: 0.1296 | KD: 1060.7240\n",
      "Train Epoch: 040 Batch: 00043/00094 | Loss: 253.7217 | CE: 0.1500 | KD: 1060.3108\n",
      "Train Epoch: 040 Batch: 00044/00094 | Loss: 253.8354 | CE: 0.1858 | KD: 1060.6368\n",
      "Train Epoch: 040 Batch: 00045/00094 | Loss: 253.9138 | CE: 0.1561 | KD: 1061.0887\n",
      "Train Epoch: 040 Batch: 00046/00094 | Loss: 253.8579 | CE: 0.1648 | KD: 1060.8182\n",
      "Train Epoch: 040 Batch: 00047/00094 | Loss: 253.9185 | CE: 0.1586 | KD: 1061.0979\n",
      "Train Epoch: 040 Batch: 00048/00094 | Loss: 253.8667 | CE: 0.1542 | KD: 1060.8994\n",
      "Train Epoch: 040 Batch: 00049/00094 | Loss: 253.8937 | CE: 0.1508 | KD: 1061.0270\n",
      "Train Epoch: 040 Batch: 00050/00094 | Loss: 253.8319 | CE: 0.1428 | KD: 1060.8018\n",
      "Train Epoch: 040 Batch: 00051/00094 | Loss: 253.8649 | CE: 0.1294 | KD: 1060.9958\n",
      "Train Epoch: 040 Batch: 00052/00094 | Loss: 253.8020 | CE: 0.1105 | KD: 1060.8119\n",
      "Train Epoch: 040 Batch: 00053/00094 | Loss: 253.7353 | CE: 0.1182 | KD: 1060.5010\n",
      "Train Epoch: 040 Batch: 00054/00094 | Loss: 253.8090 | CE: 0.1040 | KD: 1060.8687\n",
      "Train Epoch: 040 Batch: 00055/00094 | Loss: 253.6995 | CE: 0.1083 | KD: 1060.3923\n",
      "Train Epoch: 040 Batch: 00056/00094 | Loss: 253.8529 | CE: 0.1025 | KD: 1061.0580\n",
      "Train Epoch: 040 Batch: 00057/00094 | Loss: 253.9336 | CE: 0.1211 | KD: 1061.3179\n",
      "Train Epoch: 040 Batch: 00058/00094 | Loss: 253.7788 | CE: 0.1028 | KD: 1060.7472\n",
      "Train Epoch: 040 Batch: 00059/00094 | Loss: 253.8198 | CE: 0.1318 | KD: 1060.7972\n",
      "Train Epoch: 040 Batch: 00060/00094 | Loss: 253.8124 | CE: 0.0997 | KD: 1060.9006\n",
      "Train Epoch: 040 Batch: 00061/00094 | Loss: 253.8203 | CE: 0.1018 | KD: 1060.9247\n",
      "Train Epoch: 040 Batch: 00062/00094 | Loss: 253.7517 | CE: 0.1044 | KD: 1060.6273\n",
      "Train Epoch: 040 Batch: 00063/00094 | Loss: 253.8475 | CE: 0.1352 | KD: 1060.8989\n",
      "Train Epoch: 040 Batch: 00064/00094 | Loss: 253.8249 | CE: 0.1065 | KD: 1060.9241\n",
      "Train Epoch: 040 Batch: 00065/00094 | Loss: 253.8232 | CE: 0.1088 | KD: 1060.9076\n",
      "Train Epoch: 040 Batch: 00066/00094 | Loss: 253.8605 | CE: 0.1487 | KD: 1060.8969\n",
      "Train Epoch: 040 Batch: 00067/00094 | Loss: 253.9043 | CE: 0.2097 | KD: 1060.8247\n",
      "Train Epoch: 040 Batch: 00068/00094 | Loss: 253.8639 | CE: 0.1436 | KD: 1060.9323\n",
      "Train Epoch: 040 Batch: 00069/00094 | Loss: 253.8051 | CE: 0.1334 | KD: 1060.7290\n",
      "Train Epoch: 040 Batch: 00070/00094 | Loss: 253.7693 | CE: 0.1050 | KD: 1060.6978\n",
      "Train Epoch: 040 Batch: 00071/00094 | Loss: 253.7716 | CE: 0.0859 | KD: 1060.7875\n",
      "Train Epoch: 040 Batch: 00072/00094 | Loss: 253.8305 | CE: 0.1190 | KD: 1060.8955\n",
      "Train Epoch: 040 Batch: 00073/00094 | Loss: 253.8234 | CE: 0.1440 | KD: 1060.7612\n",
      "Train Epoch: 040 Batch: 00074/00094 | Loss: 253.7961 | CE: 0.1169 | KD: 1060.7605\n",
      "Train Epoch: 040 Batch: 00075/00094 | Loss: 253.8931 | CE: 0.1291 | KD: 1061.1151\n",
      "Train Epoch: 040 Batch: 00076/00094 | Loss: 253.8336 | CE: 0.1407 | KD: 1060.8176\n",
      "Train Epoch: 040 Batch: 00077/00094 | Loss: 253.9041 | CE: 0.1528 | KD: 1061.0618\n",
      "Train Epoch: 040 Batch: 00078/00094 | Loss: 253.8199 | CE: 0.1275 | KD: 1060.8154\n",
      "Train Epoch: 040 Batch: 00079/00094 | Loss: 253.8872 | CE: 0.1943 | KD: 1060.8176\n",
      "Train Epoch: 040 Batch: 00080/00094 | Loss: 253.7693 | CE: 0.1044 | KD: 1060.7007\n",
      "Train Epoch: 040 Batch: 00081/00094 | Loss: 253.8754 | CE: 0.1568 | KD: 1060.9250\n",
      "Train Epoch: 040 Batch: 00082/00094 | Loss: 253.8717 | CE: 0.1812 | KD: 1060.8074\n",
      "Train Epoch: 040 Batch: 00083/00094 | Loss: 253.8373 | CE: 0.1295 | KD: 1060.8800\n",
      "Train Epoch: 040 Batch: 00084/00094 | Loss: 253.8125 | CE: 0.1339 | KD: 1060.7581\n",
      "Train Epoch: 040 Batch: 00085/00094 | Loss: 253.8582 | CE: 0.1130 | KD: 1061.0364\n",
      "Train Epoch: 040 Batch: 00086/00094 | Loss: 253.8911 | CE: 0.1317 | KD: 1061.0959\n",
      "Train Epoch: 040 Batch: 00087/00094 | Loss: 253.7094 | CE: 0.0873 | KD: 1060.5216\n",
      "Train Epoch: 040 Batch: 00088/00094 | Loss: 253.7630 | CE: 0.1030 | KD: 1060.6802\n",
      "Train Epoch: 040 Batch: 00089/00094 | Loss: 253.9749 | CE: 0.2874 | KD: 1060.7948\n",
      "Train Epoch: 040 Batch: 00090/00094 | Loss: 253.8420 | CE: 0.1207 | KD: 1060.9368\n",
      "Train Epoch: 040 Batch: 00091/00094 | Loss: 253.7655 | CE: 0.0840 | KD: 1060.7704\n",
      "Train Epoch: 040 Batch: 00092/00094 | Loss: 254.0448 | CE: 0.2956 | KD: 1061.0529\n",
      "Train Epoch: 040 Batch: 00093/00094 | Loss: 253.7916 | CE: 0.1189 | KD: 1060.7335\n",
      "Train Epoch: 040 Batch: 00094/00094 | Loss: 253.9717 | CE: 0.1811 | KD: 1061.2261\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1411 | acc:97.4500\n",
      "[VAL Acc] Target: 97.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6770 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1997 | acc:53.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1002 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2905 | acc:53.8401\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4955 | acc:76.4325\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9707 | acc:63.5972\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8086 | acc:60.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.88%\n",
      "[VAL Acc] Avg 62.53%\n",
      "Train Epoch: 041 Batch: 00001/00094 | Loss: 253.8593 | CE: 0.1398 | KD: 1060.9287\n",
      "Train Epoch: 041 Batch: 00002/00094 | Loss: 253.8669 | CE: 0.1752 | KD: 1060.8126\n",
      "Train Epoch: 041 Batch: 00003/00094 | Loss: 253.7659 | CE: 0.1015 | KD: 1060.6985\n",
      "Train Epoch: 041 Batch: 00004/00094 | Loss: 253.8120 | CE: 0.1384 | KD: 1060.7372\n",
      "Train Epoch: 041 Batch: 00005/00094 | Loss: 253.9069 | CE: 0.1445 | KD: 1061.1083\n",
      "Train Epoch: 041 Batch: 00006/00094 | Loss: 253.8291 | CE: 0.1202 | KD: 1060.8843\n",
      "Train Epoch: 041 Batch: 00007/00094 | Loss: 253.8160 | CE: 0.1268 | KD: 1060.8022\n",
      "Train Epoch: 041 Batch: 00008/00094 | Loss: 253.8599 | CE: 0.1151 | KD: 1061.0350\n",
      "Train Epoch: 041 Batch: 00009/00094 | Loss: 253.8476 | CE: 0.1400 | KD: 1060.8790\n",
      "Train Epoch: 041 Batch: 00010/00094 | Loss: 253.8366 | CE: 0.1033 | KD: 1060.9867\n",
      "Train Epoch: 041 Batch: 00011/00094 | Loss: 253.8579 | CE: 0.1513 | KD: 1060.8748\n",
      "Train Epoch: 041 Batch: 00012/00094 | Loss: 253.7426 | CE: 0.1154 | KD: 1060.5426\n",
      "Train Epoch: 041 Batch: 00013/00094 | Loss: 253.8135 | CE: 0.1017 | KD: 1060.8967\n",
      "Train Epoch: 041 Batch: 00014/00094 | Loss: 253.7563 | CE: 0.1076 | KD: 1060.6326\n",
      "Train Epoch: 041 Batch: 00015/00094 | Loss: 254.0517 | CE: 0.1915 | KD: 1061.5171\n",
      "Train Epoch: 041 Batch: 00016/00094 | Loss: 253.7806 | CE: 0.1045 | KD: 1060.7472\n",
      "Train Epoch: 041 Batch: 00017/00094 | Loss: 253.7626 | CE: 0.1164 | KD: 1060.6224\n",
      "Train Epoch: 041 Batch: 00018/00094 | Loss: 253.9790 | CE: 0.2100 | KD: 1061.1359\n",
      "Train Epoch: 041 Batch: 00019/00094 | Loss: 253.7353 | CE: 0.1086 | KD: 1060.5409\n",
      "Train Epoch: 041 Batch: 00020/00094 | Loss: 253.7557 | CE: 0.1175 | KD: 1060.5889\n",
      "Train Epoch: 041 Batch: 00021/00094 | Loss: 253.7631 | CE: 0.1801 | KD: 1060.3582\n",
      "Train Epoch: 041 Batch: 00022/00094 | Loss: 253.7962 | CE: 0.1187 | KD: 1060.7535\n",
      "Train Epoch: 041 Batch: 00023/00094 | Loss: 253.7311 | CE: 0.1025 | KD: 1060.5487\n",
      "Train Epoch: 041 Batch: 00024/00094 | Loss: 253.9481 | CE: 0.1521 | KD: 1061.2490\n",
      "Train Epoch: 041 Batch: 00025/00094 | Loss: 253.8073 | CE: 0.1083 | KD: 1060.8433\n",
      "Train Epoch: 041 Batch: 00026/00094 | Loss: 253.7655 | CE: 0.1326 | KD: 1060.5667\n",
      "Train Epoch: 041 Batch: 00027/00094 | Loss: 253.7643 | CE: 0.1205 | KD: 1060.6125\n",
      "Train Epoch: 041 Batch: 00028/00094 | Loss: 254.1761 | CE: 0.3061 | KD: 1061.5585\n",
      "Train Epoch: 041 Batch: 00029/00094 | Loss: 253.7224 | CE: 0.1094 | KD: 1060.4836\n",
      "Train Epoch: 041 Batch: 00030/00094 | Loss: 253.7939 | CE: 0.1345 | KD: 1060.6779\n",
      "Train Epoch: 041 Batch: 00031/00094 | Loss: 253.9754 | CE: 0.1871 | KD: 1061.2166\n",
      "Train Epoch: 041 Batch: 00032/00094 | Loss: 253.9219 | CE: 0.1559 | KD: 1061.1233\n",
      "Train Epoch: 041 Batch: 00033/00094 | Loss: 253.8336 | CE: 0.1337 | KD: 1060.8469\n",
      "Train Epoch: 041 Batch: 00034/00094 | Loss: 253.7695 | CE: 0.1039 | KD: 1060.7037\n",
      "Train Epoch: 041 Batch: 00035/00094 | Loss: 253.8540 | CE: 0.1163 | KD: 1061.0049\n",
      "Train Epoch: 041 Batch: 00036/00094 | Loss: 253.9191 | CE: 0.1406 | KD: 1061.1753\n",
      "Train Epoch: 041 Batch: 00037/00094 | Loss: 253.8774 | CE: 0.1489 | KD: 1060.9667\n",
      "Train Epoch: 041 Batch: 00038/00094 | Loss: 253.9020 | CE: 0.1567 | KD: 1061.0369\n",
      "Train Epoch: 041 Batch: 00039/00094 | Loss: 253.8180 | CE: 0.1203 | KD: 1060.8378\n",
      "Train Epoch: 041 Batch: 00040/00094 | Loss: 253.9540 | CE: 0.2205 | KD: 1060.9872\n",
      "Train Epoch: 041 Batch: 00041/00094 | Loss: 253.8044 | CE: 0.1570 | KD: 1060.6276\n",
      "Train Epoch: 041 Batch: 00042/00094 | Loss: 253.8352 | CE: 0.1418 | KD: 1060.8197\n",
      "Train Epoch: 041 Batch: 00043/00094 | Loss: 253.8315 | CE: 0.1293 | KD: 1060.8566\n",
      "Train Epoch: 041 Batch: 00044/00094 | Loss: 253.9413 | CE: 0.2737 | KD: 1060.7119\n",
      "Train Epoch: 041 Batch: 00045/00094 | Loss: 253.8135 | CE: 0.1044 | KD: 1060.8856\n",
      "Train Epoch: 041 Batch: 00046/00094 | Loss: 253.8648 | CE: 0.1372 | KD: 1060.9629\n",
      "Train Epoch: 041 Batch: 00047/00094 | Loss: 253.8238 | CE: 0.1370 | KD: 1060.7922\n",
      "Train Epoch: 041 Batch: 00048/00094 | Loss: 253.6637 | CE: 0.0976 | KD: 1060.2874\n",
      "Train Epoch: 041 Batch: 00049/00094 | Loss: 253.8345 | CE: 0.1089 | KD: 1060.9543\n",
      "Train Epoch: 041 Batch: 00050/00094 | Loss: 253.7855 | CE: 0.1146 | KD: 1060.7258\n",
      "Train Epoch: 041 Batch: 00051/00094 | Loss: 253.8041 | CE: 0.1085 | KD: 1060.8287\n",
      "Train Epoch: 041 Batch: 00052/00094 | Loss: 253.8223 | CE: 0.1097 | KD: 1060.9003\n",
      "Train Epoch: 041 Batch: 00053/00094 | Loss: 253.8154 | CE: 0.1313 | KD: 1060.7805\n",
      "Train Epoch: 041 Batch: 00054/00094 | Loss: 253.8446 | CE: 0.1867 | KD: 1060.6713\n",
      "Train Epoch: 041 Batch: 00055/00094 | Loss: 253.8814 | CE: 0.1222 | KD: 1061.0948\n",
      "Train Epoch: 041 Batch: 00056/00094 | Loss: 253.7692 | CE: 0.1018 | KD: 1060.7112\n",
      "Train Epoch: 041 Batch: 00057/00094 | Loss: 253.9322 | CE: 0.1744 | KD: 1061.0891\n",
      "Train Epoch: 041 Batch: 00058/00094 | Loss: 253.8428 | CE: 0.1492 | KD: 1060.8208\n",
      "Train Epoch: 041 Batch: 00059/00094 | Loss: 253.7416 | CE: 0.1157 | KD: 1060.5374\n",
      "Train Epoch: 041 Batch: 00060/00094 | Loss: 253.8780 | CE: 0.1512 | KD: 1060.9597\n",
      "Train Epoch: 041 Batch: 00061/00094 | Loss: 253.7960 | CE: 0.1265 | KD: 1060.7197\n",
      "Train Epoch: 041 Batch: 00062/00094 | Loss: 253.8894 | CE: 0.1231 | KD: 1061.1246\n",
      "Train Epoch: 041 Batch: 00063/00094 | Loss: 253.7497 | CE: 0.1233 | KD: 1060.5398\n",
      "Train Epoch: 041 Batch: 00064/00094 | Loss: 253.8594 | CE: 0.1292 | KD: 1060.9734\n",
      "Train Epoch: 041 Batch: 00065/00094 | Loss: 253.6987 | CE: 0.0904 | KD: 1060.4635\n",
      "Train Epoch: 041 Batch: 00066/00094 | Loss: 253.6653 | CE: 0.0850 | KD: 1060.3469\n",
      "Train Epoch: 041 Batch: 00067/00094 | Loss: 253.7581 | CE: 0.1188 | KD: 1060.5939\n",
      "Train Epoch: 041 Batch: 00068/00094 | Loss: 253.9425 | CE: 0.1302 | KD: 1061.3168\n",
      "Train Epoch: 041 Batch: 00069/00094 | Loss: 253.8687 | CE: 0.0986 | KD: 1061.1403\n",
      "Train Epoch: 041 Batch: 00070/00094 | Loss: 253.7898 | CE: 0.1250 | KD: 1060.7000\n",
      "Train Epoch: 041 Batch: 00071/00094 | Loss: 253.8928 | CE: 0.2097 | KD: 1060.7765\n",
      "Train Epoch: 041 Batch: 00072/00094 | Loss: 253.7744 | CE: 0.0933 | KD: 1060.7681\n",
      "Train Epoch: 041 Batch: 00073/00094 | Loss: 254.0017 | CE: 0.1464 | KD: 1061.4969\n",
      "Train Epoch: 041 Batch: 00074/00094 | Loss: 253.7003 | CE: 0.0796 | KD: 1060.5160\n",
      "Train Epoch: 041 Batch: 00075/00094 | Loss: 253.8125 | CE: 0.1174 | KD: 1060.8269\n",
      "Train Epoch: 041 Batch: 00076/00094 | Loss: 253.8400 | CE: 0.1001 | KD: 1061.0143\n",
      "Train Epoch: 041 Batch: 00077/00094 | Loss: 253.9142 | CE: 0.1988 | KD: 1060.9119\n",
      "Train Epoch: 041 Batch: 00078/00094 | Loss: 253.7767 | CE: 0.1072 | KD: 1060.7198\n",
      "Train Epoch: 041 Batch: 00079/00094 | Loss: 253.7326 | CE: 0.0998 | KD: 1060.5665\n",
      "Train Epoch: 041 Batch: 00080/00094 | Loss: 253.8773 | CE: 0.1630 | KD: 1060.9070\n",
      "Train Epoch: 041 Batch: 00081/00094 | Loss: 253.8913 | CE: 0.1165 | KD: 1061.1603\n",
      "Train Epoch: 041 Batch: 00082/00094 | Loss: 253.9426 | CE: 0.1944 | KD: 1061.0490\n",
      "Train Epoch: 041 Batch: 00083/00094 | Loss: 253.8214 | CE: 0.1270 | KD: 1060.8240\n",
      "Train Epoch: 041 Batch: 00084/00094 | Loss: 253.9093 | CE: 0.1422 | KD: 1061.1279\n",
      "Train Epoch: 041 Batch: 00085/00094 | Loss: 253.8110 | CE: 0.1438 | KD: 1060.7103\n",
      "Train Epoch: 041 Batch: 00086/00094 | Loss: 253.8249 | CE: 0.0923 | KD: 1060.9839\n",
      "Train Epoch: 041 Batch: 00087/00094 | Loss: 253.8648 | CE: 0.1435 | KD: 1060.9365\n",
      "Train Epoch: 041 Batch: 00088/00094 | Loss: 253.8079 | CE: 0.0906 | KD: 1060.9198\n",
      "Train Epoch: 041 Batch: 00089/00094 | Loss: 253.6808 | CE: 0.0905 | KD: 1060.3885\n",
      "Train Epoch: 041 Batch: 00090/00094 | Loss: 253.8270 | CE: 0.1026 | KD: 1060.9490\n",
      "Train Epoch: 041 Batch: 00091/00094 | Loss: 253.8052 | CE: 0.1407 | KD: 1060.6987\n",
      "Train Epoch: 041 Batch: 00092/00094 | Loss: 253.8888 | CE: 0.1704 | KD: 1060.9243\n",
      "Train Epoch: 041 Batch: 00093/00094 | Loss: 253.8131 | CE: 0.1100 | KD: 1060.8606\n",
      "Train Epoch: 041 Batch: 00094/00094 | Loss: 253.7897 | CE: 0.1247 | KD: 1060.7010\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1239 | acc:97.5000\n",
      "[VAL Acc] Target: 97.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.4822 | acc:50.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9896 | acc:56.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0282 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1840 | acc:55.6034\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6342 | acc:68.5767\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7187 | acc:69.3966\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7905 | acc:60.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.44%\n",
      "[VAL Acc] Avg 63.24%\n",
      "Train Epoch: 042 Batch: 00001/00094 | Loss: 228.3954 | CE: 0.1369 | KD: 1060.5155\n",
      "Train Epoch: 042 Batch: 00002/00094 | Loss: 228.4393 | CE: 0.1659 | KD: 1060.5844\n",
      "Train Epoch: 042 Batch: 00003/00094 | Loss: 228.5184 | CE: 0.1486 | KD: 1061.0321\n",
      "Train Epoch: 042 Batch: 00004/00094 | Loss: 228.3853 | CE: 0.0926 | KD: 1060.6740\n",
      "Train Epoch: 042 Batch: 00005/00094 | Loss: 228.3794 | CE: 0.1056 | KD: 1060.5862\n",
      "Train Epoch: 042 Batch: 00006/00094 | Loss: 228.5080 | CE: 0.1101 | KD: 1061.1630\n",
      "Train Epoch: 042 Batch: 00007/00094 | Loss: 228.6100 | CE: 0.1829 | KD: 1061.2986\n",
      "Train Epoch: 042 Batch: 00008/00094 | Loss: 228.4211 | CE: 0.0955 | KD: 1060.8268\n",
      "Train Epoch: 042 Batch: 00009/00094 | Loss: 228.4718 | CE: 0.1987 | KD: 1060.5831\n",
      "Train Epoch: 042 Batch: 00010/00094 | Loss: 228.5460 | CE: 0.1400 | KD: 1061.2003\n",
      "Train Epoch: 042 Batch: 00011/00094 | Loss: 228.3820 | CE: 0.1137 | KD: 1060.5605\n",
      "Train Epoch: 042 Batch: 00012/00094 | Loss: 228.4580 | CE: 0.1027 | KD: 1060.9650\n",
      "Train Epoch: 042 Batch: 00013/00094 | Loss: 228.4731 | CE: 0.1153 | KD: 1060.9767\n",
      "Train Epoch: 042 Batch: 00014/00094 | Loss: 228.4423 | CE: 0.1112 | KD: 1060.8523\n",
      "Train Epoch: 042 Batch: 00015/00094 | Loss: 228.4767 | CE: 0.1447 | KD: 1060.8569\n",
      "Train Epoch: 042 Batch: 00016/00094 | Loss: 228.3871 | CE: 0.1031 | KD: 1060.6335\n",
      "Train Epoch: 042 Batch: 00017/00094 | Loss: 228.3486 | CE: 0.0868 | KD: 1060.5304\n",
      "Train Epoch: 042 Batch: 00018/00094 | Loss: 228.5703 | CE: 0.2453 | KD: 1060.8241\n",
      "Train Epoch: 042 Batch: 00019/00094 | Loss: 228.4463 | CE: 0.0795 | KD: 1061.0184\n",
      "Train Epoch: 042 Batch: 00020/00094 | Loss: 228.5239 | CE: 0.1520 | KD: 1061.0419\n",
      "Train Epoch: 042 Batch: 00021/00094 | Loss: 228.4794 | CE: 0.1134 | KD: 1061.0149\n",
      "Train Epoch: 042 Batch: 00022/00094 | Loss: 228.4537 | CE: 0.1203 | KD: 1060.8632\n",
      "Train Epoch: 042 Batch: 00023/00094 | Loss: 228.5928 | CE: 0.1432 | KD: 1061.4030\n",
      "Train Epoch: 042 Batch: 00024/00094 | Loss: 228.4463 | CE: 0.1147 | KD: 1060.8546\n",
      "Train Epoch: 042 Batch: 00025/00094 | Loss: 228.4423 | CE: 0.0932 | KD: 1060.9360\n",
      "Train Epoch: 042 Batch: 00026/00094 | Loss: 228.4878 | CE: 0.1618 | KD: 1060.8287\n",
      "Train Epoch: 042 Batch: 00027/00094 | Loss: 228.4446 | CE: 0.1199 | KD: 1060.8229\n",
      "Train Epoch: 042 Batch: 00028/00094 | Loss: 228.4585 | CE: 0.0995 | KD: 1060.9822\n",
      "Train Epoch: 042 Batch: 00029/00094 | Loss: 228.3860 | CE: 0.0922 | KD: 1060.6793\n",
      "Train Epoch: 042 Batch: 00030/00094 | Loss: 228.4135 | CE: 0.0955 | KD: 1060.7917\n",
      "Train Epoch: 042 Batch: 00031/00094 | Loss: 228.4331 | CE: 0.1033 | KD: 1060.8463\n",
      "Train Epoch: 042 Batch: 00032/00094 | Loss: 228.4710 | CE: 0.1228 | KD: 1060.9320\n",
      "Train Epoch: 042 Batch: 00033/00094 | Loss: 228.4475 | CE: 0.0922 | KD: 1060.9651\n",
      "Train Epoch: 042 Batch: 00034/00094 | Loss: 228.4548 | CE: 0.1153 | KD: 1060.8914\n",
      "Train Epoch: 042 Batch: 00035/00094 | Loss: 228.3817 | CE: 0.1032 | KD: 1060.6080\n",
      "Train Epoch: 042 Batch: 00036/00094 | Loss: 228.5766 | CE: 0.1496 | KD: 1061.2982\n",
      "Train Epoch: 042 Batch: 00037/00094 | Loss: 228.4222 | CE: 0.0953 | KD: 1060.8331\n",
      "Train Epoch: 042 Batch: 00038/00094 | Loss: 228.6087 | CE: 0.1324 | KD: 1061.5270\n",
      "Train Epoch: 042 Batch: 00039/00094 | Loss: 228.4484 | CE: 0.1186 | KD: 1060.8464\n",
      "Train Epoch: 042 Batch: 00040/00094 | Loss: 228.5667 | CE: 0.1547 | KD: 1061.2284\n",
      "Train Epoch: 042 Batch: 00041/00094 | Loss: 228.4714 | CE: 0.1114 | KD: 1060.9866\n",
      "Train Epoch: 042 Batch: 00042/00094 | Loss: 228.3796 | CE: 0.0830 | KD: 1060.6923\n",
      "Train Epoch: 042 Batch: 00043/00094 | Loss: 228.3723 | CE: 0.1216 | KD: 1060.4786\n",
      "Train Epoch: 042 Batch: 00044/00094 | Loss: 228.4723 | CE: 0.1740 | KD: 1060.7001\n",
      "Train Epoch: 042 Batch: 00045/00094 | Loss: 228.5056 | CE: 0.1694 | KD: 1060.8762\n",
      "Train Epoch: 042 Batch: 00046/00094 | Loss: 228.4211 | CE: 0.0955 | KD: 1060.8270\n",
      "Train Epoch: 042 Batch: 00047/00094 | Loss: 228.4337 | CE: 0.1252 | KD: 1060.7472\n",
      "Train Epoch: 042 Batch: 00048/00094 | Loss: 228.5108 | CE: 0.1518 | KD: 1060.9822\n",
      "Train Epoch: 042 Batch: 00049/00094 | Loss: 228.4617 | CE: 0.1372 | KD: 1060.8215\n",
      "Train Epoch: 042 Batch: 00050/00094 | Loss: 228.4611 | CE: 0.1265 | KD: 1060.8684\n",
      "Train Epoch: 042 Batch: 00051/00094 | Loss: 228.4809 | CE: 0.1002 | KD: 1061.0828\n",
      "Train Epoch: 042 Batch: 00052/00094 | Loss: 228.4339 | CE: 0.1079 | KD: 1060.8289\n",
      "Train Epoch: 042 Batch: 00053/00094 | Loss: 228.4367 | CE: 0.0969 | KD: 1060.8929\n",
      "Train Epoch: 042 Batch: 00054/00094 | Loss: 228.4195 | CE: 0.0975 | KD: 1060.8099\n",
      "Train Epoch: 042 Batch: 00055/00094 | Loss: 228.4460 | CE: 0.1292 | KD: 1060.7858\n",
      "Train Epoch: 042 Batch: 00056/00094 | Loss: 228.4306 | CE: 0.1010 | KD: 1060.8455\n",
      "Train Epoch: 042 Batch: 00057/00094 | Loss: 228.3934 | CE: 0.0989 | KD: 1060.6824\n",
      "Train Epoch: 042 Batch: 00058/00094 | Loss: 228.5657 | CE: 0.1661 | KD: 1061.1704\n",
      "Train Epoch: 042 Batch: 00059/00094 | Loss: 228.4617 | CE: 0.1757 | KD: 1060.6428\n",
      "Train Epoch: 042 Batch: 00060/00094 | Loss: 228.5072 | CE: 0.1202 | KD: 1061.1122\n",
      "Train Epoch: 042 Batch: 00061/00094 | Loss: 228.5220 | CE: 0.1223 | KD: 1061.1709\n",
      "Train Epoch: 042 Batch: 00062/00094 | Loss: 228.4398 | CE: 0.1025 | KD: 1060.8816\n",
      "Train Epoch: 042 Batch: 00063/00094 | Loss: 228.4272 | CE: 0.1068 | KD: 1060.8029\n",
      "Train Epoch: 042 Batch: 00064/00094 | Loss: 228.4319 | CE: 0.1014 | KD: 1060.8495\n",
      "Train Epoch: 042 Batch: 00065/00094 | Loss: 228.4086 | CE: 0.1112 | KD: 1060.6959\n",
      "Train Epoch: 042 Batch: 00066/00094 | Loss: 228.4505 | CE: 0.1182 | KD: 1060.8579\n",
      "Train Epoch: 042 Batch: 00067/00094 | Loss: 228.4880 | CE: 0.1516 | KD: 1060.8771\n",
      "Train Epoch: 042 Batch: 00068/00094 | Loss: 228.5045 | CE: 0.1909 | KD: 1060.7711\n",
      "Train Epoch: 042 Batch: 00069/00094 | Loss: 228.6017 | CE: 0.1508 | KD: 1061.4091\n",
      "Train Epoch: 042 Batch: 00070/00094 | Loss: 228.5442 | CE: 0.1480 | KD: 1061.1550\n",
      "Train Epoch: 042 Batch: 00071/00094 | Loss: 228.4137 | CE: 0.0999 | KD: 1060.7723\n",
      "Train Epoch: 042 Batch: 00072/00094 | Loss: 228.4888 | CE: 0.1119 | KD: 1061.0653\n",
      "Train Epoch: 042 Batch: 00073/00094 | Loss: 228.3902 | CE: 0.1207 | KD: 1060.5662\n",
      "Train Epoch: 042 Batch: 00074/00094 | Loss: 228.6061 | CE: 0.2311 | KD: 1061.0563\n",
      "Train Epoch: 042 Batch: 00075/00094 | Loss: 228.4393 | CE: 0.0772 | KD: 1060.9968\n",
      "Train Epoch: 042 Batch: 00076/00094 | Loss: 228.3213 | CE: 0.1130 | KD: 1060.2819\n",
      "Train Epoch: 042 Batch: 00077/00094 | Loss: 228.4121 | CE: 0.1207 | KD: 1060.6681\n",
      "Train Epoch: 042 Batch: 00078/00094 | Loss: 228.4266 | CE: 0.0898 | KD: 1060.8787\n",
      "Train Epoch: 042 Batch: 00079/00094 | Loss: 228.3970 | CE: 0.1238 | KD: 1060.5839\n",
      "Train Epoch: 042 Batch: 00080/00094 | Loss: 228.4130 | CE: 0.1138 | KD: 1060.7046\n",
      "Train Epoch: 042 Batch: 00081/00094 | Loss: 228.4999 | CE: 0.1807 | KD: 1060.7969\n",
      "Train Epoch: 042 Batch: 00082/00094 | Loss: 228.6073 | CE: 0.2184 | KD: 1061.1213\n",
      "Train Epoch: 042 Batch: 00083/00094 | Loss: 228.4931 | CE: 0.1698 | KD: 1060.8163\n",
      "Train Epoch: 042 Batch: 00084/00094 | Loss: 228.4384 | CE: 0.0944 | KD: 1060.9122\n",
      "Train Epoch: 042 Batch: 00085/00094 | Loss: 228.4159 | CE: 0.0997 | KD: 1060.7834\n",
      "Train Epoch: 042 Batch: 00086/00094 | Loss: 228.4590 | CE: 0.1300 | KD: 1060.8428\n",
      "Train Epoch: 042 Batch: 00087/00094 | Loss: 228.3853 | CE: 0.0916 | KD: 1060.6783\n",
      "Train Epoch: 042 Batch: 00088/00094 | Loss: 228.4898 | CE: 0.1181 | KD: 1061.0409\n",
      "Train Epoch: 042 Batch: 00089/00094 | Loss: 228.4669 | CE: 0.1299 | KD: 1060.8799\n",
      "Train Epoch: 042 Batch: 00090/00094 | Loss: 228.3689 | CE: 0.0836 | KD: 1060.6395\n",
      "Train Epoch: 042 Batch: 00091/00094 | Loss: 228.4568 | CE: 0.1187 | KD: 1060.8853\n",
      "Train Epoch: 042 Batch: 00092/00094 | Loss: 228.4915 | CE: 0.1455 | KD: 1060.9219\n",
      "Train Epoch: 042 Batch: 00093/00094 | Loss: 228.4542 | CE: 0.1076 | KD: 1060.9244\n",
      "Train Epoch: 042 Batch: 00094/00094 | Loss: 228.4256 | CE: 0.1046 | KD: 1060.8054\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1154 | acc:98.0500\n",
      "[VAL Acc] Target: 98.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6704 | acc:49.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1610 | acc:54.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1283 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2851 | acc:55.4467\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5729 | acc:72.5508\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8436 | acc:68.1818\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8091 | acc:60.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.25%\n",
      "[VAL Acc] Avg 63.32%\n",
      "Train Epoch: 043 Batch: 00001/00094 | Loss: 228.4252 | CE: 0.1172 | KD: 1060.7449\n",
      "Train Epoch: 043 Batch: 00002/00094 | Loss: 228.4317 | CE: 0.0863 | KD: 1060.9191\n",
      "Train Epoch: 043 Batch: 00003/00094 | Loss: 228.4648 | CE: 0.1121 | KD: 1060.9525\n",
      "Train Epoch: 043 Batch: 00004/00094 | Loss: 228.4864 | CE: 0.1161 | KD: 1061.0345\n",
      "Train Epoch: 043 Batch: 00005/00094 | Loss: 228.5048 | CE: 0.0938 | KD: 1061.2239\n",
      "Train Epoch: 043 Batch: 00006/00094 | Loss: 228.4220 | CE: 0.1018 | KD: 1060.8021\n",
      "Train Epoch: 043 Batch: 00007/00094 | Loss: 228.4486 | CE: 0.1043 | KD: 1060.9137\n",
      "Train Epoch: 043 Batch: 00008/00094 | Loss: 228.4345 | CE: 0.0913 | KD: 1060.9084\n",
      "Train Epoch: 043 Batch: 00009/00094 | Loss: 228.6008 | CE: 0.2223 | KD: 1061.0730\n",
      "Train Epoch: 043 Batch: 00010/00094 | Loss: 228.4977 | CE: 0.1385 | KD: 1060.9830\n",
      "Train Epoch: 043 Batch: 00011/00094 | Loss: 228.4745 | CE: 0.1173 | KD: 1060.9740\n",
      "Train Epoch: 043 Batch: 00012/00094 | Loss: 228.4613 | CE: 0.1511 | KD: 1060.7555\n",
      "Train Epoch: 043 Batch: 00013/00094 | Loss: 228.4748 | CE: 0.0868 | KD: 1061.1167\n",
      "Train Epoch: 043 Batch: 00014/00094 | Loss: 228.3750 | CE: 0.1032 | KD: 1060.5770\n",
      "Train Epoch: 043 Batch: 00015/00094 | Loss: 228.5411 | CE: 0.1372 | KD: 1061.1907\n",
      "Train Epoch: 043 Batch: 00016/00094 | Loss: 228.4515 | CE: 0.1016 | KD: 1060.9398\n",
      "Train Epoch: 043 Batch: 00017/00094 | Loss: 228.3844 | CE: 0.1281 | KD: 1060.5045\n",
      "Train Epoch: 043 Batch: 00018/00094 | Loss: 228.4532 | CE: 0.1298 | KD: 1060.8164\n",
      "Train Epoch: 043 Batch: 00019/00094 | Loss: 228.4470 | CE: 0.1018 | KD: 1060.9178\n",
      "Train Epoch: 043 Batch: 00020/00094 | Loss: 228.5028 | CE: 0.1490 | KD: 1060.9578\n",
      "Train Epoch: 043 Batch: 00021/00094 | Loss: 228.4768 | CE: 0.1284 | KD: 1060.9330\n",
      "Train Epoch: 043 Batch: 00022/00094 | Loss: 228.4769 | CE: 0.1662 | KD: 1060.7577\n",
      "Train Epoch: 043 Batch: 00023/00094 | Loss: 228.5021 | CE: 0.1344 | KD: 1061.0226\n",
      "Train Epoch: 043 Batch: 00024/00094 | Loss: 228.4663 | CE: 0.1392 | KD: 1060.8339\n",
      "Train Epoch: 043 Batch: 00025/00094 | Loss: 228.5080 | CE: 0.1642 | KD: 1060.9111\n",
      "Train Epoch: 043 Batch: 00026/00094 | Loss: 228.4039 | CE: 0.1049 | KD: 1060.7034\n",
      "Train Epoch: 043 Batch: 00027/00094 | Loss: 228.4377 | CE: 0.1165 | KD: 1060.8068\n",
      "Train Epoch: 043 Batch: 00028/00094 | Loss: 228.4724 | CE: 0.1409 | KD: 1060.8541\n",
      "Train Epoch: 043 Batch: 00029/00094 | Loss: 228.4491 | CE: 0.1502 | KD: 1060.7028\n",
      "Train Epoch: 043 Batch: 00030/00094 | Loss: 228.4861 | CE: 0.1173 | KD: 1061.0272\n",
      "Train Epoch: 043 Batch: 00031/00094 | Loss: 228.4615 | CE: 0.1324 | KD: 1060.8433\n",
      "Train Epoch: 043 Batch: 00032/00094 | Loss: 228.5062 | CE: 0.1489 | KD: 1060.9744\n",
      "Train Epoch: 043 Batch: 00033/00094 | Loss: 228.5161 | CE: 0.1101 | KD: 1061.2008\n",
      "Train Epoch: 043 Batch: 00034/00094 | Loss: 228.4483 | CE: 0.1226 | KD: 1060.8270\n",
      "Train Epoch: 043 Batch: 00035/00094 | Loss: 228.3907 | CE: 0.0747 | KD: 1060.7826\n",
      "Train Epoch: 043 Batch: 00036/00094 | Loss: 228.4463 | CE: 0.1420 | KD: 1060.7281\n",
      "Train Epoch: 043 Batch: 00037/00094 | Loss: 228.4863 | CE: 0.0917 | KD: 1061.1472\n",
      "Train Epoch: 043 Batch: 00038/00094 | Loss: 228.5049 | CE: 0.1703 | KD: 1060.8690\n",
      "Train Epoch: 043 Batch: 00039/00094 | Loss: 228.5255 | CE: 0.1208 | KD: 1061.1941\n",
      "Train Epoch: 043 Batch: 00040/00094 | Loss: 228.7290 | CE: 0.2981 | KD: 1061.3160\n",
      "Train Epoch: 043 Batch: 00041/00094 | Loss: 228.5288 | CE: 0.1102 | KD: 1061.2589\n",
      "Train Epoch: 043 Batch: 00042/00094 | Loss: 228.4636 | CE: 0.1468 | KD: 1060.7859\n",
      "Train Epoch: 043 Batch: 00043/00094 | Loss: 228.4067 | CE: 0.1124 | KD: 1060.6812\n",
      "Train Epoch: 043 Batch: 00044/00094 | Loss: 228.5665 | CE: 0.1352 | KD: 1061.3181\n",
      "Train Epoch: 043 Batch: 00045/00094 | Loss: 228.4723 | CE: 0.0929 | KD: 1061.0767\n",
      "Train Epoch: 043 Batch: 00046/00094 | Loss: 228.5202 | CE: 0.1572 | KD: 1061.0010\n",
      "Train Epoch: 043 Batch: 00047/00094 | Loss: 228.4636 | CE: 0.1413 | KD: 1060.8115\n",
      "Train Epoch: 043 Batch: 00048/00094 | Loss: 228.3910 | CE: 0.0903 | KD: 1060.7109\n",
      "Train Epoch: 043 Batch: 00049/00094 | Loss: 228.4451 | CE: 0.1006 | KD: 1060.9148\n",
      "Train Epoch: 043 Batch: 00050/00094 | Loss: 228.4763 | CE: 0.1455 | KD: 1060.8511\n",
      "Train Epoch: 043 Batch: 00051/00094 | Loss: 228.3872 | CE: 0.1069 | KD: 1060.6165\n",
      "Train Epoch: 043 Batch: 00052/00094 | Loss: 228.4544 | CE: 0.1071 | KD: 1060.9279\n",
      "Train Epoch: 043 Batch: 00053/00094 | Loss: 228.5212 | CE: 0.1463 | KD: 1061.0557\n",
      "Train Epoch: 043 Batch: 00054/00094 | Loss: 228.5346 | CE: 0.0863 | KD: 1061.3969\n",
      "Train Epoch: 043 Batch: 00055/00094 | Loss: 228.4609 | CE: 0.0799 | KD: 1061.0842\n",
      "Train Epoch: 043 Batch: 00056/00094 | Loss: 228.4881 | CE: 0.1165 | KD: 1061.0408\n",
      "Train Epoch: 043 Batch: 00057/00094 | Loss: 228.4284 | CE: 0.1233 | KD: 1060.7317\n",
      "Train Epoch: 043 Batch: 00058/00094 | Loss: 228.4571 | CE: 0.1309 | KD: 1060.8296\n",
      "Train Epoch: 043 Batch: 00059/00094 | Loss: 228.4390 | CE: 0.1120 | KD: 1060.8333\n",
      "Train Epoch: 043 Batch: 00060/00094 | Loss: 228.4362 | CE: 0.1105 | KD: 1060.8274\n",
      "Train Epoch: 043 Batch: 00061/00094 | Loss: 228.4673 | CE: 0.0922 | KD: 1061.0569\n",
      "Train Epoch: 043 Batch: 00062/00094 | Loss: 228.4425 | CE: 0.1271 | KD: 1060.7795\n",
      "Train Epoch: 043 Batch: 00063/00094 | Loss: 228.4207 | CE: 0.1206 | KD: 1060.7083\n",
      "Train Epoch: 043 Batch: 00064/00094 | Loss: 228.4518 | CE: 0.1489 | KD: 1060.7214\n",
      "Train Epoch: 043 Batch: 00065/00094 | Loss: 228.3729 | CE: 0.0858 | KD: 1060.6481\n",
      "Train Epoch: 043 Batch: 00066/00094 | Loss: 228.5379 | CE: 0.1309 | KD: 1061.2048\n",
      "Train Epoch: 043 Batch: 00067/00094 | Loss: 228.4090 | CE: 0.1235 | KD: 1060.6406\n",
      "Train Epoch: 043 Batch: 00068/00094 | Loss: 228.4661 | CE: 0.1111 | KD: 1060.9636\n",
      "Train Epoch: 043 Batch: 00069/00094 | Loss: 228.3884 | CE: 0.0997 | KD: 1060.6552\n",
      "Train Epoch: 043 Batch: 00070/00094 | Loss: 228.4660 | CE: 0.1173 | KD: 1060.9342\n",
      "Train Epoch: 043 Batch: 00071/00094 | Loss: 228.4352 | CE: 0.1210 | KD: 1060.7740\n",
      "Train Epoch: 043 Batch: 00072/00094 | Loss: 228.5036 | CE: 0.1591 | KD: 1060.9146\n",
      "Train Epoch: 043 Batch: 00073/00094 | Loss: 228.3390 | CE: 0.0924 | KD: 1060.4598\n",
      "Train Epoch: 043 Batch: 00074/00094 | Loss: 228.3732 | CE: 0.1093 | KD: 1060.5398\n",
      "Train Epoch: 043 Batch: 00075/00094 | Loss: 228.4491 | CE: 0.1816 | KD: 1060.5569\n",
      "Train Epoch: 043 Batch: 00076/00094 | Loss: 228.4531 | CE: 0.1659 | KD: 1060.6483\n",
      "Train Epoch: 043 Batch: 00077/00094 | Loss: 228.3504 | CE: 0.0950 | KD: 1060.5004\n",
      "Train Epoch: 043 Batch: 00078/00094 | Loss: 228.5000 | CE: 0.1606 | KD: 1060.8911\n",
      "Train Epoch: 043 Batch: 00079/00094 | Loss: 228.4131 | CE: 0.1120 | KD: 1060.7130\n",
      "Train Epoch: 043 Batch: 00080/00094 | Loss: 228.3752 | CE: 0.1098 | KD: 1060.5474\n",
      "Train Epoch: 043 Batch: 00081/00094 | Loss: 228.4701 | CE: 0.1281 | KD: 1060.9030\n",
      "Train Epoch: 043 Batch: 00082/00094 | Loss: 228.4876 | CE: 0.1367 | KD: 1060.9447\n",
      "Train Epoch: 043 Batch: 00083/00094 | Loss: 228.5461 | CE: 0.1140 | KD: 1061.3217\n",
      "Train Epoch: 043 Batch: 00084/00094 | Loss: 228.4501 | CE: 0.1165 | KD: 1060.8640\n",
      "Train Epoch: 043 Batch: 00085/00094 | Loss: 228.3971 | CE: 0.0852 | KD: 1060.7635\n",
      "Train Epoch: 043 Batch: 00086/00094 | Loss: 228.4135 | CE: 0.1063 | KD: 1060.7417\n",
      "Train Epoch: 043 Batch: 00087/00094 | Loss: 228.4342 | CE: 0.1016 | KD: 1060.8596\n",
      "Train Epoch: 043 Batch: 00088/00094 | Loss: 228.4375 | CE: 0.0967 | KD: 1060.8973\n",
      "Train Epoch: 043 Batch: 00089/00094 | Loss: 228.4680 | CE: 0.1373 | KD: 1060.8503\n",
      "Train Epoch: 043 Batch: 00090/00094 | Loss: 228.4594 | CE: 0.1448 | KD: 1060.7758\n",
      "Train Epoch: 043 Batch: 00091/00094 | Loss: 228.4443 | CE: 0.1301 | KD: 1060.7738\n",
      "Train Epoch: 043 Batch: 00092/00094 | Loss: 228.4573 | CE: 0.0911 | KD: 1061.0155\n",
      "Train Epoch: 043 Batch: 00093/00094 | Loss: 228.4926 | CE: 0.1115 | KD: 1061.0848\n",
      "Train Epoch: 043 Batch: 00094/00094 | Loss: 228.4180 | CE: 0.0796 | KD: 1060.8862\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1261 | acc:97.2500\n",
      "[VAL Acc] Target: 97.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7928 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2515 | acc:53.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1925 | acc:46.1832\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3097 | acc:55.7994\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6170 | acc:68.0222\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8899 | acc:67.8292\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.83%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8381 | acc:58.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.13%\n",
      "[VAL Acc] Avg 62.00%\n",
      "Train Epoch: 044 Batch: 00001/00094 | Loss: 228.4681 | CE: 0.0890 | KD: 1061.0756\n",
      "Train Epoch: 044 Batch: 00002/00094 | Loss: 228.4713 | CE: 0.1118 | KD: 1060.9844\n",
      "Train Epoch: 044 Batch: 00003/00094 | Loss: 228.5146 | CE: 0.1015 | KD: 1061.2334\n",
      "Train Epoch: 044 Batch: 00004/00094 | Loss: 228.4338 | CE: 0.1222 | KD: 1060.7620\n",
      "Train Epoch: 044 Batch: 00005/00094 | Loss: 228.5243 | CE: 0.1404 | KD: 1061.0973\n",
      "Train Epoch: 044 Batch: 00006/00094 | Loss: 228.4707 | CE: 0.1750 | KD: 1060.6881\n",
      "Train Epoch: 044 Batch: 00007/00094 | Loss: 228.4988 | CE: 0.1857 | KD: 1060.7684\n",
      "Train Epoch: 044 Batch: 00008/00094 | Loss: 228.5432 | CE: 0.1362 | KD: 1061.2051\n",
      "Train Epoch: 044 Batch: 00009/00094 | Loss: 228.5321 | CE: 0.1494 | KD: 1061.0920\n",
      "Train Epoch: 044 Batch: 00010/00094 | Loss: 228.4171 | CE: 0.0994 | KD: 1060.7904\n",
      "Train Epoch: 044 Batch: 00011/00094 | Loss: 228.3959 | CE: 0.1057 | KD: 1060.6625\n",
      "Train Epoch: 044 Batch: 00012/00094 | Loss: 228.3642 | CE: 0.0992 | KD: 1060.5454\n",
      "Train Epoch: 044 Batch: 00013/00094 | Loss: 228.5360 | CE: 0.1081 | KD: 1061.3020\n",
      "Train Epoch: 044 Batch: 00014/00094 | Loss: 228.4245 | CE: 0.1016 | KD: 1060.8145\n",
      "Train Epoch: 044 Batch: 00015/00094 | Loss: 228.4447 | CE: 0.0888 | KD: 1060.9673\n",
      "Train Epoch: 044 Batch: 00016/00094 | Loss: 228.4685 | CE: 0.1473 | KD: 1060.8063\n",
      "Train Epoch: 044 Batch: 00017/00094 | Loss: 228.4415 | CE: 0.1068 | KD: 1060.8689\n",
      "Train Epoch: 044 Batch: 00018/00094 | Loss: 228.4366 | CE: 0.1328 | KD: 1060.7256\n",
      "Train Epoch: 044 Batch: 00019/00094 | Loss: 228.4167 | CE: 0.0967 | KD: 1060.8010\n",
      "Train Epoch: 044 Batch: 00020/00094 | Loss: 228.5343 | CE: 0.1407 | KD: 1061.1432\n",
      "Train Epoch: 044 Batch: 00021/00094 | Loss: 228.4051 | CE: 0.1251 | KD: 1060.6150\n",
      "Train Epoch: 044 Batch: 00022/00094 | Loss: 228.4898 | CE: 0.0873 | KD: 1061.1841\n",
      "Train Epoch: 044 Batch: 00023/00094 | Loss: 228.4460 | CE: 0.1484 | KD: 1060.6968\n",
      "Train Epoch: 044 Batch: 00024/00094 | Loss: 228.5336 | CE: 0.1978 | KD: 1060.8743\n",
      "Train Epoch: 044 Batch: 00025/00094 | Loss: 228.4502 | CE: 0.1002 | KD: 1060.9402\n",
      "Train Epoch: 044 Batch: 00026/00094 | Loss: 228.4790 | CE: 0.1108 | KD: 1061.0244\n",
      "Train Epoch: 044 Batch: 00027/00094 | Loss: 228.4444 | CE: 0.1233 | KD: 1060.8063\n",
      "Train Epoch: 044 Batch: 00028/00094 | Loss: 228.4667 | CE: 0.1149 | KD: 1060.9486\n",
      "Train Epoch: 044 Batch: 00029/00094 | Loss: 228.5232 | CE: 0.1184 | KD: 1061.1947\n",
      "Train Epoch: 044 Batch: 00030/00094 | Loss: 228.5308 | CE: 0.1401 | KD: 1061.1292\n",
      "Train Epoch: 044 Batch: 00031/00094 | Loss: 228.4849 | CE: 0.1364 | KD: 1060.9331\n",
      "Train Epoch: 044 Batch: 00032/00094 | Loss: 228.3870 | CE: 0.1494 | KD: 1060.4182\n",
      "Train Epoch: 044 Batch: 00033/00094 | Loss: 228.3372 | CE: 0.1225 | KD: 1060.3118\n",
      "Train Epoch: 044 Batch: 00034/00094 | Loss: 228.3911 | CE: 0.1036 | KD: 1060.6499\n",
      "Train Epoch: 044 Batch: 00035/00094 | Loss: 228.3953 | CE: 0.0973 | KD: 1060.6986\n",
      "Train Epoch: 044 Batch: 00036/00094 | Loss: 228.3908 | CE: 0.0973 | KD: 1060.6777\n",
      "Train Epoch: 044 Batch: 00037/00094 | Loss: 228.4816 | CE: 0.1309 | KD: 1060.9437\n",
      "Train Epoch: 044 Batch: 00038/00094 | Loss: 228.4401 | CE: 0.1023 | KD: 1060.8832\n",
      "Train Epoch: 044 Batch: 00039/00094 | Loss: 228.5540 | CE: 0.1887 | KD: 1061.0111\n",
      "Train Epoch: 044 Batch: 00040/00094 | Loss: 228.4232 | CE: 0.1255 | KD: 1060.6971\n",
      "Train Epoch: 044 Batch: 00041/00094 | Loss: 228.5784 | CE: 0.1462 | KD: 1061.3223\n",
      "Train Epoch: 044 Batch: 00042/00094 | Loss: 228.3885 | CE: 0.0907 | KD: 1060.6979\n",
      "Train Epoch: 044 Batch: 00043/00094 | Loss: 228.4960 | CE: 0.1032 | KD: 1061.1390\n",
      "Train Epoch: 044 Batch: 00044/00094 | Loss: 228.4496 | CE: 0.1704 | KD: 1060.6113\n",
      "Train Epoch: 044 Batch: 00045/00094 | Loss: 228.4551 | CE: 0.1026 | KD: 1060.9521\n",
      "Train Epoch: 044 Batch: 00046/00094 | Loss: 228.4735 | CE: 0.1140 | KD: 1060.9845\n",
      "Train Epoch: 044 Batch: 00047/00094 | Loss: 228.4451 | CE: 0.0890 | KD: 1060.9684\n",
      "Train Epoch: 044 Batch: 00048/00094 | Loss: 228.5489 | CE: 0.1675 | KD: 1061.0864\n",
      "Train Epoch: 044 Batch: 00049/00094 | Loss: 228.4746 | CE: 0.1475 | KD: 1060.8337\n",
      "Train Epoch: 044 Batch: 00050/00094 | Loss: 228.6589 | CE: 0.1988 | KD: 1061.4519\n",
      "Train Epoch: 044 Batch: 00051/00094 | Loss: 228.4098 | CE: 0.1039 | KD: 1060.7352\n",
      "Train Epoch: 044 Batch: 00052/00094 | Loss: 228.4113 | CE: 0.0931 | KD: 1060.7924\n",
      "Train Epoch: 044 Batch: 00053/00094 | Loss: 228.4181 | CE: 0.1243 | KD: 1060.6791\n",
      "Train Epoch: 044 Batch: 00054/00094 | Loss: 228.5311 | CE: 0.1118 | KD: 1061.2623\n",
      "Train Epoch: 044 Batch: 00055/00094 | Loss: 228.5242 | CE: 0.1383 | KD: 1061.1068\n",
      "Train Epoch: 044 Batch: 00056/00094 | Loss: 228.4704 | CE: 0.1077 | KD: 1060.9989\n",
      "Train Epoch: 044 Batch: 00057/00094 | Loss: 228.4253 | CE: 0.1075 | KD: 1060.7905\n",
      "Train Epoch: 044 Batch: 00058/00094 | Loss: 228.3756 | CE: 0.1050 | KD: 1060.5714\n",
      "Train Epoch: 044 Batch: 00059/00094 | Loss: 228.6496 | CE: 0.2568 | KD: 1061.1389\n",
      "Train Epoch: 044 Batch: 00060/00094 | Loss: 228.4498 | CE: 0.1599 | KD: 1060.6613\n",
      "Train Epoch: 044 Batch: 00061/00094 | Loss: 228.4978 | CE: 0.1032 | KD: 1061.1476\n",
      "Train Epoch: 044 Batch: 00062/00094 | Loss: 228.4660 | CE: 0.1180 | KD: 1060.9312\n",
      "Train Epoch: 044 Batch: 00063/00094 | Loss: 228.3472 | CE: 0.0919 | KD: 1060.5000\n",
      "Train Epoch: 044 Batch: 00064/00094 | Loss: 228.5801 | CE: 0.2039 | KD: 1061.0621\n",
      "Train Epoch: 044 Batch: 00065/00094 | Loss: 228.4489 | CE: 0.1527 | KD: 1060.6906\n",
      "Train Epoch: 044 Batch: 00066/00094 | Loss: 228.4351 | CE: 0.0979 | KD: 1060.8807\n",
      "Train Epoch: 044 Batch: 00067/00094 | Loss: 228.5403 | CE: 0.1678 | KD: 1061.0450\n",
      "Train Epoch: 044 Batch: 00068/00094 | Loss: 228.4813 | CE: 0.1168 | KD: 1061.0074\n",
      "Train Epoch: 044 Batch: 00069/00094 | Loss: 228.4344 | CE: 0.1092 | KD: 1060.8253\n",
      "Train Epoch: 044 Batch: 00070/00094 | Loss: 228.4242 | CE: 0.1207 | KD: 1060.7242\n",
      "Train Epoch: 044 Batch: 00071/00094 | Loss: 228.3380 | CE: 0.1021 | KD: 1060.4102\n",
      "Train Epoch: 044 Batch: 00072/00094 | Loss: 228.6056 | CE: 0.1577 | KD: 1061.3948\n",
      "Train Epoch: 044 Batch: 00073/00094 | Loss: 228.3357 | CE: 0.0788 | KD: 1060.5077\n",
      "Train Epoch: 044 Batch: 00074/00094 | Loss: 228.4127 | CE: 0.1236 | KD: 1060.6577\n",
      "Train Epoch: 044 Batch: 00075/00094 | Loss: 228.5118 | CE: 0.1160 | KD: 1061.1531\n",
      "Train Epoch: 044 Batch: 00076/00094 | Loss: 228.5738 | CE: 0.2549 | KD: 1060.7961\n",
      "Train Epoch: 044 Batch: 00077/00094 | Loss: 228.4689 | CE: 0.1755 | KD: 1060.6772\n",
      "Train Epoch: 044 Batch: 00078/00094 | Loss: 228.4066 | CE: 0.1029 | KD: 1060.7250\n",
      "Train Epoch: 044 Batch: 00079/00094 | Loss: 228.3769 | CE: 0.0986 | KD: 1060.6069\n",
      "Train Epoch: 044 Batch: 00080/00094 | Loss: 228.4984 | CE: 0.0985 | KD: 1061.1721\n",
      "Train Epoch: 044 Batch: 00081/00094 | Loss: 228.3962 | CE: 0.0804 | KD: 1060.7811\n",
      "Train Epoch: 044 Batch: 00082/00094 | Loss: 228.6169 | CE: 0.1918 | KD: 1061.2893\n",
      "Train Epoch: 044 Batch: 00083/00094 | Loss: 228.5140 | CE: 0.1525 | KD: 1060.9937\n",
      "Train Epoch: 044 Batch: 00084/00094 | Loss: 228.4910 | CE: 0.1394 | KD: 1060.9478\n",
      "Train Epoch: 044 Batch: 00085/00094 | Loss: 228.4415 | CE: 0.1041 | KD: 1060.8813\n",
      "Train Epoch: 044 Batch: 00086/00094 | Loss: 228.5761 | CE: 0.1954 | KD: 1061.0830\n",
      "Train Epoch: 044 Batch: 00087/00094 | Loss: 228.4561 | CE: 0.1442 | KD: 1060.7632\n",
      "Train Epoch: 044 Batch: 00088/00094 | Loss: 228.5374 | CE: 0.0980 | KD: 1061.3558\n",
      "Train Epoch: 044 Batch: 00089/00094 | Loss: 228.4495 | CE: 0.1078 | KD: 1060.9016\n",
      "Train Epoch: 044 Batch: 00090/00094 | Loss: 228.5593 | CE: 0.1807 | KD: 1061.0735\n",
      "Train Epoch: 044 Batch: 00091/00094 | Loss: 228.3789 | CE: 0.1005 | KD: 1060.6077\n",
      "Train Epoch: 044 Batch: 00092/00094 | Loss: 228.4672 | CE: 0.0968 | KD: 1061.0349\n",
      "Train Epoch: 044 Batch: 00093/00094 | Loss: 228.3868 | CE: 0.1245 | KD: 1060.5330\n",
      "Train Epoch: 044 Batch: 00094/00094 | Loss: 228.4426 | CE: 0.1640 | KD: 1060.6082\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1151 | acc:98.0000\n",
      "[VAL Acc] Target: 98.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5461 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0690 | acc:55.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0583 | acc:47.3282\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2585 | acc:53.7226\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6314 | acc:67.8373\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 67.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7777 | acc:68.3386\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7736 | acc:62.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.62%\n",
      "[VAL Acc] Avg 62.87%\n",
      "Train Epoch: 045 Batch: 00001/00094 | Loss: 228.3407 | CE: 0.0815 | KD: 1060.5186\n",
      "Train Epoch: 045 Batch: 00002/00094 | Loss: 228.4537 | CE: 0.1413 | KD: 1060.7656\n",
      "Train Epoch: 045 Batch: 00003/00094 | Loss: 228.4641 | CE: 0.1045 | KD: 1060.9846\n",
      "Train Epoch: 045 Batch: 00004/00094 | Loss: 228.4201 | CE: 0.0951 | KD: 1060.8240\n",
      "Train Epoch: 045 Batch: 00005/00094 | Loss: 228.5705 | CE: 0.1550 | KD: 1061.2448\n",
      "Train Epoch: 045 Batch: 00006/00094 | Loss: 228.4109 | CE: 0.1035 | KD: 1060.7419\n",
      "Train Epoch: 045 Batch: 00007/00094 | Loss: 228.4786 | CE: 0.1472 | KD: 1060.8541\n",
      "Train Epoch: 045 Batch: 00008/00094 | Loss: 228.3175 | CE: 0.1156 | KD: 1060.2520\n",
      "Train Epoch: 045 Batch: 00009/00094 | Loss: 228.3818 | CE: 0.0821 | KD: 1060.7068\n",
      "Train Epoch: 045 Batch: 00010/00094 | Loss: 228.3921 | CE: 0.1165 | KD: 1060.5947\n",
      "Train Epoch: 045 Batch: 00011/00094 | Loss: 228.4716 | CE: 0.1140 | KD: 1060.9756\n",
      "Train Epoch: 045 Batch: 00012/00094 | Loss: 228.4734 | CE: 0.0993 | KD: 1061.0525\n",
      "Train Epoch: 045 Batch: 00013/00094 | Loss: 228.5753 | CE: 0.1842 | KD: 1061.1311\n",
      "Train Epoch: 045 Batch: 00014/00094 | Loss: 228.4714 | CE: 0.1162 | KD: 1060.9646\n",
      "Train Epoch: 045 Batch: 00015/00094 | Loss: 228.4453 | CE: 0.1330 | KD: 1060.7653\n",
      "Train Epoch: 045 Batch: 00016/00094 | Loss: 228.5214 | CE: 0.0943 | KD: 1061.2981\n",
      "Train Epoch: 045 Batch: 00017/00094 | Loss: 228.4854 | CE: 0.1189 | KD: 1061.0166\n",
      "Train Epoch: 045 Batch: 00018/00094 | Loss: 228.4590 | CE: 0.1099 | KD: 1060.9358\n",
      "Train Epoch: 045 Batch: 00019/00094 | Loss: 228.3998 | CE: 0.0919 | KD: 1060.7446\n",
      "Train Epoch: 045 Batch: 00020/00094 | Loss: 228.4414 | CE: 0.1082 | KD: 1060.8622\n",
      "Train Epoch: 045 Batch: 00021/00094 | Loss: 228.4605 | CE: 0.1085 | KD: 1060.9497\n",
      "Train Epoch: 045 Batch: 00022/00094 | Loss: 228.3871 | CE: 0.1331 | KD: 1060.4943\n",
      "Train Epoch: 045 Batch: 00023/00094 | Loss: 228.4131 | CE: 0.1192 | KD: 1060.6797\n",
      "Train Epoch: 045 Batch: 00024/00094 | Loss: 228.4683 | CE: 0.1307 | KD: 1060.8827\n",
      "Train Epoch: 045 Batch: 00025/00094 | Loss: 228.5090 | CE: 0.0890 | KD: 1061.2659\n",
      "Train Epoch: 045 Batch: 00026/00094 | Loss: 228.3791 | CE: 0.1151 | KD: 1060.5408\n",
      "Train Epoch: 045 Batch: 00027/00094 | Loss: 228.4915 | CE: 0.1489 | KD: 1060.9056\n",
      "Train Epoch: 045 Batch: 00028/00094 | Loss: 228.5759 | CE: 0.1964 | KD: 1061.0773\n",
      "Train Epoch: 045 Batch: 00029/00094 | Loss: 228.3691 | CE: 0.0874 | KD: 1060.6229\n",
      "Train Epoch: 045 Batch: 00030/00094 | Loss: 228.6153 | CE: 0.1214 | KD: 1061.6088\n",
      "Train Epoch: 045 Batch: 00031/00094 | Loss: 228.5138 | CE: 0.1060 | KD: 1061.2087\n",
      "Train Epoch: 045 Batch: 00032/00094 | Loss: 228.4890 | CE: 0.1305 | KD: 1060.9796\n",
      "Train Epoch: 045 Batch: 00033/00094 | Loss: 228.4834 | CE: 0.1188 | KD: 1061.0084\n",
      "Train Epoch: 045 Batch: 00034/00094 | Loss: 228.4518 | CE: 0.1142 | KD: 1060.8826\n",
      "Train Epoch: 045 Batch: 00035/00094 | Loss: 228.4074 | CE: 0.1299 | KD: 1060.6034\n",
      "Train Epoch: 045 Batch: 00036/00094 | Loss: 228.4686 | CE: 0.1063 | KD: 1060.9973\n",
      "Train Epoch: 045 Batch: 00037/00094 | Loss: 228.4726 | CE: 0.1559 | KD: 1060.7855\n",
      "Train Epoch: 045 Batch: 00038/00094 | Loss: 228.4406 | CE: 0.1340 | KD: 1060.7386\n",
      "Train Epoch: 045 Batch: 00039/00094 | Loss: 228.4300 | CE: 0.1183 | KD: 1060.7623\n",
      "Train Epoch: 045 Batch: 00040/00094 | Loss: 228.4555 | CE: 0.0934 | KD: 1060.9961\n",
      "Train Epoch: 045 Batch: 00041/00094 | Loss: 228.5569 | CE: 0.1669 | KD: 1061.1259\n",
      "Train Epoch: 045 Batch: 00042/00094 | Loss: 228.4538 | CE: 0.0931 | KD: 1060.9899\n",
      "Train Epoch: 045 Batch: 00043/00094 | Loss: 228.4550 | CE: 0.0867 | KD: 1061.0250\n",
      "Train Epoch: 045 Batch: 00044/00094 | Loss: 228.4751 | CE: 0.0903 | KD: 1061.1021\n",
      "Train Epoch: 045 Batch: 00045/00094 | Loss: 228.4573 | CE: 0.1018 | KD: 1060.9656\n",
      "Train Epoch: 045 Batch: 00046/00094 | Loss: 228.4027 | CE: 0.1163 | KD: 1060.6444\n",
      "Train Epoch: 045 Batch: 00047/00094 | Loss: 228.3679 | CE: 0.0756 | KD: 1060.6722\n",
      "Train Epoch: 045 Batch: 00048/00094 | Loss: 228.5000 | CE: 0.1391 | KD: 1060.9907\n",
      "Train Epoch: 045 Batch: 00049/00094 | Loss: 228.3598 | CE: 0.1163 | KD: 1060.4454\n",
      "Train Epoch: 045 Batch: 00050/00094 | Loss: 228.4820 | CE: 0.1287 | KD: 1060.9554\n",
      "Train Epoch: 045 Batch: 00051/00094 | Loss: 228.5007 | CE: 0.1262 | KD: 1061.0540\n",
      "Train Epoch: 045 Batch: 00052/00094 | Loss: 228.3187 | CE: 0.0722 | KD: 1060.4596\n",
      "Train Epoch: 045 Batch: 00053/00094 | Loss: 228.4380 | CE: 0.1177 | KD: 1060.8020\n",
      "Train Epoch: 045 Batch: 00054/00094 | Loss: 228.3885 | CE: 0.1090 | KD: 1060.6127\n",
      "Train Epoch: 045 Batch: 00055/00094 | Loss: 228.4912 | CE: 0.1228 | KD: 1061.0260\n",
      "Train Epoch: 045 Batch: 00056/00094 | Loss: 228.3907 | CE: 0.0834 | KD: 1060.7422\n",
      "Train Epoch: 045 Batch: 00057/00094 | Loss: 228.5143 | CE: 0.1168 | KD: 1061.1605\n",
      "Train Epoch: 045 Batch: 00058/00094 | Loss: 228.4961 | CE: 0.1334 | KD: 1060.9993\n",
      "Train Epoch: 045 Batch: 00059/00094 | Loss: 228.5009 | CE: 0.1034 | KD: 1061.1609\n",
      "Train Epoch: 045 Batch: 00060/00094 | Loss: 228.4746 | CE: 0.1197 | KD: 1060.9631\n",
      "Train Epoch: 045 Batch: 00061/00094 | Loss: 228.4428 | CE: 0.1324 | KD: 1060.7565\n",
      "Train Epoch: 045 Batch: 00062/00094 | Loss: 228.4474 | CE: 0.0962 | KD: 1060.9457\n",
      "Train Epoch: 045 Batch: 00063/00094 | Loss: 228.3514 | CE: 0.1029 | KD: 1060.4684\n",
      "Train Epoch: 045 Batch: 00064/00094 | Loss: 228.5186 | CE: 0.1996 | KD: 1060.7959\n",
      "Train Epoch: 045 Batch: 00065/00094 | Loss: 228.4980 | CE: 0.1128 | KD: 1061.1035\n",
      "Train Epoch: 045 Batch: 00066/00094 | Loss: 228.5577 | CE: 0.1093 | KD: 1061.3972\n",
      "Train Epoch: 045 Batch: 00067/00094 | Loss: 228.4209 | CE: 0.0988 | KD: 1060.8102\n",
      "Train Epoch: 045 Batch: 00068/00094 | Loss: 228.4227 | CE: 0.0996 | KD: 1060.8156\n",
      "Train Epoch: 045 Batch: 00069/00094 | Loss: 228.4029 | CE: 0.1226 | KD: 1060.6162\n",
      "Train Epoch: 045 Batch: 00070/00094 | Loss: 228.5035 | CE: 0.2050 | KD: 1060.7009\n",
      "Train Epoch: 045 Batch: 00071/00094 | Loss: 228.5652 | CE: 0.1944 | KD: 1061.0367\n",
      "Train Epoch: 045 Batch: 00072/00094 | Loss: 228.4806 | CE: 0.1019 | KD: 1061.0735\n",
      "Train Epoch: 045 Batch: 00073/00094 | Loss: 228.3686 | CE: 0.0824 | KD: 1060.6439\n",
      "Train Epoch: 045 Batch: 00074/00094 | Loss: 228.4522 | CE: 0.0951 | KD: 1060.9734\n",
      "Train Epoch: 045 Batch: 00075/00094 | Loss: 228.4974 | CE: 0.1175 | KD: 1061.0795\n",
      "Train Epoch: 045 Batch: 00076/00094 | Loss: 228.4599 | CE: 0.1407 | KD: 1060.7971\n",
      "Train Epoch: 045 Batch: 00077/00094 | Loss: 228.5091 | CE: 0.1360 | KD: 1061.0477\n",
      "Train Epoch: 045 Batch: 00078/00094 | Loss: 228.5383 | CE: 0.1906 | KD: 1060.9298\n",
      "Train Epoch: 045 Batch: 00079/00094 | Loss: 228.4587 | CE: 0.1197 | KD: 1060.8892\n",
      "Train Epoch: 045 Batch: 00080/00094 | Loss: 228.4939 | CE: 0.1347 | KD: 1060.9828\n",
      "Train Epoch: 045 Batch: 00081/00094 | Loss: 228.4411 | CE: 0.1030 | KD: 1060.8850\n",
      "Train Epoch: 045 Batch: 00082/00094 | Loss: 228.4347 | CE: 0.1043 | KD: 1060.8492\n",
      "Train Epoch: 045 Batch: 00083/00094 | Loss: 228.4426 | CE: 0.1101 | KD: 1060.8589\n",
      "Train Epoch: 045 Batch: 00084/00094 | Loss: 228.4307 | CE: 0.1051 | KD: 1060.8271\n",
      "Train Epoch: 045 Batch: 00085/00094 | Loss: 228.4773 | CE: 0.1184 | KD: 1060.9818\n",
      "Train Epoch: 045 Batch: 00086/00094 | Loss: 228.4402 | CE: 0.1133 | KD: 1060.8330\n",
      "Train Epoch: 045 Batch: 00087/00094 | Loss: 228.4600 | CE: 0.1268 | KD: 1060.8624\n",
      "Train Epoch: 045 Batch: 00088/00094 | Loss: 228.5495 | CE: 0.0969 | KD: 1061.4171\n",
      "Train Epoch: 045 Batch: 00089/00094 | Loss: 228.4501 | CE: 0.1337 | KD: 1060.7841\n",
      "Train Epoch: 045 Batch: 00090/00094 | Loss: 228.4248 | CE: 0.1147 | KD: 1060.7549\n",
      "Train Epoch: 045 Batch: 00091/00094 | Loss: 228.4316 | CE: 0.0997 | KD: 1060.8562\n",
      "Train Epoch: 045 Batch: 00092/00094 | Loss: 228.5180 | CE: 0.1358 | KD: 1061.0898\n",
      "Train Epoch: 045 Batch: 00093/00094 | Loss: 228.3779 | CE: 0.0699 | KD: 1060.7449\n",
      "Train Epoch: 045 Batch: 00094/00094 | Loss: 228.6764 | CE: 0.2570 | KD: 1061.2623\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1115 | acc:98.1000\n",
      "[VAL Acc] Target: 98.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5587 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0696 | acc:54.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1144 | acc:42.1756\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 42.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2256 | acc:55.8386\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6004 | acc:71.5342\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7510 | acc:69.4749\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7587 | acc:63.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 63.38%\n",
      "[VAL Acc] Avg 63.08%\n",
      "Train Epoch: 046 Batch: 00001/00094 | Loss: 205.6086 | CE: 0.1016 | KD: 1060.8993\n",
      "Train Epoch: 046 Batch: 00002/00094 | Loss: 205.7506 | CE: 0.1539 | KD: 1061.3623\n",
      "Train Epoch: 046 Batch: 00003/00094 | Loss: 205.6540 | CE: 0.1600 | KD: 1060.8314\n",
      "Train Epoch: 046 Batch: 00004/00094 | Loss: 205.5692 | CE: 0.1040 | KD: 1060.6832\n",
      "Train Epoch: 046 Batch: 00005/00094 | Loss: 205.6119 | CE: 0.1460 | KD: 1060.6870\n",
      "Train Epoch: 046 Batch: 00006/00094 | Loss: 205.5938 | CE: 0.1125 | KD: 1060.7659\n",
      "Train Epoch: 046 Batch: 00007/00094 | Loss: 205.6187 | CE: 0.1166 | KD: 1060.8735\n",
      "Train Epoch: 046 Batch: 00008/00094 | Loss: 205.7334 | CE: 0.1484 | KD: 1061.3015\n",
      "Train Epoch: 046 Batch: 00009/00094 | Loss: 205.6794 | CE: 0.0858 | KD: 1061.3459\n",
      "Train Epoch: 046 Batch: 00010/00094 | Loss: 205.6889 | CE: 0.1534 | KD: 1061.0461\n",
      "Train Epoch: 046 Batch: 00011/00094 | Loss: 205.6245 | CE: 0.1034 | KD: 1060.9719\n",
      "Train Epoch: 046 Batch: 00012/00094 | Loss: 205.6168 | CE: 0.1301 | KD: 1060.7946\n",
      "Train Epoch: 046 Batch: 00013/00094 | Loss: 205.6263 | CE: 0.0978 | KD: 1061.0101\n",
      "Train Epoch: 046 Batch: 00014/00094 | Loss: 205.5528 | CE: 0.0865 | KD: 1060.6887\n",
      "Train Epoch: 046 Batch: 00015/00094 | Loss: 205.7234 | CE: 0.1223 | KD: 1061.3848\n",
      "Train Epoch: 046 Batch: 00016/00094 | Loss: 205.6255 | CE: 0.1096 | KD: 1060.9449\n",
      "Train Epoch: 046 Batch: 00017/00094 | Loss: 205.5868 | CE: 0.1165 | KD: 1060.7096\n",
      "Train Epoch: 046 Batch: 00018/00094 | Loss: 205.6530 | CE: 0.0980 | KD: 1061.1466\n",
      "Train Epoch: 046 Batch: 00019/00094 | Loss: 205.6050 | CE: 0.0983 | KD: 1060.8973\n",
      "Train Epoch: 046 Batch: 00020/00094 | Loss: 205.6067 | CE: 0.1305 | KD: 1060.7399\n",
      "Train Epoch: 046 Batch: 00021/00094 | Loss: 205.6093 | CE: 0.1048 | KD: 1060.8861\n",
      "Train Epoch: 046 Batch: 00022/00094 | Loss: 205.6917 | CE: 0.1219 | KD: 1061.2230\n",
      "Train Epoch: 046 Batch: 00023/00094 | Loss: 205.5880 | CE: 0.0877 | KD: 1060.8641\n",
      "Train Epoch: 046 Batch: 00024/00094 | Loss: 205.6431 | CE: 0.1054 | KD: 1061.0571\n",
      "Train Epoch: 046 Batch: 00025/00094 | Loss: 205.5485 | CE: 0.0972 | KD: 1060.6112\n",
      "Train Epoch: 046 Batch: 00026/00094 | Loss: 205.5729 | CE: 0.0713 | KD: 1060.8715\n",
      "Train Epoch: 046 Batch: 00027/00094 | Loss: 205.6611 | CE: 0.0973 | KD: 1061.1924\n",
      "Train Epoch: 046 Batch: 00028/00094 | Loss: 205.6206 | CE: 0.1032 | KD: 1060.9525\n",
      "Train Epoch: 046 Batch: 00029/00094 | Loss: 205.6720 | CE: 0.1003 | KD: 1061.2332\n",
      "Train Epoch: 046 Batch: 00030/00094 | Loss: 205.6953 | CE: 0.1620 | KD: 1061.0347\n",
      "Train Epoch: 046 Batch: 00031/00094 | Loss: 205.7400 | CE: 0.1325 | KD: 1061.4180\n",
      "Train Epoch: 046 Batch: 00032/00094 | Loss: 205.7485 | CE: 0.1642 | KD: 1061.2981\n",
      "Train Epoch: 046 Batch: 00033/00094 | Loss: 205.5803 | CE: 0.1241 | KD: 1060.6371\n",
      "Train Epoch: 046 Batch: 00034/00094 | Loss: 205.6576 | CE: 0.1100 | KD: 1061.1089\n",
      "Train Epoch: 046 Batch: 00035/00094 | Loss: 205.5497 | CE: 0.0847 | KD: 1060.6818\n",
      "Train Epoch: 046 Batch: 00036/00094 | Loss: 205.7571 | CE: 0.1536 | KD: 1061.3971\n",
      "Train Epoch: 046 Batch: 00037/00094 | Loss: 205.6042 | CE: 0.1019 | KD: 1060.8748\n",
      "Train Epoch: 046 Batch: 00038/00094 | Loss: 205.6094 | CE: 0.0945 | KD: 1060.9398\n",
      "Train Epoch: 046 Batch: 00039/00094 | Loss: 205.6009 | CE: 0.0983 | KD: 1060.8761\n",
      "Train Epoch: 046 Batch: 00040/00094 | Loss: 205.6114 | CE: 0.1186 | KD: 1060.8257\n",
      "Train Epoch: 046 Batch: 00041/00094 | Loss: 205.5483 | CE: 0.1080 | KD: 1060.5547\n",
      "Train Epoch: 046 Batch: 00042/00094 | Loss: 205.5859 | CE: 0.1207 | KD: 1060.6833\n",
      "Train Epoch: 046 Batch: 00043/00094 | Loss: 205.6359 | CE: 0.1334 | KD: 1060.8755\n",
      "Train Epoch: 046 Batch: 00044/00094 | Loss: 205.6526 | CE: 0.1243 | KD: 1061.0092\n",
      "Train Epoch: 046 Batch: 00045/00094 | Loss: 205.5958 | CE: 0.0957 | KD: 1060.8632\n",
      "Train Epoch: 046 Batch: 00046/00094 | Loss: 205.6219 | CE: 0.0851 | KD: 1061.0533\n",
      "Train Epoch: 046 Batch: 00047/00094 | Loss: 205.6227 | CE: 0.1015 | KD: 1060.9727\n",
      "Train Epoch: 046 Batch: 00048/00094 | Loss: 205.5411 | CE: 0.1074 | KD: 1060.5206\n",
      "Train Epoch: 046 Batch: 00049/00094 | Loss: 205.6570 | CE: 0.1546 | KD: 1060.8754\n",
      "Train Epoch: 046 Batch: 00050/00094 | Loss: 205.5376 | CE: 0.0780 | KD: 1060.6539\n",
      "Train Epoch: 046 Batch: 00051/00094 | Loss: 205.5921 | CE: 0.0926 | KD: 1060.8600\n",
      "Train Epoch: 046 Batch: 00052/00094 | Loss: 205.6096 | CE: 0.1126 | KD: 1060.8475\n",
      "Train Epoch: 046 Batch: 00053/00094 | Loss: 205.6729 | CE: 0.1076 | KD: 1061.2001\n",
      "Train Epoch: 046 Batch: 00054/00094 | Loss: 205.6896 | CE: 0.1154 | KD: 1061.2457\n",
      "Train Epoch: 046 Batch: 00055/00094 | Loss: 205.5907 | CE: 0.0873 | KD: 1060.8806\n",
      "Train Epoch: 046 Batch: 00056/00094 | Loss: 205.6659 | CE: 0.1287 | KD: 1061.0551\n",
      "Train Epoch: 046 Batch: 00057/00094 | Loss: 205.6399 | CE: 0.1260 | KD: 1060.9348\n",
      "Train Epoch: 046 Batch: 00058/00094 | Loss: 205.6244 | CE: 0.1015 | KD: 1060.9810\n",
      "Train Epoch: 046 Batch: 00059/00094 | Loss: 205.5748 | CE: 0.1020 | KD: 1060.7224\n",
      "Train Epoch: 046 Batch: 00060/00094 | Loss: 205.6395 | CE: 0.1013 | KD: 1061.0599\n",
      "Train Epoch: 046 Batch: 00061/00094 | Loss: 205.6438 | CE: 0.1305 | KD: 1060.9312\n",
      "Train Epoch: 046 Batch: 00062/00094 | Loss: 205.6422 | CE: 0.0950 | KD: 1061.1068\n",
      "Train Epoch: 046 Batch: 00063/00094 | Loss: 205.5321 | CE: 0.0817 | KD: 1060.6066\n",
      "Train Epoch: 046 Batch: 00064/00094 | Loss: 205.5605 | CE: 0.0895 | KD: 1060.7133\n",
      "Train Epoch: 046 Batch: 00065/00094 | Loss: 205.6858 | CE: 0.1462 | KD: 1061.0676\n",
      "Train Epoch: 046 Batch: 00066/00094 | Loss: 205.7112 | CE: 0.1937 | KD: 1060.9532\n",
      "Train Epoch: 046 Batch: 00067/00094 | Loss: 205.6402 | CE: 0.1577 | KD: 1060.7727\n",
      "Train Epoch: 046 Batch: 00068/00094 | Loss: 205.5565 | CE: 0.0842 | KD: 1060.7197\n",
      "Train Epoch: 046 Batch: 00069/00094 | Loss: 205.6332 | CE: 0.0975 | KD: 1061.0472\n",
      "Train Epoch: 046 Batch: 00070/00094 | Loss: 205.6418 | CE: 0.1210 | KD: 1060.9700\n",
      "Train Epoch: 046 Batch: 00071/00094 | Loss: 205.6593 | CE: 0.1258 | KD: 1061.0356\n",
      "Train Epoch: 046 Batch: 00072/00094 | Loss: 205.7119 | CE: 0.1397 | KD: 1061.2358\n",
      "Train Epoch: 046 Batch: 00073/00094 | Loss: 205.5898 | CE: 0.1021 | KD: 1060.7997\n",
      "Train Epoch: 046 Batch: 00074/00094 | Loss: 205.6394 | CE: 0.0907 | KD: 1061.1140\n",
      "Train Epoch: 046 Batch: 00075/00094 | Loss: 205.6883 | CE: 0.1210 | KD: 1061.2103\n",
      "Train Epoch: 046 Batch: 00076/00094 | Loss: 205.5455 | CE: 0.1015 | KD: 1060.5735\n",
      "Train Epoch: 046 Batch: 00077/00094 | Loss: 205.9001 | CE: 0.1420 | KD: 1062.1954\n",
      "Train Epoch: 046 Batch: 00078/00094 | Loss: 205.8227 | CE: 0.2171 | KD: 1061.4078\n",
      "Train Epoch: 046 Batch: 00079/00094 | Loss: 205.7860 | CE: 0.2084 | KD: 1061.2633\n",
      "Train Epoch: 046 Batch: 00080/00094 | Loss: 205.6116 | CE: 0.1592 | KD: 1060.6171\n",
      "Train Epoch: 046 Batch: 00081/00094 | Loss: 205.5761 | CE: 0.1225 | KD: 1060.6235\n",
      "Train Epoch: 046 Batch: 00082/00094 | Loss: 205.6923 | CE: 0.1138 | KD: 1061.2686\n",
      "Train Epoch: 046 Batch: 00083/00094 | Loss: 205.5757 | CE: 0.1495 | KD: 1060.4817\n",
      "Train Epoch: 046 Batch: 00084/00094 | Loss: 205.6613 | CE: 0.1501 | KD: 1060.9208\n",
      "Train Epoch: 046 Batch: 00085/00094 | Loss: 205.6123 | CE: 0.0978 | KD: 1060.9379\n",
      "Train Epoch: 046 Batch: 00086/00094 | Loss: 205.5998 | CE: 0.0848 | KD: 1060.9403\n",
      "Train Epoch: 046 Batch: 00087/00094 | Loss: 205.6677 | CE: 0.1686 | KD: 1060.8582\n",
      "Train Epoch: 046 Batch: 00088/00094 | Loss: 205.5901 | CE: 0.1043 | KD: 1060.7898\n",
      "Train Epoch: 046 Batch: 00089/00094 | Loss: 205.6006 | CE: 0.1168 | KD: 1060.7792\n",
      "Train Epoch: 046 Batch: 00090/00094 | Loss: 205.5148 | CE: 0.0949 | KD: 1060.4495\n",
      "Train Epoch: 046 Batch: 00091/00094 | Loss: 205.6518 | CE: 0.1329 | KD: 1060.9604\n",
      "Train Epoch: 046 Batch: 00092/00094 | Loss: 205.6203 | CE: 0.1025 | KD: 1060.9546\n",
      "Train Epoch: 046 Batch: 00093/00094 | Loss: 205.7326 | CE: 0.1928 | KD: 1061.0684\n",
      "Train Epoch: 046 Batch: 00094/00094 | Loss: 205.6220 | CE: 0.0893 | KD: 1061.0315\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1347 | acc:97.1500\n",
      "[VAL Acc] Target: 97.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7512 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2480 | acc:52.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1501 | acc:44.4656\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4645 | acc:52.2335\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4900 | acc:76.7098\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0107 | acc:64.0674\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9035 | acc:58.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.13%\n",
      "[VAL Acc] Avg 61.91%\n",
      "Train Epoch: 047 Batch: 00001/00094 | Loss: 205.6741 | CE: 0.1242 | KD: 1061.1206\n",
      "Train Epoch: 047 Batch: 00002/00094 | Loss: 205.5614 | CE: 0.1101 | KD: 1060.6112\n",
      "Train Epoch: 047 Batch: 00003/00094 | Loss: 205.6617 | CE: 0.1188 | KD: 1061.0845\n",
      "Train Epoch: 047 Batch: 00004/00094 | Loss: 205.5596 | CE: 0.1013 | KD: 1060.6481\n",
      "Train Epoch: 047 Batch: 00005/00094 | Loss: 205.6558 | CE: 0.0996 | KD: 1061.1533\n",
      "Train Epoch: 047 Batch: 00006/00094 | Loss: 205.6783 | CE: 0.1407 | KD: 1061.0566\n",
      "Train Epoch: 047 Batch: 00007/00094 | Loss: 205.6341 | CE: 0.1128 | KD: 1060.9727\n",
      "Train Epoch: 047 Batch: 00008/00094 | Loss: 205.6221 | CE: 0.1457 | KD: 1060.7412\n",
      "Train Epoch: 047 Batch: 00009/00094 | Loss: 205.6520 | CE: 0.1391 | KD: 1060.9298\n",
      "Train Epoch: 047 Batch: 00010/00094 | Loss: 205.6277 | CE: 0.0922 | KD: 1061.0459\n",
      "Train Epoch: 047 Batch: 00011/00094 | Loss: 205.6344 | CE: 0.0990 | KD: 1061.0457\n",
      "Train Epoch: 047 Batch: 00012/00094 | Loss: 205.8238 | CE: 0.2524 | KD: 1061.2312\n",
      "Train Epoch: 047 Batch: 00013/00094 | Loss: 205.5744 | CE: 0.0957 | KD: 1060.7533\n",
      "Train Epoch: 047 Batch: 00014/00094 | Loss: 205.5815 | CE: 0.1257 | KD: 1060.6348\n",
      "Train Epoch: 047 Batch: 00015/00094 | Loss: 205.6268 | CE: 0.1510 | KD: 1060.7380\n",
      "Train Epoch: 047 Batch: 00016/00094 | Loss: 205.5215 | CE: 0.1048 | KD: 1060.4329\n",
      "Train Epoch: 047 Batch: 00017/00094 | Loss: 205.5954 | CE: 0.0862 | KD: 1060.9105\n",
      "Train Epoch: 047 Batch: 00018/00094 | Loss: 205.5866 | CE: 0.0934 | KD: 1060.8280\n",
      "Train Epoch: 047 Batch: 00019/00094 | Loss: 205.5539 | CE: 0.1030 | KD: 1060.6094\n",
      "Train Epoch: 047 Batch: 00020/00094 | Loss: 205.6954 | CE: 0.1170 | KD: 1061.2675\n",
      "Train Epoch: 047 Batch: 00021/00094 | Loss: 205.6569 | CE: 0.1262 | KD: 1061.0215\n",
      "Train Epoch: 047 Batch: 00022/00094 | Loss: 205.6373 | CE: 0.0913 | KD: 1061.1008\n",
      "Train Epoch: 047 Batch: 00023/00094 | Loss: 205.6896 | CE: 0.1569 | KD: 1061.0317\n",
      "Train Epoch: 047 Batch: 00024/00094 | Loss: 205.5553 | CE: 0.0824 | KD: 1060.7228\n",
      "Train Epoch: 047 Batch: 00025/00094 | Loss: 205.5990 | CE: 0.0975 | KD: 1060.8704\n",
      "Train Epoch: 047 Batch: 00026/00094 | Loss: 205.6741 | CE: 0.1784 | KD: 1060.8406\n",
      "Train Epoch: 047 Batch: 00027/00094 | Loss: 205.6336 | CE: 0.1208 | KD: 1060.9294\n",
      "Train Epoch: 047 Batch: 00028/00094 | Loss: 205.6791 | CE: 0.0868 | KD: 1061.3397\n",
      "Train Epoch: 047 Batch: 00029/00094 | Loss: 205.5337 | CE: 0.1089 | KD: 1060.4749\n",
      "Train Epoch: 047 Batch: 00030/00094 | Loss: 205.8045 | CE: 0.2215 | KD: 1061.2914\n",
      "Train Epoch: 047 Batch: 00031/00094 | Loss: 205.5867 | CE: 0.0878 | KD: 1060.8575\n",
      "Train Epoch: 047 Batch: 00032/00094 | Loss: 205.7209 | CE: 0.1106 | KD: 1061.4320\n",
      "Train Epoch: 047 Batch: 00033/00094 | Loss: 205.6375 | CE: 0.0909 | KD: 1061.1038\n",
      "Train Epoch: 047 Batch: 00034/00094 | Loss: 205.5473 | CE: 0.1137 | KD: 1060.5203\n",
      "Train Epoch: 047 Batch: 00035/00094 | Loss: 205.7436 | CE: 0.2127 | KD: 1061.0228\n",
      "Train Epoch: 047 Batch: 00036/00094 | Loss: 205.5487 | CE: 0.1111 | KD: 1060.5410\n",
      "Train Epoch: 047 Batch: 00037/00094 | Loss: 205.6132 | CE: 0.1409 | KD: 1060.7198\n",
      "Train Epoch: 047 Batch: 00038/00094 | Loss: 205.5896 | CE: 0.1048 | KD: 1060.7844\n",
      "Train Epoch: 047 Batch: 00039/00094 | Loss: 205.6228 | CE: 0.0956 | KD: 1061.0031\n",
      "Train Epoch: 047 Batch: 00040/00094 | Loss: 205.6638 | CE: 0.1035 | KD: 1061.1743\n",
      "Train Epoch: 047 Batch: 00041/00094 | Loss: 205.6507 | CE: 0.1085 | KD: 1061.0807\n",
      "Train Epoch: 047 Batch: 00042/00094 | Loss: 205.6248 | CE: 0.1227 | KD: 1060.8734\n",
      "Train Epoch: 047 Batch: 00043/00094 | Loss: 205.6025 | CE: 0.0959 | KD: 1060.8967\n",
      "Train Epoch: 047 Batch: 00044/00094 | Loss: 205.6003 | CE: 0.1330 | KD: 1060.6940\n",
      "Train Epoch: 047 Batch: 00045/00094 | Loss: 205.7002 | CE: 0.1195 | KD: 1061.2794\n",
      "Train Epoch: 047 Batch: 00046/00094 | Loss: 205.6141 | CE: 0.1019 | KD: 1060.9255\n",
      "Train Epoch: 047 Batch: 00047/00094 | Loss: 205.6396 | CE: 0.1009 | KD: 1061.0625\n",
      "Train Epoch: 047 Batch: 00048/00094 | Loss: 205.5574 | CE: 0.0922 | KD: 1060.6835\n",
      "Train Epoch: 047 Batch: 00049/00094 | Loss: 205.6835 | CE: 0.1038 | KD: 1061.2743\n",
      "Train Epoch: 047 Batch: 00050/00094 | Loss: 205.7299 | CE: 0.1411 | KD: 1061.3213\n",
      "Train Epoch: 047 Batch: 00051/00094 | Loss: 205.7333 | CE: 0.1621 | KD: 1061.2305\n",
      "Train Epoch: 047 Batch: 00052/00094 | Loss: 205.6943 | CE: 0.1706 | KD: 1060.9856\n",
      "Train Epoch: 047 Batch: 00053/00094 | Loss: 205.6510 | CE: 0.1016 | KD: 1061.1178\n",
      "Train Epoch: 047 Batch: 00054/00094 | Loss: 205.6094 | CE: 0.1237 | KD: 1060.7892\n",
      "Train Epoch: 047 Batch: 00055/00094 | Loss: 205.5329 | CE: 0.0964 | KD: 1060.5350\n",
      "Train Epoch: 047 Batch: 00056/00094 | Loss: 205.5516 | CE: 0.1122 | KD: 1060.5499\n",
      "Train Epoch: 047 Batch: 00057/00094 | Loss: 205.6279 | CE: 0.0901 | KD: 1061.0582\n",
      "Train Epoch: 047 Batch: 00058/00094 | Loss: 205.6937 | CE: 0.1260 | KD: 1061.2128\n",
      "Train Epoch: 047 Batch: 00059/00094 | Loss: 205.6038 | CE: 0.1380 | KD: 1060.6862\n",
      "Train Epoch: 047 Batch: 00060/00094 | Loss: 205.7050 | CE: 0.1325 | KD: 1061.2372\n",
      "Train Epoch: 047 Batch: 00061/00094 | Loss: 205.6648 | CE: 0.1097 | KD: 1061.1473\n",
      "Train Epoch: 047 Batch: 00062/00094 | Loss: 205.6889 | CE: 0.1377 | KD: 1061.1270\n",
      "Train Epoch: 047 Batch: 00063/00094 | Loss: 205.7595 | CE: 0.2118 | KD: 1061.1086\n",
      "Train Epoch: 047 Batch: 00064/00094 | Loss: 205.6048 | CE: 0.0884 | KD: 1060.9478\n",
      "Train Epoch: 047 Batch: 00065/00094 | Loss: 205.6895 | CE: 0.1220 | KD: 1061.2112\n",
      "Train Epoch: 047 Batch: 00066/00094 | Loss: 205.6681 | CE: 0.1199 | KD: 1061.1117\n",
      "Train Epoch: 047 Batch: 00067/00094 | Loss: 205.5801 | CE: 0.1251 | KD: 1060.6301\n",
      "Train Epoch: 047 Batch: 00068/00094 | Loss: 205.6122 | CE: 0.1559 | KD: 1060.6377\n",
      "Train Epoch: 047 Batch: 00069/00094 | Loss: 205.6174 | CE: 0.0878 | KD: 1061.0159\n",
      "Train Epoch: 047 Batch: 00070/00094 | Loss: 205.6319 | CE: 0.1409 | KD: 1060.8164\n",
      "Train Epoch: 047 Batch: 00071/00094 | Loss: 205.5573 | CE: 0.0897 | KD: 1060.6959\n",
      "Train Epoch: 047 Batch: 00072/00094 | Loss: 205.5328 | CE: 0.0817 | KD: 1060.6105\n",
      "Train Epoch: 047 Batch: 00073/00094 | Loss: 205.5731 | CE: 0.1065 | KD: 1060.6904\n",
      "Train Epoch: 047 Batch: 00074/00094 | Loss: 205.5991 | CE: 0.0985 | KD: 1060.8657\n",
      "Train Epoch: 047 Batch: 00075/00094 | Loss: 205.6328 | CE: 0.1015 | KD: 1061.0242\n",
      "Train Epoch: 047 Batch: 00076/00094 | Loss: 205.6042 | CE: 0.0815 | KD: 1060.9800\n",
      "Train Epoch: 047 Batch: 00077/00094 | Loss: 205.5587 | CE: 0.1024 | KD: 1060.6375\n",
      "Train Epoch: 047 Batch: 00078/00094 | Loss: 205.6360 | CE: 0.1444 | KD: 1060.8196\n",
      "Train Epoch: 047 Batch: 00079/00094 | Loss: 205.7260 | CE: 0.1585 | KD: 1061.2111\n",
      "Train Epoch: 047 Batch: 00080/00094 | Loss: 205.7199 | CE: 0.1316 | KD: 1061.3187\n",
      "Train Epoch: 047 Batch: 00081/00094 | Loss: 205.5840 | CE: 0.0919 | KD: 1060.8220\n",
      "Train Epoch: 047 Batch: 00082/00094 | Loss: 205.6198 | CE: 0.0948 | KD: 1060.9915\n",
      "Train Epoch: 047 Batch: 00083/00094 | Loss: 205.6453 | CE: 0.0909 | KD: 1061.1440\n",
      "Train Epoch: 047 Batch: 00084/00094 | Loss: 205.6103 | CE: 0.1116 | KD: 1060.8560\n",
      "Train Epoch: 047 Batch: 00085/00094 | Loss: 205.6472 | CE: 0.1233 | KD: 1060.9863\n",
      "Train Epoch: 047 Batch: 00086/00094 | Loss: 205.7016 | CE: 0.1177 | KD: 1061.2960\n",
      "Train Epoch: 047 Batch: 00087/00094 | Loss: 205.5942 | CE: 0.1047 | KD: 1060.8087\n",
      "Train Epoch: 047 Batch: 00088/00094 | Loss: 205.6936 | CE: 0.1316 | KD: 1061.1826\n",
      "Train Epoch: 047 Batch: 00089/00094 | Loss: 205.5648 | CE: 0.0995 | KD: 1060.6837\n",
      "Train Epoch: 047 Batch: 00090/00094 | Loss: 205.6040 | CE: 0.0904 | KD: 1060.9332\n",
      "Train Epoch: 047 Batch: 00091/00094 | Loss: 205.6188 | CE: 0.0874 | KD: 1061.0253\n",
      "Train Epoch: 047 Batch: 00092/00094 | Loss: 205.6353 | CE: 0.1094 | KD: 1060.9968\n",
      "Train Epoch: 047 Batch: 00093/00094 | Loss: 205.5125 | CE: 0.0773 | KD: 1060.5286\n",
      "Train Epoch: 047 Batch: 00094/00094 | Loss: 205.6093 | CE: 0.1676 | KD: 1060.5621\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1087 | acc:98.3000\n",
      "[VAL Acc] Target: 98.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6219 | acc:49.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0772 | acc:53.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1228 | acc:47.1374\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3046 | acc:54.6630\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6514 | acc:67.1904\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 67.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8753 | acc:66.8103\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8339 | acc:59.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.44%\n",
      "[VAL Acc] Avg 62.13%\n",
      "Train Epoch: 048 Batch: 00001/00094 | Loss: 205.5953 | CE: 0.0925 | KD: 1060.8773\n",
      "Train Epoch: 048 Batch: 00002/00094 | Loss: 205.6412 | CE: 0.1329 | KD: 1060.9055\n",
      "Train Epoch: 048 Batch: 00003/00094 | Loss: 205.6039 | CE: 0.1160 | KD: 1060.8004\n",
      "Train Epoch: 048 Batch: 00004/00094 | Loss: 205.6518 | CE: 0.1467 | KD: 1060.8893\n",
      "Train Epoch: 048 Batch: 00005/00094 | Loss: 205.7162 | CE: 0.1703 | KD: 1061.0995\n",
      "Train Epoch: 048 Batch: 00006/00094 | Loss: 205.6394 | CE: 0.0767 | KD: 1061.1863\n",
      "Train Epoch: 048 Batch: 00007/00094 | Loss: 205.5822 | CE: 0.1396 | KD: 1060.5662\n",
      "Train Epoch: 048 Batch: 00008/00094 | Loss: 205.5508 | CE: 0.0708 | KD: 1060.7593\n",
      "Train Epoch: 048 Batch: 00009/00094 | Loss: 205.7239 | CE: 0.2009 | KD: 1060.9816\n",
      "Train Epoch: 048 Batch: 00010/00094 | Loss: 205.8270 | CE: 0.2453 | KD: 1061.2848\n",
      "Train Epoch: 048 Batch: 00011/00094 | Loss: 205.6749 | CE: 0.0853 | KD: 1061.3251\n",
      "Train Epoch: 048 Batch: 00012/00094 | Loss: 205.5867 | CE: 0.0799 | KD: 1060.8976\n",
      "Train Epoch: 048 Batch: 00013/00094 | Loss: 205.6274 | CE: 0.0817 | KD: 1061.0990\n",
      "Train Epoch: 048 Batch: 00014/00094 | Loss: 205.6342 | CE: 0.1330 | KD: 1060.8688\n",
      "Train Epoch: 048 Batch: 00015/00094 | Loss: 205.6245 | CE: 0.0923 | KD: 1061.0289\n",
      "Train Epoch: 048 Batch: 00016/00094 | Loss: 205.6154 | CE: 0.1033 | KD: 1060.9252\n",
      "Train Epoch: 048 Batch: 00017/00094 | Loss: 205.6625 | CE: 0.1234 | KD: 1061.0651\n",
      "Train Epoch: 048 Batch: 00018/00094 | Loss: 205.7508 | CE: 0.1258 | KD: 1061.5082\n",
      "Train Epoch: 048 Batch: 00019/00094 | Loss: 205.5661 | CE: 0.0835 | KD: 1060.7733\n",
      "Train Epoch: 048 Batch: 00020/00094 | Loss: 205.6629 | CE: 0.0894 | KD: 1061.2422\n",
      "Train Epoch: 048 Batch: 00021/00094 | Loss: 205.6202 | CE: 0.1117 | KD: 1060.9070\n",
      "Train Epoch: 048 Batch: 00022/00094 | Loss: 205.6479 | CE: 0.1313 | KD: 1060.9485\n",
      "Train Epoch: 048 Batch: 00023/00094 | Loss: 205.6495 | CE: 0.1255 | KD: 1060.9866\n",
      "Train Epoch: 048 Batch: 00024/00094 | Loss: 205.6474 | CE: 0.1000 | KD: 1061.1080\n",
      "Train Epoch: 048 Batch: 00025/00094 | Loss: 205.6497 | CE: 0.1846 | KD: 1060.6825\n",
      "Train Epoch: 048 Batch: 00026/00094 | Loss: 205.6452 | CE: 0.1441 | KD: 1060.8688\n",
      "Train Epoch: 048 Batch: 00027/00094 | Loss: 205.6260 | CE: 0.0957 | KD: 1061.0193\n",
      "Train Epoch: 048 Batch: 00028/00094 | Loss: 205.5892 | CE: 0.0879 | KD: 1060.8694\n",
      "Train Epoch: 048 Batch: 00029/00094 | Loss: 205.6019 | CE: 0.1061 | KD: 1060.8409\n",
      "Train Epoch: 048 Batch: 00030/00094 | Loss: 205.6212 | CE: 0.1334 | KD: 1060.7997\n",
      "Train Epoch: 048 Batch: 00031/00094 | Loss: 205.5209 | CE: 0.0865 | KD: 1060.5240\n",
      "Train Epoch: 048 Batch: 00032/00094 | Loss: 205.5668 | CE: 0.0983 | KD: 1060.7002\n",
      "Train Epoch: 048 Batch: 00033/00094 | Loss: 205.5781 | CE: 0.0983 | KD: 1060.7587\n",
      "Train Epoch: 048 Batch: 00034/00094 | Loss: 205.5990 | CE: 0.0851 | KD: 1060.9344\n",
      "Train Epoch: 048 Batch: 00035/00094 | Loss: 205.5413 | CE: 0.1060 | KD: 1060.5291\n",
      "Train Epoch: 048 Batch: 00036/00094 | Loss: 205.5652 | CE: 0.0877 | KD: 1060.7469\n",
      "Train Epoch: 048 Batch: 00037/00094 | Loss: 205.4968 | CE: 0.0797 | KD: 1060.4353\n",
      "Train Epoch: 048 Batch: 00038/00094 | Loss: 205.6347 | CE: 0.1182 | KD: 1060.9480\n",
      "Train Epoch: 048 Batch: 00039/00094 | Loss: 205.6058 | CE: 0.1524 | KD: 1060.6224\n",
      "Train Epoch: 048 Batch: 00040/00094 | Loss: 205.5641 | CE: 0.0672 | KD: 1060.8467\n",
      "Train Epoch: 048 Batch: 00041/00094 | Loss: 205.6041 | CE: 0.1027 | KD: 1060.8701\n",
      "Train Epoch: 048 Batch: 00042/00094 | Loss: 205.6056 | CE: 0.1230 | KD: 1060.7731\n",
      "Train Epoch: 048 Batch: 00043/00094 | Loss: 205.6406 | CE: 0.1083 | KD: 1061.0298\n",
      "Train Epoch: 048 Batch: 00044/00094 | Loss: 205.7055 | CE: 0.1803 | KD: 1060.9927\n",
      "Train Epoch: 048 Batch: 00045/00094 | Loss: 205.7807 | CE: 0.2244 | KD: 1061.1534\n",
      "Train Epoch: 048 Batch: 00046/00094 | Loss: 205.7062 | CE: 0.1740 | KD: 1061.0292\n",
      "Train Epoch: 048 Batch: 00047/00094 | Loss: 205.6676 | CE: 0.0988 | KD: 1061.2183\n",
      "Train Epoch: 048 Batch: 00048/00094 | Loss: 205.6040 | CE: 0.0910 | KD: 1060.9304\n",
      "Train Epoch: 048 Batch: 00049/00094 | Loss: 205.6590 | CE: 0.1202 | KD: 1061.0631\n",
      "Train Epoch: 048 Batch: 00050/00094 | Loss: 205.6038 | CE: 0.1058 | KD: 1060.8527\n",
      "Train Epoch: 048 Batch: 00051/00094 | Loss: 205.6044 | CE: 0.1090 | KD: 1060.8387\n",
      "Train Epoch: 048 Batch: 00052/00094 | Loss: 205.6129 | CE: 0.1185 | KD: 1060.8342\n",
      "Train Epoch: 048 Batch: 00053/00094 | Loss: 205.6188 | CE: 0.1544 | KD: 1060.6787\n",
      "Train Epoch: 048 Batch: 00054/00094 | Loss: 205.5842 | CE: 0.1303 | KD: 1060.6246\n",
      "Train Epoch: 048 Batch: 00055/00094 | Loss: 205.6330 | CE: 0.1319 | KD: 1060.8682\n",
      "Train Epoch: 048 Batch: 00056/00094 | Loss: 205.7274 | CE: 0.1250 | KD: 1061.3918\n",
      "Train Epoch: 048 Batch: 00057/00094 | Loss: 205.5558 | CE: 0.0895 | KD: 1060.6892\n",
      "Train Epoch: 048 Batch: 00058/00094 | Loss: 205.6460 | CE: 0.1133 | KD: 1061.0317\n",
      "Train Epoch: 048 Batch: 00059/00094 | Loss: 205.6248 | CE: 0.1110 | KD: 1060.9341\n",
      "Train Epoch: 048 Batch: 00060/00094 | Loss: 205.6393 | CE: 0.1097 | KD: 1061.0155\n",
      "Train Epoch: 048 Batch: 00061/00094 | Loss: 205.6732 | CE: 0.1392 | KD: 1061.0380\n",
      "Train Epoch: 048 Batch: 00062/00094 | Loss: 205.5825 | CE: 0.0855 | KD: 1060.8474\n",
      "Train Epoch: 048 Batch: 00063/00094 | Loss: 205.6691 | CE: 0.1046 | KD: 1061.1958\n",
      "Train Epoch: 048 Batch: 00064/00094 | Loss: 205.4881 | CE: 0.0841 | KD: 1060.3672\n",
      "Train Epoch: 048 Batch: 00065/00094 | Loss: 205.6675 | CE: 0.1111 | KD: 1061.1544\n",
      "Train Epoch: 048 Batch: 00066/00094 | Loss: 205.6519 | CE: 0.1380 | KD: 1060.9347\n",
      "Train Epoch: 048 Batch: 00067/00094 | Loss: 205.5621 | CE: 0.1203 | KD: 1060.5626\n",
      "Train Epoch: 048 Batch: 00068/00094 | Loss: 205.6016 | CE: 0.1044 | KD: 1060.8481\n",
      "Train Epoch: 048 Batch: 00069/00094 | Loss: 205.6486 | CE: 0.0801 | KD: 1061.2164\n",
      "Train Epoch: 048 Batch: 00070/00094 | Loss: 205.5433 | CE: 0.0849 | KD: 1060.6482\n",
      "Train Epoch: 048 Batch: 00071/00094 | Loss: 205.6818 | CE: 0.1209 | KD: 1061.1772\n",
      "Train Epoch: 048 Batch: 00072/00094 | Loss: 205.6500 | CE: 0.1434 | KD: 1060.8970\n",
      "Train Epoch: 048 Batch: 00073/00094 | Loss: 205.6613 | CE: 0.1070 | KD: 1061.1434\n",
      "Train Epoch: 048 Batch: 00074/00094 | Loss: 205.5822 | CE: 0.1037 | KD: 1060.7523\n",
      "Train Epoch: 048 Batch: 00075/00094 | Loss: 205.7647 | CE: 0.1144 | KD: 1061.6388\n",
      "Train Epoch: 048 Batch: 00076/00094 | Loss: 205.5164 | CE: 0.0979 | KD: 1060.4417\n",
      "Train Epoch: 048 Batch: 00077/00094 | Loss: 205.5829 | CE: 0.1014 | KD: 1060.7672\n",
      "Train Epoch: 048 Batch: 00078/00094 | Loss: 205.6087 | CE: 0.0872 | KD: 1060.9742\n",
      "Train Epoch: 048 Batch: 00079/00094 | Loss: 205.6035 | CE: 0.1036 | KD: 1060.8622\n",
      "Train Epoch: 048 Batch: 00080/00094 | Loss: 205.5920 | CE: 0.0958 | KD: 1060.8430\n",
      "Train Epoch: 048 Batch: 00081/00094 | Loss: 205.5796 | CE: 0.0929 | KD: 1060.7942\n",
      "Train Epoch: 048 Batch: 00082/00094 | Loss: 205.6138 | CE: 0.1046 | KD: 1060.9103\n",
      "Train Epoch: 048 Batch: 00083/00094 | Loss: 205.6922 | CE: 0.1161 | KD: 1061.2557\n",
      "Train Epoch: 048 Batch: 00084/00094 | Loss: 205.6063 | CE: 0.1417 | KD: 1060.6801\n",
      "Train Epoch: 048 Batch: 00085/00094 | Loss: 205.6895 | CE: 0.1629 | KD: 1061.0001\n",
      "Train Epoch: 048 Batch: 00086/00094 | Loss: 205.5985 | CE: 0.1385 | KD: 1060.6567\n",
      "Train Epoch: 048 Batch: 00087/00094 | Loss: 205.6217 | CE: 0.1104 | KD: 1060.9208\n",
      "Train Epoch: 048 Batch: 00088/00094 | Loss: 205.5397 | CE: 0.0995 | KD: 1060.5546\n",
      "Train Epoch: 048 Batch: 00089/00094 | Loss: 205.5633 | CE: 0.0793 | KD: 1060.7804\n",
      "Train Epoch: 048 Batch: 00090/00094 | Loss: 205.6151 | CE: 0.1112 | KD: 1060.8829\n",
      "Train Epoch: 048 Batch: 00091/00094 | Loss: 205.5393 | CE: 0.1186 | KD: 1060.4536\n",
      "Train Epoch: 048 Batch: 00092/00094 | Loss: 205.6104 | CE: 0.1241 | KD: 1060.7922\n",
      "Train Epoch: 048 Batch: 00093/00094 | Loss: 205.7490 | CE: 0.1649 | KD: 1061.2970\n",
      "Train Epoch: 048 Batch: 00094/00094 | Loss: 205.5448 | CE: 0.1268 | KD: 1060.4397\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1010 | acc:98.5000\n",
      "[VAL Acc] Target: 98.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6264 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1140 | acc:54.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1108 | acc:45.8015\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3213 | acc:54.7414\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6022 | acc:70.3327\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8143 | acc:68.6129\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7948 | acc:62.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.00%\n",
      "[VAL Acc] Avg 62.96%\n",
      "Train Epoch: 049 Batch: 00001/00094 | Loss: 205.5818 | CE: 0.1349 | KD: 1060.5886\n",
      "Train Epoch: 049 Batch: 00002/00094 | Loss: 205.6629 | CE: 0.1107 | KD: 1061.1327\n",
      "Train Epoch: 049 Batch: 00003/00094 | Loss: 205.6139 | CE: 0.1083 | KD: 1060.8915\n",
      "Train Epoch: 049 Batch: 00004/00094 | Loss: 205.6164 | CE: 0.1060 | KD: 1060.9165\n",
      "Train Epoch: 049 Batch: 00005/00094 | Loss: 205.6296 | CE: 0.1195 | KD: 1060.9149\n",
      "Train Epoch: 049 Batch: 00006/00094 | Loss: 205.6748 | CE: 0.1936 | KD: 1060.7660\n",
      "Train Epoch: 049 Batch: 00007/00094 | Loss: 205.6083 | CE: 0.0883 | KD: 1060.9663\n",
      "Train Epoch: 049 Batch: 00008/00094 | Loss: 205.5479 | CE: 0.0801 | KD: 1060.6965\n",
      "Train Epoch: 049 Batch: 00009/00094 | Loss: 205.6380 | CE: 0.1686 | KD: 1060.7046\n",
      "Train Epoch: 049 Batch: 00010/00094 | Loss: 205.6084 | CE: 0.0771 | KD: 1061.0242\n",
      "Train Epoch: 049 Batch: 00011/00094 | Loss: 205.5813 | CE: 0.1215 | KD: 1060.6555\n",
      "Train Epoch: 049 Batch: 00012/00094 | Loss: 205.6031 | CE: 0.0787 | KD: 1060.9885\n",
      "Train Epoch: 049 Batch: 00013/00094 | Loss: 205.6284 | CE: 0.1103 | KD: 1060.9565\n",
      "Train Epoch: 049 Batch: 00014/00094 | Loss: 205.5800 | CE: 0.0991 | KD: 1060.7642\n",
      "Train Epoch: 049 Batch: 00015/00094 | Loss: 205.5569 | CE: 0.0722 | KD: 1060.7842\n",
      "Train Epoch: 049 Batch: 00016/00094 | Loss: 205.6149 | CE: 0.1405 | KD: 1060.7308\n",
      "Train Epoch: 049 Batch: 00017/00094 | Loss: 205.5457 | CE: 0.0980 | KD: 1060.5929\n",
      "Train Epoch: 049 Batch: 00018/00094 | Loss: 205.5932 | CE: 0.1038 | KD: 1060.8082\n",
      "Train Epoch: 049 Batch: 00019/00094 | Loss: 205.5331 | CE: 0.0853 | KD: 1060.5936\n",
      "Train Epoch: 049 Batch: 00020/00094 | Loss: 205.5376 | CE: 0.0907 | KD: 1060.5889\n",
      "Train Epoch: 049 Batch: 00021/00094 | Loss: 205.6746 | CE: 0.0884 | KD: 1061.3079\n",
      "Train Epoch: 049 Batch: 00022/00094 | Loss: 205.6423 | CE: 0.1107 | KD: 1061.0262\n",
      "Train Epoch: 049 Batch: 00023/00094 | Loss: 205.6043 | CE: 0.1229 | KD: 1060.7668\n",
      "Train Epoch: 049 Batch: 00024/00094 | Loss: 205.6213 | CE: 0.1182 | KD: 1060.8792\n",
      "Train Epoch: 049 Batch: 00025/00094 | Loss: 205.6047 | CE: 0.0930 | KD: 1060.9233\n",
      "Train Epoch: 049 Batch: 00026/00094 | Loss: 205.6174 | CE: 0.0885 | KD: 1061.0121\n",
      "Train Epoch: 049 Batch: 00027/00094 | Loss: 205.5591 | CE: 0.0724 | KD: 1060.7943\n",
      "Train Epoch: 049 Batch: 00028/00094 | Loss: 205.5988 | CE: 0.0971 | KD: 1060.8718\n",
      "Train Epoch: 049 Batch: 00029/00094 | Loss: 205.6367 | CE: 0.1083 | KD: 1061.0099\n",
      "Train Epoch: 049 Batch: 00030/00094 | Loss: 205.5736 | CE: 0.0899 | KD: 1060.7787\n",
      "Train Epoch: 049 Batch: 00031/00094 | Loss: 205.7151 | CE: 0.1431 | KD: 1061.2346\n",
      "Train Epoch: 049 Batch: 00032/00094 | Loss: 205.6514 | CE: 0.1420 | KD: 1060.9111\n",
      "Train Epoch: 049 Batch: 00033/00094 | Loss: 205.7082 | CE: 0.1009 | KD: 1061.4169\n",
      "Train Epoch: 049 Batch: 00034/00094 | Loss: 205.6331 | CE: 0.1187 | KD: 1060.9373\n",
      "Train Epoch: 049 Batch: 00035/00094 | Loss: 205.5668 | CE: 0.1029 | KD: 1060.6765\n",
      "Train Epoch: 049 Batch: 00036/00094 | Loss: 205.6195 | CE: 0.1380 | KD: 1060.7675\n",
      "Train Epoch: 049 Batch: 00037/00094 | Loss: 205.6361 | CE: 0.1106 | KD: 1060.9941\n",
      "Train Epoch: 049 Batch: 00038/00094 | Loss: 205.6505 | CE: 0.1287 | KD: 1060.9750\n",
      "Train Epoch: 049 Batch: 00039/00094 | Loss: 205.6035 | CE: 0.0891 | KD: 1060.9374\n",
      "Train Epoch: 049 Batch: 00040/00094 | Loss: 205.5678 | CE: 0.0872 | KD: 1060.7628\n",
      "Train Epoch: 049 Batch: 00041/00094 | Loss: 205.5760 | CE: 0.1044 | KD: 1060.7161\n",
      "Train Epoch: 049 Batch: 00042/00094 | Loss: 205.5809 | CE: 0.1022 | KD: 1060.7531\n",
      "Train Epoch: 049 Batch: 00043/00094 | Loss: 205.6140 | CE: 0.0868 | KD: 1061.0033\n",
      "Train Epoch: 049 Batch: 00044/00094 | Loss: 205.5917 | CE: 0.0993 | KD: 1060.8239\n",
      "Train Epoch: 049 Batch: 00045/00094 | Loss: 205.6905 | CE: 0.0986 | KD: 1061.3374\n",
      "Train Epoch: 049 Batch: 00046/00094 | Loss: 205.5788 | CE: 0.0697 | KD: 1060.9098\n",
      "Train Epoch: 049 Batch: 00047/00094 | Loss: 205.6920 | CE: 0.2247 | KD: 1060.6941\n",
      "Train Epoch: 049 Batch: 00048/00094 | Loss: 205.6694 | CE: 0.0973 | KD: 1061.2355\n",
      "Train Epoch: 049 Batch: 00049/00094 | Loss: 205.5638 | CE: 0.1092 | KD: 1060.6283\n",
      "Train Epoch: 049 Batch: 00050/00094 | Loss: 205.6284 | CE: 0.0916 | KD: 1061.0529\n",
      "Train Epoch: 049 Batch: 00051/00094 | Loss: 205.8203 | CE: 0.2519 | KD: 1061.2161\n",
      "Train Epoch: 049 Batch: 00052/00094 | Loss: 205.7160 | CE: 0.1493 | KD: 1061.2070\n",
      "Train Epoch: 049 Batch: 00053/00094 | Loss: 205.6742 | CE: 0.1605 | KD: 1060.9337\n",
      "Train Epoch: 049 Batch: 00054/00094 | Loss: 205.5090 | CE: 0.0922 | KD: 1060.4336\n",
      "Train Epoch: 049 Batch: 00055/00094 | Loss: 205.6124 | CE: 0.1475 | KD: 1060.6814\n",
      "Train Epoch: 049 Batch: 00056/00094 | Loss: 205.5293 | CE: 0.1276 | KD: 1060.3553\n",
      "Train Epoch: 049 Batch: 00057/00094 | Loss: 205.7058 | CE: 0.1105 | KD: 1061.3546\n",
      "Train Epoch: 049 Batch: 00058/00094 | Loss: 205.6466 | CE: 0.1447 | KD: 1060.8727\n",
      "Train Epoch: 049 Batch: 00059/00094 | Loss: 205.7017 | CE: 0.1144 | KD: 1061.3136\n",
      "Train Epoch: 049 Batch: 00060/00094 | Loss: 205.6435 | CE: 0.1547 | KD: 1060.8052\n",
      "Train Epoch: 049 Batch: 00061/00094 | Loss: 205.5437 | CE: 0.0776 | KD: 1060.6881\n",
      "Train Epoch: 049 Batch: 00062/00094 | Loss: 205.6500 | CE: 0.1382 | KD: 1060.9238\n",
      "Train Epoch: 049 Batch: 00063/00094 | Loss: 205.7040 | CE: 0.1814 | KD: 1060.9796\n",
      "Train Epoch: 049 Batch: 00064/00094 | Loss: 205.5688 | CE: 0.1031 | KD: 1060.6858\n",
      "Train Epoch: 049 Batch: 00065/00094 | Loss: 205.5798 | CE: 0.0924 | KD: 1060.7980\n",
      "Train Epoch: 049 Batch: 00066/00094 | Loss: 205.5777 | CE: 0.0944 | KD: 1060.7766\n",
      "Train Epoch: 049 Batch: 00067/00094 | Loss: 205.5673 | CE: 0.1056 | KD: 1060.6650\n",
      "Train Epoch: 049 Batch: 00068/00094 | Loss: 205.6194 | CE: 0.1133 | KD: 1060.8947\n",
      "Train Epoch: 049 Batch: 00069/00094 | Loss: 205.7282 | CE: 0.1571 | KD: 1061.2300\n",
      "Train Epoch: 049 Batch: 00070/00094 | Loss: 205.5993 | CE: 0.0892 | KD: 1060.9152\n",
      "Train Epoch: 049 Batch: 00071/00094 | Loss: 205.6046 | CE: 0.1022 | KD: 1060.8750\n",
      "Train Epoch: 049 Batch: 00072/00094 | Loss: 205.6671 | CE: 0.0981 | KD: 1061.2191\n",
      "Train Epoch: 049 Batch: 00073/00094 | Loss: 205.5789 | CE: 0.1262 | KD: 1060.6185\n",
      "Train Epoch: 049 Batch: 00074/00094 | Loss: 205.5960 | CE: 0.1039 | KD: 1060.8224\n",
      "Train Epoch: 049 Batch: 00075/00094 | Loss: 205.5580 | CE: 0.0940 | KD: 1060.6771\n",
      "Train Epoch: 049 Batch: 00076/00094 | Loss: 205.6740 | CE: 0.1182 | KD: 1061.1509\n",
      "Train Epoch: 049 Batch: 00077/00094 | Loss: 205.6839 | CE: 0.1965 | KD: 1060.7975\n",
      "Train Epoch: 049 Batch: 00078/00094 | Loss: 205.6151 | CE: 0.1025 | KD: 1060.9280\n",
      "Train Epoch: 049 Batch: 00079/00094 | Loss: 205.6967 | CE: 0.1870 | KD: 1060.9128\n",
      "Train Epoch: 049 Batch: 00080/00094 | Loss: 205.5988 | CE: 0.1018 | KD: 1060.8475\n",
      "Train Epoch: 049 Batch: 00081/00094 | Loss: 205.6014 | CE: 0.1090 | KD: 1060.8235\n",
      "Train Epoch: 049 Batch: 00082/00094 | Loss: 205.6439 | CE: 0.1356 | KD: 1060.9053\n",
      "Train Epoch: 049 Batch: 00083/00094 | Loss: 205.6775 | CE: 0.1040 | KD: 1061.2421\n",
      "Train Epoch: 049 Batch: 00084/00094 | Loss: 205.5663 | CE: 0.0930 | KD: 1060.7249\n",
      "Train Epoch: 049 Batch: 00085/00094 | Loss: 205.6019 | CE: 0.1772 | KD: 1060.4744\n",
      "Train Epoch: 049 Batch: 00086/00094 | Loss: 205.6361 | CE: 0.1464 | KD: 1060.8097\n",
      "Train Epoch: 049 Batch: 00087/00094 | Loss: 205.6393 | CE: 0.1215 | KD: 1060.9543\n",
      "Train Epoch: 049 Batch: 00088/00094 | Loss: 205.5341 | CE: 0.0690 | KD: 1060.6826\n",
      "Train Epoch: 049 Batch: 00089/00094 | Loss: 205.7286 | CE: 0.1143 | KD: 1061.4529\n",
      "Train Epoch: 049 Batch: 00090/00094 | Loss: 205.5764 | CE: 0.1489 | KD: 1060.4885\n",
      "Train Epoch: 049 Batch: 00091/00094 | Loss: 205.6727 | CE: 0.1341 | KD: 1061.0623\n",
      "Train Epoch: 049 Batch: 00092/00094 | Loss: 205.5899 | CE: 0.0841 | KD: 1060.8927\n",
      "Train Epoch: 049 Batch: 00093/00094 | Loss: 205.6200 | CE: 0.1100 | KD: 1060.9147\n",
      "Train Epoch: 049 Batch: 00094/00094 | Loss: 205.5614 | CE: 0.1336 | KD: 1060.4905\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1079 | acc:98.3500\n",
      "[VAL Acc] Target: 98.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6414 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1384 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1165 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3202 | acc:54.3495\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6396 | acc:68.3919\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8430 | acc:67.7116\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8098 | acc:59.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.94%\n",
      "[VAL Acc] Avg 62.08%\n",
      "Train Epoch: 050 Batch: 00001/00094 | Loss: 185.0258 | CE: 0.0903 | KD: 1060.7798\n",
      "Train Epoch: 050 Batch: 00002/00094 | Loss: 185.0734 | CE: 0.1120 | KD: 1060.9286\n",
      "Train Epoch: 050 Batch: 00003/00094 | Loss: 185.0924 | CE: 0.1300 | KD: 1060.9341\n",
      "Train Epoch: 050 Batch: 00004/00094 | Loss: 185.1037 | CE: 0.1134 | KD: 1061.0942\n",
      "Train Epoch: 050 Batch: 00005/00094 | Loss: 185.0286 | CE: 0.0811 | KD: 1060.8484\n",
      "Train Epoch: 050 Batch: 00006/00094 | Loss: 185.1027 | CE: 0.1325 | KD: 1060.9786\n",
      "Train Epoch: 050 Batch: 00007/00094 | Loss: 185.1130 | CE: 0.1360 | KD: 1061.0177\n",
      "Train Epoch: 050 Batch: 00008/00094 | Loss: 185.0443 | CE: 0.0985 | KD: 1060.8386\n",
      "Train Epoch: 050 Batch: 00009/00094 | Loss: 185.0899 | CE: 0.1086 | KD: 1061.0421\n",
      "Train Epoch: 050 Batch: 00010/00094 | Loss: 185.0178 | CE: 0.0942 | KD: 1060.7114\n",
      "Train Epoch: 050 Batch: 00011/00094 | Loss: 185.0201 | CE: 0.0850 | KD: 1060.7773\n",
      "Train Epoch: 050 Batch: 00012/00094 | Loss: 185.0029 | CE: 0.1100 | KD: 1060.5352\n",
      "Train Epoch: 050 Batch: 00013/00094 | Loss: 185.0970 | CE: 0.1342 | KD: 1060.9366\n",
      "Train Epoch: 050 Batch: 00014/00094 | Loss: 185.0677 | CE: 0.0790 | KD: 1061.0850\n",
      "Train Epoch: 050 Batch: 00015/00094 | Loss: 185.0643 | CE: 0.1122 | KD: 1060.8744\n",
      "Train Epoch: 050 Batch: 00016/00094 | Loss: 185.1080 | CE: 0.1476 | KD: 1060.9225\n",
      "Train Epoch: 050 Batch: 00017/00094 | Loss: 185.0719 | CE: 0.1185 | KD: 1060.8822\n",
      "Train Epoch: 050 Batch: 00018/00094 | Loss: 185.0530 | CE: 0.1194 | KD: 1060.7684\n",
      "Train Epoch: 050 Batch: 00019/00094 | Loss: 185.0042 | CE: 0.1117 | KD: 1060.5326\n",
      "Train Epoch: 050 Batch: 00020/00094 | Loss: 185.0897 | CE: 0.1309 | KD: 1060.9133\n",
      "Train Epoch: 050 Batch: 00021/00094 | Loss: 185.0457 | CE: 0.0940 | KD: 1060.8732\n",
      "Train Epoch: 050 Batch: 00022/00094 | Loss: 184.9637 | CE: 0.0894 | KD: 1060.4287\n",
      "Train Epoch: 050 Batch: 00023/00094 | Loss: 185.1074 | CE: 0.1305 | KD: 1061.0173\n",
      "Train Epoch: 050 Batch: 00024/00094 | Loss: 185.0608 | CE: 0.0847 | KD: 1061.0123\n",
      "Train Epoch: 050 Batch: 00025/00094 | Loss: 185.1707 | CE: 0.1547 | KD: 1061.2416\n",
      "Train Epoch: 050 Batch: 00026/00094 | Loss: 185.1838 | CE: 0.1369 | KD: 1061.4183\n",
      "Train Epoch: 050 Batch: 00027/00094 | Loss: 185.0563 | CE: 0.0820 | KD: 1061.0024\n",
      "Train Epoch: 050 Batch: 00028/00094 | Loss: 185.0764 | CE: 0.1087 | KD: 1060.9642\n",
      "Train Epoch: 050 Batch: 00029/00094 | Loss: 185.1719 | CE: 0.1890 | KD: 1061.0514\n",
      "Train Epoch: 050 Batch: 00030/00094 | Loss: 185.0926 | CE: 0.0872 | KD: 1061.1805\n",
      "Train Epoch: 050 Batch: 00031/00094 | Loss: 185.0720 | CE: 0.1226 | KD: 1060.8591\n",
      "Train Epoch: 050 Batch: 00032/00094 | Loss: 185.0704 | CE: 0.1390 | KD: 1060.7557\n",
      "Train Epoch: 050 Batch: 00033/00094 | Loss: 185.1202 | CE: 0.1258 | KD: 1061.1178\n",
      "Train Epoch: 050 Batch: 00034/00094 | Loss: 185.1320 | CE: 0.0862 | KD: 1061.4125\n",
      "Train Epoch: 050 Batch: 00035/00094 | Loss: 185.0455 | CE: 0.0838 | KD: 1060.9303\n",
      "Train Epoch: 050 Batch: 00036/00094 | Loss: 185.0502 | CE: 0.1223 | KD: 1060.7360\n",
      "Train Epoch: 050 Batch: 00037/00094 | Loss: 185.0597 | CE: 0.0994 | KD: 1060.9218\n",
      "Train Epoch: 050 Batch: 00038/00094 | Loss: 185.0466 | CE: 0.0821 | KD: 1060.9460\n",
      "Train Epoch: 050 Batch: 00039/00094 | Loss: 185.1954 | CE: 0.2360 | KD: 1060.9169\n",
      "Train Epoch: 050 Batch: 00040/00094 | Loss: 185.0224 | CE: 0.0780 | KD: 1060.8307\n",
      "Train Epoch: 050 Batch: 00041/00094 | Loss: 185.0285 | CE: 0.0950 | KD: 1060.7679\n",
      "Train Epoch: 050 Batch: 00042/00094 | Loss: 185.0411 | CE: 0.1135 | KD: 1060.7346\n",
      "Train Epoch: 050 Batch: 00043/00094 | Loss: 185.1273 | CE: 0.0811 | KD: 1061.4146\n",
      "Train Epoch: 050 Batch: 00044/00094 | Loss: 185.0312 | CE: 0.0845 | KD: 1060.8442\n",
      "Train Epoch: 050 Batch: 00045/00094 | Loss: 185.0436 | CE: 0.1111 | KD: 1060.7626\n",
      "Train Epoch: 050 Batch: 00046/00094 | Loss: 185.0131 | CE: 0.0952 | KD: 1060.6787\n",
      "Train Epoch: 050 Batch: 00047/00094 | Loss: 185.0572 | CE: 0.1143 | KD: 1060.8220\n",
      "Train Epoch: 050 Batch: 00048/00094 | Loss: 185.1284 | CE: 0.1473 | KD: 1061.0414\n",
      "Train Epoch: 050 Batch: 00049/00094 | Loss: 185.1097 | CE: 0.1858 | KD: 1060.7135\n",
      "Train Epoch: 050 Batch: 00050/00094 | Loss: 185.0238 | CE: 0.0898 | KD: 1060.7708\n",
      "Train Epoch: 050 Batch: 00051/00094 | Loss: 185.0417 | CE: 0.1216 | KD: 1060.6914\n",
      "Train Epoch: 050 Batch: 00052/00094 | Loss: 185.0953 | CE: 0.1200 | KD: 1061.0079\n",
      "Train Epoch: 050 Batch: 00053/00094 | Loss: 185.0295 | CE: 0.0948 | KD: 1060.7751\n",
      "Train Epoch: 050 Batch: 00054/00094 | Loss: 185.0286 | CE: 0.1059 | KD: 1060.7062\n",
      "Train Epoch: 050 Batch: 00055/00094 | Loss: 185.0543 | CE: 0.1077 | KD: 1060.8433\n",
      "Train Epoch: 050 Batch: 00056/00094 | Loss: 185.2381 | CE: 0.1727 | KD: 1061.5248\n",
      "Train Epoch: 050 Batch: 00057/00094 | Loss: 185.0075 | CE: 0.0948 | KD: 1060.6489\n",
      "Train Epoch: 050 Batch: 00058/00094 | Loss: 184.9783 | CE: 0.0882 | KD: 1060.5197\n",
      "Train Epoch: 050 Batch: 00059/00094 | Loss: 185.3321 | CE: 0.2980 | KD: 1061.3457\n",
      "Train Epoch: 050 Batch: 00060/00094 | Loss: 184.9911 | CE: 0.0894 | KD: 1060.5859\n",
      "Train Epoch: 050 Batch: 00061/00094 | Loss: 185.0830 | CE: 0.0980 | KD: 1061.0632\n",
      "Train Epoch: 050 Batch: 00062/00094 | Loss: 185.0688 | CE: 0.1457 | KD: 1060.7086\n",
      "Train Epoch: 050 Batch: 00063/00094 | Loss: 185.0500 | CE: 0.1292 | KD: 1060.6951\n",
      "Train Epoch: 050 Batch: 00064/00094 | Loss: 185.1921 | CE: 0.1127 | KD: 1061.6056\n",
      "Train Epoch: 050 Batch: 00065/00094 | Loss: 185.3627 | CE: 0.2966 | KD: 1061.5284\n",
      "Train Epoch: 050 Batch: 00066/00094 | Loss: 185.0715 | CE: 0.0912 | KD: 1061.0366\n",
      "Train Epoch: 050 Batch: 00067/00094 | Loss: 185.0165 | CE: 0.0807 | KD: 1060.7811\n",
      "Train Epoch: 050 Batch: 00068/00094 | Loss: 185.0064 | CE: 0.0911 | KD: 1060.6642\n",
      "Train Epoch: 050 Batch: 00069/00094 | Loss: 185.0579 | CE: 0.1248 | KD: 1060.7657\n",
      "Train Epoch: 050 Batch: 00070/00094 | Loss: 185.1402 | CE: 0.1036 | KD: 1061.3596\n",
      "Train Epoch: 050 Batch: 00071/00094 | Loss: 184.9858 | CE: 0.0986 | KD: 1060.5026\n",
      "Train Epoch: 050 Batch: 00072/00094 | Loss: 185.0517 | CE: 0.0839 | KD: 1060.9647\n",
      "Train Epoch: 050 Batch: 00073/00094 | Loss: 185.1120 | CE: 0.1183 | KD: 1061.1136\n",
      "Train Epoch: 050 Batch: 00074/00094 | Loss: 185.0905 | CE: 0.1297 | KD: 1060.9247\n",
      "Train Epoch: 050 Batch: 00075/00094 | Loss: 185.0441 | CE: 0.0807 | KD: 1060.9398\n",
      "Train Epoch: 050 Batch: 00076/00094 | Loss: 184.9686 | CE: 0.0673 | KD: 1060.5835\n",
      "Train Epoch: 050 Batch: 00077/00094 | Loss: 185.1020 | CE: 0.1212 | KD: 1061.0394\n",
      "Train Epoch: 050 Batch: 00078/00094 | Loss: 185.0332 | CE: 0.0924 | KD: 1060.8102\n",
      "Train Epoch: 050 Batch: 00079/00094 | Loss: 185.0458 | CE: 0.1154 | KD: 1060.7505\n",
      "Train Epoch: 050 Batch: 00080/00094 | Loss: 185.0332 | CE: 0.1325 | KD: 1060.5801\n",
      "Train Epoch: 050 Batch: 00081/00094 | Loss: 185.1143 | CE: 0.1449 | KD: 1060.9739\n",
      "Train Epoch: 050 Batch: 00082/00094 | Loss: 185.0616 | CE: 0.0886 | KD: 1060.9946\n",
      "Train Epoch: 050 Batch: 00083/00094 | Loss: 185.1656 | CE: 0.1176 | KD: 1061.4250\n",
      "Train Epoch: 050 Batch: 00084/00094 | Loss: 185.0847 | CE: 0.0766 | KD: 1061.1958\n",
      "Train Epoch: 050 Batch: 00085/00094 | Loss: 185.0545 | CE: 0.0836 | KD: 1060.9827\n",
      "Train Epoch: 050 Batch: 00086/00094 | Loss: 185.0123 | CE: 0.0870 | KD: 1060.7212\n",
      "Train Epoch: 050 Batch: 00087/00094 | Loss: 185.0868 | CE: 0.1082 | KD: 1061.0267\n",
      "Train Epoch: 050 Batch: 00088/00094 | Loss: 185.0514 | CE: 0.0753 | KD: 1061.0125\n",
      "Train Epoch: 050 Batch: 00089/00094 | Loss: 185.0645 | CE: 0.0878 | KD: 1061.0162\n",
      "Train Epoch: 050 Batch: 00090/00094 | Loss: 185.0743 | CE: 0.0919 | KD: 1061.0491\n",
      "Train Epoch: 050 Batch: 00091/00094 | Loss: 184.9924 | CE: 0.0801 | KD: 1060.6467\n",
      "Train Epoch: 050 Batch: 00092/00094 | Loss: 185.1261 | CE: 0.1293 | KD: 1061.1311\n",
      "Train Epoch: 050 Batch: 00093/00094 | Loss: 185.0369 | CE: 0.1229 | KD: 1060.6564\n",
      "Train Epoch: 050 Batch: 00094/00094 | Loss: 184.9991 | CE: 0.1001 | KD: 1060.5699\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1096 | acc:98.5500\n",
      "[VAL Acc] Target: 98.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6057 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1098 | acc:55.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1427 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2584 | acc:55.3292\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6728 | acc:66.6359\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8169 | acc:68.0643\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8131 | acc:61.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.00%\n",
      "[VAL Acc] Avg 62.59%\n",
      "Train Epoch: 051 Batch: 00001/00094 | Loss: 185.1301 | CE: 0.1593 | KD: 1060.9818\n",
      "Train Epoch: 051 Batch: 00002/00094 | Loss: 185.1124 | CE: 0.1236 | KD: 1061.0857\n",
      "Train Epoch: 051 Batch: 00003/00094 | Loss: 185.0114 | CE: 0.0949 | KD: 1060.6705\n",
      "Train Epoch: 051 Batch: 00004/00094 | Loss: 185.1423 | CE: 0.1328 | KD: 1061.2039\n",
      "Train Epoch: 051 Batch: 00005/00094 | Loss: 185.0147 | CE: 0.0866 | KD: 1060.7371\n",
      "Train Epoch: 051 Batch: 00006/00094 | Loss: 185.0043 | CE: 0.0848 | KD: 1060.6875\n",
      "Train Epoch: 051 Batch: 00007/00094 | Loss: 185.1295 | CE: 0.1490 | KD: 1061.0378\n",
      "Train Epoch: 051 Batch: 00008/00094 | Loss: 185.0373 | CE: 0.1225 | KD: 1060.6606\n",
      "Train Epoch: 051 Batch: 00009/00094 | Loss: 185.0544 | CE: 0.0853 | KD: 1060.9725\n",
      "Train Epoch: 051 Batch: 00010/00094 | Loss: 185.1163 | CE: 0.1287 | KD: 1061.0785\n",
      "Train Epoch: 051 Batch: 00011/00094 | Loss: 185.0883 | CE: 0.0952 | KD: 1061.1099\n",
      "Train Epoch: 051 Batch: 00012/00094 | Loss: 185.0469 | CE: 0.0772 | KD: 1060.9757\n",
      "Train Epoch: 051 Batch: 00013/00094 | Loss: 185.1117 | CE: 0.1299 | KD: 1061.0449\n",
      "Train Epoch: 051 Batch: 00014/00094 | Loss: 185.1036 | CE: 0.0966 | KD: 1061.1895\n",
      "Train Epoch: 051 Batch: 00015/00094 | Loss: 185.0452 | CE: 0.0991 | KD: 1060.8408\n",
      "Train Epoch: 051 Batch: 00016/00094 | Loss: 185.0838 | CE: 0.1238 | KD: 1060.9208\n",
      "Train Epoch: 051 Batch: 00017/00094 | Loss: 185.0783 | CE: 0.0893 | KD: 1061.0864\n",
      "Train Epoch: 051 Batch: 00018/00094 | Loss: 185.0893 | CE: 0.1605 | KD: 1060.7410\n",
      "Train Epoch: 051 Batch: 00019/00094 | Loss: 185.0606 | CE: 0.1001 | KD: 1060.9229\n",
      "Train Epoch: 051 Batch: 00020/00094 | Loss: 185.0696 | CE: 0.0991 | KD: 1060.9800\n",
      "Train Epoch: 051 Batch: 00021/00094 | Loss: 185.1335 | CE: 0.1230 | KD: 1061.2097\n",
      "Train Epoch: 051 Batch: 00022/00094 | Loss: 185.0711 | CE: 0.1044 | KD: 1060.9587\n",
      "Train Epoch: 051 Batch: 00023/00094 | Loss: 185.0716 | CE: 0.1151 | KD: 1060.9003\n",
      "Train Epoch: 051 Batch: 00024/00094 | Loss: 184.9776 | CE: 0.0958 | KD: 1060.4717\n",
      "Train Epoch: 051 Batch: 00025/00094 | Loss: 185.0497 | CE: 0.0783 | KD: 1060.9852\n",
      "Train Epoch: 051 Batch: 00026/00094 | Loss: 185.0841 | CE: 0.1055 | KD: 1061.0271\n",
      "Train Epoch: 051 Batch: 00027/00094 | Loss: 185.0124 | CE: 0.0955 | KD: 1060.6730\n",
      "Train Epoch: 051 Batch: 00028/00094 | Loss: 184.9788 | CE: 0.0923 | KD: 1060.4990\n",
      "Train Epoch: 051 Batch: 00029/00094 | Loss: 185.0038 | CE: 0.0703 | KD: 1060.7683\n",
      "Train Epoch: 051 Batch: 00030/00094 | Loss: 185.1375 | CE: 0.1603 | KD: 1061.0192\n",
      "Train Epoch: 051 Batch: 00031/00094 | Loss: 185.1497 | CE: 0.1172 | KD: 1061.3361\n",
      "Train Epoch: 051 Batch: 00032/00094 | Loss: 185.0909 | CE: 0.1420 | KD: 1060.8567\n",
      "Train Epoch: 051 Batch: 00033/00094 | Loss: 185.1065 | CE: 0.1245 | KD: 1061.0464\n",
      "Train Epoch: 051 Batch: 00034/00094 | Loss: 185.0311 | CE: 0.1085 | KD: 1060.7058\n",
      "Train Epoch: 051 Batch: 00035/00094 | Loss: 185.0630 | CE: 0.1377 | KD: 1060.7211\n",
      "Train Epoch: 051 Batch: 00036/00094 | Loss: 185.0490 | CE: 0.0904 | KD: 1060.9122\n",
      "Train Epoch: 051 Batch: 00037/00094 | Loss: 185.0897 | CE: 0.0866 | KD: 1061.1676\n",
      "Train Epoch: 051 Batch: 00038/00094 | Loss: 185.0376 | CE: 0.0938 | KD: 1060.8273\n",
      "Train Epoch: 051 Batch: 00039/00094 | Loss: 185.0287 | CE: 0.1208 | KD: 1060.6210\n",
      "Train Epoch: 051 Batch: 00040/00094 | Loss: 185.0954 | CE: 0.1026 | KD: 1061.1082\n",
      "Train Epoch: 051 Batch: 00041/00094 | Loss: 185.0689 | CE: 0.1237 | KD: 1060.8357\n",
      "Train Epoch: 051 Batch: 00042/00094 | Loss: 185.0199 | CE: 0.1084 | KD: 1060.6420\n",
      "Train Epoch: 051 Batch: 00043/00094 | Loss: 184.9848 | CE: 0.0907 | KD: 1060.5421\n",
      "Train Epoch: 051 Batch: 00044/00094 | Loss: 185.1370 | CE: 0.1290 | KD: 1061.1956\n",
      "Train Epoch: 051 Batch: 00045/00094 | Loss: 185.0915 | CE: 0.1001 | KD: 1061.1002\n",
      "Train Epoch: 051 Batch: 00046/00094 | Loss: 185.1322 | CE: 0.1111 | KD: 1061.2706\n",
      "Train Epoch: 051 Batch: 00047/00094 | Loss: 185.1096 | CE: 0.1375 | KD: 1060.9899\n",
      "Train Epoch: 051 Batch: 00048/00094 | Loss: 185.0570 | CE: 0.1042 | KD: 1060.8790\n",
      "Train Epoch: 051 Batch: 00049/00094 | Loss: 185.0538 | CE: 0.1002 | KD: 1060.8834\n",
      "Train Epoch: 051 Batch: 00050/00094 | Loss: 185.0436 | CE: 0.1121 | KD: 1060.7567\n",
      "Train Epoch: 051 Batch: 00051/00094 | Loss: 185.0150 | CE: 0.0953 | KD: 1060.6891\n",
      "Train Epoch: 051 Batch: 00052/00094 | Loss: 185.0607 | CE: 0.1099 | KD: 1060.8669\n",
      "Train Epoch: 051 Batch: 00053/00094 | Loss: 185.0781 | CE: 0.0994 | KD: 1061.0272\n",
      "Train Epoch: 051 Batch: 00054/00094 | Loss: 185.1345 | CE: 0.1336 | KD: 1061.1547\n",
      "Train Epoch: 051 Batch: 00055/00094 | Loss: 185.0467 | CE: 0.1172 | KD: 1060.7454\n",
      "Train Epoch: 051 Batch: 00056/00094 | Loss: 185.0850 | CE: 0.1454 | KD: 1060.8029\n",
      "Train Epoch: 051 Batch: 00057/00094 | Loss: 185.0332 | CE: 0.0843 | KD: 1060.8568\n",
      "Train Epoch: 051 Batch: 00058/00094 | Loss: 185.0802 | CE: 0.0986 | KD: 1061.0439\n",
      "Train Epoch: 051 Batch: 00059/00094 | Loss: 185.0788 | CE: 0.1264 | KD: 1060.8766\n",
      "Train Epoch: 051 Batch: 00060/00094 | Loss: 185.0437 | CE: 0.0996 | KD: 1060.8291\n",
      "Train Epoch: 051 Batch: 00061/00094 | Loss: 185.1403 | CE: 0.1467 | KD: 1061.1133\n",
      "Train Epoch: 051 Batch: 00062/00094 | Loss: 185.0450 | CE: 0.1034 | KD: 1060.8145\n",
      "Train Epoch: 051 Batch: 00063/00094 | Loss: 185.0985 | CE: 0.0939 | KD: 1061.1763\n",
      "Train Epoch: 051 Batch: 00064/00094 | Loss: 185.0545 | CE: 0.0745 | KD: 1061.0348\n",
      "Train Epoch: 051 Batch: 00065/00094 | Loss: 185.1288 | CE: 0.1617 | KD: 1060.9609\n",
      "Train Epoch: 051 Batch: 00066/00094 | Loss: 185.0309 | CE: 0.0977 | KD: 1060.7661\n",
      "Train Epoch: 051 Batch: 00067/00094 | Loss: 185.0860 | CE: 0.1176 | KD: 1060.9680\n",
      "Train Epoch: 051 Batch: 00068/00094 | Loss: 184.9992 | CE: 0.0897 | KD: 1060.6307\n",
      "Train Epoch: 051 Batch: 00069/00094 | Loss: 185.0639 | CE: 0.1154 | KD: 1060.8541\n",
      "Train Epoch: 051 Batch: 00070/00094 | Loss: 185.0172 | CE: 0.1117 | KD: 1060.6075\n",
      "Train Epoch: 051 Batch: 00071/00094 | Loss: 185.1469 | CE: 0.1779 | KD: 1060.9719\n",
      "Train Epoch: 051 Batch: 00072/00094 | Loss: 185.0172 | CE: 0.1130 | KD: 1060.5997\n",
      "Train Epoch: 051 Batch: 00073/00094 | Loss: 185.1202 | CE: 0.1373 | KD: 1061.0515\n",
      "Train Epoch: 051 Batch: 00074/00094 | Loss: 185.3096 | CE: 0.2754 | KD: 1061.3459\n",
      "Train Epoch: 051 Batch: 00075/00094 | Loss: 184.9683 | CE: 0.0722 | KD: 1060.5532\n",
      "Train Epoch: 051 Batch: 00076/00094 | Loss: 185.0759 | CE: 0.0997 | KD: 1061.0131\n",
      "Train Epoch: 051 Batch: 00077/00094 | Loss: 185.0873 | CE: 0.1536 | KD: 1060.7694\n",
      "Train Epoch: 051 Batch: 00078/00094 | Loss: 185.0200 | CE: 0.0834 | KD: 1060.7858\n",
      "Train Epoch: 051 Batch: 00079/00094 | Loss: 185.0247 | CE: 0.0889 | KD: 1060.7815\n",
      "Train Epoch: 051 Batch: 00080/00094 | Loss: 185.0560 | CE: 0.1186 | KD: 1060.7910\n",
      "Train Epoch: 051 Batch: 00081/00094 | Loss: 185.1015 | CE: 0.1478 | KD: 1060.8844\n",
      "Train Epoch: 051 Batch: 00082/00094 | Loss: 185.0903 | CE: 0.0845 | KD: 1061.1829\n",
      "Train Epoch: 051 Batch: 00083/00094 | Loss: 185.0903 | CE: 0.0995 | KD: 1061.0970\n",
      "Train Epoch: 051 Batch: 00084/00094 | Loss: 185.0178 | CE: 0.1078 | KD: 1060.6334\n",
      "Train Epoch: 051 Batch: 00085/00094 | Loss: 185.0831 | CE: 0.1187 | KD: 1060.9453\n",
      "Train Epoch: 051 Batch: 00086/00094 | Loss: 185.0929 | CE: 0.1252 | KD: 1060.9645\n",
      "Train Epoch: 051 Batch: 00087/00094 | Loss: 185.1052 | CE: 0.0980 | KD: 1061.1908\n",
      "Train Epoch: 051 Batch: 00088/00094 | Loss: 185.0515 | CE: 0.0955 | KD: 1060.8973\n",
      "Train Epoch: 051 Batch: 00089/00094 | Loss: 185.0490 | CE: 0.1011 | KD: 1060.8506\n",
      "Train Epoch: 051 Batch: 00090/00094 | Loss: 185.0540 | CE: 0.0920 | KD: 1060.9314\n",
      "Train Epoch: 051 Batch: 00091/00094 | Loss: 184.9602 | CE: 0.0760 | KD: 1060.4858\n",
      "Train Epoch: 051 Batch: 00092/00094 | Loss: 185.0326 | CE: 0.0974 | KD: 1060.7778\n",
      "Train Epoch: 051 Batch: 00093/00094 | Loss: 185.0664 | CE: 0.1268 | KD: 1060.8033\n",
      "Train Epoch: 051 Batch: 00094/00094 | Loss: 185.1152 | CE: 0.1561 | KD: 1060.9148\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1055 | acc:98.3500\n",
      "[VAL Acc] Target: 98.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5953 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1255 | acc:53.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1197 | acc:45.8015\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2648 | acc:54.4671\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6808 | acc:66.6359\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8032 | acc:68.8088\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8114 | acc:60.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.69%\n",
      "[VAL Acc] Avg 62.13%\n",
      "Train Epoch: 052 Batch: 00001/00094 | Loss: 185.0385 | CE: 0.1250 | KD: 1060.6539\n",
      "Train Epoch: 052 Batch: 00002/00094 | Loss: 184.9649 | CE: 0.0724 | KD: 1060.5326\n",
      "Train Epoch: 052 Batch: 00003/00094 | Loss: 184.9911 | CE: 0.0818 | KD: 1060.6295\n",
      "Train Epoch: 052 Batch: 00004/00094 | Loss: 185.0019 | CE: 0.0770 | KD: 1060.7188\n",
      "Train Epoch: 052 Batch: 00005/00094 | Loss: 185.1039 | CE: 0.1556 | KD: 1060.8529\n",
      "Train Epoch: 052 Batch: 00006/00094 | Loss: 185.0467 | CE: 0.1040 | KD: 1060.8208\n",
      "Train Epoch: 052 Batch: 00007/00094 | Loss: 185.0093 | CE: 0.0949 | KD: 1060.6583\n",
      "Train Epoch: 052 Batch: 00008/00094 | Loss: 185.0933 | CE: 0.1027 | KD: 1061.0958\n",
      "Train Epoch: 052 Batch: 00009/00094 | Loss: 185.0759 | CE: 0.1104 | KD: 1060.9517\n",
      "Train Epoch: 052 Batch: 00010/00094 | Loss: 185.1519 | CE: 0.1428 | KD: 1061.2020\n",
      "Train Epoch: 052 Batch: 00011/00094 | Loss: 185.1317 | CE: 0.1037 | KD: 1061.3099\n",
      "Train Epoch: 052 Batch: 00012/00094 | Loss: 185.0110 | CE: 0.0805 | KD: 1060.7515\n",
      "Train Epoch: 052 Batch: 00013/00094 | Loss: 185.0746 | CE: 0.1063 | KD: 1060.9680\n",
      "Train Epoch: 052 Batch: 00014/00094 | Loss: 185.1582 | CE: 0.1535 | KD: 1061.1766\n",
      "Train Epoch: 052 Batch: 00015/00094 | Loss: 185.1202 | CE: 0.0710 | KD: 1061.4319\n",
      "Train Epoch: 052 Batch: 00016/00094 | Loss: 185.0444 | CE: 0.0796 | KD: 1060.9480\n",
      "Train Epoch: 052 Batch: 00017/00094 | Loss: 185.1139 | CE: 0.1050 | KD: 1061.2009\n",
      "Train Epoch: 052 Batch: 00018/00094 | Loss: 185.1000 | CE: 0.0998 | KD: 1061.1508\n",
      "Train Epoch: 052 Batch: 00019/00094 | Loss: 185.1188 | CE: 0.1201 | KD: 1061.1421\n",
      "Train Epoch: 052 Batch: 00020/00094 | Loss: 185.0721 | CE: 0.0921 | KD: 1061.0350\n",
      "Train Epoch: 052 Batch: 00021/00094 | Loss: 185.0957 | CE: 0.0899 | KD: 1061.1826\n",
      "Train Epoch: 052 Batch: 00022/00094 | Loss: 185.0671 | CE: 0.1012 | KD: 1060.9540\n",
      "Train Epoch: 052 Batch: 00023/00094 | Loss: 185.0325 | CE: 0.1244 | KD: 1060.6224\n",
      "Train Epoch: 052 Batch: 00024/00094 | Loss: 185.0221 | CE: 0.1404 | KD: 1060.4709\n",
      "Train Epoch: 052 Batch: 00025/00094 | Loss: 185.1262 | CE: 0.1479 | KD: 1061.0250\n",
      "Train Epoch: 052 Batch: 00026/00094 | Loss: 185.0639 | CE: 0.1010 | KD: 1060.9368\n",
      "Train Epoch: 052 Batch: 00027/00094 | Loss: 185.0157 | CE: 0.0793 | KD: 1060.7848\n",
      "Train Epoch: 052 Batch: 00028/00094 | Loss: 185.0493 | CE: 0.1088 | KD: 1060.8083\n",
      "Train Epoch: 052 Batch: 00029/00094 | Loss: 185.0334 | CE: 0.1005 | KD: 1060.7646\n",
      "Train Epoch: 052 Batch: 00030/00094 | Loss: 184.9898 | CE: 0.0765 | KD: 1060.6522\n",
      "Train Epoch: 052 Batch: 00031/00094 | Loss: 185.1484 | CE: 0.1386 | KD: 1061.2054\n",
      "Train Epoch: 052 Batch: 00032/00094 | Loss: 185.0817 | CE: 0.1033 | KD: 1061.0255\n",
      "Train Epoch: 052 Batch: 00033/00094 | Loss: 185.1207 | CE: 0.1503 | KD: 1060.9795\n",
      "Train Epoch: 052 Batch: 00034/00094 | Loss: 185.0114 | CE: 0.1080 | KD: 1060.5955\n",
      "Train Epoch: 052 Batch: 00035/00094 | Loss: 185.0266 | CE: 0.0855 | KD: 1060.8119\n",
      "Train Epoch: 052 Batch: 00036/00094 | Loss: 185.0171 | CE: 0.0952 | KD: 1060.7014\n",
      "Train Epoch: 052 Batch: 00037/00094 | Loss: 185.0527 | CE: 0.1230 | KD: 1060.7463\n",
      "Train Epoch: 052 Batch: 00038/00094 | Loss: 185.0747 | CE: 0.0885 | KD: 1061.0708\n",
      "Train Epoch: 052 Batch: 00039/00094 | Loss: 185.1134 | CE: 0.1194 | KD: 1061.1154\n",
      "Train Epoch: 052 Batch: 00040/00094 | Loss: 185.1108 | CE: 0.1480 | KD: 1060.9363\n",
      "Train Epoch: 052 Batch: 00041/00094 | Loss: 185.0594 | CE: 0.1034 | KD: 1060.8970\n",
      "Train Epoch: 052 Batch: 00042/00094 | Loss: 185.0513 | CE: 0.0995 | KD: 1060.8730\n",
      "Train Epoch: 052 Batch: 00043/00094 | Loss: 185.0010 | CE: 0.1122 | KD: 1060.5115\n",
      "Train Epoch: 052 Batch: 00044/00094 | Loss: 185.0773 | CE: 0.1191 | KD: 1060.9100\n",
      "Train Epoch: 052 Batch: 00045/00094 | Loss: 184.9983 | CE: 0.0825 | KD: 1060.6667\n",
      "Train Epoch: 052 Batch: 00046/00094 | Loss: 185.0909 | CE: 0.1188 | KD: 1060.9894\n",
      "Train Epoch: 052 Batch: 00047/00094 | Loss: 184.9909 | CE: 0.0723 | KD: 1060.6830\n",
      "Train Epoch: 052 Batch: 00048/00094 | Loss: 185.0040 | CE: 0.0791 | KD: 1060.7190\n",
      "Train Epoch: 052 Batch: 00049/00094 | Loss: 185.0290 | CE: 0.0814 | KD: 1060.8491\n",
      "Train Epoch: 052 Batch: 00050/00094 | Loss: 185.0878 | CE: 0.0915 | KD: 1061.1285\n",
      "Train Epoch: 052 Batch: 00051/00094 | Loss: 185.1242 | CE: 0.1303 | KD: 1061.1145\n",
      "Train Epoch: 052 Batch: 00052/00094 | Loss: 185.0566 | CE: 0.1412 | KD: 1060.6643\n",
      "Train Epoch: 052 Batch: 00053/00094 | Loss: 184.9942 | CE: 0.1273 | KD: 1060.3861\n",
      "Train Epoch: 052 Batch: 00054/00094 | Loss: 185.2107 | CE: 0.1689 | KD: 1061.3896\n",
      "Train Epoch: 052 Batch: 00055/00094 | Loss: 184.9989 | CE: 0.1120 | KD: 1060.5013\n",
      "Train Epoch: 052 Batch: 00056/00094 | Loss: 185.1075 | CE: 0.1003 | KD: 1061.1908\n",
      "Train Epoch: 052 Batch: 00057/00094 | Loss: 185.0844 | CE: 0.0925 | KD: 1061.1033\n",
      "Train Epoch: 052 Batch: 00058/00094 | Loss: 185.1815 | CE: 0.1594 | KD: 1061.2765\n",
      "Train Epoch: 052 Batch: 00059/00094 | Loss: 185.0361 | CE: 0.0786 | KD: 1060.9059\n",
      "Train Epoch: 052 Batch: 00060/00094 | Loss: 185.0153 | CE: 0.0884 | KD: 1060.7302\n",
      "Train Epoch: 052 Batch: 00061/00094 | Loss: 185.0977 | CE: 0.0988 | KD: 1061.1434\n",
      "Train Epoch: 052 Batch: 00062/00094 | Loss: 185.1087 | CE: 0.1266 | KD: 1061.0469\n",
      "Train Epoch: 052 Batch: 00063/00094 | Loss: 185.0034 | CE: 0.0916 | KD: 1060.6438\n",
      "Train Epoch: 052 Batch: 00064/00094 | Loss: 185.1457 | CE: 0.1252 | KD: 1061.2677\n",
      "Train Epoch: 052 Batch: 00065/00094 | Loss: 185.1317 | CE: 0.1091 | KD: 1061.2793\n",
      "Train Epoch: 052 Batch: 00066/00094 | Loss: 185.0124 | CE: 0.0793 | KD: 1060.7659\n",
      "Train Epoch: 052 Batch: 00067/00094 | Loss: 185.0627 | CE: 0.1247 | KD: 1060.7937\n",
      "Train Epoch: 052 Batch: 00068/00094 | Loss: 185.2062 | CE: 0.2328 | KD: 1060.9973\n",
      "Train Epoch: 052 Batch: 00069/00094 | Loss: 185.0869 | CE: 0.0823 | KD: 1061.1763\n",
      "Train Epoch: 052 Batch: 00070/00094 | Loss: 185.0942 | CE: 0.1145 | KD: 1061.0331\n",
      "Train Epoch: 052 Batch: 00071/00094 | Loss: 185.0694 | CE: 0.1184 | KD: 1060.8687\n",
      "Train Epoch: 052 Batch: 00072/00094 | Loss: 185.0291 | CE: 0.1298 | KD: 1060.5723\n",
      "Train Epoch: 052 Batch: 00073/00094 | Loss: 185.0655 | CE: 0.1079 | KD: 1060.9062\n",
      "Train Epoch: 052 Batch: 00074/00094 | Loss: 185.2061 | CE: 0.2135 | KD: 1061.1071\n",
      "Train Epoch: 052 Batch: 00075/00094 | Loss: 185.0209 | CE: 0.0963 | KD: 1060.7172\n",
      "Train Epoch: 052 Batch: 00076/00094 | Loss: 185.1705 | CE: 0.1457 | KD: 1061.2920\n",
      "Train Epoch: 052 Batch: 00077/00094 | Loss: 185.1624 | CE: 0.1506 | KD: 1061.2175\n",
      "Train Epoch: 052 Batch: 00078/00094 | Loss: 185.0314 | CE: 0.1322 | KD: 1060.5712\n",
      "Train Epoch: 052 Batch: 00079/00094 | Loss: 185.0933 | CE: 0.1496 | KD: 1060.8264\n",
      "Train Epoch: 052 Batch: 00080/00094 | Loss: 185.0689 | CE: 0.1156 | KD: 1060.8816\n",
      "Train Epoch: 052 Batch: 00081/00094 | Loss: 185.0211 | CE: 0.1336 | KD: 1060.5046\n",
      "Train Epoch: 052 Batch: 00082/00094 | Loss: 185.0531 | CE: 0.0832 | KD: 1060.9774\n",
      "Train Epoch: 052 Batch: 00083/00094 | Loss: 185.0711 | CE: 0.0842 | KD: 1061.0743\n",
      "Train Epoch: 052 Batch: 00084/00094 | Loss: 185.0246 | CE: 0.0910 | KD: 1060.7688\n",
      "Train Epoch: 052 Batch: 00085/00094 | Loss: 185.1470 | CE: 0.1478 | KD: 1061.1454\n",
      "Train Epoch: 052 Batch: 00086/00094 | Loss: 185.1832 | CE: 0.2144 | KD: 1060.9707\n",
      "Train Epoch: 052 Batch: 00087/00094 | Loss: 185.0238 | CE: 0.0889 | KD: 1060.7759\n",
      "Train Epoch: 052 Batch: 00088/00094 | Loss: 185.0408 | CE: 0.0886 | KD: 1060.8752\n",
      "Train Epoch: 052 Batch: 00089/00094 | Loss: 185.0294 | CE: 0.1003 | KD: 1060.7427\n",
      "Train Epoch: 052 Batch: 00090/00094 | Loss: 185.0691 | CE: 0.0832 | KD: 1061.0687\n",
      "Train Epoch: 052 Batch: 00091/00094 | Loss: 185.0861 | CE: 0.0925 | KD: 1061.1132\n",
      "Train Epoch: 052 Batch: 00092/00094 | Loss: 185.0928 | CE: 0.1044 | KD: 1061.0826\n",
      "Train Epoch: 052 Batch: 00093/00094 | Loss: 185.0435 | CE: 0.1006 | KD: 1060.8223\n",
      "Train Epoch: 052 Batch: 00094/00094 | Loss: 185.0459 | CE: 0.0886 | KD: 1060.9047\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0997 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5858 | acc:50.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1128 | acc:54.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1455 | acc:44.4656\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2620 | acc:54.7022\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6877 | acc:65.4344\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 65.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8095 | acc:68.5737\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8203 | acc:60.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.88%\n",
      "[VAL Acc] Avg 62.15%\n",
      "Train Epoch: 053 Batch: 00001/00094 | Loss: 185.0103 | CE: 0.0779 | KD: 1060.7616\n",
      "Train Epoch: 053 Batch: 00002/00094 | Loss: 185.0268 | CE: 0.1106 | KD: 1060.6691\n",
      "Train Epoch: 053 Batch: 00003/00094 | Loss: 184.9943 | CE: 0.0876 | KD: 1060.6144\n",
      "Train Epoch: 053 Batch: 00004/00094 | Loss: 184.9915 | CE: 0.0930 | KD: 1060.5676\n",
      "Train Epoch: 053 Batch: 00005/00094 | Loss: 185.1434 | CE: 0.1520 | KD: 1061.1002\n",
      "Train Epoch: 053 Batch: 00006/00094 | Loss: 185.0409 | CE: 0.0923 | KD: 1060.8549\n",
      "Train Epoch: 053 Batch: 00007/00094 | Loss: 185.0289 | CE: 0.0951 | KD: 1060.7697\n",
      "Train Epoch: 053 Batch: 00008/00094 | Loss: 185.0636 | CE: 0.0878 | KD: 1061.0105\n",
      "Train Epoch: 053 Batch: 00009/00094 | Loss: 185.0671 | CE: 0.0942 | KD: 1060.9945\n",
      "Train Epoch: 053 Batch: 00010/00094 | Loss: 185.0112 | CE: 0.1158 | KD: 1060.5500\n",
      "Train Epoch: 053 Batch: 00011/00094 | Loss: 185.1348 | CE: 0.1502 | KD: 1061.0613\n",
      "Train Epoch: 053 Batch: 00012/00094 | Loss: 185.0583 | CE: 0.0833 | KD: 1061.0063\n",
      "Train Epoch: 053 Batch: 00013/00094 | Loss: 185.0314 | CE: 0.1075 | KD: 1060.7129\n",
      "Train Epoch: 053 Batch: 00014/00094 | Loss: 185.1296 | CE: 0.1174 | KD: 1061.2195\n",
      "Train Epoch: 053 Batch: 00015/00094 | Loss: 185.0574 | CE: 0.1473 | KD: 1060.6342\n",
      "Train Epoch: 053 Batch: 00016/00094 | Loss: 185.0282 | CE: 0.0954 | KD: 1060.7643\n",
      "Train Epoch: 053 Batch: 00017/00094 | Loss: 185.0703 | CE: 0.0853 | KD: 1061.0635\n",
      "Train Epoch: 053 Batch: 00018/00094 | Loss: 185.1069 | CE: 0.1225 | KD: 1061.0604\n",
      "Train Epoch: 053 Batch: 00019/00094 | Loss: 185.0175 | CE: 0.0965 | KD: 1060.6965\n",
      "Train Epoch: 053 Batch: 00020/00094 | Loss: 185.1212 | CE: 0.0973 | KD: 1061.2865\n",
      "Train Epoch: 053 Batch: 00021/00094 | Loss: 185.0643 | CE: 0.0820 | KD: 1061.0480\n",
      "Train Epoch: 053 Batch: 00022/00094 | Loss: 185.1066 | CE: 0.1342 | KD: 1060.9915\n",
      "Train Epoch: 053 Batch: 00023/00094 | Loss: 185.1022 | CE: 0.1293 | KD: 1060.9940\n",
      "Train Epoch: 053 Batch: 00024/00094 | Loss: 185.1059 | CE: 0.1261 | KD: 1061.0341\n",
      "Train Epoch: 053 Batch: 00025/00094 | Loss: 185.1007 | CE: 0.0762 | KD: 1061.2903\n",
      "Train Epoch: 053 Batch: 00026/00094 | Loss: 185.0751 | CE: 0.0858 | KD: 1061.0880\n",
      "Train Epoch: 053 Batch: 00027/00094 | Loss: 185.0509 | CE: 0.1028 | KD: 1060.8517\n",
      "Train Epoch: 053 Batch: 00028/00094 | Loss: 185.0525 | CE: 0.0854 | KD: 1060.9609\n",
      "Train Epoch: 053 Batch: 00029/00094 | Loss: 185.0429 | CE: 0.1021 | KD: 1060.8102\n",
      "Train Epoch: 053 Batch: 00030/00094 | Loss: 185.1080 | CE: 0.0998 | KD: 1061.1969\n",
      "Train Epoch: 053 Batch: 00031/00094 | Loss: 185.2675 | CE: 0.2006 | KD: 1061.5332\n",
      "Train Epoch: 053 Batch: 00032/00094 | Loss: 185.0540 | CE: 0.1139 | KD: 1060.8060\n",
      "Train Epoch: 053 Batch: 00033/00094 | Loss: 184.9911 | CE: 0.0843 | KD: 1060.6150\n",
      "Train Epoch: 053 Batch: 00034/00094 | Loss: 185.0020 | CE: 0.0948 | KD: 1060.6172\n",
      "Train Epoch: 053 Batch: 00035/00094 | Loss: 185.0541 | CE: 0.0931 | KD: 1060.9259\n",
      "Train Epoch: 053 Batch: 00036/00094 | Loss: 184.9766 | CE: 0.1163 | KD: 1060.3481\n",
      "Train Epoch: 053 Batch: 00037/00094 | Loss: 185.0296 | CE: 0.1158 | KD: 1060.6554\n",
      "Train Epoch: 053 Batch: 00038/00094 | Loss: 185.0148 | CE: 0.0804 | KD: 1060.7733\n",
      "Train Epoch: 053 Batch: 00039/00094 | Loss: 185.0470 | CE: 0.0751 | KD: 1060.9880\n",
      "Train Epoch: 053 Batch: 00040/00094 | Loss: 185.0386 | CE: 0.0765 | KD: 1060.9323\n",
      "Train Epoch: 053 Batch: 00041/00094 | Loss: 185.1390 | CE: 0.1716 | KD: 1060.9622\n",
      "Train Epoch: 053 Batch: 00042/00094 | Loss: 185.0321 | CE: 0.0841 | KD: 1060.8516\n",
      "Train Epoch: 053 Batch: 00043/00094 | Loss: 185.0062 | CE: 0.0832 | KD: 1060.7079\n",
      "Train Epoch: 053 Batch: 00044/00094 | Loss: 185.0163 | CE: 0.1040 | KD: 1060.6467\n",
      "Train Epoch: 053 Batch: 00045/00094 | Loss: 185.1505 | CE: 0.1282 | KD: 1061.2775\n",
      "Train Epoch: 053 Batch: 00046/00094 | Loss: 185.0866 | CE: 0.1383 | KD: 1060.8535\n",
      "Train Epoch: 053 Batch: 00047/00094 | Loss: 185.1374 | CE: 0.1513 | KD: 1061.0697\n",
      "Train Epoch: 053 Batch: 00048/00094 | Loss: 184.9747 | CE: 0.1149 | KD: 1060.3457\n",
      "Train Epoch: 053 Batch: 00049/00094 | Loss: 185.0557 | CE: 0.1097 | KD: 1060.8400\n",
      "Train Epoch: 053 Batch: 00050/00094 | Loss: 185.0105 | CE: 0.0835 | KD: 1060.7313\n",
      "Train Epoch: 053 Batch: 00051/00094 | Loss: 185.0456 | CE: 0.0936 | KD: 1060.8744\n",
      "Train Epoch: 053 Batch: 00052/00094 | Loss: 185.1011 | CE: 0.1002 | KD: 1061.1549\n",
      "Train Epoch: 053 Batch: 00053/00094 | Loss: 185.0701 | CE: 0.0772 | KD: 1061.1089\n",
      "Train Epoch: 053 Batch: 00054/00094 | Loss: 185.1053 | CE: 0.1148 | KD: 1061.0950\n",
      "Train Epoch: 053 Batch: 00055/00094 | Loss: 185.0647 | CE: 0.1247 | KD: 1060.8055\n",
      "Train Epoch: 053 Batch: 00056/00094 | Loss: 185.0945 | CE: 0.0870 | KD: 1061.1925\n",
      "Train Epoch: 053 Batch: 00057/00094 | Loss: 184.9985 | CE: 0.0849 | KD: 1060.6542\n",
      "Train Epoch: 053 Batch: 00058/00094 | Loss: 185.1479 | CE: 0.1932 | KD: 1060.8898\n",
      "Train Epoch: 053 Batch: 00059/00094 | Loss: 185.0039 | CE: 0.0840 | KD: 1060.6902\n",
      "Train Epoch: 053 Batch: 00060/00094 | Loss: 185.1042 | CE: 0.0809 | KD: 1061.2833\n",
      "Train Epoch: 053 Batch: 00061/00094 | Loss: 185.0387 | CE: 0.1202 | KD: 1060.6826\n",
      "Train Epoch: 053 Batch: 00062/00094 | Loss: 185.0914 | CE: 0.0865 | KD: 1061.1775\n",
      "Train Epoch: 053 Batch: 00063/00094 | Loss: 185.1079 | CE: 0.1575 | KD: 1060.8654\n",
      "Train Epoch: 053 Batch: 00064/00094 | Loss: 185.0817 | CE: 0.1677 | KD: 1060.6564\n",
      "Train Epoch: 053 Batch: 00065/00094 | Loss: 185.1963 | CE: 0.1845 | KD: 1061.2175\n",
      "Train Epoch: 053 Batch: 00066/00094 | Loss: 185.0132 | CE: 0.0898 | KD: 1060.7100\n",
      "Train Epoch: 053 Batch: 00067/00094 | Loss: 185.0182 | CE: 0.0916 | KD: 1060.7288\n",
      "Train Epoch: 053 Batch: 00068/00094 | Loss: 185.0766 | CE: 0.0999 | KD: 1061.0160\n",
      "Train Epoch: 053 Batch: 00069/00094 | Loss: 185.1266 | CE: 0.1313 | KD: 1061.1223\n",
      "Train Epoch: 053 Batch: 00070/00094 | Loss: 185.1492 | CE: 0.1584 | KD: 1061.0966\n",
      "Train Epoch: 053 Batch: 00071/00094 | Loss: 185.0437 | CE: 0.1001 | KD: 1060.8262\n",
      "Train Epoch: 053 Batch: 00072/00094 | Loss: 185.1174 | CE: 0.1222 | KD: 1061.1224\n",
      "Train Epoch: 053 Batch: 00073/00094 | Loss: 185.0654 | CE: 0.0898 | KD: 1061.0096\n",
      "Train Epoch: 053 Batch: 00074/00094 | Loss: 185.0857 | CE: 0.1231 | KD: 1060.9351\n",
      "Train Epoch: 053 Batch: 00075/00094 | Loss: 184.9755 | CE: 0.1170 | KD: 1060.3378\n",
      "Train Epoch: 053 Batch: 00076/00094 | Loss: 185.0218 | CE: 0.0905 | KD: 1060.7560\n",
      "Train Epoch: 053 Batch: 00077/00094 | Loss: 184.9752 | CE: 0.0785 | KD: 1060.5571\n",
      "Train Epoch: 053 Batch: 00078/00094 | Loss: 184.9964 | CE: 0.0869 | KD: 1060.6304\n",
      "Train Epoch: 053 Batch: 00079/00094 | Loss: 185.0677 | CE: 0.1220 | KD: 1060.8386\n",
      "Train Epoch: 053 Batch: 00080/00094 | Loss: 185.2168 | CE: 0.2339 | KD: 1061.0511\n",
      "Train Epoch: 053 Batch: 00081/00094 | Loss: 185.0576 | CE: 0.1288 | KD: 1060.7413\n",
      "Train Epoch: 053 Batch: 00082/00094 | Loss: 185.0831 | CE: 0.1032 | KD: 1061.0343\n",
      "Train Epoch: 053 Batch: 00083/00094 | Loss: 185.1611 | CE: 0.1415 | KD: 1061.2623\n",
      "Train Epoch: 053 Batch: 00084/00094 | Loss: 185.0479 | CE: 0.1132 | KD: 1060.7753\n",
      "Train Epoch: 053 Batch: 00085/00094 | Loss: 184.9690 | CE: 0.0818 | KD: 1060.5031\n",
      "Train Epoch: 053 Batch: 00086/00094 | Loss: 185.0056 | CE: 0.1291 | KD: 1060.4414\n",
      "Train Epoch: 053 Batch: 00087/00094 | Loss: 185.1183 | CE: 0.0828 | KD: 1061.3534\n",
      "Train Epoch: 053 Batch: 00088/00094 | Loss: 185.0788 | CE: 0.1103 | KD: 1060.9692\n",
      "Train Epoch: 053 Batch: 00089/00094 | Loss: 185.1525 | CE: 0.1265 | KD: 1061.2986\n",
      "Train Epoch: 053 Batch: 00090/00094 | Loss: 185.1309 | CE: 0.1175 | KD: 1061.2262\n",
      "Train Epoch: 053 Batch: 00091/00094 | Loss: 185.0023 | CE: 0.0738 | KD: 1060.7394\n",
      "Train Epoch: 053 Batch: 00092/00094 | Loss: 184.9770 | CE: 0.0839 | KD: 1060.5364\n",
      "Train Epoch: 053 Batch: 00093/00094 | Loss: 185.1187 | CE: 0.0986 | KD: 1061.2649\n",
      "Train Epoch: 053 Batch: 00094/00094 | Loss: 185.1171 | CE: 0.1172 | KD: 1061.1492\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0991 | acc:98.7000\n",
      "[VAL Acc] Target: 98.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6617 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1577 | acc:53.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1730 | acc:44.8473\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3203 | acc:54.8589\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6806 | acc:66.5434\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8491 | acc:67.4373\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8257 | acc:59.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.94%\n",
      "[VAL Acc] Avg 61.90%\n",
      "Train Epoch: 054 Batch: 00001/00094 | Loss: 166.6675 | CE: 0.2001 | KD: 1060.9417\n",
      "Train Epoch: 054 Batch: 00002/00094 | Loss: 166.5694 | CE: 0.0984 | KD: 1060.9650\n",
      "Train Epoch: 054 Batch: 00003/00094 | Loss: 166.5530 | CE: 0.1138 | KD: 1060.7617\n",
      "Train Epoch: 054 Batch: 00004/00094 | Loss: 166.5462 | CE: 0.0958 | KD: 1060.8335\n",
      "Train Epoch: 054 Batch: 00005/00094 | Loss: 166.5930 | CE: 0.1531 | KD: 1060.7665\n",
      "Train Epoch: 054 Batch: 00006/00094 | Loss: 166.6689 | CE: 0.2336 | KD: 1060.7377\n",
      "Train Epoch: 054 Batch: 00007/00094 | Loss: 166.5866 | CE: 0.1047 | KD: 1061.0343\n",
      "Train Epoch: 054 Batch: 00008/00094 | Loss: 166.5950 | CE: 0.1298 | KD: 1060.9285\n",
      "Train Epoch: 054 Batch: 00009/00094 | Loss: 166.6020 | CE: 0.1310 | KD: 1060.9647\n",
      "Train Epoch: 054 Batch: 00010/00094 | Loss: 166.5425 | CE: 0.1100 | KD: 1060.7198\n",
      "Train Epoch: 054 Batch: 00011/00094 | Loss: 166.4753 | CE: 0.0651 | KD: 1060.5770\n",
      "Train Epoch: 054 Batch: 00012/00094 | Loss: 166.5606 | CE: 0.1053 | KD: 1060.8646\n",
      "Train Epoch: 054 Batch: 00013/00094 | Loss: 166.5140 | CE: 0.0924 | KD: 1060.6495\n",
      "Train Epoch: 054 Batch: 00014/00094 | Loss: 166.5604 | CE: 0.0951 | KD: 1060.9287\n",
      "Train Epoch: 054 Batch: 00015/00094 | Loss: 166.5230 | CE: 0.0730 | KD: 1060.8312\n",
      "Train Epoch: 054 Batch: 00016/00094 | Loss: 166.5980 | CE: 0.1047 | KD: 1061.1067\n",
      "Train Epoch: 054 Batch: 00017/00094 | Loss: 166.6286 | CE: 0.1065 | KD: 1061.2903\n",
      "Train Epoch: 054 Batch: 00018/00094 | Loss: 166.4875 | CE: 0.0820 | KD: 1060.5476\n",
      "Train Epoch: 054 Batch: 00019/00094 | Loss: 166.5463 | CE: 0.0846 | KD: 1060.9056\n",
      "Train Epoch: 054 Batch: 00020/00094 | Loss: 166.6397 | CE: 0.1619 | KD: 1061.0079\n",
      "Train Epoch: 054 Batch: 00021/00094 | Loss: 166.6082 | CE: 0.1358 | KD: 1060.9736\n",
      "Train Epoch: 054 Batch: 00022/00094 | Loss: 166.5499 | CE: 0.1020 | KD: 1060.8176\n",
      "Train Epoch: 054 Batch: 00023/00094 | Loss: 166.5246 | CE: 0.0944 | KD: 1060.7046\n",
      "Train Epoch: 054 Batch: 00024/00094 | Loss: 166.6142 | CE: 0.1713 | KD: 1060.7856\n",
      "Train Epoch: 054 Batch: 00025/00094 | Loss: 166.5918 | CE: 0.1132 | KD: 1061.0135\n",
      "Train Epoch: 054 Batch: 00026/00094 | Loss: 166.5364 | CE: 0.1032 | KD: 1060.7240\n",
      "Train Epoch: 054 Batch: 00027/00094 | Loss: 166.6129 | CE: 0.1320 | KD: 1061.0278\n",
      "Train Epoch: 054 Batch: 00028/00094 | Loss: 166.6809 | CE: 0.1774 | KD: 1061.1716\n",
      "Train Epoch: 054 Batch: 00029/00094 | Loss: 166.5890 | CE: 0.1173 | KD: 1060.9697\n",
      "Train Epoch: 054 Batch: 00030/00094 | Loss: 166.6588 | CE: 0.1760 | KD: 1061.0402\n",
      "Train Epoch: 054 Batch: 00031/00094 | Loss: 166.5522 | CE: 0.0687 | KD: 1061.0448\n",
      "Train Epoch: 054 Batch: 00032/00094 | Loss: 166.5591 | CE: 0.1069 | KD: 1060.8450\n",
      "Train Epoch: 054 Batch: 00033/00094 | Loss: 166.5253 | CE: 0.1009 | KD: 1060.6683\n",
      "Train Epoch: 054 Batch: 00034/00094 | Loss: 166.6233 | CE: 0.1259 | KD: 1061.1331\n",
      "Train Epoch: 054 Batch: 00035/00094 | Loss: 166.5188 | CE: 0.0906 | KD: 1060.6923\n",
      "Train Epoch: 054 Batch: 00036/00094 | Loss: 166.5988 | CE: 0.1182 | KD: 1061.0261\n",
      "Train Epoch: 054 Batch: 00037/00094 | Loss: 166.6225 | CE: 0.1020 | KD: 1061.2804\n",
      "Train Epoch: 054 Batch: 00038/00094 | Loss: 166.5210 | CE: 0.0784 | KD: 1060.7838\n",
      "Train Epoch: 054 Batch: 00039/00094 | Loss: 166.5654 | CE: 0.0838 | KD: 1061.0323\n",
      "Train Epoch: 054 Batch: 00040/00094 | Loss: 166.5409 | CE: 0.0771 | KD: 1060.9187\n",
      "Train Epoch: 054 Batch: 00041/00094 | Loss: 166.5530 | CE: 0.0974 | KD: 1060.8671\n",
      "Train Epoch: 054 Batch: 00042/00094 | Loss: 166.6166 | CE: 0.1350 | KD: 1061.0328\n",
      "Train Epoch: 054 Batch: 00043/00094 | Loss: 166.5653 | CE: 0.1101 | KD: 1060.8641\n",
      "Train Epoch: 054 Batch: 00044/00094 | Loss: 166.5022 | CE: 0.0894 | KD: 1060.5944\n",
      "Train Epoch: 054 Batch: 00045/00094 | Loss: 166.5887 | CE: 0.1260 | KD: 1060.9114\n",
      "Train Epoch: 054 Batch: 00046/00094 | Loss: 166.5486 | CE: 0.1017 | KD: 1060.8109\n",
      "Train Epoch: 054 Batch: 00047/00094 | Loss: 166.5674 | CE: 0.1090 | KD: 1060.8846\n",
      "Train Epoch: 054 Batch: 00048/00094 | Loss: 166.5818 | CE: 0.1134 | KD: 1060.9486\n",
      "Train Epoch: 054 Batch: 00049/00094 | Loss: 166.5331 | CE: 0.0995 | KD: 1060.7264\n",
      "Train Epoch: 054 Batch: 00050/00094 | Loss: 166.5631 | CE: 0.1143 | KD: 1060.8235\n",
      "Train Epoch: 054 Batch: 00051/00094 | Loss: 166.5409 | CE: 0.0989 | KD: 1060.7802\n",
      "Train Epoch: 054 Batch: 00052/00094 | Loss: 166.5718 | CE: 0.1299 | KD: 1060.7795\n",
      "Train Epoch: 054 Batch: 00053/00094 | Loss: 166.5317 | CE: 0.0984 | KD: 1060.7242\n",
      "Train Epoch: 054 Batch: 00054/00094 | Loss: 166.5486 | CE: 0.1525 | KD: 1060.4869\n",
      "Train Epoch: 054 Batch: 00055/00094 | Loss: 166.5266 | CE: 0.0742 | KD: 1060.8462\n",
      "Train Epoch: 054 Batch: 00056/00094 | Loss: 166.5638 | CE: 0.1055 | KD: 1060.8838\n",
      "Train Epoch: 054 Batch: 00057/00094 | Loss: 166.6082 | CE: 0.1148 | KD: 1061.1072\n",
      "Train Epoch: 054 Batch: 00058/00094 | Loss: 166.5505 | CE: 0.1030 | KD: 1060.8147\n",
      "Train Epoch: 054 Batch: 00059/00094 | Loss: 166.5579 | CE: 0.0983 | KD: 1060.8922\n",
      "Train Epoch: 054 Batch: 00060/00094 | Loss: 166.5378 | CE: 0.0876 | KD: 1060.8322\n",
      "Train Epoch: 054 Batch: 00061/00094 | Loss: 166.5361 | CE: 0.1085 | KD: 1060.6881\n",
      "Train Epoch: 054 Batch: 00062/00094 | Loss: 166.5091 | CE: 0.0809 | KD: 1060.6924\n",
      "Train Epoch: 054 Batch: 00063/00094 | Loss: 166.5145 | CE: 0.0928 | KD: 1060.6501\n",
      "Train Epoch: 054 Batch: 00064/00094 | Loss: 166.6616 | CE: 0.1356 | KD: 1061.3157\n",
      "Train Epoch: 054 Batch: 00065/00094 | Loss: 166.5414 | CE: 0.0861 | KD: 1060.8649\n",
      "Train Epoch: 054 Batch: 00066/00094 | Loss: 166.5495 | CE: 0.0832 | KD: 1060.9349\n",
      "Train Epoch: 054 Batch: 00067/00094 | Loss: 166.5446 | CE: 0.1352 | KD: 1060.5720\n",
      "Train Epoch: 054 Batch: 00068/00094 | Loss: 166.5575 | CE: 0.0972 | KD: 1060.8965\n",
      "Train Epoch: 054 Batch: 00069/00094 | Loss: 166.5387 | CE: 0.0893 | KD: 1060.8268\n",
      "Train Epoch: 054 Batch: 00070/00094 | Loss: 166.6316 | CE: 0.0843 | KD: 1061.4506\n",
      "Train Epoch: 054 Batch: 00071/00094 | Loss: 166.5988 | CE: 0.0984 | KD: 1061.1526\n",
      "Train Epoch: 054 Batch: 00072/00094 | Loss: 166.6127 | CE: 0.1064 | KD: 1061.1902\n",
      "Train Epoch: 054 Batch: 00073/00094 | Loss: 166.5620 | CE: 0.0776 | KD: 1061.0503\n",
      "Train Epoch: 054 Batch: 00074/00094 | Loss: 166.5939 | CE: 0.0811 | KD: 1061.2310\n",
      "Train Epoch: 054 Batch: 00075/00094 | Loss: 166.5805 | CE: 0.1009 | KD: 1061.0197\n",
      "Train Epoch: 054 Batch: 00076/00094 | Loss: 166.5190 | CE: 0.0776 | KD: 1060.7762\n",
      "Train Epoch: 054 Batch: 00077/00094 | Loss: 166.5456 | CE: 0.0774 | KD: 1060.9471\n",
      "Train Epoch: 054 Batch: 00078/00094 | Loss: 166.5618 | CE: 0.1184 | KD: 1060.7894\n",
      "Train Epoch: 054 Batch: 00079/00094 | Loss: 166.5695 | CE: 0.0892 | KD: 1061.0242\n",
      "Train Epoch: 054 Batch: 00080/00094 | Loss: 166.6120 | CE: 0.1271 | KD: 1061.0533\n",
      "Train Epoch: 054 Batch: 00081/00094 | Loss: 166.6133 | CE: 0.1552 | KD: 1060.8828\n",
      "Train Epoch: 054 Batch: 00082/00094 | Loss: 166.5987 | CE: 0.1573 | KD: 1060.7760\n",
      "Train Epoch: 054 Batch: 00083/00094 | Loss: 166.6306 | CE: 0.0939 | KD: 1061.3838\n",
      "Train Epoch: 054 Batch: 00084/00094 | Loss: 166.5742 | CE: 0.1112 | KD: 1060.9142\n",
      "Train Epoch: 054 Batch: 00085/00094 | Loss: 166.6530 | CE: 0.1244 | KD: 1061.3318\n",
      "Train Epoch: 054 Batch: 00086/00094 | Loss: 166.5026 | CE: 0.0715 | KD: 1060.7111\n",
      "Train Epoch: 054 Batch: 00087/00094 | Loss: 166.5735 | CE: 0.0933 | KD: 1061.0237\n",
      "Train Epoch: 054 Batch: 00088/00094 | Loss: 166.5043 | CE: 0.0686 | KD: 1060.7400\n",
      "Train Epoch: 054 Batch: 00089/00094 | Loss: 166.4853 | CE: 0.0676 | KD: 1060.6248\n",
      "Train Epoch: 054 Batch: 00090/00094 | Loss: 166.5657 | CE: 0.0773 | KD: 1061.0762\n",
      "Train Epoch: 054 Batch: 00091/00094 | Loss: 166.6220 | CE: 0.1544 | KD: 1060.9429\n",
      "Train Epoch: 054 Batch: 00092/00094 | Loss: 166.8141 | CE: 0.2220 | KD: 1061.7367\n",
      "Train Epoch: 054 Batch: 00093/00094 | Loss: 166.5801 | CE: 0.0791 | KD: 1061.1558\n",
      "Train Epoch: 054 Batch: 00094/00094 | Loss: 166.5322 | CE: 0.0710 | KD: 1060.9027\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0964 | acc:98.6500\n",
      "[VAL Acc] Target: 98.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6418 | acc:50.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1978 | acc:52.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1582 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3417 | acc:54.2712\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6677 | acc:68.6691\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.67%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8411 | acc:67.7900\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8503 | acc:60.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.06%\n",
      "[VAL Acc] Avg 62.25%\n",
      "Train Epoch: 055 Batch: 00001/00094 | Loss: 166.5957 | CE: 0.0957 | KD: 1061.1498\n",
      "Train Epoch: 055 Batch: 00002/00094 | Loss: 166.5261 | CE: 0.0907 | KD: 1060.7380\n",
      "Train Epoch: 055 Batch: 00003/00094 | Loss: 166.7165 | CE: 0.1689 | KD: 1061.4531\n",
      "Train Epoch: 055 Batch: 00004/00094 | Loss: 166.5643 | CE: 0.1145 | KD: 1060.8300\n",
      "Train Epoch: 055 Batch: 00005/00094 | Loss: 166.7066 | CE: 0.1641 | KD: 1061.4202\n",
      "Train Epoch: 055 Batch: 00006/00094 | Loss: 166.5353 | CE: 0.0864 | KD: 1060.8239\n",
      "Train Epoch: 055 Batch: 00007/00094 | Loss: 166.6207 | CE: 0.0928 | KD: 1061.3273\n",
      "Train Epoch: 055 Batch: 00008/00094 | Loss: 166.5199 | CE: 0.0817 | KD: 1060.7554\n",
      "Train Epoch: 055 Batch: 00009/00094 | Loss: 166.5204 | CE: 0.0854 | KD: 1060.7351\n",
      "Train Epoch: 055 Batch: 00010/00094 | Loss: 166.6228 | CE: 0.1190 | KD: 1061.1733\n",
      "Train Epoch: 055 Batch: 00011/00094 | Loss: 166.4695 | CE: 0.0896 | KD: 1060.3844\n",
      "Train Epoch: 055 Batch: 00012/00094 | Loss: 166.5490 | CE: 0.1168 | KD: 1060.7175\n",
      "Train Epoch: 055 Batch: 00013/00094 | Loss: 166.5891 | CE: 0.1213 | KD: 1060.9445\n",
      "Train Epoch: 055 Batch: 00014/00094 | Loss: 166.5435 | CE: 0.0947 | KD: 1060.8234\n",
      "Train Epoch: 055 Batch: 00015/00094 | Loss: 166.5800 | CE: 0.1175 | KD: 1060.9105\n",
      "Train Epoch: 055 Batch: 00016/00094 | Loss: 166.5187 | CE: 0.1023 | KD: 1060.6165\n",
      "Train Epoch: 055 Batch: 00017/00094 | Loss: 166.6321 | CE: 0.1179 | KD: 1061.2405\n",
      "Train Epoch: 055 Batch: 00018/00094 | Loss: 166.5430 | CE: 0.1059 | KD: 1060.7485\n",
      "Train Epoch: 055 Batch: 00019/00094 | Loss: 166.5161 | CE: 0.0797 | KD: 1060.7443\n",
      "Train Epoch: 055 Batch: 00020/00094 | Loss: 166.5988 | CE: 0.1041 | KD: 1061.1155\n",
      "Train Epoch: 055 Batch: 00021/00094 | Loss: 166.5465 | CE: 0.1115 | KD: 1060.7352\n",
      "Train Epoch: 055 Batch: 00022/00094 | Loss: 166.5510 | CE: 0.0925 | KD: 1060.8854\n",
      "Train Epoch: 055 Batch: 00023/00094 | Loss: 166.5648 | CE: 0.1321 | KD: 1060.7213\n",
      "Train Epoch: 055 Batch: 00024/00094 | Loss: 166.5316 | CE: 0.0926 | KD: 1060.7610\n",
      "Train Epoch: 055 Batch: 00025/00094 | Loss: 166.5670 | CE: 0.0797 | KD: 1061.0691\n",
      "Train Epoch: 055 Batch: 00026/00094 | Loss: 166.5313 | CE: 0.0813 | KD: 1060.8314\n",
      "Train Epoch: 055 Batch: 00027/00094 | Loss: 166.5656 | CE: 0.0945 | KD: 1060.9650\n",
      "Train Epoch: 055 Batch: 00028/00094 | Loss: 166.5644 | CE: 0.0812 | KD: 1061.0426\n",
      "Train Epoch: 055 Batch: 00029/00094 | Loss: 166.5644 | CE: 0.1210 | KD: 1060.7888\n",
      "Train Epoch: 055 Batch: 00030/00094 | Loss: 166.5879 | CE: 0.1224 | KD: 1060.9296\n",
      "Train Epoch: 055 Batch: 00031/00094 | Loss: 166.5812 | CE: 0.0813 | KD: 1061.1488\n",
      "Train Epoch: 055 Batch: 00032/00094 | Loss: 166.6494 | CE: 0.1581 | KD: 1061.0946\n",
      "Train Epoch: 055 Batch: 00033/00094 | Loss: 166.5475 | CE: 0.0884 | KD: 1060.8894\n",
      "Train Epoch: 055 Batch: 00034/00094 | Loss: 166.5400 | CE: 0.1274 | KD: 1060.5930\n",
      "Train Epoch: 055 Batch: 00035/00094 | Loss: 166.5378 | CE: 0.0951 | KD: 1060.7844\n",
      "Train Epoch: 055 Batch: 00036/00094 | Loss: 166.5469 | CE: 0.0791 | KD: 1060.9441\n",
      "Train Epoch: 055 Batch: 00037/00094 | Loss: 166.5914 | CE: 0.0788 | KD: 1061.2296\n",
      "Train Epoch: 055 Batch: 00038/00094 | Loss: 166.5284 | CE: 0.0772 | KD: 1060.8386\n",
      "Train Epoch: 055 Batch: 00039/00094 | Loss: 166.5888 | CE: 0.1044 | KD: 1061.0507\n",
      "Train Epoch: 055 Batch: 00040/00094 | Loss: 166.6468 | CE: 0.1537 | KD: 1061.1055\n",
      "Train Epoch: 055 Batch: 00041/00094 | Loss: 166.5528 | CE: 0.0724 | KD: 1061.0245\n",
      "Train Epoch: 055 Batch: 00042/00094 | Loss: 166.5251 | CE: 0.0875 | KD: 1060.7517\n",
      "Train Epoch: 055 Batch: 00043/00094 | Loss: 166.5827 | CE: 0.1534 | KD: 1060.6995\n",
      "Train Epoch: 055 Batch: 00044/00094 | Loss: 166.6136 | CE: 0.1023 | KD: 1061.2216\n",
      "Train Epoch: 055 Batch: 00045/00094 | Loss: 166.6364 | CE: 0.0841 | KD: 1061.4830\n",
      "Train Epoch: 055 Batch: 00046/00094 | Loss: 166.5516 | CE: 0.0760 | KD: 1060.9937\n",
      "Train Epoch: 055 Batch: 00047/00094 | Loss: 166.5950 | CE: 0.1363 | KD: 1060.8865\n",
      "Train Epoch: 055 Batch: 00048/00094 | Loss: 166.5473 | CE: 0.0968 | KD: 1060.8342\n",
      "Train Epoch: 055 Batch: 00049/00094 | Loss: 166.5611 | CE: 0.0701 | KD: 1061.0924\n",
      "Train Epoch: 055 Batch: 00050/00094 | Loss: 166.6132 | CE: 0.1590 | KD: 1060.8578\n",
      "Train Epoch: 055 Batch: 00051/00094 | Loss: 166.6268 | CE: 0.1697 | KD: 1060.8763\n",
      "Train Epoch: 055 Batch: 00052/00094 | Loss: 166.5667 | CE: 0.1084 | KD: 1060.8838\n",
      "Train Epoch: 055 Batch: 00053/00094 | Loss: 166.5552 | CE: 0.0858 | KD: 1060.9547\n",
      "Train Epoch: 055 Batch: 00054/00094 | Loss: 166.5174 | CE: 0.0851 | KD: 1060.7185\n",
      "Train Epoch: 055 Batch: 00055/00094 | Loss: 166.6279 | CE: 0.1674 | KD: 1060.8982\n",
      "Train Epoch: 055 Batch: 00056/00094 | Loss: 166.5318 | CE: 0.0974 | KD: 1060.7316\n",
      "Train Epoch: 055 Batch: 00057/00094 | Loss: 166.5904 | CE: 0.0806 | KD: 1061.2119\n",
      "Train Epoch: 055 Batch: 00058/00094 | Loss: 166.6011 | CE: 0.1320 | KD: 1060.9526\n",
      "Train Epoch: 055 Batch: 00059/00094 | Loss: 166.6454 | CE: 0.0999 | KD: 1061.4397\n",
      "Train Epoch: 055 Batch: 00060/00094 | Loss: 166.5804 | CE: 0.1213 | KD: 1060.8892\n",
      "Train Epoch: 055 Batch: 00061/00094 | Loss: 166.5638 | CE: 0.1317 | KD: 1060.7166\n",
      "Train Epoch: 055 Batch: 00062/00094 | Loss: 166.5340 | CE: 0.1284 | KD: 1060.5481\n",
      "Train Epoch: 055 Batch: 00063/00094 | Loss: 166.5672 | CE: 0.0702 | KD: 1061.1307\n",
      "Train Epoch: 055 Batch: 00064/00094 | Loss: 166.5811 | CE: 0.0936 | KD: 1061.0701\n",
      "Train Epoch: 055 Batch: 00065/00094 | Loss: 166.6248 | CE: 0.0891 | KD: 1061.3773\n",
      "Train Epoch: 055 Batch: 00066/00094 | Loss: 166.5962 | CE: 0.1066 | KD: 1061.0829\n",
      "Train Epoch: 055 Batch: 00067/00094 | Loss: 166.5832 | CE: 0.1088 | KD: 1060.9867\n",
      "Train Epoch: 055 Batch: 00068/00094 | Loss: 166.5724 | CE: 0.0960 | KD: 1060.9990\n",
      "Train Epoch: 055 Batch: 00069/00094 | Loss: 166.5593 | CE: 0.0828 | KD: 1060.9999\n",
      "Train Epoch: 055 Batch: 00070/00094 | Loss: 166.5325 | CE: 0.1158 | KD: 1060.6191\n",
      "Train Epoch: 055 Batch: 00071/00094 | Loss: 166.5586 | CE: 0.0749 | KD: 1061.0454\n",
      "Train Epoch: 055 Batch: 00072/00094 | Loss: 166.5533 | CE: 0.1021 | KD: 1060.8384\n",
      "Train Epoch: 055 Batch: 00073/00094 | Loss: 166.5639 | CE: 0.1046 | KD: 1060.8905\n",
      "Train Epoch: 055 Batch: 00074/00094 | Loss: 166.4802 | CE: 0.0855 | KD: 1060.4789\n",
      "Train Epoch: 055 Batch: 00075/00094 | Loss: 166.5535 | CE: 0.0735 | KD: 1061.0221\n",
      "Train Epoch: 055 Batch: 00076/00094 | Loss: 166.5526 | CE: 0.0853 | KD: 1060.9414\n",
      "Train Epoch: 055 Batch: 00077/00094 | Loss: 166.5898 | CE: 0.1057 | KD: 1061.0490\n",
      "Train Epoch: 055 Batch: 00078/00094 | Loss: 166.5609 | CE: 0.0850 | KD: 1060.9963\n",
      "Train Epoch: 055 Batch: 00079/00094 | Loss: 166.6604 | CE: 0.0975 | KD: 1061.5511\n",
      "Train Epoch: 055 Batch: 00080/00094 | Loss: 166.6723 | CE: 0.1793 | KD: 1061.1053\n",
      "Train Epoch: 055 Batch: 00081/00094 | Loss: 166.5551 | CE: 0.1032 | KD: 1060.8433\n",
      "Train Epoch: 055 Batch: 00082/00094 | Loss: 166.5355 | CE: 0.0884 | KD: 1060.8127\n",
      "Train Epoch: 055 Batch: 00083/00094 | Loss: 166.5636 | CE: 0.0825 | KD: 1061.0288\n",
      "Train Epoch: 055 Batch: 00084/00094 | Loss: 166.6436 | CE: 0.1265 | KD: 1061.2583\n",
      "Train Epoch: 055 Batch: 00085/00094 | Loss: 166.6367 | CE: 0.1171 | KD: 1061.2749\n",
      "Train Epoch: 055 Batch: 00086/00094 | Loss: 166.5527 | CE: 0.1021 | KD: 1060.8346\n",
      "Train Epoch: 055 Batch: 00087/00094 | Loss: 166.5369 | CE: 0.0967 | KD: 1060.7681\n",
      "Train Epoch: 055 Batch: 00088/00094 | Loss: 166.5632 | CE: 0.1030 | KD: 1060.8955\n",
      "Train Epoch: 055 Batch: 00089/00094 | Loss: 166.5391 | CE: 0.1044 | KD: 1060.7338\n",
      "Train Epoch: 055 Batch: 00090/00094 | Loss: 166.6257 | CE: 0.1069 | KD: 1061.2693\n",
      "Train Epoch: 055 Batch: 00091/00094 | Loss: 166.5976 | CE: 0.1599 | KD: 1060.7527\n",
      "Train Epoch: 055 Batch: 00092/00094 | Loss: 166.6707 | CE: 0.1827 | KD: 1061.0729\n",
      "Train Epoch: 055 Batch: 00093/00094 | Loss: 166.5102 | CE: 0.0729 | KD: 1060.7502\n",
      "Train Epoch: 055 Batch: 00094/00094 | Loss: 166.5040 | CE: 0.1001 | KD: 1060.5371\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0963 | acc:98.5500\n",
      "[VAL Acc] Target: 98.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6660 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1519 | acc:54.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1897 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3542 | acc:54.8589\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5879 | acc:72.7357\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8124 | acc:68.5345\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8446 | acc:58.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.75%\n",
      "[VAL Acc] Avg 62.84%\n",
      "Train Epoch: 056 Batch: 00001/00094 | Loss: 166.6078 | CE: 0.1377 | KD: 1060.9590\n",
      "Train Epoch: 056 Batch: 00002/00094 | Loss: 166.5920 | CE: 0.0936 | KD: 1061.1395\n",
      "Train Epoch: 056 Batch: 00003/00094 | Loss: 166.6301 | CE: 0.1448 | KD: 1061.0558\n",
      "Train Epoch: 056 Batch: 00004/00094 | Loss: 166.5587 | CE: 0.0918 | KD: 1060.9386\n",
      "Train Epoch: 056 Batch: 00005/00094 | Loss: 166.6143 | CE: 0.1467 | KD: 1060.9431\n",
      "Train Epoch: 056 Batch: 00006/00094 | Loss: 166.6248 | CE: 0.1739 | KD: 1060.8367\n",
      "Train Epoch: 056 Batch: 00007/00094 | Loss: 166.5556 | CE: 0.0954 | KD: 1060.8964\n",
      "Train Epoch: 056 Batch: 00008/00094 | Loss: 166.5564 | CE: 0.1601 | KD: 1060.4885\n",
      "Train Epoch: 056 Batch: 00009/00094 | Loss: 166.5702 | CE: 0.1334 | KD: 1060.7468\n",
      "Train Epoch: 056 Batch: 00010/00094 | Loss: 166.5790 | CE: 0.1016 | KD: 1061.0055\n",
      "Train Epoch: 056 Batch: 00011/00094 | Loss: 166.5495 | CE: 0.0917 | KD: 1060.8809\n",
      "Train Epoch: 056 Batch: 00012/00094 | Loss: 166.6323 | CE: 0.1065 | KD: 1061.3140\n",
      "Train Epoch: 056 Batch: 00013/00094 | Loss: 166.4881 | CE: 0.0729 | KD: 1060.6088\n",
      "Train Epoch: 056 Batch: 00014/00094 | Loss: 166.5612 | CE: 0.0853 | KD: 1060.9963\n",
      "Train Epoch: 056 Batch: 00015/00094 | Loss: 166.5876 | CE: 0.1019 | KD: 1061.0585\n",
      "Train Epoch: 056 Batch: 00016/00094 | Loss: 166.6474 | CE: 0.1526 | KD: 1061.1166\n",
      "Train Epoch: 056 Batch: 00017/00094 | Loss: 166.5552 | CE: 0.0842 | KD: 1060.9648\n",
      "Train Epoch: 056 Batch: 00018/00094 | Loss: 166.5978 | CE: 0.0922 | KD: 1061.1857\n",
      "Train Epoch: 056 Batch: 00019/00094 | Loss: 166.5140 | CE: 0.0882 | KD: 1060.6766\n",
      "Train Epoch: 056 Batch: 00020/00094 | Loss: 166.5277 | CE: 0.0981 | KD: 1060.7009\n",
      "Train Epoch: 056 Batch: 00021/00094 | Loss: 166.5484 | CE: 0.0824 | KD: 1060.9329\n",
      "Train Epoch: 056 Batch: 00022/00094 | Loss: 166.6012 | CE: 0.0819 | KD: 1061.2726\n",
      "Train Epoch: 056 Batch: 00023/00094 | Loss: 166.6228 | CE: 0.1120 | KD: 1061.2183\n",
      "Train Epoch: 056 Batch: 00024/00094 | Loss: 166.6121 | CE: 0.1101 | KD: 1061.1625\n",
      "Train Epoch: 056 Batch: 00025/00094 | Loss: 166.5621 | CE: 0.0909 | KD: 1060.9661\n",
      "Train Epoch: 056 Batch: 00026/00094 | Loss: 166.5487 | CE: 0.0856 | KD: 1060.9143\n",
      "Train Epoch: 056 Batch: 00027/00094 | Loss: 166.5168 | CE: 0.0873 | KD: 1060.7003\n",
      "Train Epoch: 056 Batch: 00028/00094 | Loss: 166.6082 | CE: 0.1011 | KD: 1061.1952\n",
      "Train Epoch: 056 Batch: 00029/00094 | Loss: 166.5688 | CE: 0.0939 | KD: 1060.9896\n",
      "Train Epoch: 056 Batch: 00030/00094 | Loss: 166.5857 | CE: 0.1202 | KD: 1060.9301\n",
      "Train Epoch: 056 Batch: 00031/00094 | Loss: 166.5454 | CE: 0.0869 | KD: 1060.8853\n",
      "Train Epoch: 056 Batch: 00032/00094 | Loss: 166.5145 | CE: 0.0817 | KD: 1060.7219\n",
      "Train Epoch: 056 Batch: 00033/00094 | Loss: 166.7826 | CE: 0.2546 | KD: 1061.3279\n",
      "Train Epoch: 056 Batch: 00034/00094 | Loss: 166.6684 | CE: 0.2095 | KD: 1060.8882\n",
      "Train Epoch: 056 Batch: 00035/00094 | Loss: 166.5408 | CE: 0.0769 | KD: 1060.9200\n",
      "Train Epoch: 056 Batch: 00036/00094 | Loss: 166.5014 | CE: 0.0844 | KD: 1060.6206\n",
      "Train Epoch: 056 Batch: 00037/00094 | Loss: 166.6490 | CE: 0.1024 | KD: 1061.4464\n",
      "Train Epoch: 056 Batch: 00038/00094 | Loss: 166.5259 | CE: 0.0749 | KD: 1060.8370\n",
      "Train Epoch: 056 Batch: 00039/00094 | Loss: 166.5919 | CE: 0.0906 | KD: 1061.1578\n",
      "Train Epoch: 056 Batch: 00040/00094 | Loss: 166.6964 | CE: 0.0994 | KD: 1061.7681\n",
      "Train Epoch: 056 Batch: 00041/00094 | Loss: 166.5377 | CE: 0.1033 | KD: 1060.7321\n",
      "Train Epoch: 056 Batch: 00042/00094 | Loss: 166.5237 | CE: 0.0826 | KD: 1060.7742\n",
      "Train Epoch: 056 Batch: 00043/00094 | Loss: 166.4854 | CE: 0.0647 | KD: 1060.6442\n",
      "Train Epoch: 056 Batch: 00044/00094 | Loss: 166.5320 | CE: 0.0732 | KD: 1060.8872\n",
      "Train Epoch: 056 Batch: 00045/00094 | Loss: 166.5973 | CE: 0.1217 | KD: 1060.9944\n",
      "Train Epoch: 056 Batch: 00046/00094 | Loss: 166.5717 | CE: 0.1716 | KD: 1060.5126\n",
      "Train Epoch: 056 Batch: 00047/00094 | Loss: 166.5972 | CE: 0.1129 | KD: 1061.0491\n",
      "Train Epoch: 056 Batch: 00048/00094 | Loss: 166.6212 | CE: 0.1102 | KD: 1061.2198\n",
      "Train Epoch: 056 Batch: 00049/00094 | Loss: 166.5804 | CE: 0.0922 | KD: 1061.0743\n",
      "Train Epoch: 056 Batch: 00050/00094 | Loss: 166.6338 | CE: 0.1252 | KD: 1061.2045\n",
      "Train Epoch: 056 Batch: 00051/00094 | Loss: 166.6029 | CE: 0.1069 | KD: 1061.1240\n",
      "Train Epoch: 056 Batch: 00052/00094 | Loss: 166.6170 | CE: 0.1191 | KD: 1061.1364\n",
      "Train Epoch: 056 Batch: 00053/00094 | Loss: 166.8234 | CE: 0.2648 | KD: 1061.5233\n",
      "Train Epoch: 056 Batch: 00054/00094 | Loss: 166.6401 | CE: 0.1187 | KD: 1061.2856\n",
      "Train Epoch: 056 Batch: 00055/00094 | Loss: 166.5012 | CE: 0.0936 | KD: 1060.5609\n",
      "Train Epoch: 056 Batch: 00056/00094 | Loss: 166.5398 | CE: 0.1174 | KD: 1060.6550\n",
      "Train Epoch: 056 Batch: 00057/00094 | Loss: 166.5843 | CE: 0.0820 | KD: 1061.1638\n",
      "Train Epoch: 056 Batch: 00058/00094 | Loss: 166.5697 | CE: 0.1032 | KD: 1060.9362\n",
      "Train Epoch: 056 Batch: 00059/00094 | Loss: 166.5589 | CE: 0.1015 | KD: 1060.8787\n",
      "Train Epoch: 056 Batch: 00060/00094 | Loss: 166.6613 | CE: 0.1541 | KD: 1061.1949\n",
      "Train Epoch: 056 Batch: 00061/00094 | Loss: 166.5246 | CE: 0.0800 | KD: 1060.7969\n",
      "Train Epoch: 056 Batch: 00062/00094 | Loss: 166.6604 | CE: 0.1135 | KD: 1061.4490\n",
      "Train Epoch: 056 Batch: 00063/00094 | Loss: 166.5728 | CE: 0.0680 | KD: 1061.1798\n",
      "Train Epoch: 056 Batch: 00064/00094 | Loss: 166.4903 | CE: 0.0898 | KD: 1060.5155\n",
      "Train Epoch: 056 Batch: 00065/00094 | Loss: 166.5282 | CE: 0.0918 | KD: 1060.7444\n",
      "Train Epoch: 056 Batch: 00066/00094 | Loss: 166.6002 | CE: 0.1320 | KD: 1060.9470\n",
      "Train Epoch: 056 Batch: 00067/00094 | Loss: 166.6117 | CE: 0.1469 | KD: 1060.9253\n",
      "Train Epoch: 056 Batch: 00068/00094 | Loss: 166.5778 | CE: 0.1228 | KD: 1060.8630\n",
      "Train Epoch: 056 Batch: 00069/00094 | Loss: 166.6242 | CE: 0.1426 | KD: 1061.0327\n",
      "Train Epoch: 056 Batch: 00070/00094 | Loss: 166.6406 | CE: 0.1224 | KD: 1061.2654\n",
      "Train Epoch: 056 Batch: 00071/00094 | Loss: 166.5112 | CE: 0.0831 | KD: 1060.6909\n",
      "Train Epoch: 056 Batch: 00072/00094 | Loss: 166.5870 | CE: 0.1125 | KD: 1060.9871\n",
      "Train Epoch: 056 Batch: 00073/00094 | Loss: 166.5508 | CE: 0.0750 | KD: 1060.9950\n",
      "Train Epoch: 056 Batch: 00074/00094 | Loss: 166.6555 | CE: 0.1378 | KD: 1061.2625\n",
      "Train Epoch: 056 Batch: 00075/00094 | Loss: 166.5583 | CE: 0.1025 | KD: 1060.8679\n",
      "Train Epoch: 056 Batch: 00076/00094 | Loss: 166.5524 | CE: 0.1287 | KD: 1060.6635\n",
      "Train Epoch: 056 Batch: 00077/00094 | Loss: 166.6087 | CE: 0.1145 | KD: 1061.1124\n",
      "Train Epoch: 056 Batch: 00078/00094 | Loss: 166.6404 | CE: 0.1659 | KD: 1060.9868\n",
      "Train Epoch: 056 Batch: 00079/00094 | Loss: 166.5549 | CE: 0.0714 | KD: 1061.0448\n",
      "Train Epoch: 056 Batch: 00080/00094 | Loss: 166.5006 | CE: 0.0638 | KD: 1060.7465\n",
      "Train Epoch: 056 Batch: 00081/00094 | Loss: 166.6693 | CE: 0.1913 | KD: 1061.0099\n",
      "Train Epoch: 056 Batch: 00082/00094 | Loss: 166.5172 | CE: 0.1002 | KD: 1060.6207\n",
      "Train Epoch: 056 Batch: 00083/00094 | Loss: 166.5277 | CE: 0.0881 | KD: 1060.7650\n",
      "Train Epoch: 056 Batch: 00084/00094 | Loss: 166.5446 | CE: 0.0987 | KD: 1060.8042\n",
      "Train Epoch: 056 Batch: 00085/00094 | Loss: 166.5324 | CE: 0.1076 | KD: 1060.6705\n",
      "Train Epoch: 056 Batch: 00086/00094 | Loss: 166.5844 | CE: 0.0947 | KD: 1061.0845\n",
      "Train Epoch: 056 Batch: 00087/00094 | Loss: 166.6307 | CE: 0.1814 | KD: 1060.8267\n",
      "Train Epoch: 056 Batch: 00088/00094 | Loss: 166.5296 | CE: 0.1117 | KD: 1060.6267\n",
      "Train Epoch: 056 Batch: 00089/00094 | Loss: 166.5654 | CE: 0.0718 | KD: 1061.1090\n",
      "Train Epoch: 056 Batch: 00090/00094 | Loss: 166.6666 | CE: 0.2055 | KD: 1060.9021\n",
      "Train Epoch: 056 Batch: 00091/00094 | Loss: 166.5623 | CE: 0.0715 | KD: 1061.0914\n",
      "Train Epoch: 056 Batch: 00092/00094 | Loss: 166.5372 | CE: 0.0752 | KD: 1060.9070\n",
      "Train Epoch: 056 Batch: 00093/00094 | Loss: 166.6145 | CE: 0.1101 | KD: 1061.1776\n",
      "Train Epoch: 056 Batch: 00094/00094 | Loss: 166.6377 | CE: 0.1374 | KD: 1061.1515\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0961 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5319 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1003 | acc:55.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1143 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1982 | acc:55.9169\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5714 | acc:73.6599\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7155 | acc:71.6301\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8346 | acc:60.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.81%\n",
      "[VAL Acc] Avg 63.84%\n",
      "VAL Acc improve from 63.52% to 63.84%\n",
      "Save best model\n",
      "Train Epoch: 057 Batch: 00001/00094 | Loss: 166.6253 | CE: 0.0729 | KD: 1061.4835\n",
      "Train Epoch: 057 Batch: 00002/00094 | Loss: 166.6023 | CE: 0.1499 | KD: 1060.8466\n",
      "Train Epoch: 057 Batch: 00003/00094 | Loss: 166.6215 | CE: 0.0889 | KD: 1061.3579\n",
      "Train Epoch: 057 Batch: 00004/00094 | Loss: 166.5724 | CE: 0.1098 | KD: 1060.9115\n",
      "Train Epoch: 057 Batch: 00005/00094 | Loss: 166.5943 | CE: 0.1286 | KD: 1060.9313\n",
      "Train Epoch: 057 Batch: 00006/00094 | Loss: 166.5984 | CE: 0.0909 | KD: 1061.1978\n",
      "Train Epoch: 057 Batch: 00007/00094 | Loss: 166.6124 | CE: 0.1055 | KD: 1061.1936\n",
      "Train Epoch: 057 Batch: 00008/00094 | Loss: 166.5470 | CE: 0.0782 | KD: 1060.9509\n",
      "Train Epoch: 057 Batch: 00009/00094 | Loss: 166.5594 | CE: 0.1178 | KD: 1060.7773\n",
      "Train Epoch: 057 Batch: 00010/00094 | Loss: 166.5468 | CE: 0.0769 | KD: 1060.9578\n",
      "Train Epoch: 057 Batch: 00011/00094 | Loss: 166.6144 | CE: 0.1373 | KD: 1061.0039\n",
      "Train Epoch: 057 Batch: 00012/00094 | Loss: 166.7231 | CE: 0.1150 | KD: 1061.8387\n",
      "Train Epoch: 057 Batch: 00013/00094 | Loss: 166.5290 | CE: 0.0992 | KD: 1060.7021\n",
      "Train Epoch: 057 Batch: 00014/00094 | Loss: 166.6205 | CE: 0.1074 | KD: 1061.2329\n",
      "Train Epoch: 057 Batch: 00015/00094 | Loss: 166.5813 | CE: 0.0833 | KD: 1061.1367\n",
      "Train Epoch: 057 Batch: 00016/00094 | Loss: 166.5675 | CE: 0.0656 | KD: 1061.1613\n",
      "Train Epoch: 057 Batch: 00017/00094 | Loss: 166.7233 | CE: 0.2200 | KD: 1061.1710\n",
      "Train Epoch: 057 Batch: 00018/00094 | Loss: 166.5276 | CE: 0.1185 | KD: 1060.5701\n",
      "Train Epoch: 057 Batch: 00019/00094 | Loss: 166.5621 | CE: 0.1179 | KD: 1060.7937\n",
      "Train Epoch: 057 Batch: 00020/00094 | Loss: 166.5596 | CE: 0.0808 | KD: 1061.0145\n",
      "Train Epoch: 057 Batch: 00021/00094 | Loss: 166.5927 | CE: 0.1146 | KD: 1061.0101\n",
      "Train Epoch: 057 Batch: 00022/00094 | Loss: 166.6608 | CE: 0.1693 | KD: 1061.0952\n",
      "Train Epoch: 057 Batch: 00023/00094 | Loss: 166.5313 | CE: 0.0886 | KD: 1060.7844\n",
      "Train Epoch: 057 Batch: 00024/00094 | Loss: 166.5770 | CE: 0.1008 | KD: 1060.9982\n",
      "Train Epoch: 057 Batch: 00025/00094 | Loss: 166.6549 | CE: 0.0972 | KD: 1061.5175\n",
      "Train Epoch: 057 Batch: 00026/00094 | Loss: 166.6049 | CE: 0.1026 | KD: 1061.1648\n",
      "Train Epoch: 057 Batch: 00027/00094 | Loss: 166.5517 | CE: 0.0854 | KD: 1060.9349\n",
      "Train Epoch: 057 Batch: 00028/00094 | Loss: 166.5403 | CE: 0.0852 | KD: 1060.8634\n",
      "Train Epoch: 057 Batch: 00029/00094 | Loss: 166.5750 | CE: 0.1236 | KD: 1060.8401\n",
      "Train Epoch: 057 Batch: 00030/00094 | Loss: 166.5937 | CE: 0.1326 | KD: 1060.9016\n",
      "Train Epoch: 057 Batch: 00031/00094 | Loss: 166.5046 | CE: 0.0717 | KD: 1060.7220\n",
      "Train Epoch: 057 Batch: 00032/00094 | Loss: 166.5336 | CE: 0.0813 | KD: 1060.8457\n",
      "Train Epoch: 057 Batch: 00033/00094 | Loss: 166.4684 | CE: 0.0774 | KD: 1060.4550\n",
      "Train Epoch: 057 Batch: 00034/00094 | Loss: 166.5895 | CE: 0.1505 | KD: 1060.7606\n",
      "Train Epoch: 057 Batch: 00035/00094 | Loss: 166.5307 | CE: 0.0804 | KD: 1060.8331\n",
      "Train Epoch: 057 Batch: 00036/00094 | Loss: 166.6902 | CE: 0.2103 | KD: 1061.0221\n",
      "Train Epoch: 057 Batch: 00037/00094 | Loss: 166.5591 | CE: 0.0983 | KD: 1060.8997\n",
      "Train Epoch: 057 Batch: 00038/00094 | Loss: 166.5324 | CE: 0.0755 | KD: 1060.8748\n",
      "Train Epoch: 057 Batch: 00039/00094 | Loss: 166.5343 | CE: 0.0713 | KD: 1060.9141\n",
      "Train Epoch: 057 Batch: 00040/00094 | Loss: 166.6254 | CE: 0.1093 | KD: 1061.2527\n",
      "Train Epoch: 057 Batch: 00041/00094 | Loss: 166.5299 | CE: 0.0966 | KD: 1060.7250\n",
      "Train Epoch: 057 Batch: 00042/00094 | Loss: 166.5648 | CE: 0.0995 | KD: 1060.9287\n",
      "Train Epoch: 057 Batch: 00043/00094 | Loss: 166.6381 | CE: 0.1192 | KD: 1061.2703\n",
      "Train Epoch: 057 Batch: 00044/00094 | Loss: 166.5032 | CE: 0.0816 | KD: 1060.6504\n",
      "Train Epoch: 057 Batch: 00045/00094 | Loss: 166.6105 | CE: 0.1246 | KD: 1061.0592\n",
      "Train Epoch: 057 Batch: 00046/00094 | Loss: 166.5032 | CE: 0.0612 | KD: 1060.7794\n",
      "Train Epoch: 057 Batch: 00047/00094 | Loss: 166.5865 | CE: 0.1077 | KD: 1061.0143\n",
      "Train Epoch: 057 Batch: 00048/00094 | Loss: 166.5681 | CE: 0.1009 | KD: 1060.9406\n",
      "Train Epoch: 057 Batch: 00049/00094 | Loss: 166.5310 | CE: 0.1142 | KD: 1060.6194\n",
      "Train Epoch: 057 Batch: 00050/00094 | Loss: 166.4962 | CE: 0.0931 | KD: 1060.5323\n",
      "Train Epoch: 057 Batch: 00051/00094 | Loss: 166.6037 | CE: 0.1741 | KD: 1060.7015\n",
      "Train Epoch: 057 Batch: 00052/00094 | Loss: 166.6344 | CE: 0.1026 | KD: 1061.3524\n",
      "Train Epoch: 057 Batch: 00053/00094 | Loss: 166.7049 | CE: 0.1601 | KD: 1061.4349\n",
      "Train Epoch: 057 Batch: 00054/00094 | Loss: 166.5893 | CE: 0.1269 | KD: 1060.9102\n",
      "Train Epoch: 057 Batch: 00055/00094 | Loss: 166.5410 | CE: 0.0671 | KD: 1060.9832\n",
      "Train Epoch: 057 Batch: 00056/00094 | Loss: 166.7016 | CE: 0.1292 | KD: 1061.6112\n",
      "Train Epoch: 057 Batch: 00057/00094 | Loss: 166.6194 | CE: 0.1169 | KD: 1061.1661\n",
      "Train Epoch: 057 Batch: 00058/00094 | Loss: 166.7258 | CE: 0.2378 | KD: 1061.0736\n",
      "Train Epoch: 057 Batch: 00059/00094 | Loss: 166.5717 | CE: 0.1176 | KD: 1060.8572\n",
      "Train Epoch: 057 Batch: 00060/00094 | Loss: 166.5590 | CE: 0.0721 | KD: 1061.0665\n",
      "Train Epoch: 057 Batch: 00061/00094 | Loss: 166.5207 | CE: 0.0758 | KD: 1060.7988\n",
      "Train Epoch: 057 Batch: 00062/00094 | Loss: 166.5149 | CE: 0.0839 | KD: 1060.7096\n",
      "Train Epoch: 057 Batch: 00063/00094 | Loss: 166.5378 | CE: 0.0865 | KD: 1060.8392\n",
      "Train Epoch: 057 Batch: 00064/00094 | Loss: 166.5210 | CE: 0.0930 | KD: 1060.6908\n",
      "Train Epoch: 057 Batch: 00065/00094 | Loss: 166.5822 | CE: 0.0978 | KD: 1061.0500\n",
      "Train Epoch: 057 Batch: 00066/00094 | Loss: 166.5650 | CE: 0.1217 | KD: 1060.7883\n",
      "Train Epoch: 057 Batch: 00067/00094 | Loss: 166.5603 | CE: 0.0860 | KD: 1060.9861\n",
      "Train Epoch: 057 Batch: 00068/00094 | Loss: 166.6510 | CE: 0.1348 | KD: 1061.2524\n",
      "Train Epoch: 057 Batch: 00069/00094 | Loss: 166.5161 | CE: 0.0844 | KD: 1060.7144\n",
      "Train Epoch: 057 Batch: 00070/00094 | Loss: 166.5554 | CE: 0.1010 | KD: 1060.8593\n",
      "Train Epoch: 057 Batch: 00071/00094 | Loss: 166.5099 | CE: 0.0761 | KD: 1060.7283\n",
      "Train Epoch: 057 Batch: 00072/00094 | Loss: 166.5200 | CE: 0.0787 | KD: 1060.7750\n",
      "Train Epoch: 057 Batch: 00073/00094 | Loss: 166.5363 | CE: 0.0642 | KD: 1060.9716\n",
      "Train Epoch: 057 Batch: 00074/00094 | Loss: 166.6302 | CE: 0.1272 | KD: 1061.1685\n",
      "Train Epoch: 057 Batch: 00075/00094 | Loss: 166.6406 | CE: 0.1111 | KD: 1061.3380\n",
      "Train Epoch: 057 Batch: 00076/00094 | Loss: 166.5800 | CE: 0.1346 | KD: 1060.8016\n",
      "Train Epoch: 057 Batch: 00077/00094 | Loss: 166.5755 | CE: 0.0956 | KD: 1061.0215\n",
      "Train Epoch: 057 Batch: 00078/00094 | Loss: 166.5822 | CE: 0.0781 | KD: 1061.1758\n",
      "Train Epoch: 057 Batch: 00079/00094 | Loss: 166.5912 | CE: 0.1230 | KD: 1060.9468\n",
      "Train Epoch: 057 Batch: 00080/00094 | Loss: 166.5806 | CE: 0.1110 | KD: 1060.9553\n",
      "Train Epoch: 057 Batch: 00081/00094 | Loss: 166.6997 | CE: 0.1773 | KD: 1061.2922\n",
      "Train Epoch: 057 Batch: 00082/00094 | Loss: 166.5553 | CE: 0.1104 | KD: 1060.7983\n",
      "Train Epoch: 057 Batch: 00083/00094 | Loss: 166.6075 | CE: 0.1304 | KD: 1061.0035\n",
      "Train Epoch: 057 Batch: 00084/00094 | Loss: 166.6180 | CE: 0.0870 | KD: 1061.3472\n",
      "Train Epoch: 057 Batch: 00085/00094 | Loss: 166.5482 | CE: 0.0855 | KD: 1060.9121\n",
      "Train Epoch: 057 Batch: 00086/00094 | Loss: 166.5713 | CE: 0.0812 | KD: 1061.0868\n",
      "Train Epoch: 057 Batch: 00087/00094 | Loss: 166.5836 | CE: 0.1250 | KD: 1060.8857\n",
      "Train Epoch: 057 Batch: 00088/00094 | Loss: 166.5164 | CE: 0.1018 | KD: 1060.6053\n",
      "Train Epoch: 057 Batch: 00089/00094 | Loss: 166.5594 | CE: 0.0649 | KD: 1061.1146\n",
      "Train Epoch: 057 Batch: 00090/00094 | Loss: 166.5483 | CE: 0.1051 | KD: 1060.7876\n",
      "Train Epoch: 057 Batch: 00091/00094 | Loss: 166.6026 | CE: 0.0856 | KD: 1061.2584\n",
      "Train Epoch: 057 Batch: 00092/00094 | Loss: 166.5561 | CE: 0.1056 | KD: 1060.8347\n",
      "Train Epoch: 057 Batch: 00093/00094 | Loss: 166.7067 | CE: 0.1261 | KD: 1061.6633\n",
      "Train Epoch: 057 Batch: 00094/00094 | Loss: 166.6413 | CE: 0.1098 | KD: 1061.3502\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1063 | acc:98.0500\n",
      "[VAL Acc] Target: 98.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7869 | acc:48.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2583 | acc:52.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2350 | acc:45.0382\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2255 | acc:57.3668\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6002 | acc:72.5508\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8159 | acc:71.0815\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8858 | acc:59.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.88%\n",
      "[VAL Acc] Avg 63.17%\n",
      "Train Epoch: 058 Batch: 00001/00094 | Loss: 166.5998 | CE: 0.0924 | KD: 1061.1968\n",
      "Train Epoch: 058 Batch: 00002/00094 | Loss: 166.6090 | CE: 0.1114 | KD: 1061.1338\n",
      "Train Epoch: 058 Batch: 00003/00094 | Loss: 166.5047 | CE: 0.0629 | KD: 1060.7788\n",
      "Train Epoch: 058 Batch: 00004/00094 | Loss: 166.5746 | CE: 0.0790 | KD: 1061.1213\n",
      "Train Epoch: 058 Batch: 00005/00094 | Loss: 166.5358 | CE: 0.0834 | KD: 1060.8462\n",
      "Train Epoch: 058 Batch: 00006/00094 | Loss: 166.5027 | CE: 0.0722 | KD: 1060.7068\n",
      "Train Epoch: 058 Batch: 00007/00094 | Loss: 166.5706 | CE: 0.1076 | KD: 1060.9141\n",
      "Train Epoch: 058 Batch: 00008/00094 | Loss: 166.5781 | CE: 0.1086 | KD: 1060.9554\n",
      "Train Epoch: 058 Batch: 00009/00094 | Loss: 166.5139 | CE: 0.0989 | KD: 1060.6078\n",
      "Train Epoch: 058 Batch: 00010/00094 | Loss: 166.5522 | CE: 0.1110 | KD: 1060.7745\n",
      "Train Epoch: 058 Batch: 00011/00094 | Loss: 166.5870 | CE: 0.0825 | KD: 1061.1787\n",
      "Train Epoch: 058 Batch: 00012/00094 | Loss: 166.5645 | CE: 0.1076 | KD: 1060.8746\n",
      "Train Epoch: 058 Batch: 00013/00094 | Loss: 166.5130 | CE: 0.0998 | KD: 1060.5963\n",
      "Train Epoch: 058 Batch: 00014/00094 | Loss: 166.5499 | CE: 0.0581 | KD: 1061.0974\n",
      "Train Epoch: 058 Batch: 00015/00094 | Loss: 166.5204 | CE: 0.0999 | KD: 1060.6432\n",
      "Train Epoch: 058 Batch: 00016/00094 | Loss: 166.5246 | CE: 0.0689 | KD: 1060.8676\n",
      "Train Epoch: 058 Batch: 00017/00094 | Loss: 166.5089 | CE: 0.0734 | KD: 1060.7383\n",
      "Train Epoch: 058 Batch: 00018/00094 | Loss: 166.5366 | CE: 0.0839 | KD: 1060.8484\n",
      "Train Epoch: 058 Batch: 00019/00094 | Loss: 166.6717 | CE: 0.1068 | KD: 1061.5635\n",
      "Train Epoch: 058 Batch: 00020/00094 | Loss: 166.6788 | CE: 0.1795 | KD: 1061.1453\n",
      "Train Epoch: 058 Batch: 00021/00094 | Loss: 166.5491 | CE: 0.1150 | KD: 1060.7296\n",
      "Train Epoch: 058 Batch: 00022/00094 | Loss: 166.6702 | CE: 0.1599 | KD: 1061.2155\n",
      "Train Epoch: 058 Batch: 00023/00094 | Loss: 166.6397 | CE: 0.1433 | KD: 1061.1266\n",
      "Train Epoch: 058 Batch: 00024/00094 | Loss: 166.5727 | CE: 0.1135 | KD: 1060.8898\n",
      "Train Epoch: 058 Batch: 00025/00094 | Loss: 166.6350 | CE: 0.1424 | KD: 1061.1029\n",
      "Train Epoch: 058 Batch: 00026/00094 | Loss: 166.6721 | CE: 0.1352 | KD: 1061.3853\n",
      "Train Epoch: 058 Batch: 00027/00094 | Loss: 166.5549 | CE: 0.1176 | KD: 1060.7498\n",
      "Train Epoch: 058 Batch: 00028/00094 | Loss: 166.6358 | CE: 0.1624 | KD: 1060.9806\n",
      "Train Epoch: 058 Batch: 00029/00094 | Loss: 166.6025 | CE: 0.0998 | KD: 1061.1666\n",
      "Train Epoch: 058 Batch: 00030/00094 | Loss: 166.6626 | CE: 0.1280 | KD: 1061.3696\n",
      "Train Epoch: 058 Batch: 00031/00094 | Loss: 166.8216 | CE: 0.3541 | KD: 1060.9427\n",
      "Train Epoch: 058 Batch: 00032/00094 | Loss: 166.6224 | CE: 0.1079 | KD: 1061.2419\n",
      "Train Epoch: 058 Batch: 00033/00094 | Loss: 166.5900 | CE: 0.0959 | KD: 1061.1119\n",
      "Train Epoch: 058 Batch: 00034/00094 | Loss: 166.6941 | CE: 0.1873 | KD: 1061.1929\n",
      "Train Epoch: 058 Batch: 00035/00094 | Loss: 166.5575 | CE: 0.0743 | KD: 1061.0420\n",
      "Train Epoch: 058 Batch: 00036/00094 | Loss: 166.5341 | CE: 0.0727 | KD: 1060.9041\n",
      "Train Epoch: 058 Batch: 00037/00094 | Loss: 166.5574 | CE: 0.1068 | KD: 1060.8351\n",
      "Train Epoch: 058 Batch: 00038/00094 | Loss: 166.5680 | CE: 0.0953 | KD: 1060.9757\n",
      "Train Epoch: 058 Batch: 00039/00094 | Loss: 166.5720 | CE: 0.1424 | KD: 1060.7009\n",
      "Train Epoch: 058 Batch: 00040/00094 | Loss: 166.5647 | CE: 0.0862 | KD: 1061.0128\n",
      "Train Epoch: 058 Batch: 00041/00094 | Loss: 166.5285 | CE: 0.0640 | KD: 1060.9237\n",
      "Train Epoch: 058 Batch: 00042/00094 | Loss: 166.6030 | CE: 0.1257 | KD: 1061.0045\n",
      "Train Epoch: 058 Batch: 00043/00094 | Loss: 166.7868 | CE: 0.2473 | KD: 1061.4020\n",
      "Train Epoch: 058 Batch: 00044/00094 | Loss: 166.5996 | CE: 0.1063 | KD: 1061.1073\n",
      "Train Epoch: 058 Batch: 00045/00094 | Loss: 166.6238 | CE: 0.1313 | KD: 1061.1017\n",
      "Train Epoch: 058 Batch: 00046/00094 | Loss: 166.4798 | CE: 0.0759 | KD: 1060.5375\n",
      "Train Epoch: 058 Batch: 00047/00094 | Loss: 166.6084 | CE: 0.1025 | KD: 1061.1870\n",
      "Train Epoch: 058 Batch: 00048/00094 | Loss: 166.6318 | CE: 0.0918 | KD: 1061.4050\n",
      "Train Epoch: 058 Batch: 00049/00094 | Loss: 166.5793 | CE: 0.1046 | KD: 1060.9888\n",
      "Train Epoch: 058 Batch: 00050/00094 | Loss: 166.4953 | CE: 0.0719 | KD: 1060.6615\n",
      "Train Epoch: 058 Batch: 00051/00094 | Loss: 166.5101 | CE: 0.1077 | KD: 1060.5277\n",
      "Train Epoch: 058 Batch: 00052/00094 | Loss: 166.5099 | CE: 0.0664 | KD: 1060.7891\n",
      "Train Epoch: 058 Batch: 00053/00094 | Loss: 166.5242 | CE: 0.0696 | KD: 1060.8599\n",
      "Train Epoch: 058 Batch: 00054/00094 | Loss: 166.6381 | CE: 0.1268 | KD: 1061.2217\n",
      "Train Epoch: 058 Batch: 00055/00094 | Loss: 166.6006 | CE: 0.1015 | KD: 1061.1438\n",
      "Train Epoch: 058 Batch: 00056/00094 | Loss: 166.5411 | CE: 0.0966 | KD: 1060.7961\n",
      "Train Epoch: 058 Batch: 00057/00094 | Loss: 166.5352 | CE: 0.0763 | KD: 1060.8882\n",
      "Train Epoch: 058 Batch: 00058/00094 | Loss: 166.6074 | CE: 0.0949 | KD: 1061.2291\n",
      "Train Epoch: 058 Batch: 00059/00094 | Loss: 166.5368 | CE: 0.1039 | KD: 1060.7223\n",
      "Train Epoch: 058 Batch: 00060/00094 | Loss: 166.6854 | CE: 0.1699 | KD: 1061.2479\n",
      "Train Epoch: 058 Batch: 00061/00094 | Loss: 166.5522 | CE: 0.1339 | KD: 1060.6289\n",
      "Train Epoch: 058 Batch: 00062/00094 | Loss: 166.7891 | CE: 0.2275 | KD: 1061.5417\n",
      "Train Epoch: 058 Batch: 00063/00094 | Loss: 166.6398 | CE: 0.1474 | KD: 1061.1012\n",
      "Train Epoch: 058 Batch: 00064/00094 | Loss: 166.5652 | CE: 0.1074 | KD: 1060.8804\n",
      "Train Epoch: 058 Batch: 00065/00094 | Loss: 166.5902 | CE: 0.0937 | KD: 1061.1276\n",
      "Train Epoch: 058 Batch: 00066/00094 | Loss: 166.5380 | CE: 0.0808 | KD: 1060.8766\n",
      "Train Epoch: 058 Batch: 00067/00094 | Loss: 166.5941 | CE: 0.0919 | KD: 1061.1637\n",
      "Train Epoch: 058 Batch: 00068/00094 | Loss: 166.6383 | CE: 0.1087 | KD: 1061.3378\n",
      "Train Epoch: 058 Batch: 00069/00094 | Loss: 166.6472 | CE: 0.1641 | KD: 1061.0419\n",
      "Train Epoch: 058 Batch: 00070/00094 | Loss: 166.5876 | CE: 0.0827 | KD: 1061.1814\n",
      "Train Epoch: 058 Batch: 00071/00094 | Loss: 166.6236 | CE: 0.1155 | KD: 1061.2015\n",
      "Train Epoch: 058 Batch: 00072/00094 | Loss: 166.5073 | CE: 0.0637 | KD: 1060.7897\n",
      "Train Epoch: 058 Batch: 00073/00094 | Loss: 166.5698 | CE: 0.1186 | KD: 1060.8386\n",
      "Train Epoch: 058 Batch: 00074/00094 | Loss: 166.4914 | CE: 0.0745 | KD: 1060.6202\n",
      "Train Epoch: 058 Batch: 00075/00094 | Loss: 166.5374 | CE: 0.0901 | KD: 1060.8136\n",
      "Train Epoch: 058 Batch: 00076/00094 | Loss: 166.7474 | CE: 0.1124 | KD: 1062.0105\n",
      "Train Epoch: 058 Batch: 00077/00094 | Loss: 166.5851 | CE: 0.0977 | KD: 1061.0695\n",
      "Train Epoch: 058 Batch: 00078/00094 | Loss: 166.5484 | CE: 0.1309 | KD: 1060.6241\n",
      "Train Epoch: 058 Batch: 00079/00094 | Loss: 166.5188 | CE: 0.0977 | KD: 1060.6464\n",
      "Train Epoch: 058 Batch: 00080/00094 | Loss: 166.5852 | CE: 0.1395 | KD: 1060.8032\n",
      "Train Epoch: 058 Batch: 00081/00094 | Loss: 166.5758 | CE: 0.0937 | KD: 1061.0354\n",
      "Train Epoch: 058 Batch: 00082/00094 | Loss: 166.5526 | CE: 0.1114 | KD: 1060.7748\n",
      "Train Epoch: 058 Batch: 00083/00094 | Loss: 166.5979 | CE: 0.0740 | KD: 1061.3021\n",
      "Train Epoch: 058 Batch: 00084/00094 | Loss: 166.5056 | CE: 0.0731 | KD: 1060.7194\n",
      "Train Epoch: 058 Batch: 00085/00094 | Loss: 166.7370 | CE: 0.2421 | KD: 1061.1168\n",
      "Train Epoch: 058 Batch: 00086/00094 | Loss: 166.6024 | CE: 0.1159 | KD: 1061.0641\n",
      "Train Epoch: 058 Batch: 00087/00094 | Loss: 166.5510 | CE: 0.0933 | KD: 1060.8798\n",
      "Train Epoch: 058 Batch: 00088/00094 | Loss: 166.6707 | CE: 0.1641 | KD: 1061.1918\n",
      "Train Epoch: 058 Batch: 00089/00094 | Loss: 166.5408 | CE: 0.1483 | KD: 1060.4645\n",
      "Train Epoch: 058 Batch: 00090/00094 | Loss: 166.5618 | CE: 0.0787 | KD: 1061.0415\n",
      "Train Epoch: 058 Batch: 00091/00094 | Loss: 166.5661 | CE: 0.0964 | KD: 1060.9569\n",
      "Train Epoch: 058 Batch: 00092/00094 | Loss: 166.5755 | CE: 0.0947 | KD: 1061.0270\n",
      "Train Epoch: 058 Batch: 00093/00094 | Loss: 166.5613 | CE: 0.0813 | KD: 1061.0219\n",
      "Train Epoch: 058 Batch: 00094/00094 | Loss: 166.5786 | CE: 0.0786 | KD: 1061.1498\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0868 | acc:99.0500\n",
      "[VAL Acc] Target: 99.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6719 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1927 | acc:53.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1782 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2113 | acc:56.9357\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7793 | acc:62.5693\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 62.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7356 | acc:71.9044\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8220 | acc:61.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.81%\n",
      "[VAL Acc] Avg 62.44%\n",
      "Train Epoch: 059 Batch: 00001/00094 | Loss: 166.6270 | CE: 0.0805 | KD: 1061.4456\n",
      "Train Epoch: 059 Batch: 00002/00094 | Loss: 166.5695 | CE: 0.0939 | KD: 1060.9940\n",
      "Train Epoch: 059 Batch: 00003/00094 | Loss: 166.5797 | CE: 0.0930 | KD: 1061.0648\n",
      "Train Epoch: 059 Batch: 00004/00094 | Loss: 166.5948 | CE: 0.1281 | KD: 1060.9371\n",
      "Train Epoch: 059 Batch: 00005/00094 | Loss: 166.5533 | CE: 0.0863 | KD: 1060.9396\n",
      "Train Epoch: 059 Batch: 00006/00094 | Loss: 166.4973 | CE: 0.0938 | KD: 1060.5343\n",
      "Train Epoch: 059 Batch: 00007/00094 | Loss: 166.7046 | CE: 0.1786 | KD: 1061.3153\n",
      "Train Epoch: 059 Batch: 00008/00094 | Loss: 166.6038 | CE: 0.1320 | KD: 1060.9696\n",
      "Train Epoch: 059 Batch: 00009/00094 | Loss: 166.6074 | CE: 0.1170 | KD: 1061.0880\n",
      "Train Epoch: 059 Batch: 00010/00094 | Loss: 166.5335 | CE: 0.0791 | KD: 1060.8591\n",
      "Train Epoch: 059 Batch: 00011/00094 | Loss: 166.6078 | CE: 0.1010 | KD: 1061.1929\n",
      "Train Epoch: 059 Batch: 00012/00094 | Loss: 166.6281 | CE: 0.1073 | KD: 1061.2823\n",
      "Train Epoch: 059 Batch: 00013/00094 | Loss: 166.6068 | CE: 0.0769 | KD: 1061.3400\n",
      "Train Epoch: 059 Batch: 00014/00094 | Loss: 166.5727 | CE: 0.0965 | KD: 1060.9983\n",
      "Train Epoch: 059 Batch: 00015/00094 | Loss: 166.5555 | CE: 0.1305 | KD: 1060.6718\n",
      "Train Epoch: 059 Batch: 00016/00094 | Loss: 166.6011 | CE: 0.0859 | KD: 1061.2463\n",
      "Train Epoch: 059 Batch: 00017/00094 | Loss: 166.5799 | CE: 0.0920 | KD: 1061.0721\n",
      "Train Epoch: 059 Batch: 00018/00094 | Loss: 166.5758 | CE: 0.1074 | KD: 1060.9485\n",
      "Train Epoch: 059 Batch: 00019/00094 | Loss: 166.5375 | CE: 0.0991 | KD: 1060.7570\n",
      "Train Epoch: 059 Batch: 00020/00094 | Loss: 166.5933 | CE: 0.0810 | KD: 1061.2278\n",
      "Train Epoch: 059 Batch: 00021/00094 | Loss: 166.5754 | CE: 0.0990 | KD: 1060.9991\n",
      "Train Epoch: 059 Batch: 00022/00094 | Loss: 166.5761 | CE: 0.0935 | KD: 1061.0389\n",
      "Train Epoch: 059 Batch: 00023/00094 | Loss: 166.5287 | CE: 0.1136 | KD: 1060.6088\n",
      "Train Epoch: 059 Batch: 00024/00094 | Loss: 166.5634 | CE: 0.0879 | KD: 1060.9940\n",
      "Train Epoch: 059 Batch: 00025/00094 | Loss: 166.5117 | CE: 0.1003 | KD: 1060.5850\n",
      "Train Epoch: 059 Batch: 00026/00094 | Loss: 166.5184 | CE: 0.0770 | KD: 1060.7759\n",
      "Train Epoch: 059 Batch: 00027/00094 | Loss: 166.5278 | CE: 0.0641 | KD: 1060.9180\n",
      "Train Epoch: 059 Batch: 00028/00094 | Loss: 166.5980 | CE: 0.1252 | KD: 1060.9763\n",
      "Train Epoch: 059 Batch: 00029/00094 | Loss: 166.5995 | CE: 0.1135 | KD: 1061.0602\n",
      "Train Epoch: 059 Batch: 00030/00094 | Loss: 166.5363 | CE: 0.1179 | KD: 1060.6296\n",
      "Train Epoch: 059 Batch: 00031/00094 | Loss: 166.5678 | CE: 0.0841 | KD: 1061.0463\n",
      "Train Epoch: 059 Batch: 00032/00094 | Loss: 166.5685 | CE: 0.1000 | KD: 1060.9489\n",
      "Train Epoch: 059 Batch: 00033/00094 | Loss: 166.7285 | CE: 0.1577 | KD: 1061.6011\n",
      "Train Epoch: 059 Batch: 00034/00094 | Loss: 166.5896 | CE: 0.1398 | KD: 1060.8301\n",
      "Train Epoch: 059 Batch: 00035/00094 | Loss: 166.5386 | CE: 0.0661 | KD: 1060.9740\n",
      "Train Epoch: 059 Batch: 00036/00094 | Loss: 166.5604 | CE: 0.1063 | KD: 1060.8574\n",
      "Train Epoch: 059 Batch: 00037/00094 | Loss: 166.5222 | CE: 0.0951 | KD: 1060.6851\n",
      "Train Epoch: 059 Batch: 00038/00094 | Loss: 166.5862 | CE: 0.0903 | KD: 1061.1230\n",
      "Train Epoch: 059 Batch: 00039/00094 | Loss: 166.5875 | CE: 0.1131 | KD: 1060.9869\n",
      "Train Epoch: 059 Batch: 00040/00094 | Loss: 166.6092 | CE: 0.0848 | KD: 1061.3055\n",
      "Train Epoch: 059 Batch: 00041/00094 | Loss: 166.6893 | CE: 0.1573 | KD: 1061.3540\n",
      "Train Epoch: 059 Batch: 00042/00094 | Loss: 166.5618 | CE: 0.1188 | KD: 1060.7861\n",
      "Train Epoch: 059 Batch: 00043/00094 | Loss: 166.6303 | CE: 0.1475 | KD: 1061.0402\n",
      "Train Epoch: 059 Batch: 00044/00094 | Loss: 166.6124 | CE: 0.0940 | KD: 1061.2667\n",
      "Train Epoch: 059 Batch: 00045/00094 | Loss: 166.5347 | CE: 0.0902 | KD: 1060.7960\n",
      "Train Epoch: 059 Batch: 00046/00094 | Loss: 166.5280 | CE: 0.0783 | KD: 1060.8287\n",
      "Train Epoch: 059 Batch: 00047/00094 | Loss: 166.6069 | CE: 0.1263 | KD: 1061.0255\n",
      "Train Epoch: 059 Batch: 00048/00094 | Loss: 166.5575 | CE: 0.0656 | KD: 1061.0977\n",
      "Train Epoch: 059 Batch: 00049/00094 | Loss: 166.6095 | CE: 0.1305 | KD: 1061.0157\n",
      "Train Epoch: 059 Batch: 00050/00094 | Loss: 166.5455 | CE: 0.0920 | KD: 1060.8533\n",
      "Train Epoch: 059 Batch: 00051/00094 | Loss: 166.5950 | CE: 0.1160 | KD: 1061.0155\n",
      "Train Epoch: 059 Batch: 00052/00094 | Loss: 166.5713 | CE: 0.0872 | KD: 1061.0482\n",
      "Train Epoch: 059 Batch: 00053/00094 | Loss: 166.6015 | CE: 0.1356 | KD: 1060.9325\n",
      "Train Epoch: 059 Batch: 00054/00094 | Loss: 166.5703 | CE: 0.0759 | KD: 1061.1139\n",
      "Train Epoch: 059 Batch: 00055/00094 | Loss: 166.6665 | CE: 0.1910 | KD: 1060.9932\n",
      "Train Epoch: 059 Batch: 00056/00094 | Loss: 166.6401 | CE: 0.0903 | KD: 1061.4668\n",
      "Train Epoch: 059 Batch: 00057/00094 | Loss: 166.5585 | CE: 0.0893 | KD: 1060.9531\n",
      "Train Epoch: 059 Batch: 00058/00094 | Loss: 166.5514 | CE: 0.0992 | KD: 1060.8455\n",
      "Train Epoch: 059 Batch: 00059/00094 | Loss: 166.6059 | CE: 0.1273 | KD: 1061.0134\n",
      "Train Epoch: 059 Batch: 00060/00094 | Loss: 166.4969 | CE: 0.0766 | KD: 1060.6420\n",
      "Train Epoch: 059 Batch: 00061/00094 | Loss: 166.6425 | CE: 0.1412 | KD: 1061.1584\n",
      "Train Epoch: 059 Batch: 00062/00094 | Loss: 166.5747 | CE: 0.1064 | KD: 1060.9474\n",
      "Train Epoch: 059 Batch: 00063/00094 | Loss: 166.6188 | CE: 0.1245 | KD: 1061.1134\n",
      "Train Epoch: 059 Batch: 00064/00094 | Loss: 166.6354 | CE: 0.1143 | KD: 1061.2839\n",
      "Train Epoch: 059 Batch: 00065/00094 | Loss: 166.5885 | CE: 0.0850 | KD: 1061.1719\n",
      "Train Epoch: 059 Batch: 00066/00094 | Loss: 166.5620 | CE: 0.1592 | KD: 1060.5303\n",
      "Train Epoch: 059 Batch: 00067/00094 | Loss: 166.5755 | CE: 0.0946 | KD: 1061.0276\n",
      "Train Epoch: 059 Batch: 00068/00094 | Loss: 166.5567 | CE: 0.0909 | KD: 1060.9319\n",
      "Train Epoch: 059 Batch: 00069/00094 | Loss: 166.6090 | CE: 0.1180 | KD: 1061.0925\n",
      "Train Epoch: 059 Batch: 00070/00094 | Loss: 166.5938 | CE: 0.1576 | KD: 1060.7433\n",
      "Train Epoch: 059 Batch: 00071/00094 | Loss: 166.5904 | CE: 0.0896 | KD: 1061.1545\n",
      "Train Epoch: 059 Batch: 00072/00094 | Loss: 166.6498 | CE: 0.1405 | KD: 1061.2090\n",
      "Train Epoch: 059 Batch: 00073/00094 | Loss: 166.5195 | CE: 0.0796 | KD: 1060.7668\n",
      "Train Epoch: 059 Batch: 00074/00094 | Loss: 166.6003 | CE: 0.0911 | KD: 1061.2084\n",
      "Train Epoch: 059 Batch: 00075/00094 | Loss: 166.5253 | CE: 0.0869 | KD: 1060.7567\n",
      "Train Epoch: 059 Batch: 00076/00094 | Loss: 166.5845 | CE: 0.1350 | KD: 1060.8278\n",
      "Train Epoch: 059 Batch: 00077/00094 | Loss: 166.6105 | CE: 0.1442 | KD: 1060.9351\n",
      "Train Epoch: 059 Batch: 00078/00094 | Loss: 166.5670 | CE: 0.0848 | KD: 1061.0361\n",
      "Train Epoch: 059 Batch: 00079/00094 | Loss: 166.6756 | CE: 0.1589 | KD: 1061.2562\n",
      "Train Epoch: 059 Batch: 00080/00094 | Loss: 166.5157 | CE: 0.0741 | KD: 1060.7777\n",
      "Train Epoch: 059 Batch: 00081/00094 | Loss: 166.5876 | CE: 0.1034 | KD: 1061.0493\n",
      "Train Epoch: 059 Batch: 00082/00094 | Loss: 166.5820 | CE: 0.0733 | KD: 1061.2053\n",
      "Train Epoch: 059 Batch: 00083/00094 | Loss: 166.4960 | CE: 0.0749 | KD: 1060.6466\n",
      "Train Epoch: 059 Batch: 00084/00094 | Loss: 166.5782 | CE: 0.0948 | KD: 1061.0442\n",
      "Train Epoch: 059 Batch: 00085/00094 | Loss: 166.4968 | CE: 0.0911 | KD: 1060.5485\n",
      "Train Epoch: 059 Batch: 00086/00094 | Loss: 166.5216 | CE: 0.0741 | KD: 1060.8153\n",
      "Train Epoch: 059 Batch: 00087/00094 | Loss: 166.6445 | CE: 0.1643 | KD: 1061.0232\n",
      "Train Epoch: 059 Batch: 00088/00094 | Loss: 166.6008 | CE: 0.0869 | KD: 1061.2383\n",
      "Train Epoch: 059 Batch: 00089/00094 | Loss: 166.6582 | CE: 0.1728 | KD: 1061.0565\n",
      "Train Epoch: 059 Batch: 00090/00094 | Loss: 166.5194 | CE: 0.0851 | KD: 1060.7306\n",
      "Train Epoch: 059 Batch: 00091/00094 | Loss: 166.5769 | CE: 0.0933 | KD: 1061.0448\n",
      "Train Epoch: 059 Batch: 00092/00094 | Loss: 166.5590 | CE: 0.1274 | KD: 1060.7139\n",
      "Train Epoch: 059 Batch: 00093/00094 | Loss: 166.5653 | CE: 0.1398 | KD: 1060.6747\n",
      "Train Epoch: 059 Batch: 00094/00094 | Loss: 166.5848 | CE: 0.0977 | KD: 1061.0671\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0944 | acc:98.2500\n",
      "[VAL Acc] Target: 98.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6368 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1590 | acc:51.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1687 | acc:45.2290\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1857 | acc:56.4655\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9018 | acc:58.7800\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 58.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7209 | acc:71.1599\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8522 | acc:60.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.38%\n",
      "[VAL Acc] Avg 61.45%\n",
      "Train Epoch: 060 Batch: 00001/00094 | Loss: 166.6711 | CE: 0.1503 | KD: 1061.2825\n",
      "Train Epoch: 060 Batch: 00002/00094 | Loss: 166.5241 | CE: 0.0859 | KD: 1060.7561\n",
      "Train Epoch: 060 Batch: 00003/00094 | Loss: 166.5540 | CE: 0.1075 | KD: 1060.8087\n",
      "Train Epoch: 060 Batch: 00004/00094 | Loss: 166.6409 | CE: 0.1528 | KD: 1061.0737\n",
      "Train Epoch: 060 Batch: 00005/00094 | Loss: 166.5797 | CE: 0.1080 | KD: 1060.9695\n",
      "Train Epoch: 060 Batch: 00006/00094 | Loss: 166.6058 | CE: 0.0849 | KD: 1061.2826\n",
      "Train Epoch: 060 Batch: 00007/00094 | Loss: 166.6079 | CE: 0.1043 | KD: 1061.1725\n",
      "Train Epoch: 060 Batch: 00008/00094 | Loss: 166.5999 | CE: 0.0937 | KD: 1061.1892\n",
      "Train Epoch: 060 Batch: 00009/00094 | Loss: 166.5868 | CE: 0.1056 | KD: 1061.0300\n",
      "Train Epoch: 060 Batch: 00010/00094 | Loss: 166.4940 | CE: 0.0727 | KD: 1060.6477\n",
      "Train Epoch: 060 Batch: 00011/00094 | Loss: 166.5705 | CE: 0.0852 | KD: 1061.0563\n",
      "Train Epoch: 060 Batch: 00012/00094 | Loss: 166.5556 | CE: 0.0907 | KD: 1060.9260\n",
      "Train Epoch: 060 Batch: 00013/00094 | Loss: 166.5252 | CE: 0.0885 | KD: 1060.7460\n",
      "Train Epoch: 060 Batch: 00014/00094 | Loss: 166.5844 | CE: 0.1027 | KD: 1061.0328\n",
      "Train Epoch: 060 Batch: 00015/00094 | Loss: 166.5294 | CE: 0.0788 | KD: 1060.8345\n",
      "Train Epoch: 060 Batch: 00016/00094 | Loss: 166.5293 | CE: 0.0681 | KD: 1060.9027\n",
      "Train Epoch: 060 Batch: 00017/00094 | Loss: 166.5780 | CE: 0.1350 | KD: 1060.7860\n",
      "Train Epoch: 060 Batch: 00018/00094 | Loss: 166.6077 | CE: 0.1216 | KD: 1061.0610\n",
      "Train Epoch: 060 Batch: 00019/00094 | Loss: 166.5915 | CE: 0.0846 | KD: 1061.1938\n",
      "Train Epoch: 060 Batch: 00020/00094 | Loss: 166.5977 | CE: 0.0934 | KD: 1061.1770\n",
      "Train Epoch: 060 Batch: 00021/00094 | Loss: 166.5669 | CE: 0.0981 | KD: 1060.9504\n",
      "Train Epoch: 060 Batch: 00022/00094 | Loss: 166.7062 | CE: 0.2330 | KD: 1060.9790\n",
      "Train Epoch: 060 Batch: 00023/00094 | Loss: 166.5156 | CE: 0.0714 | KD: 1060.7942\n",
      "Train Epoch: 060 Batch: 00024/00094 | Loss: 166.6208 | CE: 0.0931 | KD: 1061.3262\n",
      "Train Epoch: 060 Batch: 00025/00094 | Loss: 166.5670 | CE: 0.1185 | KD: 1060.8219\n",
      "Train Epoch: 060 Batch: 00026/00094 | Loss: 166.5908 | CE: 0.0829 | KD: 1061.2004\n",
      "Train Epoch: 060 Batch: 00027/00094 | Loss: 166.4762 | CE: 0.0670 | KD: 1060.5708\n",
      "Train Epoch: 060 Batch: 00028/00094 | Loss: 166.5802 | CE: 0.1153 | KD: 1060.9261\n",
      "Train Epoch: 060 Batch: 00029/00094 | Loss: 166.6081 | CE: 0.0802 | KD: 1061.3273\n",
      "Train Epoch: 060 Batch: 00030/00094 | Loss: 166.4863 | CE: 0.0752 | KD: 1060.5835\n",
      "Train Epoch: 060 Batch: 00031/00094 | Loss: 166.5439 | CE: 0.0759 | KD: 1060.9462\n",
      "Train Epoch: 060 Batch: 00032/00094 | Loss: 166.5924 | CE: 0.0676 | KD: 1061.3077\n",
      "Train Epoch: 060 Batch: 00033/00094 | Loss: 166.5159 | CE: 0.0669 | KD: 1060.8245\n",
      "Train Epoch: 060 Batch: 00034/00094 | Loss: 166.5596 | CE: 0.0892 | KD: 1060.9613\n",
      "Train Epoch: 060 Batch: 00035/00094 | Loss: 166.6306 | CE: 0.1139 | KD: 1061.2560\n",
      "Train Epoch: 060 Batch: 00036/00094 | Loss: 166.6243 | CE: 0.1743 | KD: 1060.8307\n",
      "Train Epoch: 060 Batch: 00037/00094 | Loss: 166.6030 | CE: 0.0835 | KD: 1061.2739\n",
      "Train Epoch: 060 Batch: 00038/00094 | Loss: 166.5175 | CE: 0.0766 | KD: 1060.7729\n",
      "Train Epoch: 060 Batch: 00039/00094 | Loss: 166.6330 | CE: 0.1110 | KD: 1061.2899\n",
      "Train Epoch: 060 Batch: 00040/00094 | Loss: 166.5910 | CE: 0.1125 | KD: 1061.0123\n",
      "Train Epoch: 060 Batch: 00041/00094 | Loss: 166.6120 | CE: 0.1217 | KD: 1061.0878\n",
      "Train Epoch: 060 Batch: 00042/00094 | Loss: 166.6157 | CE: 0.1471 | KD: 1060.9495\n",
      "Train Epoch: 060 Batch: 00043/00094 | Loss: 166.6229 | CE: 0.1255 | KD: 1061.1329\n",
      "Train Epoch: 060 Batch: 00044/00094 | Loss: 166.6313 | CE: 0.1239 | KD: 1061.1967\n",
      "Train Epoch: 060 Batch: 00045/00094 | Loss: 166.5175 | CE: 0.1021 | KD: 1060.6106\n",
      "Train Epoch: 060 Batch: 00046/00094 | Loss: 166.5941 | CE: 0.0805 | KD: 1061.2363\n",
      "Train Epoch: 060 Batch: 00047/00094 | Loss: 166.5372 | CE: 0.1042 | KD: 1060.7224\n",
      "Train Epoch: 060 Batch: 00048/00094 | Loss: 166.5127 | CE: 0.0839 | KD: 1060.6958\n",
      "Train Epoch: 060 Batch: 00049/00094 | Loss: 166.6781 | CE: 0.0903 | KD: 1061.7089\n",
      "Train Epoch: 060 Batch: 00050/00094 | Loss: 166.6256 | CE: 0.1465 | KD: 1061.0165\n",
      "Train Epoch: 060 Batch: 00051/00094 | Loss: 166.5160 | CE: 0.0864 | KD: 1060.7012\n",
      "Train Epoch: 060 Batch: 00052/00094 | Loss: 166.5574 | CE: 0.0810 | KD: 1060.9994\n",
      "Train Epoch: 060 Batch: 00053/00094 | Loss: 166.6008 | CE: 0.0917 | KD: 1061.2074\n",
      "Train Epoch: 060 Batch: 00054/00094 | Loss: 166.5586 | CE: 0.1208 | KD: 1060.7528\n",
      "Train Epoch: 060 Batch: 00055/00094 | Loss: 166.5567 | CE: 0.0780 | KD: 1061.0139\n",
      "Train Epoch: 060 Batch: 00056/00094 | Loss: 166.5431 | CE: 0.0980 | KD: 1060.7997\n",
      "Train Epoch: 060 Batch: 00057/00094 | Loss: 166.5537 | CE: 0.0911 | KD: 1060.9110\n",
      "Train Epoch: 060 Batch: 00058/00094 | Loss: 166.5887 | CE: 0.1233 | KD: 1060.9291\n",
      "Train Epoch: 060 Batch: 00059/00094 | Loss: 166.5600 | CE: 0.1125 | KD: 1060.8148\n",
      "Train Epoch: 060 Batch: 00060/00094 | Loss: 166.7106 | CE: 0.1265 | KD: 1061.6858\n",
      "Train Epoch: 060 Batch: 00061/00094 | Loss: 166.5349 | CE: 0.0904 | KD: 1060.7959\n",
      "Train Epoch: 060 Batch: 00062/00094 | Loss: 166.5635 | CE: 0.0935 | KD: 1060.9584\n",
      "Train Epoch: 060 Batch: 00063/00094 | Loss: 166.5540 | CE: 0.0819 | KD: 1060.9717\n",
      "Train Epoch: 060 Batch: 00064/00094 | Loss: 166.5551 | CE: 0.0932 | KD: 1060.9062\n",
      "Train Epoch: 060 Batch: 00065/00094 | Loss: 166.6642 | CE: 0.1271 | KD: 1061.3864\n",
      "Train Epoch: 060 Batch: 00066/00094 | Loss: 166.7411 | CE: 0.1744 | KD: 1061.5745\n",
      "Train Epoch: 060 Batch: 00067/00094 | Loss: 166.5590 | CE: 0.0717 | KD: 1061.0688\n",
      "Train Epoch: 060 Batch: 00068/00094 | Loss: 166.7214 | CE: 0.1610 | KD: 1061.5349\n",
      "Train Epoch: 060 Batch: 00069/00094 | Loss: 166.5678 | CE: 0.0934 | KD: 1060.9862\n",
      "Train Epoch: 060 Batch: 00070/00094 | Loss: 166.5820 | CE: 0.0896 | KD: 1061.1010\n",
      "Train Epoch: 060 Batch: 00071/00094 | Loss: 166.7336 | CE: 0.2024 | KD: 1061.3489\n",
      "Train Epoch: 060 Batch: 00072/00094 | Loss: 166.5824 | CE: 0.0789 | KD: 1061.1718\n",
      "Train Epoch: 060 Batch: 00073/00094 | Loss: 166.6011 | CE: 0.1277 | KD: 1060.9803\n",
      "Train Epoch: 060 Batch: 00074/00094 | Loss: 166.5691 | CE: 0.0933 | KD: 1060.9956\n",
      "Train Epoch: 060 Batch: 00075/00094 | Loss: 166.5406 | CE: 0.0940 | KD: 1060.8094\n",
      "Train Epoch: 060 Batch: 00076/00094 | Loss: 166.6099 | CE: 0.1884 | KD: 1060.6492\n",
      "Train Epoch: 060 Batch: 00077/00094 | Loss: 166.5652 | CE: 0.1260 | KD: 1060.7627\n",
      "Train Epoch: 060 Batch: 00078/00094 | Loss: 166.5075 | CE: 0.1197 | KD: 1060.4342\n",
      "Train Epoch: 060 Batch: 00079/00094 | Loss: 166.5710 | CE: 0.0907 | KD: 1061.0239\n",
      "Train Epoch: 060 Batch: 00080/00094 | Loss: 166.5667 | CE: 0.1245 | KD: 1060.7809\n",
      "Train Epoch: 060 Batch: 00081/00094 | Loss: 166.6127 | CE: 0.1025 | KD: 1061.2148\n",
      "Train Epoch: 060 Batch: 00082/00094 | Loss: 166.6638 | CE: 0.1748 | KD: 1061.0797\n",
      "Train Epoch: 060 Batch: 00083/00094 | Loss: 166.5178 | CE: 0.1112 | KD: 1060.5546\n",
      "Train Epoch: 060 Batch: 00084/00094 | Loss: 166.6711 | CE: 0.0962 | KD: 1061.6274\n",
      "Train Epoch: 060 Batch: 00085/00094 | Loss: 166.5563 | CE: 0.0782 | KD: 1061.0106\n",
      "Train Epoch: 060 Batch: 00086/00094 | Loss: 166.5287 | CE: 0.0928 | KD: 1060.7412\n",
      "Train Epoch: 060 Batch: 00087/00094 | Loss: 166.5378 | CE: 0.0790 | KD: 1060.8871\n",
      "Train Epoch: 060 Batch: 00088/00094 | Loss: 166.5448 | CE: 0.0723 | KD: 1060.9740\n",
      "Train Epoch: 060 Batch: 00089/00094 | Loss: 166.5386 | CE: 0.0841 | KD: 1060.8595\n",
      "Train Epoch: 060 Batch: 00090/00094 | Loss: 166.5343 | CE: 0.0837 | KD: 1060.8351\n",
      "Train Epoch: 060 Batch: 00091/00094 | Loss: 166.5316 | CE: 0.1081 | KD: 1060.6622\n",
      "Train Epoch: 060 Batch: 00092/00094 | Loss: 166.4964 | CE: 0.0729 | KD: 1060.6620\n",
      "Train Epoch: 060 Batch: 00093/00094 | Loss: 166.5737 | CE: 0.1146 | KD: 1060.8884\n",
      "Train Epoch: 060 Batch: 00094/00094 | Loss: 166.5653 | CE: 0.1049 | KD: 1060.8976\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0900 | acc:98.4500\n",
      "[VAL Acc] Target: 98.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6146 | acc:48.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1087 | acc:55.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1882 | acc:44.0840\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1469 | acc:57.4060\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6086 | acc:70.9797\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.98%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6839 | acc:72.6881\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8877 | acc:58.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.75%\n",
      "[VAL Acc] Avg 63.21%\n",
      "Train Epoch: 061 Batch: 00001/00094 | Loss: 149.9432 | CE: 0.1166 | KD: 1060.9844\n",
      "Train Epoch: 061 Batch: 00002/00094 | Loss: 149.8857 | CE: 0.0906 | KD: 1060.7604\n",
      "Train Epoch: 061 Batch: 00003/00094 | Loss: 149.8880 | CE: 0.0871 | KD: 1060.8019\n",
      "Train Epoch: 061 Batch: 00004/00094 | Loss: 149.9734 | CE: 0.1078 | KD: 1061.2603\n",
      "Train Epoch: 061 Batch: 00005/00094 | Loss: 149.9402 | CE: 0.1246 | KD: 1060.9060\n",
      "Train Epoch: 061 Batch: 00006/00094 | Loss: 149.9472 | CE: 0.1004 | KD: 1061.1273\n",
      "Train Epoch: 061 Batch: 00007/00094 | Loss: 149.8725 | CE: 0.1080 | KD: 1060.5441\n",
      "Train Epoch: 061 Batch: 00008/00094 | Loss: 149.8574 | CE: 0.0659 | KD: 1060.7349\n",
      "Train Epoch: 061 Batch: 00009/00094 | Loss: 149.9711 | CE: 0.0828 | KD: 1061.4211\n",
      "Train Epoch: 061 Batch: 00010/00094 | Loss: 149.8383 | CE: 0.0792 | KD: 1060.5059\n",
      "Train Epoch: 061 Batch: 00011/00094 | Loss: 149.8834 | CE: 0.0897 | KD: 1060.7509\n",
      "Train Epoch: 061 Batch: 00012/00094 | Loss: 149.9359 | CE: 0.0870 | KD: 1061.1416\n",
      "Train Epoch: 061 Batch: 00013/00094 | Loss: 149.8660 | CE: 0.0715 | KD: 1060.7566\n",
      "Train Epoch: 061 Batch: 00014/00094 | Loss: 149.9130 | CE: 0.0980 | KD: 1060.9015\n",
      "Train Epoch: 061 Batch: 00015/00094 | Loss: 149.8763 | CE: 0.0831 | KD: 1060.7471\n",
      "Train Epoch: 061 Batch: 00016/00094 | Loss: 149.8905 | CE: 0.0772 | KD: 1060.8896\n",
      "Train Epoch: 061 Batch: 00017/00094 | Loss: 149.9072 | CE: 0.1043 | KD: 1060.8158\n",
      "Train Epoch: 061 Batch: 00018/00094 | Loss: 149.9684 | CE: 0.1665 | KD: 1060.8088\n",
      "Train Epoch: 061 Batch: 00019/00094 | Loss: 149.8964 | CE: 0.0881 | KD: 1060.8546\n",
      "Train Epoch: 061 Batch: 00020/00094 | Loss: 149.9109 | CE: 0.1155 | KD: 1060.7629\n",
      "Train Epoch: 061 Batch: 00021/00094 | Loss: 149.9077 | CE: 0.0911 | KD: 1060.9136\n",
      "Train Epoch: 061 Batch: 00022/00094 | Loss: 149.8431 | CE: 0.0713 | KD: 1060.5957\n",
      "Train Epoch: 061 Batch: 00023/00094 | Loss: 149.9235 | CE: 0.0831 | KD: 1061.0813\n",
      "Train Epoch: 061 Batch: 00024/00094 | Loss: 149.8974 | CE: 0.0803 | KD: 1060.9163\n",
      "Train Epoch: 061 Batch: 00025/00094 | Loss: 149.9292 | CE: 0.0802 | KD: 1061.1425\n",
      "Train Epoch: 061 Batch: 00026/00094 | Loss: 149.9134 | CE: 0.0920 | KD: 1060.9473\n",
      "Train Epoch: 061 Batch: 00027/00094 | Loss: 149.9668 | CE: 0.0976 | KD: 1061.2855\n",
      "Train Epoch: 061 Batch: 00028/00094 | Loss: 149.9638 | CE: 0.1165 | KD: 1061.1309\n",
      "Train Epoch: 061 Batch: 00029/00094 | Loss: 149.9488 | CE: 0.1459 | KD: 1060.8159\n",
      "Train Epoch: 061 Batch: 00030/00094 | Loss: 149.9118 | CE: 0.0741 | KD: 1061.0625\n",
      "Train Epoch: 061 Batch: 00031/00094 | Loss: 150.0482 | CE: 0.1956 | KD: 1061.1681\n",
      "Train Epoch: 061 Batch: 00032/00094 | Loss: 149.9047 | CE: 0.1015 | KD: 1060.8180\n",
      "Train Epoch: 061 Batch: 00033/00094 | Loss: 149.9178 | CE: 0.0769 | KD: 1061.0854\n",
      "Train Epoch: 061 Batch: 00034/00094 | Loss: 149.9564 | CE: 0.1042 | KD: 1061.1652\n",
      "Train Epoch: 061 Batch: 00035/00094 | Loss: 149.8978 | CE: 0.0847 | KD: 1060.8885\n",
      "Train Epoch: 061 Batch: 00036/00094 | Loss: 149.8943 | CE: 0.0802 | KD: 1060.8960\n",
      "Train Epoch: 061 Batch: 00037/00094 | Loss: 149.8853 | CE: 0.0630 | KD: 1060.9531\n",
      "Train Epoch: 061 Batch: 00038/00094 | Loss: 149.8359 | CE: 0.0736 | KD: 1060.5286\n",
      "Train Epoch: 061 Batch: 00039/00094 | Loss: 149.9213 | CE: 0.1044 | KD: 1060.9154\n",
      "Train Epoch: 061 Batch: 00040/00094 | Loss: 149.9917 | CE: 0.1988 | KD: 1060.7446\n",
      "Train Epoch: 061 Batch: 00041/00094 | Loss: 149.9668 | CE: 0.1191 | KD: 1061.1329\n",
      "Train Epoch: 061 Batch: 00042/00094 | Loss: 150.0098 | CE: 0.1617 | KD: 1061.1357\n",
      "Train Epoch: 061 Batch: 00043/00094 | Loss: 149.9311 | CE: 0.0975 | KD: 1061.0333\n",
      "Train Epoch: 061 Batch: 00044/00094 | Loss: 149.9843 | CE: 0.1587 | KD: 1060.9768\n",
      "Train Epoch: 061 Batch: 00045/00094 | Loss: 149.9296 | CE: 0.1039 | KD: 1060.9774\n",
      "Train Epoch: 061 Batch: 00046/00094 | Loss: 149.9905 | CE: 0.1952 | KD: 1060.7623\n",
      "Train Epoch: 061 Batch: 00047/00094 | Loss: 149.8937 | CE: 0.1152 | KD: 1060.6433\n",
      "Train Epoch: 061 Batch: 00048/00094 | Loss: 149.9403 | CE: 0.0694 | KD: 1061.2980\n",
      "Train Epoch: 061 Batch: 00049/00094 | Loss: 149.8926 | CE: 0.0873 | KD: 1060.8335\n",
      "Train Epoch: 061 Batch: 00050/00094 | Loss: 149.8784 | CE: 0.0804 | KD: 1060.7812\n",
      "Train Epoch: 061 Batch: 00051/00094 | Loss: 149.9452 | CE: 0.0823 | KD: 1061.2407\n",
      "Train Epoch: 061 Batch: 00052/00094 | Loss: 149.8645 | CE: 0.0730 | KD: 1060.7356\n",
      "Train Epoch: 061 Batch: 00053/00094 | Loss: 149.9330 | CE: 0.0793 | KD: 1061.1753\n",
      "Train Epoch: 061 Batch: 00054/00094 | Loss: 149.9027 | CE: 0.1084 | KD: 1060.7554\n",
      "Train Epoch: 061 Batch: 00055/00094 | Loss: 149.8906 | CE: 0.0626 | KD: 1060.9933\n",
      "Train Epoch: 061 Batch: 00056/00094 | Loss: 149.8808 | CE: 0.0754 | KD: 1060.8334\n",
      "Train Epoch: 061 Batch: 00057/00094 | Loss: 149.9190 | CE: 0.1014 | KD: 1060.9205\n",
      "Train Epoch: 061 Batch: 00058/00094 | Loss: 149.9000 | CE: 0.1145 | KD: 1060.6930\n",
      "Train Epoch: 061 Batch: 00059/00094 | Loss: 149.9390 | CE: 0.0860 | KD: 1061.1707\n",
      "Train Epoch: 061 Batch: 00060/00094 | Loss: 149.9921 | CE: 0.1714 | KD: 1060.9421\n",
      "Train Epoch: 061 Batch: 00061/00094 | Loss: 149.8943 | CE: 0.0676 | KD: 1060.9847\n",
      "Train Epoch: 061 Batch: 00062/00094 | Loss: 149.8817 | CE: 0.1117 | KD: 1060.5830\n",
      "Train Epoch: 061 Batch: 00063/00094 | Loss: 149.9659 | CE: 0.1250 | KD: 1061.0851\n",
      "Train Epoch: 061 Batch: 00064/00094 | Loss: 149.9717 | CE: 0.0889 | KD: 1061.3813\n",
      "Train Epoch: 061 Batch: 00065/00094 | Loss: 149.9069 | CE: 0.1246 | KD: 1060.6702\n",
      "Train Epoch: 061 Batch: 00066/00094 | Loss: 150.0476 | CE: 0.1730 | KD: 1061.3236\n",
      "Train Epoch: 061 Batch: 00067/00094 | Loss: 149.9067 | CE: 0.1137 | KD: 1060.7463\n",
      "Train Epoch: 061 Batch: 00068/00094 | Loss: 149.8223 | CE: 0.0740 | KD: 1060.4298\n",
      "Train Epoch: 061 Batch: 00069/00094 | Loss: 149.9959 | CE: 0.1015 | KD: 1061.4637\n",
      "Train Epoch: 061 Batch: 00070/00094 | Loss: 149.9762 | CE: 0.1233 | KD: 1061.1700\n",
      "Train Epoch: 061 Batch: 00071/00094 | Loss: 149.9786 | CE: 0.0987 | KD: 1061.3618\n",
      "Train Epoch: 061 Batch: 00072/00094 | Loss: 149.9247 | CE: 0.0840 | KD: 1061.0835\n",
      "Train Epoch: 061 Batch: 00073/00094 | Loss: 149.9090 | CE: 0.0639 | KD: 1061.1150\n",
      "Train Epoch: 061 Batch: 00074/00094 | Loss: 149.9028 | CE: 0.0845 | KD: 1060.9252\n",
      "Train Epoch: 061 Batch: 00075/00094 | Loss: 149.9666 | CE: 0.0677 | KD: 1061.4957\n",
      "Train Epoch: 061 Batch: 00076/00094 | Loss: 149.9534 | CE: 0.1040 | KD: 1061.1449\n",
      "Train Epoch: 061 Batch: 00077/00094 | Loss: 149.9397 | CE: 0.0893 | KD: 1061.1526\n",
      "Train Epoch: 061 Batch: 00078/00094 | Loss: 150.0584 | CE: 0.1455 | KD: 1061.5950\n",
      "Train Epoch: 061 Batch: 00079/00094 | Loss: 149.9536 | CE: 0.0792 | KD: 1061.3225\n",
      "Train Epoch: 061 Batch: 00080/00094 | Loss: 149.8754 | CE: 0.0779 | KD: 1060.7783\n",
      "Train Epoch: 061 Batch: 00081/00094 | Loss: 149.8713 | CE: 0.0633 | KD: 1060.8518\n",
      "Train Epoch: 061 Batch: 00082/00094 | Loss: 149.9537 | CE: 0.1241 | KD: 1061.0050\n",
      "Train Epoch: 061 Batch: 00083/00094 | Loss: 149.9780 | CE: 0.0898 | KD: 1061.4197\n",
      "Train Epoch: 061 Batch: 00084/00094 | Loss: 149.9064 | CE: 0.0812 | KD: 1060.9738\n",
      "Train Epoch: 061 Batch: 00085/00094 | Loss: 149.9408 | CE: 0.0792 | KD: 1061.2317\n",
      "Train Epoch: 061 Batch: 00086/00094 | Loss: 149.8985 | CE: 0.0829 | KD: 1060.9060\n",
      "Train Epoch: 061 Batch: 00087/00094 | Loss: 150.1283 | CE: 0.2352 | KD: 1061.4546\n",
      "Train Epoch: 061 Batch: 00088/00094 | Loss: 149.8871 | CE: 0.0671 | KD: 1060.9369\n",
      "Train Epoch: 061 Batch: 00089/00094 | Loss: 149.9240 | CE: 0.1107 | KD: 1060.8898\n",
      "Train Epoch: 061 Batch: 00090/00094 | Loss: 149.9427 | CE: 0.0902 | KD: 1061.1670\n",
      "Train Epoch: 061 Batch: 00091/00094 | Loss: 150.0275 | CE: 0.1182 | KD: 1061.5697\n",
      "Train Epoch: 061 Batch: 00092/00094 | Loss: 149.9155 | CE: 0.0865 | KD: 1061.0009\n",
      "Train Epoch: 061 Batch: 00093/00094 | Loss: 149.9113 | CE: 0.0908 | KD: 1060.9407\n",
      "Train Epoch: 061 Batch: 00094/00094 | Loss: 149.9392 | CE: 0.0865 | KD: 1061.1689\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0873 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.5952 | acc:49.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0900 | acc:54.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1730 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1047 | acc:59.0125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.01%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0074 | acc:55.2680\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6844 | acc:72.6489\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9222 | acc:58.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.44%\n",
      "[VAL Acc] Avg 61.92%\n",
      "Train Epoch: 062 Batch: 00001/00094 | Loss: 149.9276 | CE: 0.0796 | KD: 1061.1350\n",
      "Train Epoch: 062 Batch: 00002/00094 | Loss: 149.8990 | CE: 0.0719 | KD: 1060.9872\n",
      "Train Epoch: 062 Batch: 00003/00094 | Loss: 149.9147 | CE: 0.1036 | KD: 1060.8741\n",
      "Train Epoch: 062 Batch: 00004/00094 | Loss: 149.9555 | CE: 0.1279 | KD: 1060.9915\n",
      "Train Epoch: 062 Batch: 00005/00094 | Loss: 149.8822 | CE: 0.0901 | KD: 1060.7401\n",
      "Train Epoch: 062 Batch: 00006/00094 | Loss: 149.9961 | CE: 0.1001 | KD: 1061.4749\n",
      "Train Epoch: 062 Batch: 00007/00094 | Loss: 149.9632 | CE: 0.1905 | KD: 1060.6024\n",
      "Train Epoch: 062 Batch: 00008/00094 | Loss: 149.9277 | CE: 0.0952 | KD: 1061.0259\n",
      "Train Epoch: 062 Batch: 00009/00094 | Loss: 149.9049 | CE: 0.0867 | KD: 1060.9242\n",
      "Train Epoch: 062 Batch: 00010/00094 | Loss: 150.0013 | CE: 0.1773 | KD: 1060.9653\n",
      "Train Epoch: 062 Batch: 00011/00094 | Loss: 149.8719 | CE: 0.0757 | KD: 1060.7689\n",
      "Train Epoch: 062 Batch: 00012/00094 | Loss: 150.0211 | CE: 0.1740 | KD: 1061.1288\n",
      "Train Epoch: 062 Batch: 00013/00094 | Loss: 149.9156 | CE: 0.0792 | KD: 1061.0537\n",
      "Train Epoch: 062 Batch: 00014/00094 | Loss: 149.9621 | CE: 0.0854 | KD: 1061.3386\n",
      "Train Epoch: 062 Batch: 00015/00094 | Loss: 149.8999 | CE: 0.1032 | KD: 1060.7722\n",
      "Train Epoch: 062 Batch: 00016/00094 | Loss: 149.9094 | CE: 0.0887 | KD: 1060.9418\n",
      "Train Epoch: 062 Batch: 00017/00094 | Loss: 149.9415 | CE: 0.0985 | KD: 1061.0997\n",
      "Train Epoch: 062 Batch: 00018/00094 | Loss: 149.9183 | CE: 0.1077 | KD: 1060.8711\n",
      "Train Epoch: 062 Batch: 00019/00094 | Loss: 149.9214 | CE: 0.0747 | KD: 1061.1262\n",
      "Train Epoch: 062 Batch: 00020/00094 | Loss: 149.8979 | CE: 0.0741 | KD: 1060.9635\n",
      "Train Epoch: 062 Batch: 00021/00094 | Loss: 149.8962 | CE: 0.0803 | KD: 1060.9082\n",
      "Train Epoch: 062 Batch: 00022/00094 | Loss: 149.9240 | CE: 0.0574 | KD: 1061.2672\n",
      "Train Epoch: 062 Batch: 00023/00094 | Loss: 150.0286 | CE: 0.1295 | KD: 1061.4971\n",
      "Train Epoch: 062 Batch: 00024/00094 | Loss: 149.9036 | CE: 0.0806 | KD: 1060.9581\n",
      "Train Epoch: 062 Batch: 00025/00094 | Loss: 149.9038 | CE: 0.1010 | KD: 1060.8149\n",
      "Train Epoch: 062 Batch: 00026/00094 | Loss: 149.8998 | CE: 0.0750 | KD: 1060.9713\n",
      "Train Epoch: 062 Batch: 00027/00094 | Loss: 149.9444 | CE: 0.0769 | KD: 1061.2732\n",
      "Train Epoch: 062 Batch: 00028/00094 | Loss: 149.9922 | CE: 0.1274 | KD: 1061.2540\n",
      "Train Epoch: 062 Batch: 00029/00094 | Loss: 149.9198 | CE: 0.0856 | KD: 1061.0372\n",
      "Train Epoch: 062 Batch: 00030/00094 | Loss: 149.8905 | CE: 0.0808 | KD: 1060.8645\n",
      "Train Epoch: 062 Batch: 00031/00094 | Loss: 149.9301 | CE: 0.1089 | KD: 1060.9459\n",
      "Train Epoch: 062 Batch: 00032/00094 | Loss: 149.9400 | CE: 0.0832 | KD: 1061.1976\n",
      "Train Epoch: 062 Batch: 00033/00094 | Loss: 149.9482 | CE: 0.1369 | KD: 1060.8757\n",
      "Train Epoch: 062 Batch: 00034/00094 | Loss: 149.8444 | CE: 0.0650 | KD: 1060.6497\n",
      "Train Epoch: 062 Batch: 00035/00094 | Loss: 150.0114 | CE: 0.1274 | KD: 1061.3904\n",
      "Train Epoch: 062 Batch: 00036/00094 | Loss: 149.9175 | CE: 0.0766 | KD: 1061.0850\n",
      "Train Epoch: 062 Batch: 00037/00094 | Loss: 149.9281 | CE: 0.1080 | KD: 1060.9380\n",
      "Train Epoch: 062 Batch: 00038/00094 | Loss: 149.8664 | CE: 0.0732 | KD: 1060.7471\n",
      "Train Epoch: 062 Batch: 00039/00094 | Loss: 149.9411 | CE: 0.1032 | KD: 1061.0640\n",
      "Train Epoch: 062 Batch: 00040/00094 | Loss: 149.9275 | CE: 0.0819 | KD: 1061.1187\n",
      "Train Epoch: 062 Batch: 00041/00094 | Loss: 149.8763 | CE: 0.0707 | KD: 1060.8348\n",
      "Train Epoch: 062 Batch: 00042/00094 | Loss: 149.9267 | CE: 0.1078 | KD: 1060.9291\n",
      "Train Epoch: 062 Batch: 00043/00094 | Loss: 149.8777 | CE: 0.0718 | KD: 1060.8372\n",
      "Train Epoch: 062 Batch: 00044/00094 | Loss: 149.9383 | CE: 0.1087 | KD: 1061.0051\n",
      "Train Epoch: 062 Batch: 00045/00094 | Loss: 149.9663 | CE: 0.1177 | KD: 1061.1399\n",
      "Train Epoch: 062 Batch: 00046/00094 | Loss: 149.9331 | CE: 0.0873 | KD: 1061.1196\n",
      "Train Epoch: 062 Batch: 00047/00094 | Loss: 149.9708 | CE: 0.0937 | KD: 1061.3414\n",
      "Train Epoch: 062 Batch: 00048/00094 | Loss: 150.0667 | CE: 0.2031 | KD: 1061.2454\n",
      "Train Epoch: 062 Batch: 00049/00094 | Loss: 149.9413 | CE: 0.0915 | KD: 1061.1476\n",
      "Train Epoch: 062 Batch: 00050/00094 | Loss: 149.9572 | CE: 0.0925 | KD: 1061.2538\n",
      "Train Epoch: 062 Batch: 00051/00094 | Loss: 149.9651 | CE: 0.0918 | KD: 1061.3152\n",
      "Train Epoch: 062 Batch: 00052/00094 | Loss: 149.9288 | CE: 0.0752 | KD: 1061.1753\n",
      "Train Epoch: 062 Batch: 00053/00094 | Loss: 149.9011 | CE: 0.0721 | KD: 1061.0006\n",
      "Train Epoch: 062 Batch: 00054/00094 | Loss: 149.9035 | CE: 0.0886 | KD: 1060.9015\n",
      "Train Epoch: 062 Batch: 00055/00094 | Loss: 149.9216 | CE: 0.1274 | KD: 1060.7549\n",
      "Train Epoch: 062 Batch: 00056/00094 | Loss: 149.8857 | CE: 0.0616 | KD: 1060.9663\n",
      "Train Epoch: 062 Batch: 00057/00094 | Loss: 149.8927 | CE: 0.0743 | KD: 1060.9260\n",
      "Train Epoch: 062 Batch: 00058/00094 | Loss: 149.9426 | CE: 0.1231 | KD: 1060.9341\n",
      "Train Epoch: 062 Batch: 00059/00094 | Loss: 149.9293 | CE: 0.0875 | KD: 1061.0914\n",
      "Train Epoch: 062 Batch: 00060/00094 | Loss: 149.9532 | CE: 0.1125 | KD: 1061.0840\n",
      "Train Epoch: 062 Batch: 00061/00094 | Loss: 149.8685 | CE: 0.0635 | KD: 1060.8311\n",
      "Train Epoch: 062 Batch: 00062/00094 | Loss: 149.9801 | CE: 0.1287 | KD: 1061.1595\n",
      "Train Epoch: 062 Batch: 00063/00094 | Loss: 149.9371 | CE: 0.0762 | KD: 1061.2273\n",
      "Train Epoch: 062 Batch: 00064/00094 | Loss: 149.9024 | CE: 0.0756 | KD: 1060.9854\n",
      "Train Epoch: 062 Batch: 00065/00094 | Loss: 149.9389 | CE: 0.0857 | KD: 1061.1720\n",
      "Train Epoch: 062 Batch: 00066/00094 | Loss: 149.9260 | CE: 0.0719 | KD: 1061.1783\n",
      "Train Epoch: 062 Batch: 00067/00094 | Loss: 150.0192 | CE: 0.1794 | KD: 1061.0770\n",
      "Train Epoch: 062 Batch: 00068/00094 | Loss: 149.9526 | CE: 0.0954 | KD: 1061.2002\n",
      "Train Epoch: 062 Batch: 00069/00094 | Loss: 150.0062 | CE: 0.1371 | KD: 1061.2845\n",
      "Train Epoch: 062 Batch: 00070/00094 | Loss: 149.8903 | CE: 0.0594 | KD: 1061.0143\n",
      "Train Epoch: 062 Batch: 00071/00094 | Loss: 149.9415 | CE: 0.0998 | KD: 1061.0909\n",
      "Train Epoch: 062 Batch: 00072/00094 | Loss: 149.9492 | CE: 0.1041 | KD: 1061.1149\n",
      "Train Epoch: 062 Batch: 00073/00094 | Loss: 149.9123 | CE: 0.1223 | KD: 1060.7241\n",
      "Train Epoch: 062 Batch: 00074/00094 | Loss: 149.8528 | CE: 0.0777 | KD: 1060.6196\n",
      "Train Epoch: 062 Batch: 00075/00094 | Loss: 150.0229 | CE: 0.1454 | KD: 1061.3444\n",
      "Train Epoch: 062 Batch: 00076/00094 | Loss: 149.9660 | CE: 0.1049 | KD: 1061.2284\n",
      "Train Epoch: 062 Batch: 00077/00094 | Loss: 149.9397 | CE: 0.1075 | KD: 1061.0239\n",
      "Train Epoch: 062 Batch: 00078/00094 | Loss: 149.9481 | CE: 0.0964 | KD: 1061.1616\n",
      "Train Epoch: 062 Batch: 00079/00094 | Loss: 149.9099 | CE: 0.0687 | KD: 1061.0872\n",
      "Train Epoch: 062 Batch: 00080/00094 | Loss: 149.9618 | CE: 0.1012 | KD: 1061.2247\n",
      "Train Epoch: 062 Batch: 00081/00094 | Loss: 149.9162 | CE: 0.1036 | KD: 1060.8850\n",
      "Train Epoch: 062 Batch: 00082/00094 | Loss: 149.8737 | CE: 0.0658 | KD: 1060.8516\n",
      "Train Epoch: 062 Batch: 00083/00094 | Loss: 149.8579 | CE: 0.0744 | KD: 1060.6790\n",
      "Train Epoch: 062 Batch: 00084/00094 | Loss: 149.9770 | CE: 0.1031 | KD: 1061.3187\n",
      "Train Epoch: 062 Batch: 00085/00094 | Loss: 149.8849 | CE: 0.0682 | KD: 1060.9136\n",
      "Train Epoch: 062 Batch: 00086/00094 | Loss: 149.8973 | CE: 0.0745 | KD: 1060.9569\n",
      "Train Epoch: 062 Batch: 00087/00094 | Loss: 149.9637 | CE: 0.1079 | KD: 1061.1904\n",
      "Train Epoch: 062 Batch: 00088/00094 | Loss: 149.8938 | CE: 0.0710 | KD: 1060.9573\n",
      "Train Epoch: 062 Batch: 00089/00094 | Loss: 149.9819 | CE: 0.0804 | KD: 1061.5146\n",
      "Train Epoch: 062 Batch: 00090/00094 | Loss: 149.8605 | CE: 0.0751 | KD: 1060.6925\n",
      "Train Epoch: 062 Batch: 00091/00094 | Loss: 149.9345 | CE: 0.1337 | KD: 1060.8010\n",
      "Train Epoch: 062 Batch: 00092/00094 | Loss: 149.9738 | CE: 0.1370 | KD: 1061.0566\n",
      "Train Epoch: 062 Batch: 00093/00094 | Loss: 149.9857 | CE: 0.1205 | KD: 1061.2578\n",
      "Train Epoch: 062 Batch: 00094/00094 | Loss: 149.8533 | CE: 0.1049 | KD: 1060.4302\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0953 | acc:98.4000\n",
      "[VAL Acc] Target: 98.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8246 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2907 | acc:52.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3174 | acc:43.1298\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2525 | acc:57.6411\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5308 | acc:75.6007\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7055 | acc:72.8840\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8830 | acc:59.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.69%\n",
      "[VAL Acc] Avg 63.63%\n",
      "Train Epoch: 063 Batch: 00001/00094 | Loss: 149.9027 | CE: 0.1061 | KD: 1060.7709\n",
      "Train Epoch: 063 Batch: 00002/00094 | Loss: 149.9394 | CE: 0.0999 | KD: 1061.0757\n",
      "Train Epoch: 063 Batch: 00003/00094 | Loss: 149.8956 | CE: 0.0719 | KD: 1060.9630\n",
      "Train Epoch: 063 Batch: 00004/00094 | Loss: 149.9167 | CE: 0.0881 | KD: 1060.9977\n",
      "Train Epoch: 063 Batch: 00005/00094 | Loss: 149.8588 | CE: 0.0932 | KD: 1060.5518\n",
      "Train Epoch: 063 Batch: 00006/00094 | Loss: 149.9655 | CE: 0.0819 | KD: 1061.3875\n",
      "Train Epoch: 063 Batch: 00007/00094 | Loss: 149.8751 | CE: 0.0634 | KD: 1060.8784\n",
      "Train Epoch: 063 Batch: 00008/00094 | Loss: 149.9789 | CE: 0.0859 | KD: 1061.4540\n",
      "Train Epoch: 063 Batch: 00009/00094 | Loss: 149.9118 | CE: 0.0888 | KD: 1060.9586\n",
      "Train Epoch: 063 Batch: 00010/00094 | Loss: 149.8287 | CE: 0.0575 | KD: 1060.5913\n",
      "Train Epoch: 063 Batch: 00011/00094 | Loss: 149.9097 | CE: 0.0732 | KD: 1061.0540\n",
      "Train Epoch: 063 Batch: 00012/00094 | Loss: 149.9338 | CE: 0.0729 | KD: 1061.2264\n",
      "Train Epoch: 063 Batch: 00013/00094 | Loss: 149.9420 | CE: 0.0783 | KD: 1061.2467\n",
      "Train Epoch: 063 Batch: 00014/00094 | Loss: 149.8849 | CE: 0.0685 | KD: 1060.9115\n",
      "Train Epoch: 063 Batch: 00015/00094 | Loss: 150.0026 | CE: 0.1285 | KD: 1061.3206\n",
      "Train Epoch: 063 Batch: 00016/00094 | Loss: 149.9180 | CE: 0.1029 | KD: 1060.9030\n",
      "Train Epoch: 063 Batch: 00017/00094 | Loss: 149.9996 | CE: 0.1296 | KD: 1061.2916\n",
      "Train Epoch: 063 Batch: 00018/00094 | Loss: 149.8993 | CE: 0.1043 | KD: 1060.7596\n",
      "Train Epoch: 063 Batch: 00019/00094 | Loss: 149.9209 | CE: 0.1154 | KD: 1060.8344\n",
      "Train Epoch: 063 Batch: 00020/00094 | Loss: 149.9230 | CE: 0.1171 | KD: 1060.8369\n",
      "Train Epoch: 063 Batch: 00021/00094 | Loss: 149.8873 | CE: 0.0923 | KD: 1060.7605\n",
      "Train Epoch: 063 Batch: 00022/00094 | Loss: 149.8439 | CE: 0.0583 | KD: 1060.6935\n",
      "Train Epoch: 063 Batch: 00023/00094 | Loss: 149.9356 | CE: 0.0890 | KD: 1061.1251\n",
      "Train Epoch: 063 Batch: 00024/00094 | Loss: 149.9619 | CE: 0.0856 | KD: 1061.3357\n",
      "Train Epoch: 063 Batch: 00025/00094 | Loss: 149.9521 | CE: 0.0890 | KD: 1061.2421\n",
      "Train Epoch: 063 Batch: 00026/00094 | Loss: 149.8891 | CE: 0.0714 | KD: 1060.9211\n",
      "Train Epoch: 063 Batch: 00027/00094 | Loss: 149.8928 | CE: 0.0761 | KD: 1060.9136\n",
      "Train Epoch: 063 Batch: 00028/00094 | Loss: 149.9888 | CE: 0.1052 | KD: 1061.3871\n",
      "Train Epoch: 063 Batch: 00029/00094 | Loss: 149.9733 | CE: 0.1176 | KD: 1061.1902\n",
      "Train Epoch: 063 Batch: 00030/00094 | Loss: 149.9511 | CE: 0.1579 | KD: 1060.7478\n",
      "Train Epoch: 063 Batch: 00031/00094 | Loss: 149.8977 | CE: 0.0894 | KD: 1060.8545\n",
      "Train Epoch: 063 Batch: 00032/00094 | Loss: 149.8845 | CE: 0.0573 | KD: 1060.9879\n",
      "Train Epoch: 063 Batch: 00033/00094 | Loss: 150.0793 | CE: 0.1891 | KD: 1061.4340\n",
      "Train Epoch: 063 Batch: 00034/00094 | Loss: 149.8958 | CE: 0.0866 | KD: 1060.8602\n",
      "Train Epoch: 063 Batch: 00035/00094 | Loss: 149.9124 | CE: 0.0930 | KD: 1060.9326\n",
      "Train Epoch: 063 Batch: 00036/00094 | Loss: 149.9261 | CE: 0.0939 | KD: 1061.0232\n",
      "Train Epoch: 063 Batch: 00037/00094 | Loss: 149.8697 | CE: 0.1160 | KD: 1060.4679\n",
      "Train Epoch: 063 Batch: 00038/00094 | Loss: 149.8555 | CE: 0.0764 | KD: 1060.6473\n",
      "Train Epoch: 063 Batch: 00039/00094 | Loss: 149.9728 | CE: 0.1023 | KD: 1061.2948\n",
      "Train Epoch: 063 Batch: 00040/00094 | Loss: 149.9689 | CE: 0.0768 | KD: 1061.4476\n",
      "Train Epoch: 063 Batch: 00041/00094 | Loss: 149.9610 | CE: 0.1037 | KD: 1061.2009\n",
      "Train Epoch: 063 Batch: 00042/00094 | Loss: 149.8916 | CE: 0.0623 | KD: 1061.0026\n",
      "Train Epoch: 063 Batch: 00043/00094 | Loss: 150.0188 | CE: 0.0984 | KD: 1061.6481\n",
      "Train Epoch: 063 Batch: 00044/00094 | Loss: 149.8459 | CE: 0.0821 | KD: 1060.5393\n",
      "Train Epoch: 063 Batch: 00045/00094 | Loss: 149.9264 | CE: 0.0784 | KD: 1061.1356\n",
      "Train Epoch: 063 Batch: 00046/00094 | Loss: 149.9544 | CE: 0.0766 | KD: 1061.3462\n",
      "Train Epoch: 063 Batch: 00047/00094 | Loss: 149.9148 | CE: 0.0900 | KD: 1060.9713\n",
      "Train Epoch: 063 Batch: 00048/00094 | Loss: 149.9077 | CE: 0.1182 | KD: 1060.7208\n",
      "Train Epoch: 063 Batch: 00049/00094 | Loss: 149.9021 | CE: 0.0977 | KD: 1060.8267\n",
      "Train Epoch: 063 Batch: 00050/00094 | Loss: 149.8788 | CE: 0.0764 | KD: 1060.8126\n",
      "Train Epoch: 063 Batch: 00051/00094 | Loss: 150.0154 | CE: 0.1640 | KD: 1061.1592\n",
      "Train Epoch: 063 Batch: 00052/00094 | Loss: 149.9451 | CE: 0.0986 | KD: 1061.1245\n",
      "Train Epoch: 063 Batch: 00053/00094 | Loss: 149.8611 | CE: 0.0916 | KD: 1060.5792\n",
      "Train Epoch: 063 Batch: 00054/00094 | Loss: 149.8786 | CE: 0.0710 | KD: 1060.8500\n",
      "Train Epoch: 063 Batch: 00055/00094 | Loss: 149.9080 | CE: 0.0916 | KD: 1060.9117\n",
      "Train Epoch: 063 Batch: 00056/00094 | Loss: 149.8800 | CE: 0.0633 | KD: 1060.9135\n",
      "Train Epoch: 063 Batch: 00057/00094 | Loss: 149.8522 | CE: 0.0598 | KD: 1060.7417\n",
      "Train Epoch: 063 Batch: 00058/00094 | Loss: 149.9460 | CE: 0.0746 | KD: 1061.3014\n",
      "Train Epoch: 063 Batch: 00059/00094 | Loss: 149.8912 | CE: 0.0942 | KD: 1060.7744\n",
      "Train Epoch: 063 Batch: 00060/00094 | Loss: 149.9021 | CE: 0.0833 | KD: 1060.9293\n",
      "Train Epoch: 063 Batch: 00061/00094 | Loss: 149.9060 | CE: 0.0760 | KD: 1061.0083\n",
      "Train Epoch: 063 Batch: 00062/00094 | Loss: 149.9396 | CE: 0.1068 | KD: 1061.0282\n",
      "Train Epoch: 063 Batch: 00063/00094 | Loss: 149.8556 | CE: 0.0549 | KD: 1060.8004\n",
      "Train Epoch: 063 Batch: 00064/00094 | Loss: 149.9315 | CE: 0.1177 | KD: 1060.8936\n",
      "Train Epoch: 063 Batch: 00065/00094 | Loss: 149.9607 | CE: 0.1247 | KD: 1061.0507\n",
      "Train Epoch: 063 Batch: 00066/00094 | Loss: 149.8929 | CE: 0.0853 | KD: 1060.8490\n",
      "Train Epoch: 063 Batch: 00067/00094 | Loss: 150.0231 | CE: 0.1647 | KD: 1061.2094\n",
      "Train Epoch: 063 Batch: 00068/00094 | Loss: 149.8949 | CE: 0.0704 | KD: 1060.9690\n",
      "Train Epoch: 063 Batch: 00069/00094 | Loss: 149.8771 | CE: 0.0693 | KD: 1060.8512\n",
      "Train Epoch: 063 Batch: 00070/00094 | Loss: 149.8607 | CE: 0.0759 | KD: 1060.6875\n",
      "Train Epoch: 063 Batch: 00071/00094 | Loss: 149.9503 | CE: 0.1254 | KD: 1060.9714\n",
      "Train Epoch: 063 Batch: 00072/00094 | Loss: 149.8739 | CE: 0.0662 | KD: 1060.8503\n",
      "Train Epoch: 063 Batch: 00073/00094 | Loss: 149.8559 | CE: 0.0789 | KD: 1060.6322\n",
      "Train Epoch: 063 Batch: 00074/00094 | Loss: 149.9203 | CE: 0.0683 | KD: 1061.1639\n",
      "Train Epoch: 063 Batch: 00075/00094 | Loss: 149.8803 | CE: 0.0760 | KD: 1060.8259\n",
      "Train Epoch: 063 Batch: 00076/00094 | Loss: 149.9414 | CE: 0.0776 | KD: 1061.2473\n",
      "Train Epoch: 063 Batch: 00077/00094 | Loss: 149.9938 | CE: 0.1736 | KD: 1060.9390\n",
      "Train Epoch: 063 Batch: 00078/00094 | Loss: 149.9060 | CE: 0.0731 | KD: 1061.0283\n",
      "Train Epoch: 063 Batch: 00079/00094 | Loss: 149.9663 | CE: 0.0906 | KD: 1061.3314\n",
      "Train Epoch: 063 Batch: 00080/00094 | Loss: 149.9544 | CE: 0.1389 | KD: 1060.9053\n",
      "Train Epoch: 063 Batch: 00081/00094 | Loss: 149.8484 | CE: 0.0580 | KD: 1060.7271\n",
      "Train Epoch: 063 Batch: 00082/00094 | Loss: 149.9170 | CE: 0.1269 | KD: 1060.7258\n",
      "Train Epoch: 063 Batch: 00083/00094 | Loss: 149.8307 | CE: 0.0659 | KD: 1060.5461\n",
      "Train Epoch: 063 Batch: 00084/00094 | Loss: 149.9467 | CE: 0.1207 | KD: 1060.9800\n",
      "Train Epoch: 063 Batch: 00085/00094 | Loss: 149.9180 | CE: 0.0662 | KD: 1061.1621\n",
      "Train Epoch: 063 Batch: 00086/00094 | Loss: 149.9488 | CE: 0.1136 | KD: 1061.0449\n",
      "Train Epoch: 063 Batch: 00087/00094 | Loss: 149.9342 | CE: 0.0987 | KD: 1061.0464\n",
      "Train Epoch: 063 Batch: 00088/00094 | Loss: 149.9628 | CE: 0.1528 | KD: 1060.8665\n",
      "Train Epoch: 063 Batch: 00089/00094 | Loss: 149.9966 | CE: 0.1107 | KD: 1061.4039\n",
      "Train Epoch: 063 Batch: 00090/00094 | Loss: 149.9680 | CE: 0.1280 | KD: 1061.0789\n",
      "Train Epoch: 063 Batch: 00091/00094 | Loss: 149.9382 | CE: 0.1295 | KD: 1060.8572\n",
      "Train Epoch: 063 Batch: 00092/00094 | Loss: 149.9042 | CE: 0.1214 | KD: 1060.6738\n",
      "Train Epoch: 063 Batch: 00093/00094 | Loss: 149.9694 | CE: 0.1159 | KD: 1061.1742\n",
      "Train Epoch: 063 Batch: 00094/00094 | Loss: 149.8619 | CE: 0.0601 | KD: 1060.8083\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0826 | acc:98.8500\n",
      "[VAL Acc] Target: 98.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6898 | acc:49.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2291 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1796 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3034 | acc:53.9577\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6471 | acc:68.1146\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7513 | acc:69.7492\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8686 | acc:60.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.06%\n",
      "[VAL Acc] Avg 62.10%\n",
      "Train Epoch: 064 Batch: 00001/00094 | Loss: 149.9358 | CE: 0.0967 | KD: 1061.0724\n",
      "Train Epoch: 064 Batch: 00002/00094 | Loss: 150.0525 | CE: 0.1806 | KD: 1061.3047\n",
      "Train Epoch: 064 Batch: 00003/00094 | Loss: 149.9169 | CE: 0.0827 | KD: 1061.0375\n",
      "Train Epoch: 064 Batch: 00004/00094 | Loss: 149.9568 | CE: 0.0792 | KD: 1061.3448\n",
      "Train Epoch: 064 Batch: 00005/00094 | Loss: 149.8937 | CE: 0.0888 | KD: 1060.8303\n",
      "Train Epoch: 064 Batch: 00006/00094 | Loss: 149.9092 | CE: 0.0898 | KD: 1060.9331\n",
      "Train Epoch: 064 Batch: 00007/00094 | Loss: 149.9212 | CE: 0.0692 | KD: 1061.1639\n",
      "Train Epoch: 064 Batch: 00008/00094 | Loss: 149.9322 | CE: 0.0776 | KD: 1061.1821\n",
      "Train Epoch: 064 Batch: 00009/00094 | Loss: 149.9328 | CE: 0.0989 | KD: 1061.0358\n",
      "Train Epoch: 064 Batch: 00010/00094 | Loss: 149.9227 | CE: 0.1063 | KD: 1060.9116\n",
      "Train Epoch: 064 Batch: 00011/00094 | Loss: 149.9636 | CE: 0.1031 | KD: 1061.2239\n",
      "Train Epoch: 064 Batch: 00012/00094 | Loss: 149.9052 | CE: 0.0714 | KD: 1061.0347\n",
      "Train Epoch: 064 Batch: 00013/00094 | Loss: 149.9372 | CE: 0.0847 | KD: 1061.1678\n",
      "Train Epoch: 064 Batch: 00014/00094 | Loss: 149.8765 | CE: 0.0857 | KD: 1060.7303\n",
      "Train Epoch: 064 Batch: 00015/00094 | Loss: 149.8717 | CE: 0.0697 | KD: 1060.8099\n",
      "Train Epoch: 064 Batch: 00016/00094 | Loss: 149.9063 | CE: 0.1138 | KD: 1060.7423\n",
      "Train Epoch: 064 Batch: 00017/00094 | Loss: 149.9274 | CE: 0.1128 | KD: 1060.8987\n",
      "Train Epoch: 064 Batch: 00018/00094 | Loss: 149.9017 | CE: 0.0954 | KD: 1060.8397\n",
      "Train Epoch: 064 Batch: 00019/00094 | Loss: 149.8847 | CE: 0.0763 | KD: 1060.8549\n",
      "Train Epoch: 064 Batch: 00020/00094 | Loss: 149.9258 | CE: 0.0992 | KD: 1060.9835\n",
      "Train Epoch: 064 Batch: 00021/00094 | Loss: 149.9507 | CE: 0.0875 | KD: 1061.2432\n",
      "Train Epoch: 064 Batch: 00022/00094 | Loss: 150.0717 | CE: 0.1790 | KD: 1061.4518\n",
      "Train Epoch: 064 Batch: 00023/00094 | Loss: 149.9824 | CE: 0.1062 | KD: 1061.3348\n",
      "Train Epoch: 064 Batch: 00024/00094 | Loss: 149.9182 | CE: 0.0852 | KD: 1061.0294\n",
      "Train Epoch: 064 Batch: 00025/00094 | Loss: 149.9286 | CE: 0.0973 | KD: 1061.0173\n",
      "Train Epoch: 064 Batch: 00026/00094 | Loss: 150.0040 | CE: 0.1268 | KD: 1061.3422\n",
      "Train Epoch: 064 Batch: 00027/00094 | Loss: 149.9383 | CE: 0.0629 | KD: 1061.3290\n",
      "Train Epoch: 064 Batch: 00028/00094 | Loss: 149.9299 | CE: 0.0784 | KD: 1061.1604\n",
      "Train Epoch: 064 Batch: 00029/00094 | Loss: 149.9762 | CE: 0.1627 | KD: 1060.8912\n",
      "Train Epoch: 064 Batch: 00030/00094 | Loss: 149.8871 | CE: 0.0962 | KD: 1060.7307\n",
      "Train Epoch: 064 Batch: 00031/00094 | Loss: 149.9705 | CE: 0.1203 | KD: 1061.1505\n",
      "Train Epoch: 064 Batch: 00032/00094 | Loss: 149.9494 | CE: 0.1097 | KD: 1061.0768\n",
      "Train Epoch: 064 Batch: 00033/00094 | Loss: 149.8731 | CE: 0.0713 | KD: 1060.8083\n",
      "Train Epoch: 064 Batch: 00034/00094 | Loss: 149.9328 | CE: 0.1295 | KD: 1060.8184\n",
      "Train Epoch: 064 Batch: 00035/00094 | Loss: 149.9727 | CE: 0.1245 | KD: 1061.1370\n",
      "Train Epoch: 064 Batch: 00036/00094 | Loss: 149.8883 | CE: 0.0925 | KD: 1060.7657\n",
      "Train Epoch: 064 Batch: 00037/00094 | Loss: 149.8850 | CE: 0.0705 | KD: 1060.8981\n",
      "Train Epoch: 064 Batch: 00038/00094 | Loss: 149.8941 | CE: 0.0782 | KD: 1060.9080\n",
      "Train Epoch: 064 Batch: 00039/00094 | Loss: 149.9152 | CE: 0.0980 | KD: 1060.9174\n",
      "Train Epoch: 064 Batch: 00040/00094 | Loss: 149.9061 | CE: 0.0712 | KD: 1061.0425\n",
      "Train Epoch: 064 Batch: 00041/00094 | Loss: 149.8876 | CE: 0.0657 | KD: 1060.9507\n",
      "Train Epoch: 064 Batch: 00042/00094 | Loss: 149.9614 | CE: 0.1397 | KD: 1060.9492\n",
      "Train Epoch: 064 Batch: 00043/00094 | Loss: 149.9107 | CE: 0.0670 | KD: 1061.1046\n",
      "Train Epoch: 064 Batch: 00044/00094 | Loss: 149.9134 | CE: 0.0777 | KD: 1061.0479\n",
      "Train Epoch: 064 Batch: 00045/00094 | Loss: 149.9789 | CE: 0.1237 | KD: 1061.1859\n",
      "Train Epoch: 064 Batch: 00046/00094 | Loss: 149.8495 | CE: 0.0892 | KD: 1060.5150\n",
      "Train Epoch: 064 Batch: 00047/00094 | Loss: 149.8486 | CE: 0.0675 | KD: 1060.6620\n",
      "Train Epoch: 064 Batch: 00048/00094 | Loss: 149.8869 | CE: 0.0734 | KD: 1060.8905\n",
      "Train Epoch: 064 Batch: 00049/00094 | Loss: 149.8398 | CE: 0.0721 | KD: 1060.5669\n",
      "Train Epoch: 064 Batch: 00050/00094 | Loss: 149.8795 | CE: 0.0848 | KD: 1060.7582\n",
      "Train Epoch: 064 Batch: 00051/00094 | Loss: 149.9218 | CE: 0.0602 | KD: 1061.2314\n",
      "Train Epoch: 064 Batch: 00052/00094 | Loss: 149.8939 | CE: 0.0725 | KD: 1060.9473\n",
      "Train Epoch: 064 Batch: 00053/00094 | Loss: 149.9100 | CE: 0.0755 | KD: 1061.0397\n",
      "Train Epoch: 064 Batch: 00054/00094 | Loss: 149.9144 | CE: 0.0795 | KD: 1061.0425\n",
      "Train Epoch: 064 Batch: 00055/00094 | Loss: 150.0064 | CE: 0.1356 | KD: 1061.2969\n",
      "Train Epoch: 064 Batch: 00056/00094 | Loss: 149.9294 | CE: 0.1166 | KD: 1060.8857\n",
      "Train Epoch: 064 Batch: 00057/00094 | Loss: 149.9143 | CE: 0.1071 | KD: 1060.8463\n",
      "Train Epoch: 064 Batch: 00058/00094 | Loss: 149.9269 | CE: 0.0639 | KD: 1061.2417\n",
      "Train Epoch: 064 Batch: 00059/00094 | Loss: 149.9808 | CE: 0.0997 | KD: 1061.3698\n",
      "Train Epoch: 064 Batch: 00060/00094 | Loss: 149.8900 | CE: 0.0657 | KD: 1060.9679\n",
      "Train Epoch: 064 Batch: 00061/00094 | Loss: 149.8511 | CE: 0.0568 | KD: 1060.7550\n",
      "Train Epoch: 064 Batch: 00062/00094 | Loss: 149.9043 | CE: 0.0735 | KD: 1061.0137\n",
      "Train Epoch: 064 Batch: 00063/00094 | Loss: 149.9313 | CE: 0.1017 | KD: 1061.0050\n",
      "Train Epoch: 064 Batch: 00064/00094 | Loss: 150.0985 | CE: 0.2076 | KD: 1061.4392\n",
      "Train Epoch: 064 Batch: 00065/00094 | Loss: 149.9639 | CE: 0.1000 | KD: 1061.2484\n",
      "Train Epoch: 064 Batch: 00066/00094 | Loss: 149.8886 | CE: 0.0586 | KD: 1061.0078\n",
      "Train Epoch: 064 Batch: 00067/00094 | Loss: 149.8858 | CE: 0.0718 | KD: 1060.8948\n",
      "Train Epoch: 064 Batch: 00068/00094 | Loss: 149.9082 | CE: 0.1124 | KD: 1060.7662\n",
      "Train Epoch: 064 Batch: 00069/00094 | Loss: 149.8956 | CE: 0.0619 | KD: 1061.0342\n",
      "Train Epoch: 064 Batch: 00070/00094 | Loss: 149.8841 | CE: 0.0781 | KD: 1060.8381\n",
      "Train Epoch: 064 Batch: 00071/00094 | Loss: 150.0057 | CE: 0.1545 | KD: 1061.1576\n",
      "Train Epoch: 064 Batch: 00072/00094 | Loss: 149.9035 | CE: 0.0914 | KD: 1060.8813\n",
      "Train Epoch: 064 Batch: 00073/00094 | Loss: 149.9411 | CE: 0.0905 | KD: 1061.1534\n",
      "Train Epoch: 064 Batch: 00074/00094 | Loss: 149.9104 | CE: 0.0667 | KD: 1061.1049\n",
      "Train Epoch: 064 Batch: 00075/00094 | Loss: 149.8698 | CE: 0.0645 | KD: 1060.8336\n",
      "Train Epoch: 064 Batch: 00076/00094 | Loss: 149.8479 | CE: 0.0594 | KD: 1060.7137\n",
      "Train Epoch: 064 Batch: 00077/00094 | Loss: 149.9309 | CE: 0.1315 | KD: 1060.7911\n",
      "Train Epoch: 064 Batch: 00078/00094 | Loss: 149.8420 | CE: 0.0770 | KD: 1060.5477\n",
      "Train Epoch: 064 Batch: 00079/00094 | Loss: 149.8980 | CE: 0.1057 | KD: 1060.7408\n",
      "Train Epoch: 064 Batch: 00080/00094 | Loss: 149.9458 | CE: 0.1076 | KD: 1061.0662\n",
      "Train Epoch: 064 Batch: 00081/00094 | Loss: 150.0204 | CE: 0.1229 | KD: 1061.4861\n",
      "Train Epoch: 064 Batch: 00082/00094 | Loss: 149.8758 | CE: 0.0713 | KD: 1060.8276\n",
      "Train Epoch: 064 Batch: 00083/00094 | Loss: 149.8778 | CE: 0.0925 | KD: 1060.6910\n",
      "Train Epoch: 064 Batch: 00084/00094 | Loss: 149.9211 | CE: 0.0884 | KD: 1061.0271\n",
      "Train Epoch: 064 Batch: 00085/00094 | Loss: 149.9286 | CE: 0.0649 | KD: 1061.2461\n",
      "Train Epoch: 064 Batch: 00086/00094 | Loss: 149.8796 | CE: 0.0889 | KD: 1060.7295\n",
      "Train Epoch: 064 Batch: 00087/00094 | Loss: 149.9449 | CE: 0.1224 | KD: 1060.9547\n",
      "Train Epoch: 064 Batch: 00088/00094 | Loss: 149.9644 | CE: 0.1588 | KD: 1060.8353\n",
      "Train Epoch: 064 Batch: 00089/00094 | Loss: 149.8852 | CE: 0.0643 | KD: 1060.9437\n",
      "Train Epoch: 064 Batch: 00090/00094 | Loss: 149.9826 | CE: 0.1743 | KD: 1060.8542\n",
      "Train Epoch: 064 Batch: 00091/00094 | Loss: 149.9126 | CE: 0.0737 | KD: 1061.0709\n",
      "Train Epoch: 064 Batch: 00092/00094 | Loss: 149.9632 | CE: 0.0970 | KD: 1061.2646\n",
      "Train Epoch: 064 Batch: 00093/00094 | Loss: 149.9785 | CE: 0.1417 | KD: 1061.0560\n",
      "Train Epoch: 064 Batch: 00094/00094 | Loss: 149.8274 | CE: 0.0618 | KD: 1060.5519\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0940 | acc:98.2500\n",
      "[VAL Acc] Target: 98.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6921 | acc:49.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2066 | acc:53.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2144 | acc:46.5649\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2969 | acc:54.8981\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6428 | acc:69.2237\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6939 | acc:71.7868\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8975 | acc:58.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.13%\n",
      "[VAL Acc] Avg 62.65%\n",
      "Train Epoch: 065 Batch: 00001/00094 | Loss: 134.9400 | CE: 0.1068 | KD: 1060.8997\n",
      "Train Epoch: 065 Batch: 00002/00094 | Loss: 134.9229 | CE: 0.0788 | KD: 1060.9847\n",
      "Train Epoch: 065 Batch: 00003/00094 | Loss: 134.9074 | CE: 0.0709 | KD: 1060.9253\n",
      "Train Epoch: 065 Batch: 00004/00094 | Loss: 134.8956 | CE: 0.0888 | KD: 1060.6917\n",
      "Train Epoch: 065 Batch: 00005/00094 | Loss: 134.9243 | CE: 0.1030 | KD: 1060.8063\n",
      "Train Epoch: 065 Batch: 00006/00094 | Loss: 134.8885 | CE: 0.0732 | KD: 1060.7588\n",
      "Train Epoch: 065 Batch: 00007/00094 | Loss: 134.9031 | CE: 0.0790 | KD: 1060.8274\n",
      "Train Epoch: 065 Batch: 00008/00094 | Loss: 134.8947 | CE: 0.0671 | KD: 1060.8557\n",
      "Train Epoch: 065 Batch: 00009/00094 | Loss: 134.9212 | CE: 0.0657 | KD: 1061.0747\n",
      "Train Epoch: 065 Batch: 00010/00094 | Loss: 135.0529 | CE: 0.1715 | KD: 1061.2793\n",
      "Train Epoch: 065 Batch: 00011/00094 | Loss: 134.9264 | CE: 0.0862 | KD: 1060.9552\n",
      "Train Epoch: 065 Batch: 00012/00094 | Loss: 134.9076 | CE: 0.0741 | KD: 1060.9019\n",
      "Train Epoch: 065 Batch: 00013/00094 | Loss: 134.8901 | CE: 0.0585 | KD: 1060.8867\n",
      "Train Epoch: 065 Batch: 00014/00094 | Loss: 135.0444 | CE: 0.1489 | KD: 1061.3896\n",
      "Train Epoch: 065 Batch: 00015/00094 | Loss: 135.0252 | CE: 0.1637 | KD: 1061.1221\n",
      "Train Epoch: 065 Batch: 00016/00094 | Loss: 134.9038 | CE: 0.0780 | KD: 1060.8412\n",
      "Train Epoch: 065 Batch: 00017/00094 | Loss: 134.9787 | CE: 0.1024 | KD: 1061.2386\n",
      "Train Epoch: 065 Batch: 00018/00094 | Loss: 135.0423 | CE: 0.1592 | KD: 1061.2917\n",
      "Train Epoch: 065 Batch: 00019/00094 | Loss: 134.9839 | CE: 0.0875 | KD: 1061.3969\n",
      "Train Epoch: 065 Batch: 00020/00094 | Loss: 134.9267 | CE: 0.0989 | KD: 1060.8575\n",
      "Train Epoch: 065 Batch: 00021/00094 | Loss: 134.9193 | CE: 0.0743 | KD: 1060.9922\n",
      "Train Epoch: 065 Batch: 00022/00094 | Loss: 134.9530 | CE: 0.0758 | KD: 1061.2457\n",
      "Train Epoch: 065 Batch: 00023/00094 | Loss: 134.9606 | CE: 0.0955 | KD: 1061.1498\n",
      "Train Epoch: 065 Batch: 00024/00094 | Loss: 134.9574 | CE: 0.1268 | KD: 1060.8788\n",
      "Train Epoch: 065 Batch: 00025/00094 | Loss: 135.0898 | CE: 0.1815 | KD: 1061.4907\n",
      "Train Epoch: 065 Batch: 00026/00094 | Loss: 134.9185 | CE: 0.0621 | KD: 1061.0823\n",
      "Train Epoch: 065 Batch: 00027/00094 | Loss: 134.9131 | CE: 0.0849 | KD: 1060.8604\n",
      "Train Epoch: 065 Batch: 00028/00094 | Loss: 134.9413 | CE: 0.0804 | KD: 1061.1173\n",
      "Train Epoch: 065 Batch: 00029/00094 | Loss: 134.9119 | CE: 0.1026 | KD: 1060.7119\n",
      "Train Epoch: 065 Batch: 00030/00094 | Loss: 134.9752 | CE: 0.1187 | KD: 1061.0834\n",
      "Train Epoch: 065 Batch: 00031/00094 | Loss: 134.8802 | CE: 0.0844 | KD: 1060.6052\n",
      "Train Epoch: 065 Batch: 00032/00094 | Loss: 134.9038 | CE: 0.0909 | KD: 1060.7393\n",
      "Train Epoch: 065 Batch: 00033/00094 | Loss: 135.0054 | CE: 0.1288 | KD: 1061.2411\n",
      "Train Epoch: 065 Batch: 00034/00094 | Loss: 134.9118 | CE: 0.0796 | KD: 1060.8914\n",
      "Train Epoch: 065 Batch: 00035/00094 | Loss: 134.9376 | CE: 0.0617 | KD: 1061.2355\n",
      "Train Epoch: 065 Batch: 00036/00094 | Loss: 134.9246 | CE: 0.0967 | KD: 1060.8574\n",
      "Train Epoch: 065 Batch: 00037/00094 | Loss: 134.9492 | CE: 0.1143 | KD: 1060.9126\n",
      "Train Epoch: 065 Batch: 00038/00094 | Loss: 134.9101 | CE: 0.0677 | KD: 1060.9718\n",
      "Train Epoch: 065 Batch: 00039/00094 | Loss: 134.9159 | CE: 0.0808 | KD: 1060.9149\n",
      "Train Epoch: 065 Batch: 00040/00094 | Loss: 134.9030 | CE: 0.0859 | KD: 1060.7732\n",
      "Train Epoch: 065 Batch: 00041/00094 | Loss: 134.8892 | CE: 0.0725 | KD: 1060.7693\n",
      "Train Epoch: 065 Batch: 00042/00094 | Loss: 135.0081 | CE: 0.1070 | KD: 1061.4341\n",
      "Train Epoch: 065 Batch: 00043/00094 | Loss: 134.9268 | CE: 0.0728 | KD: 1061.0632\n",
      "Train Epoch: 065 Batch: 00044/00094 | Loss: 134.9492 | CE: 0.0832 | KD: 1061.1577\n",
      "Train Epoch: 065 Batch: 00045/00094 | Loss: 134.9189 | CE: 0.0730 | KD: 1060.9996\n",
      "Train Epoch: 065 Batch: 00046/00094 | Loss: 134.9035 | CE: 0.0689 | KD: 1060.9099\n",
      "Train Epoch: 065 Batch: 00047/00094 | Loss: 134.9012 | CE: 0.0548 | KD: 1061.0037\n",
      "Train Epoch: 065 Batch: 00048/00094 | Loss: 134.9363 | CE: 0.0832 | KD: 1061.0560\n",
      "Train Epoch: 065 Batch: 00049/00094 | Loss: 134.9180 | CE: 0.0723 | KD: 1060.9980\n",
      "Train Epoch: 065 Batch: 00050/00094 | Loss: 134.8995 | CE: 0.0658 | KD: 1060.9028\n",
      "Train Epoch: 065 Batch: 00051/00094 | Loss: 134.9649 | CE: 0.1214 | KD: 1060.9803\n",
      "Train Epoch: 065 Batch: 00052/00094 | Loss: 134.9355 | CE: 0.1350 | KD: 1060.6420\n",
      "Train Epoch: 065 Batch: 00053/00094 | Loss: 134.9060 | CE: 0.0756 | KD: 1060.8779\n",
      "Train Epoch: 065 Batch: 00054/00094 | Loss: 134.9840 | CE: 0.0759 | KD: 1061.4885\n",
      "Train Epoch: 065 Batch: 00055/00094 | Loss: 134.9429 | CE: 0.0654 | KD: 1061.2480\n",
      "Train Epoch: 065 Batch: 00056/00094 | Loss: 134.9136 | CE: 0.0673 | KD: 1061.0020\n",
      "Train Epoch: 065 Batch: 00057/00094 | Loss: 134.9520 | CE: 0.0984 | KD: 1061.0602\n",
      "Train Epoch: 065 Batch: 00058/00094 | Loss: 134.9460 | CE: 0.1006 | KD: 1060.9954\n",
      "Train Epoch: 065 Batch: 00059/00094 | Loss: 134.8783 | CE: 0.0664 | KD: 1060.7322\n",
      "Train Epoch: 065 Batch: 00060/00094 | Loss: 134.8996 | CE: 0.0584 | KD: 1060.9623\n",
      "Train Epoch: 065 Batch: 00061/00094 | Loss: 134.9005 | CE: 0.0852 | KD: 1060.7587\n",
      "Train Epoch: 065 Batch: 00062/00094 | Loss: 134.9212 | CE: 0.0822 | KD: 1060.9451\n",
      "Train Epoch: 065 Batch: 00063/00094 | Loss: 134.8952 | CE: 0.0498 | KD: 1060.9951\n",
      "Train Epoch: 065 Batch: 00064/00094 | Loss: 134.9505 | CE: 0.1252 | KD: 1060.8372\n",
      "Train Epoch: 065 Batch: 00065/00094 | Loss: 134.8952 | CE: 0.0789 | KD: 1060.7664\n",
      "Train Epoch: 065 Batch: 00066/00094 | Loss: 134.9045 | CE: 0.0767 | KD: 1060.8567\n",
      "Train Epoch: 065 Batch: 00067/00094 | Loss: 134.9634 | CE: 0.1507 | KD: 1060.7384\n",
      "Train Epoch: 065 Batch: 00068/00094 | Loss: 135.0970 | CE: 0.1832 | KD: 1061.5337\n",
      "Train Epoch: 065 Batch: 00069/00094 | Loss: 134.9618 | CE: 0.0721 | KD: 1061.3439\n",
      "Train Epoch: 065 Batch: 00070/00094 | Loss: 134.9536 | CE: 0.0624 | KD: 1061.3562\n",
      "Train Epoch: 065 Batch: 00071/00094 | Loss: 134.8920 | CE: 0.0697 | KD: 1060.8140\n",
      "Train Epoch: 065 Batch: 00072/00094 | Loss: 134.9221 | CE: 0.0706 | KD: 1061.0436\n",
      "Train Epoch: 065 Batch: 00073/00094 | Loss: 134.9574 | CE: 0.0954 | KD: 1061.1263\n",
      "Train Epoch: 065 Batch: 00074/00094 | Loss: 134.9404 | CE: 0.0718 | KD: 1061.1785\n",
      "Train Epoch: 065 Batch: 00075/00094 | Loss: 134.9628 | CE: 0.0650 | KD: 1061.4077\n",
      "Train Epoch: 065 Batch: 00076/00094 | Loss: 134.9620 | CE: 0.0773 | KD: 1061.3046\n",
      "Train Epoch: 065 Batch: 00077/00094 | Loss: 134.8812 | CE: 0.0749 | KD: 1060.6880\n",
      "Train Epoch: 065 Batch: 00078/00094 | Loss: 134.9857 | CE: 0.1553 | KD: 1060.8772\n",
      "Train Epoch: 065 Batch: 00079/00094 | Loss: 134.9358 | CE: 0.1097 | KD: 1060.8440\n",
      "Train Epoch: 065 Batch: 00080/00094 | Loss: 134.9743 | CE: 0.1119 | KD: 1061.1288\n",
      "Train Epoch: 065 Batch: 00081/00094 | Loss: 135.0282 | CE: 0.1567 | KD: 1061.2007\n",
      "Train Epoch: 065 Batch: 00082/00094 | Loss: 134.9247 | CE: 0.0949 | KD: 1060.8723\n",
      "Train Epoch: 065 Batch: 00083/00094 | Loss: 134.9159 | CE: 0.0721 | KD: 1060.9823\n",
      "Train Epoch: 065 Batch: 00084/00094 | Loss: 134.9509 | CE: 0.0946 | KD: 1061.0813\n",
      "Train Epoch: 065 Batch: 00085/00094 | Loss: 134.9406 | CE: 0.0769 | KD: 1061.1395\n",
      "Train Epoch: 065 Batch: 00086/00094 | Loss: 134.9804 | CE: 0.1107 | KD: 1061.1873\n",
      "Train Epoch: 065 Batch: 00087/00094 | Loss: 134.9495 | CE: 0.1044 | KD: 1060.9932\n",
      "Train Epoch: 065 Batch: 00088/00094 | Loss: 134.9810 | CE: 0.1571 | KD: 1060.8259\n",
      "Train Epoch: 065 Batch: 00089/00094 | Loss: 134.8928 | CE: 0.0640 | KD: 1060.8655\n",
      "Train Epoch: 065 Batch: 00090/00094 | Loss: 134.9298 | CE: 0.0705 | KD: 1061.1051\n",
      "Train Epoch: 065 Batch: 00091/00094 | Loss: 134.9070 | CE: 0.0721 | KD: 1060.9125\n",
      "Train Epoch: 065 Batch: 00092/00094 | Loss: 134.9281 | CE: 0.0655 | KD: 1061.1311\n",
      "Train Epoch: 065 Batch: 00093/00094 | Loss: 134.9266 | CE: 0.0644 | KD: 1061.1276\n",
      "Train Epoch: 065 Batch: 00094/00094 | Loss: 134.8962 | CE: 0.0684 | KD: 1060.8568\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0823 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7407 | acc:48.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2144 | acc:53.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2031 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2723 | acc:55.8386\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5885 | acc:72.5508\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7376 | acc:71.0031\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8748 | acc:59.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.75%\n",
      "[VAL Acc] Avg 63.10%\n",
      "Train Epoch: 066 Batch: 00001/00094 | Loss: 134.9065 | CE: 0.0713 | KD: 1060.9153\n",
      "Train Epoch: 066 Batch: 00002/00094 | Loss: 135.0007 | CE: 0.1328 | KD: 1061.1726\n",
      "Train Epoch: 066 Batch: 00003/00094 | Loss: 134.9727 | CE: 0.0878 | KD: 1061.3060\n",
      "Train Epoch: 066 Batch: 00004/00094 | Loss: 134.9462 | CE: 0.0899 | KD: 1061.0811\n",
      "Train Epoch: 066 Batch: 00005/00094 | Loss: 134.9123 | CE: 0.0721 | KD: 1060.9545\n",
      "Train Epoch: 066 Batch: 00006/00094 | Loss: 135.0657 | CE: 0.1310 | KD: 1061.6975\n",
      "Train Epoch: 066 Batch: 00007/00094 | Loss: 134.9305 | CE: 0.0686 | KD: 1061.1255\n",
      "Train Epoch: 066 Batch: 00008/00094 | Loss: 134.9268 | CE: 0.0704 | KD: 1061.0824\n",
      "Train Epoch: 066 Batch: 00009/00094 | Loss: 134.9667 | CE: 0.0826 | KD: 1061.3003\n",
      "Train Epoch: 066 Batch: 00010/00094 | Loss: 134.9214 | CE: 0.0686 | KD: 1061.0531\n",
      "Train Epoch: 066 Batch: 00011/00094 | Loss: 134.9106 | CE: 0.0674 | KD: 1060.9779\n",
      "Train Epoch: 066 Batch: 00012/00094 | Loss: 134.9515 | CE: 0.0804 | KD: 1061.1979\n",
      "Train Epoch: 066 Batch: 00013/00094 | Loss: 134.9981 | CE: 0.1220 | KD: 1061.2375\n",
      "Train Epoch: 066 Batch: 00014/00094 | Loss: 134.8748 | CE: 0.0526 | KD: 1060.8131\n",
      "Train Epoch: 066 Batch: 00015/00094 | Loss: 134.9652 | CE: 0.1011 | KD: 1061.1431\n",
      "Train Epoch: 066 Batch: 00016/00094 | Loss: 134.9242 | CE: 0.0826 | KD: 1060.9658\n",
      "Train Epoch: 066 Batch: 00017/00094 | Loss: 135.0004 | CE: 0.0986 | KD: 1061.4393\n",
      "Train Epoch: 066 Batch: 00018/00094 | Loss: 134.9192 | CE: 0.0575 | KD: 1061.1237\n",
      "Train Epoch: 066 Batch: 00019/00094 | Loss: 134.9180 | CE: 0.0658 | KD: 1061.0490\n",
      "Train Epoch: 066 Batch: 00020/00094 | Loss: 134.9412 | CE: 0.0905 | KD: 1061.0377\n",
      "Train Epoch: 066 Batch: 00021/00094 | Loss: 134.8976 | CE: 0.0981 | KD: 1060.6348\n",
      "Train Epoch: 066 Batch: 00022/00094 | Loss: 134.8994 | CE: 0.0656 | KD: 1060.9043\n",
      "Train Epoch: 066 Batch: 00023/00094 | Loss: 134.8881 | CE: 0.1007 | KD: 1060.5392\n",
      "Train Epoch: 066 Batch: 00024/00094 | Loss: 134.8351 | CE: 0.0577 | KD: 1060.4609\n",
      "Train Epoch: 066 Batch: 00025/00094 | Loss: 135.0544 | CE: 0.2036 | KD: 1061.0376\n",
      "Train Epoch: 066 Batch: 00026/00094 | Loss: 134.9097 | CE: 0.0825 | KD: 1060.8523\n",
      "Train Epoch: 066 Batch: 00027/00094 | Loss: 134.9687 | CE: 0.1181 | KD: 1061.0365\n",
      "Train Epoch: 066 Batch: 00028/00094 | Loss: 134.9305 | CE: 0.0994 | KD: 1060.8837\n",
      "Train Epoch: 066 Batch: 00029/00094 | Loss: 134.9541 | CE: 0.1153 | KD: 1060.9440\n",
      "Train Epoch: 066 Batch: 00030/00094 | Loss: 134.9251 | CE: 0.1044 | KD: 1060.8008\n",
      "Train Epoch: 066 Batch: 00031/00094 | Loss: 134.9490 | CE: 0.0598 | KD: 1061.3402\n",
      "Train Epoch: 066 Batch: 00032/00094 | Loss: 134.9673 | CE: 0.1149 | KD: 1061.0507\n",
      "Train Epoch: 066 Batch: 00033/00094 | Loss: 134.9407 | CE: 0.1121 | KD: 1060.8640\n",
      "Train Epoch: 066 Batch: 00034/00094 | Loss: 134.9421 | CE: 0.0674 | KD: 1061.2256\n",
      "Train Epoch: 066 Batch: 00035/00094 | Loss: 134.8796 | CE: 0.1048 | KD: 1060.4398\n",
      "Train Epoch: 066 Batch: 00036/00094 | Loss: 134.8676 | CE: 0.0725 | KD: 1060.5995\n",
      "Train Epoch: 066 Batch: 00037/00094 | Loss: 134.8997 | CE: 0.0644 | KD: 1060.9160\n",
      "Train Epoch: 066 Batch: 00038/00094 | Loss: 134.9036 | CE: 0.0840 | KD: 1060.7925\n",
      "Train Epoch: 066 Batch: 00039/00094 | Loss: 134.9592 | CE: 0.1090 | KD: 1061.0337\n",
      "Train Epoch: 066 Batch: 00040/00094 | Loss: 134.9972 | CE: 0.1014 | KD: 1061.3925\n",
      "Train Epoch: 066 Batch: 00041/00094 | Loss: 134.8748 | CE: 0.0834 | KD: 1060.5704\n",
      "Train Epoch: 066 Batch: 00042/00094 | Loss: 134.9214 | CE: 0.0621 | KD: 1061.1050\n",
      "Train Epoch: 066 Batch: 00043/00094 | Loss: 134.9662 | CE: 0.1068 | KD: 1061.1052\n",
      "Train Epoch: 066 Batch: 00044/00094 | Loss: 134.9243 | CE: 0.0586 | KD: 1061.1545\n",
      "Train Epoch: 066 Batch: 00045/00094 | Loss: 134.9236 | CE: 0.1369 | KD: 1060.5341\n",
      "Train Epoch: 066 Batch: 00046/00094 | Loss: 134.9958 | CE: 0.1115 | KD: 1061.3016\n",
      "Train Epoch: 066 Batch: 00047/00094 | Loss: 134.9638 | CE: 0.0953 | KD: 1061.1770\n",
      "Train Epoch: 066 Batch: 00048/00094 | Loss: 134.9161 | CE: 0.0709 | KD: 1060.9939\n",
      "Train Epoch: 066 Batch: 00049/00094 | Loss: 134.9560 | CE: 0.1035 | KD: 1061.0513\n",
      "Train Epoch: 066 Batch: 00050/00094 | Loss: 134.9571 | CE: 0.1111 | KD: 1061.0004\n",
      "Train Epoch: 066 Batch: 00051/00094 | Loss: 135.0243 | CE: 0.1692 | KD: 1061.0715\n",
      "Train Epoch: 066 Batch: 00052/00094 | Loss: 135.0388 | CE: 0.1645 | KD: 1061.2230\n",
      "Train Epoch: 066 Batch: 00053/00094 | Loss: 134.8973 | CE: 0.0768 | KD: 1060.7999\n",
      "Train Epoch: 066 Batch: 00054/00094 | Loss: 134.8773 | CE: 0.0624 | KD: 1060.7549\n",
      "Train Epoch: 066 Batch: 00055/00094 | Loss: 134.9486 | CE: 0.0665 | KD: 1061.2842\n",
      "Train Epoch: 066 Batch: 00056/00094 | Loss: 135.0327 | CE: 0.1908 | KD: 1060.9684\n",
      "Train Epoch: 066 Batch: 00057/00094 | Loss: 134.9681 | CE: 0.0847 | KD: 1061.2943\n",
      "Train Epoch: 066 Batch: 00058/00094 | Loss: 134.9073 | CE: 0.0691 | KD: 1060.9387\n",
      "Train Epoch: 066 Batch: 00059/00094 | Loss: 134.9365 | CE: 0.1143 | KD: 1060.8132\n",
      "Train Epoch: 066 Batch: 00060/00094 | Loss: 134.8924 | CE: 0.0715 | KD: 1060.8024\n",
      "Train Epoch: 066 Batch: 00061/00094 | Loss: 134.9285 | CE: 0.1054 | KD: 1060.8206\n",
      "Train Epoch: 066 Batch: 00062/00094 | Loss: 134.9067 | CE: 0.0781 | KD: 1060.8634\n",
      "Train Epoch: 066 Batch: 00063/00094 | Loss: 134.9165 | CE: 0.1149 | KD: 1060.6504\n",
      "Train Epoch: 066 Batch: 00064/00094 | Loss: 134.9159 | CE: 0.0616 | KD: 1061.0652\n",
      "Train Epoch: 066 Batch: 00065/00094 | Loss: 135.0084 | CE: 0.0729 | KD: 1061.7048\n",
      "Train Epoch: 066 Batch: 00066/00094 | Loss: 134.9781 | CE: 0.0738 | KD: 1061.4586\n",
      "Train Epoch: 066 Batch: 00067/00094 | Loss: 134.9220 | CE: 0.0970 | KD: 1060.8354\n",
      "Train Epoch: 066 Batch: 00068/00094 | Loss: 134.8958 | CE: 0.0516 | KD: 1060.9863\n",
      "Train Epoch: 066 Batch: 00069/00094 | Loss: 134.9155 | CE: 0.0710 | KD: 1060.9879\n",
      "Train Epoch: 066 Batch: 00070/00094 | Loss: 134.9245 | CE: 0.0755 | KD: 1061.0237\n",
      "Train Epoch: 066 Batch: 00071/00094 | Loss: 134.8716 | CE: 0.0632 | KD: 1060.7048\n",
      "Train Epoch: 066 Batch: 00072/00094 | Loss: 134.8787 | CE: 0.0521 | KD: 1060.8474\n",
      "Train Epoch: 066 Batch: 00073/00094 | Loss: 134.8756 | CE: 0.0676 | KD: 1060.7013\n",
      "Train Epoch: 066 Batch: 00074/00094 | Loss: 134.9188 | CE: 0.0727 | KD: 1061.0006\n",
      "Train Epoch: 066 Batch: 00075/00094 | Loss: 134.9407 | CE: 0.0799 | KD: 1061.1168\n",
      "Train Epoch: 066 Batch: 00076/00094 | Loss: 134.9133 | CE: 0.0768 | KD: 1060.9252\n",
      "Train Epoch: 066 Batch: 00077/00094 | Loss: 134.9508 | CE: 0.0634 | KD: 1061.3254\n",
      "Train Epoch: 066 Batch: 00078/00094 | Loss: 134.9868 | CE: 0.1049 | KD: 1061.2823\n",
      "Train Epoch: 066 Batch: 00079/00094 | Loss: 134.8917 | CE: 0.0829 | KD: 1060.7078\n",
      "Train Epoch: 066 Batch: 00080/00094 | Loss: 134.9303 | CE: 0.0586 | KD: 1061.2024\n",
      "Train Epoch: 066 Batch: 00081/00094 | Loss: 134.9717 | CE: 0.0929 | KD: 1061.2584\n",
      "Train Epoch: 066 Batch: 00082/00094 | Loss: 135.0395 | CE: 0.1212 | KD: 1061.5687\n",
      "Train Epoch: 066 Batch: 00083/00094 | Loss: 134.9669 | CE: 0.0737 | KD: 1061.3719\n",
      "Train Epoch: 066 Batch: 00084/00094 | Loss: 134.9723 | CE: 0.0657 | KD: 1061.4772\n",
      "Train Epoch: 066 Batch: 00085/00094 | Loss: 134.9901 | CE: 0.1222 | KD: 1061.1721\n",
      "Train Epoch: 066 Batch: 00086/00094 | Loss: 134.9487 | CE: 0.0834 | KD: 1061.1516\n",
      "Train Epoch: 066 Batch: 00087/00094 | Loss: 134.9672 | CE: 0.0865 | KD: 1061.2727\n",
      "Train Epoch: 066 Batch: 00088/00094 | Loss: 134.9250 | CE: 0.0672 | KD: 1061.0930\n",
      "Train Epoch: 066 Batch: 00089/00094 | Loss: 134.9535 | CE: 0.1346 | KD: 1060.7866\n",
      "Train Epoch: 066 Batch: 00090/00094 | Loss: 134.8995 | CE: 0.0680 | KD: 1060.8861\n",
      "Train Epoch: 066 Batch: 00091/00094 | Loss: 134.9257 | CE: 0.0619 | KD: 1061.1404\n",
      "Train Epoch: 066 Batch: 00092/00094 | Loss: 134.9175 | CE: 0.0780 | KD: 1060.9491\n",
      "Train Epoch: 066 Batch: 00093/00094 | Loss: 134.9065 | CE: 0.0729 | KD: 1060.9026\n",
      "Train Epoch: 066 Batch: 00094/00094 | Loss: 134.9318 | CE: 0.0881 | KD: 1060.9819\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0818 | acc:98.5500\n",
      "[VAL Acc] Target: 98.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6793 | acc:48.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1728 | acc:52.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1700 | acc:46.1832\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2369 | acc:56.0737\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6360 | acc:71.5342\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6966 | acc:72.1787\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8770 | acc:59.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.38%\n",
      "[VAL Acc] Avg 63.18%\n",
      "Train Epoch: 067 Batch: 00001/00094 | Loss: 134.9214 | CE: 0.1114 | KD: 1060.7170\n",
      "Train Epoch: 067 Batch: 00002/00094 | Loss: 134.9102 | CE: 0.0605 | KD: 1061.0293\n",
      "Train Epoch: 067 Batch: 00003/00094 | Loss: 135.0048 | CE: 0.1289 | KD: 1061.2356\n",
      "Train Epoch: 067 Batch: 00004/00094 | Loss: 134.8717 | CE: 0.0848 | KD: 1060.5350\n",
      "Train Epoch: 067 Batch: 00005/00094 | Loss: 134.8886 | CE: 0.0618 | KD: 1060.8495\n",
      "Train Epoch: 067 Batch: 00006/00094 | Loss: 134.9152 | CE: 0.0797 | KD: 1060.9177\n",
      "Train Epoch: 067 Batch: 00007/00094 | Loss: 134.9802 | CE: 0.1024 | KD: 1061.2510\n",
      "Train Epoch: 067 Batch: 00008/00094 | Loss: 134.9600 | CE: 0.1030 | KD: 1061.0869\n",
      "Train Epoch: 067 Batch: 00009/00094 | Loss: 134.9306 | CE: 0.1177 | KD: 1060.7400\n",
      "Train Epoch: 067 Batch: 00010/00094 | Loss: 134.9133 | CE: 0.0854 | KD: 1060.8579\n",
      "Train Epoch: 067 Batch: 00011/00094 | Loss: 135.0228 | CE: 0.1087 | KD: 1061.5360\n",
      "Train Epoch: 067 Batch: 00012/00094 | Loss: 134.8702 | CE: 0.0889 | KD: 1060.4911\n",
      "Train Epoch: 067 Batch: 00013/00094 | Loss: 134.9486 | CE: 0.1080 | KD: 1060.9578\n",
      "Train Epoch: 067 Batch: 00014/00094 | Loss: 134.8763 | CE: 0.0786 | KD: 1060.6201\n",
      "Train Epoch: 067 Batch: 00015/00094 | Loss: 134.9412 | CE: 0.1035 | KD: 1060.9346\n",
      "Train Epoch: 067 Batch: 00016/00094 | Loss: 134.9205 | CE: 0.0732 | KD: 1061.0098\n",
      "Train Epoch: 067 Batch: 00017/00094 | Loss: 134.8700 | CE: 0.0634 | KD: 1060.6902\n",
      "Train Epoch: 067 Batch: 00018/00094 | Loss: 134.9451 | CE: 0.0648 | KD: 1061.2699\n",
      "Train Epoch: 067 Batch: 00019/00094 | Loss: 134.9184 | CE: 0.0655 | KD: 1061.0546\n",
      "Train Epoch: 067 Batch: 00020/00094 | Loss: 134.9097 | CE: 0.0617 | KD: 1061.0157\n",
      "Train Epoch: 067 Batch: 00021/00094 | Loss: 134.9417 | CE: 0.1112 | KD: 1060.8781\n",
      "Train Epoch: 067 Batch: 00022/00094 | Loss: 134.9169 | CE: 0.0668 | KD: 1061.0328\n",
      "Train Epoch: 067 Batch: 00023/00094 | Loss: 134.9255 | CE: 0.0872 | KD: 1060.9396\n",
      "Train Epoch: 067 Batch: 00024/00094 | Loss: 134.9083 | CE: 0.1165 | KD: 1060.5735\n",
      "Train Epoch: 067 Batch: 00025/00094 | Loss: 134.9394 | CE: 0.0959 | KD: 1060.9806\n",
      "Train Epoch: 067 Batch: 00026/00094 | Loss: 134.9063 | CE: 0.0645 | KD: 1060.9677\n",
      "Train Epoch: 067 Batch: 00027/00094 | Loss: 134.9319 | CE: 0.0801 | KD: 1061.0461\n",
      "Train Epoch: 067 Batch: 00028/00094 | Loss: 134.9532 | CE: 0.1202 | KD: 1060.8986\n",
      "Train Epoch: 067 Batch: 00029/00094 | Loss: 134.9961 | CE: 0.1244 | KD: 1061.2025\n",
      "Train Epoch: 067 Batch: 00030/00094 | Loss: 134.9231 | CE: 0.0770 | KD: 1061.0016\n",
      "Train Epoch: 067 Batch: 00031/00094 | Loss: 134.9682 | CE: 0.0874 | KD: 1061.2740\n",
      "Train Epoch: 067 Batch: 00032/00094 | Loss: 134.9509 | CE: 0.0653 | KD: 1061.3115\n",
      "Train Epoch: 067 Batch: 00033/00094 | Loss: 135.0239 | CE: 0.1038 | KD: 1061.5831\n",
      "Train Epoch: 067 Batch: 00034/00094 | Loss: 134.8835 | CE: 0.0622 | KD: 1060.8063\n",
      "Train Epoch: 067 Batch: 00035/00094 | Loss: 134.9615 | CE: 0.0616 | KD: 1061.4249\n",
      "Train Epoch: 067 Batch: 00036/00094 | Loss: 135.0405 | CE: 0.1894 | KD: 1061.0404\n",
      "Train Epoch: 067 Batch: 00037/00094 | Loss: 134.9154 | CE: 0.0710 | KD: 1060.9878\n",
      "Train Epoch: 067 Batch: 00038/00094 | Loss: 134.9111 | CE: 0.0990 | KD: 1060.7336\n",
      "Train Epoch: 067 Batch: 00039/00094 | Loss: 134.9237 | CE: 0.0747 | KD: 1061.0233\n",
      "Train Epoch: 067 Batch: 00040/00094 | Loss: 134.8993 | CE: 0.0547 | KD: 1060.9889\n",
      "Train Epoch: 067 Batch: 00041/00094 | Loss: 134.9172 | CE: 0.0692 | KD: 1061.0156\n",
      "Train Epoch: 067 Batch: 00042/00094 | Loss: 134.9284 | CE: 0.0928 | KD: 1060.9185\n",
      "Train Epoch: 067 Batch: 00043/00094 | Loss: 134.9008 | CE: 0.0728 | KD: 1060.8590\n",
      "Train Epoch: 067 Batch: 00044/00094 | Loss: 134.8472 | CE: 0.0550 | KD: 1060.5770\n",
      "Train Epoch: 067 Batch: 00045/00094 | Loss: 134.9099 | CE: 0.0769 | KD: 1060.8977\n",
      "Train Epoch: 067 Batch: 00046/00094 | Loss: 134.9158 | CE: 0.0708 | KD: 1060.9922\n",
      "Train Epoch: 067 Batch: 00047/00094 | Loss: 134.9299 | CE: 0.0990 | KD: 1060.8816\n",
      "Train Epoch: 067 Batch: 00048/00094 | Loss: 134.9276 | CE: 0.0904 | KD: 1060.9312\n",
      "Train Epoch: 067 Batch: 00049/00094 | Loss: 134.8472 | CE: 0.0499 | KD: 1060.6169\n",
      "Train Epoch: 067 Batch: 00050/00094 | Loss: 134.9148 | CE: 0.0789 | KD: 1060.9208\n",
      "Train Epoch: 067 Batch: 00051/00094 | Loss: 134.9566 | CE: 0.0774 | KD: 1061.2615\n",
      "Train Epoch: 067 Batch: 00052/00094 | Loss: 134.9456 | CE: 0.0870 | KD: 1061.0991\n",
      "Train Epoch: 067 Batch: 00053/00094 | Loss: 134.8930 | CE: 0.0865 | KD: 1060.6901\n",
      "Train Epoch: 067 Batch: 00054/00094 | Loss: 135.0148 | CE: 0.1003 | KD: 1061.5396\n",
      "Train Epoch: 067 Batch: 00055/00094 | Loss: 135.0589 | CE: 0.1548 | KD: 1061.4570\n",
      "Train Epoch: 067 Batch: 00056/00094 | Loss: 134.9012 | CE: 0.0702 | KD: 1060.8821\n",
      "Train Epoch: 067 Batch: 00057/00094 | Loss: 134.9360 | CE: 0.0889 | KD: 1061.0083\n",
      "Train Epoch: 067 Batch: 00058/00094 | Loss: 134.9737 | CE: 0.0668 | KD: 1061.4795\n",
      "Train Epoch: 067 Batch: 00059/00094 | Loss: 134.9216 | CE: 0.0626 | KD: 1061.1022\n",
      "Train Epoch: 067 Batch: 00060/00094 | Loss: 134.9239 | CE: 0.0819 | KD: 1060.9686\n",
      "Train Epoch: 067 Batch: 00061/00094 | Loss: 134.9401 | CE: 0.0650 | KD: 1061.2290\n",
      "Train Epoch: 067 Batch: 00062/00094 | Loss: 134.9469 | CE: 0.1201 | KD: 1060.8491\n",
      "Train Epoch: 067 Batch: 00063/00094 | Loss: 134.9089 | CE: 0.0925 | KD: 1060.7672\n",
      "Train Epoch: 067 Batch: 00064/00094 | Loss: 134.8878 | CE: 0.0709 | KD: 1060.7715\n",
      "Train Epoch: 067 Batch: 00065/00094 | Loss: 134.8907 | CE: 0.0605 | KD: 1060.8762\n",
      "Train Epoch: 067 Batch: 00066/00094 | Loss: 134.8877 | CE: 0.0709 | KD: 1060.7703\n",
      "Train Epoch: 067 Batch: 00067/00094 | Loss: 134.9507 | CE: 0.1158 | KD: 1060.9124\n",
      "Train Epoch: 067 Batch: 00068/00094 | Loss: 134.9223 | CE: 0.0814 | KD: 1060.9601\n",
      "Train Epoch: 067 Batch: 00069/00094 | Loss: 134.9527 | CE: 0.0638 | KD: 1061.3381\n",
      "Train Epoch: 067 Batch: 00070/00094 | Loss: 135.0043 | CE: 0.0863 | KD: 1061.5662\n",
      "Train Epoch: 067 Batch: 00071/00094 | Loss: 134.9105 | CE: 0.0783 | KD: 1060.8910\n",
      "Train Epoch: 067 Batch: 00072/00094 | Loss: 134.9393 | CE: 0.0728 | KD: 1061.1617\n",
      "Train Epoch: 067 Batch: 00073/00094 | Loss: 134.9339 | CE: 0.0945 | KD: 1060.9487\n",
      "Train Epoch: 067 Batch: 00074/00094 | Loss: 134.9405 | CE: 0.0768 | KD: 1061.1395\n",
      "Train Epoch: 067 Batch: 00075/00094 | Loss: 134.9123 | CE: 0.0627 | KD: 1061.0286\n",
      "Train Epoch: 067 Batch: 00076/00094 | Loss: 134.9356 | CE: 0.0747 | KD: 1061.1169\n",
      "Train Epoch: 067 Batch: 00077/00094 | Loss: 134.9258 | CE: 0.0705 | KD: 1061.0735\n",
      "Train Epoch: 067 Batch: 00078/00094 | Loss: 134.9162 | CE: 0.0780 | KD: 1060.9385\n",
      "Train Epoch: 067 Batch: 00079/00094 | Loss: 134.9576 | CE: 0.0905 | KD: 1061.1669\n",
      "Train Epoch: 067 Batch: 00080/00094 | Loss: 134.8781 | CE: 0.0547 | KD: 1060.8224\n",
      "Train Epoch: 067 Batch: 00081/00094 | Loss: 134.9816 | CE: 0.0844 | KD: 1061.4036\n",
      "Train Epoch: 067 Batch: 00082/00094 | Loss: 134.8742 | CE: 0.0761 | KD: 1060.6230\n",
      "Train Epoch: 067 Batch: 00083/00094 | Loss: 134.8586 | CE: 0.0612 | KD: 1060.6178\n",
      "Train Epoch: 067 Batch: 00084/00094 | Loss: 134.9915 | CE: 0.1222 | KD: 1061.1832\n",
      "Train Epoch: 067 Batch: 00085/00094 | Loss: 134.9061 | CE: 0.0771 | KD: 1060.8665\n",
      "Train Epoch: 067 Batch: 00086/00094 | Loss: 134.9411 | CE: 0.0811 | KD: 1061.1100\n",
      "Train Epoch: 067 Batch: 00087/00094 | Loss: 134.9104 | CE: 0.0639 | KD: 1061.0042\n",
      "Train Epoch: 067 Batch: 00088/00094 | Loss: 134.9811 | CE: 0.0957 | KD: 1061.3104\n",
      "Train Epoch: 067 Batch: 00089/00094 | Loss: 135.0508 | CE: 0.2070 | KD: 1060.9824\n",
      "Train Epoch: 067 Batch: 00090/00094 | Loss: 134.9052 | CE: 0.0825 | KD: 1060.8170\n",
      "Train Epoch: 067 Batch: 00091/00094 | Loss: 134.9242 | CE: 0.0658 | KD: 1061.0983\n",
      "Train Epoch: 067 Batch: 00092/00094 | Loss: 134.9088 | CE: 0.0905 | KD: 1060.7817\n",
      "Train Epoch: 067 Batch: 00093/00094 | Loss: 135.0185 | CE: 0.1280 | KD: 1061.3503\n",
      "Train Epoch: 067 Batch: 00094/00094 | Loss: 134.9007 | CE: 0.0584 | KD: 1060.9714\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0820 | acc:98.8500\n",
      "[VAL Acc] Target: 98.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7766 | acc:49.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.3015 | acc:51.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2902 | acc:46.5649\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3676 | acc:54.8589\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7202 | acc:67.6525\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 67.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8336 | acc:68.4953\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8982 | acc:58.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.63%\n",
      "[VAL Acc] Avg 61.98%\n",
      "Train Epoch: 068 Batch: 00001/00094 | Loss: 134.9913 | CE: 0.1212 | KD: 1061.1899\n",
      "Train Epoch: 068 Batch: 00002/00094 | Loss: 134.9141 | CE: 0.0817 | KD: 1060.8929\n",
      "Train Epoch: 068 Batch: 00003/00094 | Loss: 134.9974 | CE: 0.1163 | KD: 1061.2761\n",
      "Train Epoch: 068 Batch: 00004/00094 | Loss: 134.8976 | CE: 0.0634 | KD: 1060.9078\n",
      "Train Epoch: 068 Batch: 00005/00094 | Loss: 134.9207 | CE: 0.0680 | KD: 1061.0535\n",
      "Train Epoch: 068 Batch: 00006/00094 | Loss: 134.9174 | CE: 0.0679 | KD: 1061.0273\n",
      "Train Epoch: 068 Batch: 00007/00094 | Loss: 134.9026 | CE: 0.0645 | KD: 1060.9375\n",
      "Train Epoch: 068 Batch: 00008/00094 | Loss: 134.9209 | CE: 0.0787 | KD: 1060.9703\n",
      "Train Epoch: 068 Batch: 00009/00094 | Loss: 135.0063 | CE: 0.1435 | KD: 1061.1329\n",
      "Train Epoch: 068 Batch: 00010/00094 | Loss: 134.9663 | CE: 0.1160 | KD: 1061.0341\n",
      "Train Epoch: 068 Batch: 00011/00094 | Loss: 134.9756 | CE: 0.0777 | KD: 1061.4084\n",
      "Train Epoch: 068 Batch: 00012/00094 | Loss: 134.9699 | CE: 0.0768 | KD: 1061.3705\n",
      "Train Epoch: 068 Batch: 00013/00094 | Loss: 134.9027 | CE: 0.0551 | KD: 1061.0127\n",
      "Train Epoch: 068 Batch: 00014/00094 | Loss: 134.9576 | CE: 0.0850 | KD: 1061.2100\n",
      "Train Epoch: 068 Batch: 00015/00094 | Loss: 134.9494 | CE: 0.1308 | KD: 1060.7849\n",
      "Train Epoch: 068 Batch: 00016/00094 | Loss: 134.9130 | CE: 0.0829 | KD: 1060.8748\n",
      "Train Epoch: 068 Batch: 00017/00094 | Loss: 134.9248 | CE: 0.0778 | KD: 1061.0081\n",
      "Train Epoch: 068 Batch: 00018/00094 | Loss: 134.9201 | CE: 0.0643 | KD: 1061.0775\n",
      "Train Epoch: 068 Batch: 00019/00094 | Loss: 134.9049 | CE: 0.0668 | KD: 1060.9380\n",
      "Train Epoch: 068 Batch: 00020/00094 | Loss: 134.8900 | CE: 0.0681 | KD: 1060.8104\n",
      "Train Epoch: 068 Batch: 00021/00094 | Loss: 134.8829 | CE: 0.0708 | KD: 1060.7341\n",
      "Train Epoch: 068 Batch: 00022/00094 | Loss: 134.8857 | CE: 0.0606 | KD: 1060.8359\n",
      "Train Epoch: 068 Batch: 00023/00094 | Loss: 134.9742 | CE: 0.1017 | KD: 1061.2092\n",
      "Train Epoch: 068 Batch: 00024/00094 | Loss: 134.9147 | CE: 0.0633 | KD: 1061.0428\n",
      "Train Epoch: 068 Batch: 00025/00094 | Loss: 134.9462 | CE: 0.1080 | KD: 1060.9387\n",
      "Train Epoch: 068 Batch: 00026/00094 | Loss: 134.9194 | CE: 0.0831 | KD: 1060.9237\n",
      "Train Epoch: 068 Batch: 00027/00094 | Loss: 134.9292 | CE: 0.0855 | KD: 1060.9821\n",
      "Train Epoch: 068 Batch: 00028/00094 | Loss: 134.9328 | CE: 0.0965 | KD: 1060.9235\n",
      "Train Epoch: 068 Batch: 00029/00094 | Loss: 134.9037 | CE: 0.0673 | KD: 1060.9253\n",
      "Train Epoch: 068 Batch: 00030/00094 | Loss: 134.9818 | CE: 0.1193 | KD: 1061.1299\n",
      "Train Epoch: 068 Batch: 00031/00094 | Loss: 134.9681 | CE: 0.0984 | KD: 1061.1869\n",
      "Train Epoch: 068 Batch: 00032/00094 | Loss: 134.9015 | CE: 0.0702 | KD: 1060.8844\n",
      "Train Epoch: 068 Batch: 00033/00094 | Loss: 134.9158 | CE: 0.0738 | KD: 1060.9694\n",
      "Train Epoch: 068 Batch: 00034/00094 | Loss: 134.9485 | CE: 0.0662 | KD: 1061.2859\n",
      "Train Epoch: 068 Batch: 00035/00094 | Loss: 134.9189 | CE: 0.0596 | KD: 1061.1052\n",
      "Train Epoch: 068 Batch: 00036/00094 | Loss: 134.9211 | CE: 0.1018 | KD: 1060.7897\n",
      "Train Epoch: 068 Batch: 00037/00094 | Loss: 134.9760 | CE: 0.0997 | KD: 1061.2384\n",
      "Train Epoch: 068 Batch: 00038/00094 | Loss: 134.9325 | CE: 0.0687 | KD: 1061.1405\n",
      "Train Epoch: 068 Batch: 00039/00094 | Loss: 135.0051 | CE: 0.1738 | KD: 1060.8845\n",
      "Train Epoch: 068 Batch: 00040/00094 | Loss: 134.9247 | CE: 0.0935 | KD: 1060.8837\n",
      "Train Epoch: 068 Batch: 00041/00094 | Loss: 134.8631 | CE: 0.0539 | KD: 1060.7113\n",
      "Train Epoch: 068 Batch: 00042/00094 | Loss: 134.9479 | CE: 0.0721 | KD: 1061.2347\n",
      "Train Epoch: 068 Batch: 00043/00094 | Loss: 134.9319 | CE: 0.0973 | KD: 1060.9105\n",
      "Train Epoch: 068 Batch: 00044/00094 | Loss: 134.8980 | CE: 0.0705 | KD: 1060.8544\n",
      "Train Epoch: 068 Batch: 00045/00094 | Loss: 134.9579 | CE: 0.0677 | KD: 1061.3480\n",
      "Train Epoch: 068 Batch: 00046/00094 | Loss: 135.0166 | CE: 0.1051 | KD: 1061.5150\n",
      "Train Epoch: 068 Batch: 00047/00094 | Loss: 134.9430 | CE: 0.1019 | KD: 1060.9612\n",
      "Train Epoch: 068 Batch: 00048/00094 | Loss: 134.8922 | CE: 0.0617 | KD: 1060.8776\n",
      "Train Epoch: 068 Batch: 00049/00094 | Loss: 134.9285 | CE: 0.0710 | KD: 1061.0907\n",
      "Train Epoch: 068 Batch: 00050/00094 | Loss: 134.9090 | CE: 0.0666 | KD: 1060.9717\n",
      "Train Epoch: 068 Batch: 00051/00094 | Loss: 134.8976 | CE: 0.0659 | KD: 1060.8877\n",
      "Train Epoch: 068 Batch: 00052/00094 | Loss: 134.9291 | CE: 0.0674 | KD: 1061.1232\n",
      "Train Epoch: 068 Batch: 00053/00094 | Loss: 134.9257 | CE: 0.0978 | KD: 1060.8578\n",
      "Train Epoch: 068 Batch: 00054/00094 | Loss: 134.9532 | CE: 0.1003 | KD: 1061.0546\n",
      "Train Epoch: 068 Batch: 00055/00094 | Loss: 134.8985 | CE: 0.0660 | KD: 1060.8934\n",
      "Train Epoch: 068 Batch: 00056/00094 | Loss: 134.9664 | CE: 0.1318 | KD: 1060.9102\n",
      "Train Epoch: 068 Batch: 00057/00094 | Loss: 134.9370 | CE: 0.0872 | KD: 1061.0302\n",
      "Train Epoch: 068 Batch: 00058/00094 | Loss: 134.8922 | CE: 0.0767 | KD: 1060.7603\n",
      "Train Epoch: 068 Batch: 00059/00094 | Loss: 134.9630 | CE: 0.0733 | KD: 1061.3438\n",
      "Train Epoch: 068 Batch: 00060/00094 | Loss: 134.9442 | CE: 0.1460 | KD: 1060.6240\n",
      "Train Epoch: 068 Batch: 00061/00094 | Loss: 135.0383 | CE: 0.2059 | KD: 1060.8936\n",
      "Train Epoch: 068 Batch: 00062/00094 | Loss: 134.9165 | CE: 0.0829 | KD: 1060.9030\n",
      "Train Epoch: 068 Batch: 00063/00094 | Loss: 134.9626 | CE: 0.0921 | KD: 1061.1934\n",
      "Train Epoch: 068 Batch: 00064/00094 | Loss: 134.9207 | CE: 0.0704 | KD: 1061.0343\n",
      "Train Epoch: 068 Batch: 00065/00094 | Loss: 134.9343 | CE: 0.0959 | KD: 1060.9404\n",
      "Train Epoch: 068 Batch: 00066/00094 | Loss: 134.9295 | CE: 0.0696 | KD: 1061.1094\n",
      "Train Epoch: 068 Batch: 00067/00094 | Loss: 134.9468 | CE: 0.0684 | KD: 1061.2551\n",
      "Train Epoch: 068 Batch: 00068/00094 | Loss: 134.9249 | CE: 0.0773 | KD: 1061.0125\n",
      "Train Epoch: 068 Batch: 00069/00094 | Loss: 135.0120 | CE: 0.0881 | KD: 1061.6133\n",
      "Train Epoch: 068 Batch: 00070/00094 | Loss: 134.9268 | CE: 0.0765 | KD: 1061.0337\n",
      "Train Epoch: 068 Batch: 00071/00094 | Loss: 135.0047 | CE: 0.1116 | KD: 1061.3707\n",
      "Train Epoch: 068 Batch: 00072/00094 | Loss: 135.0010 | CE: 0.0837 | KD: 1061.5614\n",
      "Train Epoch: 068 Batch: 00073/00094 | Loss: 134.9454 | CE: 0.0616 | KD: 1061.2980\n",
      "Train Epoch: 068 Batch: 00074/00094 | Loss: 134.8790 | CE: 0.0763 | KD: 1060.6598\n",
      "Train Epoch: 068 Batch: 00075/00094 | Loss: 135.0103 | CE: 0.1231 | KD: 1061.3245\n",
      "Train Epoch: 068 Batch: 00076/00094 | Loss: 135.0119 | CE: 0.1037 | KD: 1061.4899\n",
      "Train Epoch: 068 Batch: 00077/00094 | Loss: 134.8920 | CE: 0.0746 | KD: 1060.7749\n",
      "Train Epoch: 068 Batch: 00078/00094 | Loss: 134.9020 | CE: 0.0566 | KD: 1060.9954\n",
      "Train Epoch: 068 Batch: 00079/00094 | Loss: 134.9313 | CE: 0.0702 | KD: 1061.1190\n",
      "Train Epoch: 068 Batch: 00080/00094 | Loss: 134.8699 | CE: 0.0586 | KD: 1060.7269\n",
      "Train Epoch: 068 Batch: 00081/00094 | Loss: 134.9405 | CE: 0.1066 | KD: 1060.9052\n",
      "Train Epoch: 068 Batch: 00082/00094 | Loss: 134.9466 | CE: 0.0571 | KD: 1061.3425\n",
      "Train Epoch: 068 Batch: 00083/00094 | Loss: 134.8927 | CE: 0.0702 | KD: 1060.8152\n",
      "Train Epoch: 068 Batch: 00084/00094 | Loss: 134.9408 | CE: 0.0941 | KD: 1061.0057\n",
      "Train Epoch: 068 Batch: 00085/00094 | Loss: 134.9437 | CE: 0.0673 | KD: 1061.2389\n",
      "Train Epoch: 068 Batch: 00086/00094 | Loss: 135.0343 | CE: 0.1197 | KD: 1061.5403\n",
      "Train Epoch: 068 Batch: 00087/00094 | Loss: 134.9083 | CE: 0.0680 | KD: 1060.9550\n",
      "Train Epoch: 068 Batch: 00088/00094 | Loss: 134.9346 | CE: 0.0921 | KD: 1060.9725\n",
      "Train Epoch: 068 Batch: 00089/00094 | Loss: 134.9085 | CE: 0.0705 | KD: 1060.9374\n",
      "Train Epoch: 068 Batch: 00090/00094 | Loss: 134.9587 | CE: 0.0911 | KD: 1061.1705\n",
      "Train Epoch: 068 Batch: 00091/00094 | Loss: 134.8884 | CE: 0.0680 | KD: 1060.7991\n",
      "Train Epoch: 068 Batch: 00092/00094 | Loss: 134.9583 | CE: 0.0639 | KD: 1061.3811\n",
      "Train Epoch: 068 Batch: 00093/00094 | Loss: 134.9730 | CE: 0.0912 | KD: 1061.2819\n",
      "Train Epoch: 068 Batch: 00094/00094 | Loss: 134.9466 | CE: 0.1038 | KD: 1060.9750\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0775 | acc:98.9000\n",
      "[VAL Acc] Target: 98.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7660 | acc:49.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2431 | acc:51.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1930 | acc:45.0382\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3144 | acc:55.1724\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.17%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8374 | acc:61.7375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 61.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8172 | acc:68.0643\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9036 | acc:58.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.88%\n",
      "[VAL Acc] Avg 61.06%\n",
      "Train Epoch: 069 Batch: 00001/00094 | Loss: 121.4427 | CE: 0.0766 | KD: 1061.0410\n",
      "Train Epoch: 069 Batch: 00002/00094 | Loss: 121.3839 | CE: 0.0601 | KD: 1060.6719\n",
      "Train Epoch: 069 Batch: 00003/00094 | Loss: 121.4169 | CE: 0.0551 | KD: 1061.0043\n",
      "Train Epoch: 069 Batch: 00004/00094 | Loss: 121.4882 | CE: 0.0737 | KD: 1061.4641\n",
      "Train Epoch: 069 Batch: 00005/00094 | Loss: 121.4112 | CE: 0.0714 | KD: 1060.8109\n",
      "Train Epoch: 069 Batch: 00006/00094 | Loss: 121.3925 | CE: 0.0667 | KD: 1060.6891\n",
      "Train Epoch: 069 Batch: 00007/00094 | Loss: 121.4908 | CE: 0.0870 | KD: 1061.3706\n",
      "Train Epoch: 069 Batch: 00008/00094 | Loss: 121.4475 | CE: 0.0816 | KD: 1061.0393\n",
      "Train Epoch: 069 Batch: 00009/00094 | Loss: 121.4462 | CE: 0.0981 | KD: 1060.8834\n",
      "Train Epoch: 069 Batch: 00010/00094 | Loss: 121.4762 | CE: 0.1473 | KD: 1060.7159\n",
      "Train Epoch: 069 Batch: 00011/00094 | Loss: 121.4763 | CE: 0.0950 | KD: 1061.1738\n",
      "Train Epoch: 069 Batch: 00012/00094 | Loss: 121.4363 | CE: 0.0915 | KD: 1060.8547\n",
      "Train Epoch: 069 Batch: 00013/00094 | Loss: 121.4558 | CE: 0.0615 | KD: 1061.2883\n",
      "Train Epoch: 069 Batch: 00014/00094 | Loss: 121.4674 | CE: 0.0902 | KD: 1061.1379\n",
      "Train Epoch: 069 Batch: 00015/00094 | Loss: 121.4679 | CE: 0.0731 | KD: 1061.2925\n",
      "Train Epoch: 069 Batch: 00016/00094 | Loss: 121.4701 | CE: 0.1060 | KD: 1061.0242\n",
      "Train Epoch: 069 Batch: 00017/00094 | Loss: 121.4395 | CE: 0.0644 | KD: 1061.1206\n",
      "Train Epoch: 069 Batch: 00018/00094 | Loss: 121.4817 | CE: 0.1216 | KD: 1060.9890\n",
      "Train Epoch: 069 Batch: 00019/00094 | Loss: 121.4092 | CE: 0.0699 | KD: 1060.8074\n",
      "Train Epoch: 069 Batch: 00020/00094 | Loss: 121.5044 | CE: 0.0683 | KD: 1061.6534\n",
      "Train Epoch: 069 Batch: 00021/00094 | Loss: 121.3726 | CE: 0.0573 | KD: 1060.5970\n",
      "Train Epoch: 069 Batch: 00022/00094 | Loss: 121.4822 | CE: 0.1104 | KD: 1061.0916\n",
      "Train Epoch: 069 Batch: 00023/00094 | Loss: 121.4578 | CE: 0.0883 | KD: 1061.0710\n",
      "Train Epoch: 069 Batch: 00024/00094 | Loss: 121.4275 | CE: 0.0534 | KD: 1061.1107\n",
      "Train Epoch: 069 Batch: 00025/00094 | Loss: 121.5140 | CE: 0.1090 | KD: 1061.3810\n",
      "Train Epoch: 069 Batch: 00026/00094 | Loss: 121.3962 | CE: 0.0792 | KD: 1060.6123\n",
      "Train Epoch: 069 Batch: 00027/00094 | Loss: 121.4724 | CE: 0.0970 | KD: 1061.1224\n",
      "Train Epoch: 069 Batch: 00028/00094 | Loss: 121.4095 | CE: 0.0551 | KD: 1060.9381\n",
      "Train Epoch: 069 Batch: 00029/00094 | Loss: 121.4532 | CE: 0.0642 | KD: 1061.2416\n",
      "Train Epoch: 069 Batch: 00030/00094 | Loss: 121.3716 | CE: 0.0509 | KD: 1060.6437\n",
      "Train Epoch: 069 Batch: 00031/00094 | Loss: 121.4457 | CE: 0.0647 | KD: 1061.1714\n",
      "Train Epoch: 069 Batch: 00032/00094 | Loss: 121.4063 | CE: 0.0661 | KD: 1060.8148\n",
      "Train Epoch: 069 Batch: 00033/00094 | Loss: 121.4084 | CE: 0.0783 | KD: 1060.7264\n",
      "Train Epoch: 069 Batch: 00034/00094 | Loss: 121.6861 | CE: 0.2804 | KD: 1061.3879\n",
      "Train Epoch: 069 Batch: 00035/00094 | Loss: 121.4260 | CE: 0.0516 | KD: 1061.1139\n",
      "Train Epoch: 069 Batch: 00036/00094 | Loss: 121.4696 | CE: 0.0822 | KD: 1061.2274\n",
      "Train Epoch: 069 Batch: 00037/00094 | Loss: 121.5852 | CE: 0.2291 | KD: 1060.9539\n",
      "Train Epoch: 069 Batch: 00038/00094 | Loss: 121.4386 | CE: 0.0669 | KD: 1061.0901\n",
      "Train Epoch: 069 Batch: 00039/00094 | Loss: 121.4660 | CE: 0.0925 | KD: 1061.1056\n",
      "Train Epoch: 069 Batch: 00040/00094 | Loss: 121.4403 | CE: 0.0774 | KD: 1061.0128\n",
      "Train Epoch: 069 Batch: 00041/00094 | Loss: 121.4373 | CE: 0.1115 | KD: 1060.6891\n",
      "Train Epoch: 069 Batch: 00042/00094 | Loss: 121.4803 | CE: 0.0976 | KD: 1061.1857\n",
      "Train Epoch: 069 Batch: 00043/00094 | Loss: 121.4644 | CE: 0.0767 | KD: 1061.2299\n",
      "Train Epoch: 069 Batch: 00044/00094 | Loss: 121.4109 | CE: 0.0664 | KD: 1060.8525\n",
      "Train Epoch: 069 Batch: 00045/00094 | Loss: 121.3969 | CE: 0.0717 | KD: 1060.6833\n",
      "Train Epoch: 069 Batch: 00046/00094 | Loss: 121.4306 | CE: 0.0781 | KD: 1060.9224\n",
      "Train Epoch: 069 Batch: 00047/00094 | Loss: 121.4845 | CE: 0.1097 | KD: 1061.1169\n",
      "Train Epoch: 069 Batch: 00048/00094 | Loss: 121.4005 | CE: 0.0667 | KD: 1060.7582\n",
      "Train Epoch: 069 Batch: 00049/00094 | Loss: 121.5975 | CE: 0.1887 | KD: 1061.4148\n",
      "Train Epoch: 069 Batch: 00050/00094 | Loss: 121.4064 | CE: 0.0902 | KD: 1060.6047\n",
      "Train Epoch: 069 Batch: 00051/00094 | Loss: 121.3979 | CE: 0.0566 | KD: 1060.8250\n",
      "Train Epoch: 069 Batch: 00052/00094 | Loss: 121.4684 | CE: 0.0757 | KD: 1061.2738\n",
      "Train Epoch: 069 Batch: 00053/00094 | Loss: 121.4131 | CE: 0.0648 | KD: 1060.8861\n",
      "Train Epoch: 069 Batch: 00054/00094 | Loss: 121.4112 | CE: 0.0622 | KD: 1060.8920\n",
      "Train Epoch: 069 Batch: 00055/00094 | Loss: 121.4733 | CE: 0.0951 | KD: 1061.1467\n",
      "Train Epoch: 069 Batch: 00056/00094 | Loss: 121.4791 | CE: 0.1237 | KD: 1060.9479\n",
      "Train Epoch: 069 Batch: 00057/00094 | Loss: 121.4691 | CE: 0.0666 | KD: 1061.3597\n",
      "Train Epoch: 069 Batch: 00058/00094 | Loss: 121.4288 | CE: 0.0780 | KD: 1060.9072\n",
      "Train Epoch: 069 Batch: 00059/00094 | Loss: 121.4996 | CE: 0.1212 | KD: 1061.1488\n",
      "Train Epoch: 069 Batch: 00060/00094 | Loss: 121.4857 | CE: 0.0985 | KD: 1061.2256\n",
      "Train Epoch: 069 Batch: 00061/00094 | Loss: 121.4500 | CE: 0.0607 | KD: 1061.2434\n",
      "Train Epoch: 069 Batch: 00062/00094 | Loss: 121.4621 | CE: 0.0940 | KD: 1061.0590\n",
      "Train Epoch: 069 Batch: 00063/00094 | Loss: 121.3640 | CE: 0.0676 | KD: 1060.4325\n",
      "Train Epoch: 069 Batch: 00064/00094 | Loss: 121.4420 | CE: 0.0603 | KD: 1061.1774\n",
      "Train Epoch: 069 Batch: 00065/00094 | Loss: 121.4770 | CE: 0.0707 | KD: 1061.3920\n",
      "Train Epoch: 069 Batch: 00066/00094 | Loss: 121.4349 | CE: 0.0752 | KD: 1060.9856\n",
      "Train Epoch: 069 Batch: 00067/00094 | Loss: 121.4340 | CE: 0.0723 | KD: 1061.0021\n",
      "Train Epoch: 069 Batch: 00068/00094 | Loss: 121.4101 | CE: 0.0550 | KD: 1060.9449\n",
      "Train Epoch: 069 Batch: 00069/00094 | Loss: 121.4688 | CE: 0.1045 | KD: 1061.0251\n",
      "Train Epoch: 069 Batch: 00070/00094 | Loss: 121.4437 | CE: 0.0664 | KD: 1061.1389\n",
      "Train Epoch: 069 Batch: 00071/00094 | Loss: 121.4136 | CE: 0.0683 | KD: 1060.8599\n",
      "Train Epoch: 069 Batch: 00072/00094 | Loss: 121.4720 | CE: 0.0788 | KD: 1061.2788\n",
      "Train Epoch: 069 Batch: 00073/00094 | Loss: 121.4874 | CE: 0.0871 | KD: 1061.3398\n",
      "Train Epoch: 069 Batch: 00074/00094 | Loss: 121.4802 | CE: 0.0831 | KD: 1061.3123\n",
      "Train Epoch: 069 Batch: 00075/00094 | Loss: 121.4183 | CE: 0.0893 | KD: 1060.7173\n",
      "Train Epoch: 069 Batch: 00076/00094 | Loss: 121.3765 | CE: 0.0537 | KD: 1060.6633\n",
      "Train Epoch: 069 Batch: 00077/00094 | Loss: 121.4693 | CE: 0.0798 | KD: 1061.2457\n",
      "Train Epoch: 069 Batch: 00078/00094 | Loss: 121.4338 | CE: 0.0768 | KD: 1060.9619\n",
      "Train Epoch: 069 Batch: 00079/00094 | Loss: 121.4702 | CE: 0.1149 | KD: 1060.9467\n",
      "Train Epoch: 069 Batch: 00080/00094 | Loss: 121.3947 | CE: 0.0756 | KD: 1060.6301\n",
      "Train Epoch: 069 Batch: 00081/00094 | Loss: 121.4634 | CE: 0.0857 | KD: 1061.1421\n",
      "Train Epoch: 069 Batch: 00082/00094 | Loss: 121.5136 | CE: 0.1513 | KD: 1061.0081\n",
      "Train Epoch: 069 Batch: 00083/00094 | Loss: 121.5054 | CE: 0.1668 | KD: 1060.8004\n",
      "Train Epoch: 069 Batch: 00084/00094 | Loss: 121.4454 | CE: 0.0766 | KD: 1061.0654\n",
      "Train Epoch: 069 Batch: 00085/00094 | Loss: 121.3941 | CE: 0.0516 | KD: 1060.8353\n",
      "Train Epoch: 069 Batch: 00086/00094 | Loss: 121.5493 | CE: 0.1709 | KD: 1061.1493\n",
      "Train Epoch: 069 Batch: 00087/00094 | Loss: 121.4793 | CE: 0.0995 | KD: 1061.1604\n",
      "Train Epoch: 069 Batch: 00088/00094 | Loss: 121.4413 | CE: 0.0773 | KD: 1061.0229\n",
      "Train Epoch: 069 Batch: 00089/00094 | Loss: 121.4201 | CE: 0.0709 | KD: 1060.8934\n",
      "Train Epoch: 069 Batch: 00090/00094 | Loss: 121.4112 | CE: 0.0888 | KD: 1060.6592\n",
      "Train Epoch: 069 Batch: 00091/00094 | Loss: 121.4717 | CE: 0.0885 | KD: 1061.1902\n",
      "Train Epoch: 069 Batch: 00092/00094 | Loss: 121.4179 | CE: 0.0926 | KD: 1060.6840\n",
      "Train Epoch: 069 Batch: 00093/00094 | Loss: 121.4337 | CE: 0.0792 | KD: 1060.9396\n",
      "Train Epoch: 069 Batch: 00094/00094 | Loss: 121.5003 | CE: 0.1187 | KD: 1061.1772\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0769 | acc:98.9500\n",
      "[VAL Acc] Target: 98.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6799 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1988 | acc:52.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1631 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2464 | acc:56.3088\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7744 | acc:64.5102\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 64.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7289 | acc:70.3370\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9123 | acc:58.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.88%\n",
      "[VAL Acc] Avg 62.26%\n",
      "Train Epoch: 070 Batch: 00001/00094 | Loss: 121.4267 | CE: 0.0571 | KD: 1061.0710\n",
      "Train Epoch: 070 Batch: 00002/00094 | Loss: 121.4110 | CE: 0.0812 | KD: 1060.7240\n",
      "Train Epoch: 070 Batch: 00003/00094 | Loss: 121.4309 | CE: 0.0813 | KD: 1060.8976\n",
      "Train Epoch: 070 Batch: 00004/00094 | Loss: 121.4588 | CE: 0.0705 | KD: 1061.2350\n",
      "Train Epoch: 070 Batch: 00005/00094 | Loss: 121.4885 | CE: 0.0900 | KD: 1061.3241\n",
      "Train Epoch: 070 Batch: 00006/00094 | Loss: 121.4358 | CE: 0.0705 | KD: 1061.0338\n",
      "Train Epoch: 070 Batch: 00007/00094 | Loss: 121.4479 | CE: 0.0890 | KD: 1060.9786\n",
      "Train Epoch: 070 Batch: 00008/00094 | Loss: 121.4150 | CE: 0.0622 | KD: 1060.9246\n",
      "Train Epoch: 070 Batch: 00009/00094 | Loss: 121.4646 | CE: 0.1187 | KD: 1060.8645\n",
      "Train Epoch: 070 Batch: 00010/00094 | Loss: 121.4613 | CE: 0.0781 | KD: 1061.1907\n",
      "Train Epoch: 070 Batch: 00011/00094 | Loss: 121.4967 | CE: 0.0686 | KD: 1061.5826\n",
      "Train Epoch: 070 Batch: 00012/00094 | Loss: 121.4151 | CE: 0.0619 | KD: 1060.9287\n",
      "Train Epoch: 070 Batch: 00013/00094 | Loss: 121.4461 | CE: 0.0753 | KD: 1061.0823\n",
      "Train Epoch: 070 Batch: 00014/00094 | Loss: 121.4045 | CE: 0.0668 | KD: 1060.7922\n",
      "Train Epoch: 070 Batch: 00015/00094 | Loss: 121.4058 | CE: 0.0534 | KD: 1060.9215\n",
      "Train Epoch: 070 Batch: 00016/00094 | Loss: 121.4540 | CE: 0.0794 | KD: 1061.1151\n",
      "Train Epoch: 070 Batch: 00017/00094 | Loss: 121.7413 | CE: 0.3565 | KD: 1061.2046\n",
      "Train Epoch: 070 Batch: 00018/00094 | Loss: 121.4518 | CE: 0.0703 | KD: 1061.1759\n",
      "Train Epoch: 070 Batch: 00019/00094 | Loss: 121.4916 | CE: 0.1368 | KD: 1060.9424\n",
      "Train Epoch: 070 Batch: 00020/00094 | Loss: 121.4073 | CE: 0.0694 | KD: 1060.7942\n",
      "Train Epoch: 070 Batch: 00021/00094 | Loss: 121.4176 | CE: 0.0777 | KD: 1060.8119\n",
      "Train Epoch: 070 Batch: 00022/00094 | Loss: 121.4724 | CE: 0.0740 | KD: 1061.3231\n",
      "Train Epoch: 070 Batch: 00023/00094 | Loss: 121.4665 | CE: 0.0770 | KD: 1061.2457\n",
      "Train Epoch: 070 Batch: 00024/00094 | Loss: 121.4380 | CE: 0.1071 | KD: 1060.7341\n",
      "Train Epoch: 070 Batch: 00025/00094 | Loss: 121.4252 | CE: 0.0693 | KD: 1060.9520\n",
      "Train Epoch: 070 Batch: 00026/00094 | Loss: 121.4791 | CE: 0.1061 | KD: 1061.1017\n",
      "Train Epoch: 070 Batch: 00027/00094 | Loss: 121.3960 | CE: 0.0630 | KD: 1060.7522\n",
      "Train Epoch: 070 Batch: 00028/00094 | Loss: 121.5292 | CE: 0.1595 | KD: 1061.0726\n",
      "Train Epoch: 070 Batch: 00029/00094 | Loss: 121.4080 | CE: 0.0611 | KD: 1060.8735\n",
      "Train Epoch: 070 Batch: 00030/00094 | Loss: 121.4901 | CE: 0.0675 | KD: 1061.5355\n",
      "Train Epoch: 070 Batch: 00031/00094 | Loss: 121.4360 | CE: 0.0666 | KD: 1061.0702\n",
      "Train Epoch: 070 Batch: 00032/00094 | Loss: 121.5284 | CE: 0.1232 | KD: 1061.3835\n",
      "Train Epoch: 070 Batch: 00033/00094 | Loss: 121.3662 | CE: 0.0596 | KD: 1060.5208\n",
      "Train Epoch: 070 Batch: 00034/00094 | Loss: 121.3821 | CE: 0.0586 | KD: 1060.6687\n",
      "Train Epoch: 070 Batch: 00035/00094 | Loss: 121.4266 | CE: 0.0854 | KD: 1060.8230\n",
      "Train Epoch: 070 Batch: 00036/00094 | Loss: 121.3716 | CE: 0.0590 | KD: 1060.5732\n",
      "Train Epoch: 070 Batch: 00037/00094 | Loss: 121.5046 | CE: 0.1304 | KD: 1061.1116\n",
      "Train Epoch: 070 Batch: 00038/00094 | Loss: 121.5394 | CE: 0.1029 | KD: 1061.6571\n",
      "Train Epoch: 070 Batch: 00039/00094 | Loss: 121.4476 | CE: 0.0792 | KD: 1061.0612\n",
      "Train Epoch: 070 Batch: 00040/00094 | Loss: 121.4185 | CE: 0.0627 | KD: 1060.9507\n",
      "Train Epoch: 070 Batch: 00041/00094 | Loss: 121.4495 | CE: 0.0789 | KD: 1061.0800\n",
      "Train Epoch: 070 Batch: 00042/00094 | Loss: 121.5264 | CE: 0.0831 | KD: 1061.7159\n",
      "Train Epoch: 070 Batch: 00043/00094 | Loss: 121.4719 | CE: 0.0802 | KD: 1061.2648\n",
      "Train Epoch: 070 Batch: 00044/00094 | Loss: 121.4243 | CE: 0.0617 | KD: 1061.0106\n",
      "Train Epoch: 070 Batch: 00045/00094 | Loss: 121.4310 | CE: 0.0647 | KD: 1061.0431\n",
      "Train Epoch: 070 Batch: 00046/00094 | Loss: 121.4733 | CE: 0.0863 | KD: 1061.2239\n",
      "Train Epoch: 070 Batch: 00047/00094 | Loss: 121.3980 | CE: 0.0567 | KD: 1060.8248\n",
      "Train Epoch: 070 Batch: 00048/00094 | Loss: 121.4261 | CE: 0.0608 | KD: 1061.0343\n",
      "Train Epoch: 070 Batch: 00049/00094 | Loss: 121.4889 | CE: 0.0672 | KD: 1061.5278\n",
      "Train Epoch: 070 Batch: 00050/00094 | Loss: 121.4493 | CE: 0.0816 | KD: 1061.0557\n",
      "Train Epoch: 070 Batch: 00051/00094 | Loss: 121.5544 | CE: 0.1294 | KD: 1061.5559\n",
      "Train Epoch: 070 Batch: 00052/00094 | Loss: 121.4809 | CE: 0.0700 | KD: 1061.4333\n",
      "Train Epoch: 070 Batch: 00053/00094 | Loss: 121.4307 | CE: 0.0652 | KD: 1061.0367\n",
      "Train Epoch: 070 Batch: 00054/00094 | Loss: 121.4751 | CE: 0.0596 | KD: 1061.4729\n",
      "Train Epoch: 070 Batch: 00055/00094 | Loss: 121.4906 | CE: 0.1182 | KD: 1061.0964\n",
      "Train Epoch: 070 Batch: 00056/00094 | Loss: 121.4550 | CE: 0.0742 | KD: 1061.1697\n",
      "Train Epoch: 070 Batch: 00057/00094 | Loss: 121.5322 | CE: 0.1292 | KD: 1061.3635\n",
      "Train Epoch: 070 Batch: 00058/00094 | Loss: 121.3822 | CE: 0.0594 | KD: 1060.6630\n",
      "Train Epoch: 070 Batch: 00059/00094 | Loss: 121.4723 | CE: 0.0856 | KD: 1061.2218\n",
      "Train Epoch: 070 Batch: 00060/00094 | Loss: 121.4324 | CE: 0.0747 | KD: 1060.9683\n",
      "Train Epoch: 070 Batch: 00061/00094 | Loss: 121.4324 | CE: 0.0722 | KD: 1060.9900\n",
      "Train Epoch: 070 Batch: 00062/00094 | Loss: 121.3965 | CE: 0.0548 | KD: 1060.8286\n",
      "Train Epoch: 070 Batch: 00063/00094 | Loss: 121.4199 | CE: 0.0597 | KD: 1060.9900\n",
      "Train Epoch: 070 Batch: 00064/00094 | Loss: 121.3836 | CE: 0.0595 | KD: 1060.6741\n",
      "Train Epoch: 070 Batch: 00065/00094 | Loss: 121.4297 | CE: 0.0583 | KD: 1061.0875\n",
      "Train Epoch: 070 Batch: 00066/00094 | Loss: 121.4505 | CE: 0.0594 | KD: 1061.2593\n",
      "Train Epoch: 070 Batch: 00067/00094 | Loss: 121.4388 | CE: 0.0600 | KD: 1061.1528\n",
      "Train Epoch: 070 Batch: 00068/00094 | Loss: 121.4390 | CE: 0.0792 | KD: 1060.9856\n",
      "Train Epoch: 070 Batch: 00069/00094 | Loss: 121.4800 | CE: 0.0973 | KD: 1061.1864\n",
      "Train Epoch: 070 Batch: 00070/00094 | Loss: 121.4269 | CE: 0.0718 | KD: 1060.9451\n",
      "Train Epoch: 070 Batch: 00071/00094 | Loss: 121.4308 | CE: 0.1141 | KD: 1060.6094\n",
      "Train Epoch: 070 Batch: 00072/00094 | Loss: 121.4263 | CE: 0.0803 | KD: 1060.8650\n",
      "Train Epoch: 070 Batch: 00073/00094 | Loss: 121.4366 | CE: 0.1011 | KD: 1060.7743\n",
      "Train Epoch: 070 Batch: 00074/00094 | Loss: 121.5478 | CE: 0.1402 | KD: 1061.4041\n",
      "Train Epoch: 070 Batch: 00075/00094 | Loss: 121.4281 | CE: 0.0542 | KD: 1061.1093\n",
      "Train Epoch: 070 Batch: 00076/00094 | Loss: 121.4015 | CE: 0.0539 | KD: 1060.8794\n",
      "Train Epoch: 070 Batch: 00077/00094 | Loss: 121.4124 | CE: 0.0871 | KD: 1060.6851\n",
      "Train Epoch: 070 Batch: 00078/00094 | Loss: 121.4603 | CE: 0.0770 | KD: 1061.1915\n",
      "Train Epoch: 070 Batch: 00079/00094 | Loss: 121.3968 | CE: 0.0810 | KD: 1060.6016\n",
      "Train Epoch: 070 Batch: 00080/00094 | Loss: 121.4519 | CE: 0.0784 | KD: 1061.1061\n",
      "Train Epoch: 070 Batch: 00081/00094 | Loss: 121.4530 | CE: 0.0888 | KD: 1061.0249\n",
      "Train Epoch: 070 Batch: 00082/00094 | Loss: 121.4511 | CE: 0.0639 | KD: 1061.2263\n",
      "Train Epoch: 070 Batch: 00083/00094 | Loss: 121.4173 | CE: 0.0523 | KD: 1061.0314\n",
      "Train Epoch: 070 Batch: 00084/00094 | Loss: 121.4600 | CE: 0.0731 | KD: 1061.2231\n",
      "Train Epoch: 070 Batch: 00085/00094 | Loss: 121.4908 | CE: 0.1251 | KD: 1061.0378\n",
      "Train Epoch: 070 Batch: 00086/00094 | Loss: 121.4476 | CE: 0.0634 | KD: 1061.1995\n",
      "Train Epoch: 070 Batch: 00087/00094 | Loss: 121.4089 | CE: 0.0625 | KD: 1060.8689\n",
      "Train Epoch: 070 Batch: 00088/00094 | Loss: 121.3868 | CE: 0.0532 | KD: 1060.7572\n",
      "Train Epoch: 070 Batch: 00089/00094 | Loss: 121.4821 | CE: 0.1318 | KD: 1060.9030\n",
      "Train Epoch: 070 Batch: 00090/00094 | Loss: 121.4536 | CE: 0.0729 | KD: 1061.1683\n",
      "Train Epoch: 070 Batch: 00091/00094 | Loss: 121.4265 | CE: 0.0708 | KD: 1060.9508\n",
      "Train Epoch: 070 Batch: 00092/00094 | Loss: 121.4675 | CE: 0.0581 | KD: 1061.4197\n",
      "Train Epoch: 070 Batch: 00093/00094 | Loss: 121.4407 | CE: 0.0813 | KD: 1060.9827\n",
      "Train Epoch: 070 Batch: 00094/00094 | Loss: 121.3761 | CE: 0.0664 | KD: 1060.5483\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0778 | acc:98.7500\n",
      "[VAL Acc] Target: 98.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7201 | acc:49.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1837 | acc:53.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1225 | acc:46.7557\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2590 | acc:55.7994\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8324 | acc:64.1405\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 64.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7132 | acc:72.1787\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8888 | acc:59.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.44%\n",
      "[VAL Acc] Avg 62.43%\n",
      "Train Epoch: 071 Batch: 00001/00094 | Loss: 121.4314 | CE: 0.0788 | KD: 1060.9230\n",
      "Train Epoch: 071 Batch: 00002/00094 | Loss: 121.4600 | CE: 0.0871 | KD: 1061.1001\n",
      "Train Epoch: 071 Batch: 00003/00094 | Loss: 121.3952 | CE: 0.0946 | KD: 1060.4683\n",
      "Train Epoch: 071 Batch: 00004/00094 | Loss: 121.4332 | CE: 0.0915 | KD: 1060.8281\n",
      "Train Epoch: 071 Batch: 00005/00094 | Loss: 121.4127 | CE: 0.0576 | KD: 1060.9448\n",
      "Train Epoch: 071 Batch: 00006/00094 | Loss: 121.4854 | CE: 0.1172 | KD: 1061.0592\n",
      "Train Epoch: 071 Batch: 00007/00094 | Loss: 121.4612 | CE: 0.1138 | KD: 1060.8777\n",
      "Train Epoch: 071 Batch: 00008/00094 | Loss: 121.4388 | CE: 0.0742 | KD: 1061.0284\n",
      "Train Epoch: 071 Batch: 00009/00094 | Loss: 121.4603 | CE: 0.1010 | KD: 1060.9821\n",
      "Train Epoch: 071 Batch: 00010/00094 | Loss: 121.5032 | CE: 0.0750 | KD: 1061.5836\n",
      "Train Epoch: 071 Batch: 00011/00094 | Loss: 121.4550 | CE: 0.1134 | KD: 1060.8273\n",
      "Train Epoch: 071 Batch: 00012/00094 | Loss: 121.4674 | CE: 0.0788 | KD: 1061.2382\n",
      "Train Epoch: 071 Batch: 00013/00094 | Loss: 121.4957 | CE: 0.0963 | KD: 1061.3322\n",
      "Train Epoch: 071 Batch: 00014/00094 | Loss: 121.3990 | CE: 0.0580 | KD: 1060.8219\n",
      "Train Epoch: 071 Batch: 00015/00094 | Loss: 121.3785 | CE: 0.0656 | KD: 1060.5764\n",
      "Train Epoch: 071 Batch: 00016/00094 | Loss: 121.4670 | CE: 0.0910 | KD: 1061.1281\n",
      "Train Epoch: 071 Batch: 00017/00094 | Loss: 121.5271 | CE: 0.1449 | KD: 1061.1820\n",
      "Train Epoch: 071 Batch: 00018/00094 | Loss: 121.4715 | CE: 0.0645 | KD: 1061.3984\n",
      "Train Epoch: 071 Batch: 00019/00094 | Loss: 121.4188 | CE: 0.1040 | KD: 1060.5923\n",
      "Train Epoch: 071 Batch: 00020/00094 | Loss: 121.5312 | CE: 0.1470 | KD: 1061.1995\n",
      "Train Epoch: 071 Batch: 00021/00094 | Loss: 121.4273 | CE: 0.0695 | KD: 1060.9681\n",
      "Train Epoch: 071 Batch: 00022/00094 | Loss: 121.4805 | CE: 0.1083 | KD: 1061.0938\n",
      "Train Epoch: 071 Batch: 00023/00094 | Loss: 121.4489 | CE: 0.1171 | KD: 1060.7415\n",
      "Train Epoch: 071 Batch: 00024/00094 | Loss: 121.4321 | CE: 0.0855 | KD: 1060.8711\n",
      "Train Epoch: 071 Batch: 00025/00094 | Loss: 121.4017 | CE: 0.0649 | KD: 1060.7843\n",
      "Train Epoch: 071 Batch: 00026/00094 | Loss: 121.4332 | CE: 0.0734 | KD: 1060.9868\n",
      "Train Epoch: 071 Batch: 00027/00094 | Loss: 121.4442 | CE: 0.1032 | KD: 1060.8220\n",
      "Train Epoch: 071 Batch: 00028/00094 | Loss: 121.3980 | CE: 0.0594 | KD: 1060.8008\n",
      "Train Epoch: 071 Batch: 00029/00094 | Loss: 121.4737 | CE: 0.1392 | KD: 1060.7651\n",
      "Train Epoch: 071 Batch: 00030/00094 | Loss: 121.5174 | CE: 0.1320 | KD: 1061.2104\n",
      "Train Epoch: 071 Batch: 00031/00094 | Loss: 121.4544 | CE: 0.0916 | KD: 1061.0116\n",
      "Train Epoch: 071 Batch: 00032/00094 | Loss: 121.4277 | CE: 0.0702 | KD: 1060.9658\n",
      "Train Epoch: 071 Batch: 00033/00094 | Loss: 121.4031 | CE: 0.0704 | KD: 1060.7495\n",
      "Train Epoch: 071 Batch: 00034/00094 | Loss: 121.4376 | CE: 0.0650 | KD: 1061.0978\n",
      "Train Epoch: 071 Batch: 00035/00094 | Loss: 121.5064 | CE: 0.1220 | KD: 1061.2009\n",
      "Train Epoch: 071 Batch: 00036/00094 | Loss: 121.4715 | CE: 0.0863 | KD: 1061.2083\n",
      "Train Epoch: 071 Batch: 00037/00094 | Loss: 121.4508 | CE: 0.0731 | KD: 1061.1426\n",
      "Train Epoch: 071 Batch: 00038/00094 | Loss: 121.4477 | CE: 0.0810 | KD: 1061.0466\n",
      "Train Epoch: 071 Batch: 00039/00094 | Loss: 121.4807 | CE: 0.0817 | KD: 1061.3285\n",
      "Train Epoch: 071 Batch: 00040/00094 | Loss: 121.4433 | CE: 0.0656 | KD: 1061.1425\n",
      "Train Epoch: 071 Batch: 00041/00094 | Loss: 121.4137 | CE: 0.0654 | KD: 1060.8853\n",
      "Train Epoch: 071 Batch: 00042/00094 | Loss: 121.4096 | CE: 0.0563 | KD: 1060.9294\n",
      "Train Epoch: 071 Batch: 00043/00094 | Loss: 121.5196 | CE: 0.1286 | KD: 1061.2589\n",
      "Train Epoch: 071 Batch: 00044/00094 | Loss: 121.4432 | CE: 0.0805 | KD: 1061.0115\n",
      "Train Epoch: 071 Batch: 00045/00094 | Loss: 121.4577 | CE: 0.1077 | KD: 1060.9003\n",
      "Train Epoch: 071 Batch: 00046/00094 | Loss: 121.4872 | CE: 0.0812 | KD: 1061.3893\n",
      "Train Epoch: 071 Batch: 00047/00094 | Loss: 121.4402 | CE: 0.0585 | KD: 1061.1772\n",
      "Train Epoch: 071 Batch: 00048/00094 | Loss: 121.4400 | CE: 0.0817 | KD: 1060.9730\n",
      "Train Epoch: 071 Batch: 00049/00094 | Loss: 121.4877 | CE: 0.1023 | KD: 1061.2098\n",
      "Train Epoch: 071 Batch: 00050/00094 | Loss: 121.4537 | CE: 0.0994 | KD: 1060.9374\n",
      "Train Epoch: 071 Batch: 00051/00094 | Loss: 121.4151 | CE: 0.0795 | KD: 1060.7748\n",
      "Train Epoch: 071 Batch: 00052/00094 | Loss: 121.4574 | CE: 0.0850 | KD: 1061.0964\n",
      "Train Epoch: 071 Batch: 00053/00094 | Loss: 121.4364 | CE: 0.0531 | KD: 1061.1918\n",
      "Train Epoch: 071 Batch: 00054/00094 | Loss: 121.4341 | CE: 0.0734 | KD: 1060.9932\n",
      "Train Epoch: 071 Batch: 00055/00094 | Loss: 121.4584 | CE: 0.1021 | KD: 1060.9557\n",
      "Train Epoch: 071 Batch: 00056/00094 | Loss: 121.4482 | CE: 0.0731 | KD: 1061.1199\n",
      "Train Epoch: 071 Batch: 00057/00094 | Loss: 121.5081 | CE: 0.1134 | KD: 1061.2916\n",
      "Train Epoch: 071 Batch: 00058/00094 | Loss: 121.5125 | CE: 0.1110 | KD: 1061.3507\n",
      "Train Epoch: 071 Batch: 00059/00094 | Loss: 121.4512 | CE: 0.0897 | KD: 1061.0009\n",
      "Train Epoch: 071 Batch: 00060/00094 | Loss: 121.4355 | CE: 0.0498 | KD: 1061.2126\n",
      "Train Epoch: 071 Batch: 00061/00094 | Loss: 121.4260 | CE: 0.0539 | KD: 1061.0944\n",
      "Train Epoch: 071 Batch: 00062/00094 | Loss: 121.4100 | CE: 0.0596 | KD: 1060.9041\n",
      "Train Epoch: 071 Batch: 00063/00094 | Loss: 121.4063 | CE: 0.0721 | KD: 1060.7623\n",
      "Train Epoch: 071 Batch: 00064/00094 | Loss: 121.3731 | CE: 0.0556 | KD: 1060.6162\n",
      "Train Epoch: 071 Batch: 00065/00094 | Loss: 121.5086 | CE: 0.1482 | KD: 1060.9908\n",
      "Train Epoch: 071 Batch: 00066/00094 | Loss: 121.5268 | CE: 0.1471 | KD: 1061.1595\n",
      "Train Epoch: 071 Batch: 00067/00094 | Loss: 121.4674 | CE: 0.0938 | KD: 1061.1072\n",
      "Train Epoch: 071 Batch: 00068/00094 | Loss: 121.4879 | CE: 0.0755 | KD: 1061.4460\n",
      "Train Epoch: 071 Batch: 00069/00094 | Loss: 121.4203 | CE: 0.0812 | KD: 1060.8057\n",
      "Train Epoch: 071 Batch: 00070/00094 | Loss: 121.4689 | CE: 0.0964 | KD: 1061.0978\n",
      "Train Epoch: 071 Batch: 00071/00094 | Loss: 121.4472 | CE: 0.0728 | KD: 1061.1133\n",
      "Train Epoch: 071 Batch: 00072/00094 | Loss: 121.4307 | CE: 0.0699 | KD: 1060.9949\n",
      "Train Epoch: 071 Batch: 00073/00094 | Loss: 121.4355 | CE: 0.1067 | KD: 1060.7153\n",
      "Train Epoch: 071 Batch: 00074/00094 | Loss: 121.4661 | CE: 0.0723 | KD: 1061.2827\n",
      "Train Epoch: 071 Batch: 00075/00094 | Loss: 121.4586 | CE: 0.1069 | KD: 1060.9150\n",
      "Train Epoch: 071 Batch: 00076/00094 | Loss: 121.4397 | CE: 0.0656 | KD: 1061.1108\n",
      "Train Epoch: 071 Batch: 00077/00094 | Loss: 121.4102 | CE: 0.0825 | KD: 1060.7056\n",
      "Train Epoch: 071 Batch: 00078/00094 | Loss: 121.4284 | CE: 0.0787 | KD: 1060.8978\n",
      "Train Epoch: 071 Batch: 00079/00094 | Loss: 121.4031 | CE: 0.0560 | KD: 1060.8752\n",
      "Train Epoch: 071 Batch: 00080/00094 | Loss: 121.4485 | CE: 0.0730 | KD: 1061.1234\n",
      "Train Epoch: 071 Batch: 00081/00094 | Loss: 121.4223 | CE: 0.0571 | KD: 1061.0339\n",
      "Train Epoch: 071 Batch: 00082/00094 | Loss: 121.4143 | CE: 0.0629 | KD: 1060.9128\n",
      "Train Epoch: 071 Batch: 00083/00094 | Loss: 121.4186 | CE: 0.0604 | KD: 1060.9725\n",
      "Train Epoch: 071 Batch: 00084/00094 | Loss: 121.3988 | CE: 0.0670 | KD: 1060.7411\n",
      "Train Epoch: 071 Batch: 00085/00094 | Loss: 121.4472 | CE: 0.0920 | KD: 1060.9454\n",
      "Train Epoch: 071 Batch: 00086/00094 | Loss: 121.4827 | CE: 0.0835 | KD: 1061.3306\n",
      "Train Epoch: 071 Batch: 00087/00094 | Loss: 121.4285 | CE: 0.0963 | KD: 1060.7454\n",
      "Train Epoch: 071 Batch: 00088/00094 | Loss: 121.4026 | CE: 0.0707 | KD: 1060.7423\n",
      "Train Epoch: 071 Batch: 00089/00094 | Loss: 121.3664 | CE: 0.0547 | KD: 1060.5651\n",
      "Train Epoch: 071 Batch: 00090/00094 | Loss: 121.4194 | CE: 0.0977 | KD: 1060.6531\n",
      "Train Epoch: 071 Batch: 00091/00094 | Loss: 121.4940 | CE: 0.0887 | KD: 1061.3842\n",
      "Train Epoch: 071 Batch: 00092/00094 | Loss: 121.4468 | CE: 0.0936 | KD: 1060.9285\n",
      "Train Epoch: 071 Batch: 00093/00094 | Loss: 121.4459 | CE: 0.0955 | KD: 1060.9039\n",
      "Train Epoch: 071 Batch: 00094/00094 | Loss: 121.4447 | CE: 0.0695 | KD: 1061.1202\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0755 | acc:98.9500\n",
      "[VAL Acc] Target: 98.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7508 | acc:49.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2295 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2245 | acc:43.7023\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3249 | acc:55.4467\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7623 | acc:64.8799\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 64.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7835 | acc:69.2790\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8879 | acc:59.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.31%\n",
      "[VAL Acc] Avg 61.61%\n",
      "Train Epoch: 072 Batch: 00001/00094 | Loss: 121.4352 | CE: 0.0600 | KD: 1061.1213\n",
      "Train Epoch: 072 Batch: 00002/00094 | Loss: 121.4923 | CE: 0.0875 | KD: 1061.3792\n",
      "Train Epoch: 072 Batch: 00003/00094 | Loss: 121.4459 | CE: 0.0946 | KD: 1060.9122\n",
      "Train Epoch: 072 Batch: 00004/00094 | Loss: 121.4233 | CE: 0.0680 | KD: 1060.9475\n",
      "Train Epoch: 072 Batch: 00005/00094 | Loss: 121.3895 | CE: 0.0618 | KD: 1060.7054\n",
      "Train Epoch: 072 Batch: 00006/00094 | Loss: 121.4856 | CE: 0.1088 | KD: 1061.1348\n",
      "Train Epoch: 072 Batch: 00007/00094 | Loss: 121.4975 | CE: 0.1126 | KD: 1061.2057\n",
      "Train Epoch: 072 Batch: 00008/00094 | Loss: 121.3867 | CE: 0.0678 | KD: 1060.6292\n",
      "Train Epoch: 072 Batch: 00009/00094 | Loss: 121.4736 | CE: 0.0520 | KD: 1061.5260\n",
      "Train Epoch: 072 Batch: 00010/00094 | Loss: 121.3863 | CE: 0.0519 | KD: 1060.7645\n",
      "Train Epoch: 072 Batch: 00011/00094 | Loss: 121.4183 | CE: 0.0656 | KD: 1060.9237\n",
      "Train Epoch: 072 Batch: 00012/00094 | Loss: 121.4145 | CE: 0.0584 | KD: 1060.9535\n",
      "Train Epoch: 072 Batch: 00013/00094 | Loss: 121.4129 | CE: 0.0659 | KD: 1060.8748\n",
      "Train Epoch: 072 Batch: 00014/00094 | Loss: 121.4568 | CE: 0.0690 | KD: 1061.2308\n",
      "Train Epoch: 072 Batch: 00015/00094 | Loss: 121.4775 | CE: 0.0980 | KD: 1061.1581\n",
      "Train Epoch: 072 Batch: 00016/00094 | Loss: 121.4622 | CE: 0.1096 | KD: 1060.9226\n",
      "Train Epoch: 072 Batch: 00017/00094 | Loss: 121.4338 | CE: 0.0804 | KD: 1060.9297\n",
      "Train Epoch: 072 Batch: 00018/00094 | Loss: 121.4718 | CE: 0.0772 | KD: 1061.2897\n",
      "Train Epoch: 072 Batch: 00019/00094 | Loss: 121.4857 | CE: 0.1063 | KD: 1061.1573\n",
      "Train Epoch: 072 Batch: 00020/00094 | Loss: 121.4335 | CE: 0.0802 | KD: 1060.9293\n",
      "Train Epoch: 072 Batch: 00021/00094 | Loss: 121.5388 | CE: 0.1520 | KD: 1061.2229\n",
      "Train Epoch: 072 Batch: 00022/00094 | Loss: 121.4393 | CE: 0.0831 | KD: 1060.9543\n",
      "Train Epoch: 072 Batch: 00023/00094 | Loss: 121.4039 | CE: 0.0829 | KD: 1060.6472\n",
      "Train Epoch: 072 Batch: 00024/00094 | Loss: 121.5108 | CE: 0.1488 | KD: 1061.0055\n",
      "Train Epoch: 072 Batch: 00025/00094 | Loss: 121.4355 | CE: 0.0680 | KD: 1061.0532\n",
      "Train Epoch: 072 Batch: 00026/00094 | Loss: 121.3771 | CE: 0.0631 | KD: 1060.5859\n",
      "Train Epoch: 072 Batch: 00027/00094 | Loss: 121.5177 | CE: 0.1470 | KD: 1061.0812\n",
      "Train Epoch: 072 Batch: 00028/00094 | Loss: 121.4227 | CE: 0.0757 | KD: 1060.8749\n",
      "Train Epoch: 072 Batch: 00029/00094 | Loss: 121.4257 | CE: 0.0552 | KD: 1061.0798\n",
      "Train Epoch: 072 Batch: 00030/00094 | Loss: 121.4324 | CE: 0.0811 | KD: 1060.9115\n",
      "Train Epoch: 072 Batch: 00031/00094 | Loss: 121.4283 | CE: 0.0732 | KD: 1060.9458\n",
      "Train Epoch: 072 Batch: 00032/00094 | Loss: 121.4502 | CE: 0.0679 | KD: 1061.1829\n",
      "Train Epoch: 072 Batch: 00033/00094 | Loss: 121.3902 | CE: 0.0705 | KD: 1060.6361\n",
      "Train Epoch: 072 Batch: 00034/00094 | Loss: 121.4204 | CE: 0.0645 | KD: 1060.9515\n",
      "Train Epoch: 072 Batch: 00035/00094 | Loss: 121.4279 | CE: 0.0780 | KD: 1060.9003\n",
      "Train Epoch: 072 Batch: 00036/00094 | Loss: 121.4255 | CE: 0.0550 | KD: 1061.0797\n",
      "Train Epoch: 072 Batch: 00037/00094 | Loss: 121.4258 | CE: 0.0633 | KD: 1061.0092\n",
      "Train Epoch: 072 Batch: 00038/00094 | Loss: 121.4038 | CE: 0.0715 | KD: 1060.7460\n",
      "Train Epoch: 072 Batch: 00039/00094 | Loss: 121.4178 | CE: 0.0697 | KD: 1060.8835\n",
      "Train Epoch: 072 Batch: 00040/00094 | Loss: 121.4608 | CE: 0.1094 | KD: 1060.9126\n",
      "Train Epoch: 072 Batch: 00041/00094 | Loss: 121.4708 | CE: 0.0970 | KD: 1061.1085\n",
      "Train Epoch: 072 Batch: 00042/00094 | Loss: 121.4383 | CE: 0.0856 | KD: 1060.9241\n",
      "Train Epoch: 072 Batch: 00043/00094 | Loss: 121.4137 | CE: 0.0709 | KD: 1060.8368\n",
      "Train Epoch: 072 Batch: 00044/00094 | Loss: 121.4317 | CE: 0.0740 | KD: 1060.9677\n",
      "Train Epoch: 072 Batch: 00045/00094 | Loss: 121.4016 | CE: 0.0544 | KD: 1060.8762\n",
      "Train Epoch: 072 Batch: 00046/00094 | Loss: 121.4902 | CE: 0.1039 | KD: 1061.2175\n",
      "Train Epoch: 072 Batch: 00047/00094 | Loss: 121.4084 | CE: 0.0693 | KD: 1060.8049\n",
      "Train Epoch: 072 Batch: 00048/00094 | Loss: 121.4438 | CE: 0.0668 | KD: 1061.1362\n",
      "Train Epoch: 072 Batch: 00049/00094 | Loss: 121.5038 | CE: 0.1077 | KD: 1061.3032\n",
      "Train Epoch: 072 Batch: 00050/00094 | Loss: 121.4472 | CE: 0.0910 | KD: 1060.9548\n",
      "Train Epoch: 072 Batch: 00051/00094 | Loss: 121.4337 | CE: 0.0618 | KD: 1061.0924\n",
      "Train Epoch: 072 Batch: 00052/00094 | Loss: 121.4291 | CE: 0.1061 | KD: 1060.6647\n",
      "Train Epoch: 072 Batch: 00053/00094 | Loss: 121.3951 | CE: 0.0686 | KD: 1060.6954\n",
      "Train Epoch: 072 Batch: 00054/00094 | Loss: 121.4619 | CE: 0.0735 | KD: 1061.2363\n",
      "Train Epoch: 072 Batch: 00055/00094 | Loss: 121.4904 | CE: 0.1258 | KD: 1061.0284\n",
      "Train Epoch: 072 Batch: 00056/00094 | Loss: 121.4180 | CE: 0.0835 | KD: 1060.7653\n",
      "Train Epoch: 072 Batch: 00057/00094 | Loss: 121.3978 | CE: 0.0595 | KD: 1060.7975\n",
      "Train Epoch: 072 Batch: 00058/00094 | Loss: 121.4369 | CE: 0.0713 | KD: 1061.0361\n",
      "Train Epoch: 072 Batch: 00059/00094 | Loss: 121.4267 | CE: 0.0840 | KD: 1060.8370\n",
      "Train Epoch: 072 Batch: 00060/00094 | Loss: 121.4073 | CE: 0.0607 | KD: 1060.8713\n",
      "Train Epoch: 072 Batch: 00061/00094 | Loss: 121.5115 | CE: 0.1273 | KD: 1061.1993\n",
      "Train Epoch: 072 Batch: 00062/00094 | Loss: 121.4110 | CE: 0.0571 | KD: 1060.9348\n",
      "Train Epoch: 072 Batch: 00063/00094 | Loss: 121.4359 | CE: 0.0642 | KD: 1061.0903\n",
      "Train Epoch: 072 Batch: 00064/00094 | Loss: 121.5043 | CE: 0.0912 | KD: 1061.4525\n",
      "Train Epoch: 072 Batch: 00065/00094 | Loss: 121.4531 | CE: 0.0705 | KD: 1061.1859\n",
      "Train Epoch: 072 Batch: 00066/00094 | Loss: 121.4197 | CE: 0.0733 | KD: 1060.8689\n",
      "Train Epoch: 072 Batch: 00067/00094 | Loss: 121.4051 | CE: 0.0670 | KD: 1060.7963\n",
      "Train Epoch: 072 Batch: 00068/00094 | Loss: 121.4290 | CE: 0.0787 | KD: 1060.9036\n",
      "Train Epoch: 072 Batch: 00069/00094 | Loss: 121.4149 | CE: 0.0701 | KD: 1060.8553\n",
      "Train Epoch: 072 Batch: 00070/00094 | Loss: 121.4198 | CE: 0.0608 | KD: 1060.9799\n",
      "Train Epoch: 072 Batch: 00071/00094 | Loss: 121.4373 | CE: 0.0835 | KD: 1060.9341\n",
      "Train Epoch: 072 Batch: 00072/00094 | Loss: 121.4186 | CE: 0.0494 | KD: 1061.0686\n",
      "Train Epoch: 072 Batch: 00073/00094 | Loss: 121.4087 | CE: 0.0639 | KD: 1060.8552\n",
      "Train Epoch: 072 Batch: 00074/00094 | Loss: 121.4797 | CE: 0.0682 | KD: 1061.4384\n",
      "Train Epoch: 072 Batch: 00075/00094 | Loss: 121.4616 | CE: 0.0838 | KD: 1061.1432\n",
      "Train Epoch: 072 Batch: 00076/00094 | Loss: 121.4894 | CE: 0.0843 | KD: 1061.3816\n",
      "Train Epoch: 072 Batch: 00077/00094 | Loss: 121.4833 | CE: 0.1355 | KD: 1060.8811\n",
      "Train Epoch: 072 Batch: 00078/00094 | Loss: 121.4728 | CE: 0.0854 | KD: 1061.2273\n",
      "Train Epoch: 072 Batch: 00079/00094 | Loss: 121.4097 | CE: 0.0510 | KD: 1060.9769\n",
      "Train Epoch: 072 Batch: 00080/00094 | Loss: 121.4998 | CE: 0.0624 | KD: 1061.6648\n",
      "Train Epoch: 072 Batch: 00081/00094 | Loss: 121.4376 | CE: 0.0739 | KD: 1061.0199\n",
      "Train Epoch: 072 Batch: 00082/00094 | Loss: 121.3831 | CE: 0.0480 | KD: 1060.7708\n",
      "Train Epoch: 072 Batch: 00083/00094 | Loss: 121.4315 | CE: 0.0789 | KD: 1060.9226\n",
      "Train Epoch: 072 Batch: 00084/00094 | Loss: 121.5131 | CE: 0.1087 | KD: 1061.3759\n",
      "Train Epoch: 072 Batch: 00085/00094 | Loss: 121.5115 | CE: 0.0900 | KD: 1061.5260\n",
      "Train Epoch: 072 Batch: 00086/00094 | Loss: 121.4354 | CE: 0.1080 | KD: 1060.7030\n",
      "Train Epoch: 072 Batch: 00087/00094 | Loss: 121.4282 | CE: 0.0742 | KD: 1060.9354\n",
      "Train Epoch: 072 Batch: 00088/00094 | Loss: 121.6468 | CE: 0.1780 | KD: 1061.9390\n",
      "Train Epoch: 072 Batch: 00089/00094 | Loss: 121.4349 | CE: 0.0815 | KD: 1060.9301\n",
      "Train Epoch: 072 Batch: 00090/00094 | Loss: 121.4194 | CE: 0.0559 | KD: 1061.0182\n",
      "Train Epoch: 072 Batch: 00091/00094 | Loss: 121.4338 | CE: 0.0705 | KD: 1061.0165\n",
      "Train Epoch: 072 Batch: 00092/00094 | Loss: 121.4595 | CE: 0.0669 | KD: 1061.2737\n",
      "Train Epoch: 072 Batch: 00093/00094 | Loss: 121.4659 | CE: 0.0671 | KD: 1061.3278\n",
      "Train Epoch: 072 Batch: 00094/00094 | Loss: 121.4878 | CE: 0.0678 | KD: 1061.5127\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0772 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6887 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1380 | acc:54.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1736 | acc:44.8473\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3397 | acc:53.6834\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7401 | acc:66.5434\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7855 | acc:71.0815\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8837 | acc:60.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.44%\n",
      "[VAL Acc] Avg 62.40%\n",
      "Train Epoch: 073 Batch: 00001/00094 | Loss: 109.3529 | CE: 0.0837 | KD: 1061.4269\n",
      "Train Epoch: 073 Batch: 00002/00094 | Loss: 109.3372 | CE: 0.1225 | KD: 1060.8977\n",
      "Train Epoch: 073 Batch: 00003/00094 | Loss: 109.2961 | CE: 0.0602 | KD: 1061.1031\n",
      "Train Epoch: 073 Batch: 00004/00094 | Loss: 109.3769 | CE: 0.1396 | KD: 1061.1169\n",
      "Train Epoch: 073 Batch: 00005/00094 | Loss: 109.3660 | CE: 0.0726 | KD: 1061.6627\n",
      "Train Epoch: 073 Batch: 00006/00094 | Loss: 109.2827 | CE: 0.0704 | KD: 1060.8748\n",
      "Train Epoch: 073 Batch: 00007/00094 | Loss: 109.3008 | CE: 0.0868 | KD: 1060.8907\n",
      "Train Epoch: 073 Batch: 00008/00094 | Loss: 109.2873 | CE: 0.0684 | KD: 1060.9384\n",
      "Train Epoch: 073 Batch: 00009/00094 | Loss: 109.2981 | CE: 0.0535 | KD: 1061.1876\n",
      "Train Epoch: 073 Batch: 00010/00094 | Loss: 109.3376 | CE: 0.0943 | KD: 1061.1755\n",
      "Train Epoch: 073 Batch: 00011/00094 | Loss: 109.3478 | CE: 0.1247 | KD: 1060.9791\n",
      "Train Epoch: 073 Batch: 00012/00094 | Loss: 109.2855 | CE: 0.0618 | KD: 1060.9852\n",
      "Train Epoch: 073 Batch: 00013/00094 | Loss: 109.2943 | CE: 0.0777 | KD: 1060.9161\n",
      "Train Epoch: 073 Batch: 00014/00094 | Loss: 109.2563 | CE: 0.0596 | KD: 1060.7228\n",
      "Train Epoch: 073 Batch: 00015/00094 | Loss: 109.2891 | CE: 0.0623 | KD: 1061.0153\n",
      "Train Epoch: 073 Batch: 00016/00094 | Loss: 109.3046 | CE: 0.0737 | KD: 1061.0555\n",
      "Train Epoch: 073 Batch: 00017/00094 | Loss: 109.3347 | CE: 0.0663 | KD: 1061.4192\n",
      "Train Epoch: 073 Batch: 00018/00094 | Loss: 109.3565 | CE: 0.0871 | KD: 1061.4286\n",
      "Train Epoch: 073 Batch: 00019/00094 | Loss: 109.2980 | CE: 0.0706 | KD: 1061.0211\n",
      "Train Epoch: 073 Batch: 00020/00094 | Loss: 109.3117 | CE: 0.0890 | KD: 1060.9746\n",
      "Train Epoch: 073 Batch: 00021/00094 | Loss: 109.2962 | CE: 0.0657 | KD: 1061.0516\n",
      "Train Epoch: 073 Batch: 00022/00094 | Loss: 109.3447 | CE: 0.0637 | KD: 1061.5425\n",
      "Train Epoch: 073 Batch: 00023/00094 | Loss: 109.3205 | CE: 0.0851 | KD: 1061.0986\n",
      "Train Epoch: 073 Batch: 00024/00094 | Loss: 109.2840 | CE: 0.0714 | KD: 1060.8771\n",
      "Train Epoch: 073 Batch: 00025/00094 | Loss: 109.2742 | CE: 0.0665 | KD: 1060.8303\n",
      "Train Epoch: 073 Batch: 00026/00094 | Loss: 109.2868 | CE: 0.1019 | KD: 1060.6080\n",
      "Train Epoch: 073 Batch: 00027/00094 | Loss: 109.3649 | CE: 0.1061 | KD: 1061.3254\n",
      "Train Epoch: 073 Batch: 00028/00094 | Loss: 109.2949 | CE: 0.0610 | KD: 1061.0840\n",
      "Train Epoch: 073 Batch: 00029/00094 | Loss: 109.3313 | CE: 0.0772 | KD: 1061.2809\n",
      "Train Epoch: 073 Batch: 00030/00094 | Loss: 109.3199 | CE: 0.0960 | KD: 1060.9865\n",
      "Train Epoch: 073 Batch: 00031/00094 | Loss: 109.3312 | CE: 0.0795 | KD: 1061.2573\n",
      "Train Epoch: 073 Batch: 00032/00094 | Loss: 109.2906 | CE: 0.0602 | KD: 1061.0492\n",
      "Train Epoch: 073 Batch: 00033/00094 | Loss: 109.3170 | CE: 0.0649 | KD: 1061.2612\n",
      "Train Epoch: 073 Batch: 00034/00094 | Loss: 109.3338 | CE: 0.1011 | KD: 1061.0723\n",
      "Train Epoch: 073 Batch: 00035/00094 | Loss: 109.2920 | CE: 0.0570 | KD: 1061.0944\n",
      "Train Epoch: 073 Batch: 00036/00094 | Loss: 109.3087 | CE: 0.0773 | KD: 1061.0594\n",
      "Train Epoch: 073 Batch: 00037/00094 | Loss: 109.2823 | CE: 0.0555 | KD: 1061.0150\n",
      "Train Epoch: 073 Batch: 00038/00094 | Loss: 109.2563 | CE: 0.0620 | KD: 1060.7002\n",
      "Train Epoch: 073 Batch: 00039/00094 | Loss: 109.3209 | CE: 0.1134 | KD: 1060.8274\n",
      "Train Epoch: 073 Batch: 00040/00094 | Loss: 109.2967 | CE: 0.0827 | KD: 1060.8910\n",
      "Train Epoch: 073 Batch: 00041/00094 | Loss: 109.2397 | CE: 0.0544 | KD: 1060.6122\n",
      "Train Epoch: 073 Batch: 00042/00094 | Loss: 109.3089 | CE: 0.1026 | KD: 1060.8158\n",
      "Train Epoch: 073 Batch: 00043/00094 | Loss: 109.3362 | CE: 0.0749 | KD: 1061.3505\n",
      "Train Epoch: 073 Batch: 00044/00094 | Loss: 109.2861 | CE: 0.0700 | KD: 1060.9117\n",
      "Train Epoch: 073 Batch: 00045/00094 | Loss: 109.3393 | CE: 0.0834 | KD: 1061.2977\n",
      "Train Epoch: 073 Batch: 00046/00094 | Loss: 109.3366 | CE: 0.0973 | KD: 1061.1361\n",
      "Train Epoch: 073 Batch: 00047/00094 | Loss: 109.4098 | CE: 0.1403 | KD: 1061.4299\n",
      "Train Epoch: 073 Batch: 00048/00094 | Loss: 109.3288 | CE: 0.0910 | KD: 1061.1217\n",
      "Train Epoch: 073 Batch: 00049/00094 | Loss: 109.2892 | CE: 0.0747 | KD: 1060.8951\n",
      "Train Epoch: 073 Batch: 00050/00094 | Loss: 109.2685 | CE: 0.0895 | KD: 1060.5508\n",
      "Train Epoch: 073 Batch: 00051/00094 | Loss: 109.2929 | CE: 0.0869 | KD: 1060.8126\n",
      "Train Epoch: 073 Batch: 00052/00094 | Loss: 109.3732 | CE: 0.0830 | KD: 1061.6316\n",
      "Train Epoch: 073 Batch: 00053/00094 | Loss: 109.3563 | CE: 0.1160 | KD: 1061.1464\n",
      "Train Epoch: 073 Batch: 00054/00094 | Loss: 109.3434 | CE: 0.0848 | KD: 1061.3239\n",
      "Train Epoch: 073 Batch: 00055/00094 | Loss: 109.2844 | CE: 0.0570 | KD: 1061.0205\n",
      "Train Epoch: 073 Batch: 00056/00094 | Loss: 109.2816 | CE: 0.0555 | KD: 1061.0078\n",
      "Train Epoch: 073 Batch: 00057/00094 | Loss: 109.3354 | CE: 0.0777 | KD: 1061.3147\n",
      "Train Epoch: 073 Batch: 00058/00094 | Loss: 109.3602 | CE: 0.0847 | KD: 1061.4884\n",
      "Train Epoch: 073 Batch: 00059/00094 | Loss: 109.2949 | CE: 0.0644 | KD: 1061.0505\n",
      "Train Epoch: 073 Batch: 00060/00094 | Loss: 109.2671 | CE: 0.0781 | KD: 1060.6479\n",
      "Train Epoch: 073 Batch: 00061/00094 | Loss: 109.3383 | CE: 0.1318 | KD: 1060.8179\n",
      "Train Epoch: 073 Batch: 00062/00094 | Loss: 109.3352 | CE: 0.0824 | KD: 1061.2683\n",
      "Train Epoch: 073 Batch: 00063/00094 | Loss: 109.3059 | CE: 0.0861 | KD: 1060.9470\n",
      "Train Epoch: 073 Batch: 00064/00094 | Loss: 109.3455 | CE: 0.1022 | KD: 1061.1759\n",
      "Train Epoch: 073 Batch: 00065/00094 | Loss: 109.2869 | CE: 0.0663 | KD: 1060.9541\n",
      "Train Epoch: 073 Batch: 00066/00094 | Loss: 109.3844 | CE: 0.1438 | KD: 1061.1483\n",
      "Train Epoch: 073 Batch: 00067/00094 | Loss: 109.2637 | CE: 0.0533 | KD: 1060.8562\n",
      "Train Epoch: 073 Batch: 00068/00094 | Loss: 109.3644 | CE: 0.0919 | KD: 1061.4585\n",
      "Train Epoch: 073 Batch: 00069/00094 | Loss: 109.3237 | CE: 0.0916 | KD: 1061.0668\n",
      "Train Epoch: 073 Batch: 00070/00094 | Loss: 109.2728 | CE: 0.0796 | KD: 1060.6893\n",
      "Train Epoch: 073 Batch: 00071/00094 | Loss: 109.2667 | CE: 0.0659 | KD: 1060.7629\n",
      "Train Epoch: 073 Batch: 00072/00094 | Loss: 109.3251 | CE: 0.0717 | KD: 1061.2729\n",
      "Train Epoch: 073 Batch: 00073/00094 | Loss: 109.3305 | CE: 0.0900 | KD: 1061.1476\n",
      "Train Epoch: 073 Batch: 00074/00094 | Loss: 109.2808 | CE: 0.0582 | KD: 1060.9744\n",
      "Train Epoch: 073 Batch: 00075/00094 | Loss: 109.2953 | CE: 0.0702 | KD: 1060.9990\n",
      "Train Epoch: 073 Batch: 00076/00094 | Loss: 109.3021 | CE: 0.0970 | KD: 1060.8041\n",
      "Train Epoch: 073 Batch: 00077/00094 | Loss: 109.3573 | CE: 0.0936 | KD: 1061.3738\n",
      "Train Epoch: 073 Batch: 00078/00094 | Loss: 109.3100 | CE: 0.0647 | KD: 1061.1940\n",
      "Train Epoch: 073 Batch: 00079/00094 | Loss: 109.3362 | CE: 0.1024 | KD: 1061.0826\n",
      "Train Epoch: 073 Batch: 00080/00094 | Loss: 109.3458 | CE: 0.1182 | KD: 1061.0227\n",
      "Train Epoch: 073 Batch: 00081/00094 | Loss: 109.2674 | CE: 0.0700 | KD: 1060.7294\n",
      "Train Epoch: 073 Batch: 00082/00094 | Loss: 109.2942 | CE: 0.0767 | KD: 1060.9249\n",
      "Train Epoch: 073 Batch: 00083/00094 | Loss: 109.2364 | CE: 0.0496 | KD: 1060.6271\n",
      "Train Epoch: 073 Batch: 00084/00094 | Loss: 109.2975 | CE: 0.0554 | KD: 1061.1637\n",
      "Train Epoch: 073 Batch: 00085/00094 | Loss: 109.2935 | CE: 0.0720 | KD: 1060.9640\n",
      "Train Epoch: 073 Batch: 00086/00094 | Loss: 109.2859 | CE: 0.0714 | KD: 1060.8955\n",
      "Train Epoch: 073 Batch: 00087/00094 | Loss: 109.3333 | CE: 0.0713 | KD: 1061.3574\n",
      "Train Epoch: 073 Batch: 00088/00094 | Loss: 109.2824 | CE: 0.0703 | KD: 1060.8730\n",
      "Train Epoch: 073 Batch: 00089/00094 | Loss: 109.3576 | CE: 0.0994 | KD: 1061.3201\n",
      "Train Epoch: 073 Batch: 00090/00094 | Loss: 109.2806 | CE: 0.0703 | KD: 1060.8552\n",
      "Train Epoch: 073 Batch: 00091/00094 | Loss: 109.2793 | CE: 0.0634 | KD: 1060.9092\n",
      "Train Epoch: 073 Batch: 00092/00094 | Loss: 109.3408 | CE: 0.1140 | KD: 1061.0143\n",
      "Train Epoch: 073 Batch: 00093/00094 | Loss: 109.3191 | CE: 0.1121 | KD: 1060.8224\n",
      "Train Epoch: 073 Batch: 00094/00094 | Loss: 109.3190 | CE: 0.0686 | KD: 1061.2443\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0798 | acc:98.5500\n",
      "[VAL Acc] Target: 98.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7125 | acc:48.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1910 | acc:53.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1932 | acc:44.4656\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2645 | acc:56.7006\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7988 | acc:63.5860\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 63.59%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6968 | acc:72.3746\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8742 | acc:59.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.94%\n",
      "[VAL Acc] Avg 62.22%\n",
      "Train Epoch: 074 Batch: 00001/00094 | Loss: 109.3242 | CE: 0.0970 | KD: 1061.0189\n",
      "Train Epoch: 074 Batch: 00002/00094 | Loss: 109.3413 | CE: 0.0642 | KD: 1061.5038\n",
      "Train Epoch: 074 Batch: 00003/00094 | Loss: 109.2906 | CE: 0.0715 | KD: 1060.9402\n",
      "Train Epoch: 074 Batch: 00004/00094 | Loss: 109.3464 | CE: 0.0953 | KD: 1061.2512\n",
      "Train Epoch: 074 Batch: 00005/00094 | Loss: 109.3005 | CE: 0.0734 | KD: 1061.0176\n",
      "Train Epoch: 074 Batch: 00006/00094 | Loss: 109.3246 | CE: 0.0800 | KD: 1061.1875\n",
      "Train Epoch: 074 Batch: 00007/00094 | Loss: 109.2675 | CE: 0.0589 | KD: 1060.8386\n",
      "Train Epoch: 074 Batch: 00008/00094 | Loss: 109.3092 | CE: 0.0836 | KD: 1061.0039\n",
      "Train Epoch: 074 Batch: 00009/00094 | Loss: 109.3800 | CE: 0.1048 | KD: 1061.4855\n",
      "Train Epoch: 074 Batch: 00010/00094 | Loss: 109.3028 | CE: 0.0833 | KD: 1060.9442\n",
      "Train Epoch: 074 Batch: 00011/00094 | Loss: 109.3998 | CE: 0.1336 | KD: 1061.3978\n",
      "Train Epoch: 074 Batch: 00012/00094 | Loss: 109.3181 | CE: 0.0895 | KD: 1061.0323\n",
      "Train Epoch: 074 Batch: 00013/00094 | Loss: 109.2919 | CE: 0.0690 | KD: 1060.9768\n",
      "Train Epoch: 074 Batch: 00014/00094 | Loss: 109.2902 | CE: 0.0648 | KD: 1061.0018\n",
      "Train Epoch: 074 Batch: 00015/00094 | Loss: 109.3044 | CE: 0.0572 | KD: 1061.2134\n",
      "Train Epoch: 074 Batch: 00016/00094 | Loss: 109.3762 | CE: 0.0861 | KD: 1061.6305\n",
      "Train Epoch: 074 Batch: 00017/00094 | Loss: 109.2814 | CE: 0.0784 | KD: 1060.7839\n",
      "Train Epoch: 074 Batch: 00018/00094 | Loss: 109.3179 | CE: 0.0599 | KD: 1061.3187\n",
      "Train Epoch: 074 Batch: 00019/00094 | Loss: 109.2976 | CE: 0.0572 | KD: 1061.1477\n",
      "Train Epoch: 074 Batch: 00020/00094 | Loss: 109.3549 | CE: 0.1219 | KD: 1061.0753\n",
      "Train Epoch: 074 Batch: 00021/00094 | Loss: 109.3314 | CE: 0.0816 | KD: 1061.2379\n",
      "Train Epoch: 074 Batch: 00022/00094 | Loss: 109.2745 | CE: 0.0523 | KD: 1060.9705\n",
      "Train Epoch: 074 Batch: 00023/00094 | Loss: 109.2885 | CE: 0.0620 | KD: 1061.0122\n",
      "Train Epoch: 074 Batch: 00024/00094 | Loss: 109.3795 | CE: 0.0834 | KD: 1061.6886\n",
      "Train Epoch: 074 Batch: 00025/00094 | Loss: 109.3617 | CE: 0.1065 | KD: 1061.2915\n",
      "Train Epoch: 074 Batch: 00026/00094 | Loss: 109.3259 | CE: 0.0625 | KD: 1061.3701\n",
      "Train Epoch: 074 Batch: 00027/00094 | Loss: 109.3606 | CE: 0.0668 | KD: 1061.6666\n",
      "Train Epoch: 074 Batch: 00028/00094 | Loss: 109.3143 | CE: 0.1106 | KD: 1060.7909\n",
      "Train Epoch: 074 Batch: 00029/00094 | Loss: 109.3818 | CE: 0.1238 | KD: 1061.3180\n",
      "Train Epoch: 074 Batch: 00030/00094 | Loss: 109.3135 | CE: 0.0681 | KD: 1061.1952\n",
      "Train Epoch: 074 Batch: 00031/00094 | Loss: 109.3211 | CE: 0.0776 | KD: 1061.1780\n",
      "Train Epoch: 074 Batch: 00032/00094 | Loss: 109.2648 | CE: 0.0678 | KD: 1060.7258\n",
      "Train Epoch: 074 Batch: 00033/00094 | Loss: 109.3018 | CE: 0.0592 | KD: 1061.1685\n",
      "Train Epoch: 074 Batch: 00034/00094 | Loss: 109.3987 | CE: 0.1345 | KD: 1061.3776\n",
      "Train Epoch: 074 Batch: 00035/00094 | Loss: 109.3096 | CE: 0.0924 | KD: 1060.9222\n",
      "Train Epoch: 074 Batch: 00036/00094 | Loss: 109.3810 | CE: 0.1522 | KD: 1061.0345\n",
      "Train Epoch: 074 Batch: 00037/00094 | Loss: 109.2751 | CE: 0.0557 | KD: 1060.9434\n",
      "Train Epoch: 074 Batch: 00038/00094 | Loss: 109.2733 | CE: 0.0785 | KD: 1060.7046\n",
      "Train Epoch: 074 Batch: 00039/00094 | Loss: 109.3201 | CE: 0.0793 | KD: 1061.1509\n",
      "Train Epoch: 074 Batch: 00040/00094 | Loss: 109.2970 | CE: 0.0645 | KD: 1061.0703\n",
      "Train Epoch: 074 Batch: 00041/00094 | Loss: 109.3316 | CE: 0.0634 | KD: 1061.4172\n",
      "Train Epoch: 074 Batch: 00042/00094 | Loss: 109.3819 | CE: 0.1126 | KD: 1061.4279\n",
      "Train Epoch: 074 Batch: 00043/00094 | Loss: 109.3020 | CE: 0.0803 | KD: 1060.9650\n",
      "Train Epoch: 074 Batch: 00044/00094 | Loss: 109.3014 | CE: 0.0664 | KD: 1061.0947\n",
      "Train Epoch: 074 Batch: 00045/00094 | Loss: 109.2733 | CE: 0.0564 | KD: 1060.9188\n",
      "Train Epoch: 074 Batch: 00046/00094 | Loss: 109.3165 | CE: 0.0543 | KD: 1061.3588\n",
      "Train Epoch: 074 Batch: 00047/00094 | Loss: 109.3396 | CE: 0.1176 | KD: 1060.9690\n",
      "Train Epoch: 074 Batch: 00048/00094 | Loss: 109.4017 | CE: 0.1360 | KD: 1061.3929\n",
      "Train Epoch: 074 Batch: 00049/00094 | Loss: 109.2780 | CE: 0.0573 | KD: 1060.9559\n",
      "Train Epoch: 074 Batch: 00050/00094 | Loss: 109.3277 | CE: 0.0880 | KD: 1061.1400\n",
      "Train Epoch: 074 Batch: 00051/00094 | Loss: 109.2493 | CE: 0.0491 | KD: 1060.7565\n",
      "Train Epoch: 074 Batch: 00052/00094 | Loss: 109.3396 | CE: 0.0901 | KD: 1061.2355\n",
      "Train Epoch: 074 Batch: 00053/00094 | Loss: 109.3278 | CE: 0.0849 | KD: 1061.1719\n",
      "Train Epoch: 074 Batch: 00054/00094 | Loss: 109.3351 | CE: 0.1220 | KD: 1060.8821\n",
      "Train Epoch: 074 Batch: 00055/00094 | Loss: 109.3203 | CE: 0.0711 | KD: 1061.2321\n",
      "Train Epoch: 074 Batch: 00056/00094 | Loss: 109.2983 | CE: 0.0639 | KD: 1061.0889\n",
      "Train Epoch: 074 Batch: 00057/00094 | Loss: 109.2975 | CE: 0.0678 | KD: 1061.0432\n",
      "Train Epoch: 074 Batch: 00058/00094 | Loss: 109.3190 | CE: 0.0715 | KD: 1061.2167\n",
      "Train Epoch: 074 Batch: 00059/00094 | Loss: 109.2832 | CE: 0.0771 | KD: 1060.8134\n",
      "Train Epoch: 074 Batch: 00060/00094 | Loss: 109.3409 | CE: 0.0717 | KD: 1061.4265\n",
      "Train Epoch: 074 Batch: 00061/00094 | Loss: 109.3445 | CE: 0.0841 | KD: 1061.3418\n",
      "Train Epoch: 074 Batch: 00062/00094 | Loss: 109.2894 | CE: 0.0579 | KD: 1061.0616\n",
      "Train Epoch: 074 Batch: 00063/00094 | Loss: 109.3475 | CE: 0.0828 | KD: 1061.3829\n",
      "Train Epoch: 074 Batch: 00064/00094 | Loss: 109.3008 | CE: 0.0731 | KD: 1061.0240\n",
      "Train Epoch: 074 Batch: 00065/00094 | Loss: 109.3687 | CE: 0.1262 | KD: 1061.1677\n",
      "Train Epoch: 074 Batch: 00066/00094 | Loss: 109.3269 | CE: 0.0714 | KD: 1061.2941\n",
      "Train Epoch: 074 Batch: 00067/00094 | Loss: 109.2922 | CE: 0.0772 | KD: 1060.9009\n",
      "Train Epoch: 074 Batch: 00068/00094 | Loss: 109.3093 | CE: 0.0901 | KD: 1060.9408\n",
      "Train Epoch: 074 Batch: 00069/00094 | Loss: 109.3011 | CE: 0.0762 | KD: 1060.9963\n",
      "Train Epoch: 074 Batch: 00070/00094 | Loss: 109.3046 | CE: 0.0628 | KD: 1061.1608\n",
      "Train Epoch: 074 Batch: 00071/00094 | Loss: 109.3461 | CE: 0.0946 | KD: 1061.2557\n",
      "Train Epoch: 074 Batch: 00072/00094 | Loss: 109.2667 | CE: 0.0589 | KD: 1060.8300\n",
      "Train Epoch: 074 Batch: 00073/00094 | Loss: 109.3675 | CE: 0.1275 | KD: 1061.1432\n",
      "Train Epoch: 074 Batch: 00074/00094 | Loss: 109.3201 | CE: 0.0745 | KD: 1061.1976\n",
      "Train Epoch: 074 Batch: 00075/00094 | Loss: 109.2933 | CE: 0.0636 | KD: 1061.0437\n",
      "Train Epoch: 074 Batch: 00076/00094 | Loss: 109.3067 | CE: 0.0689 | KD: 1061.1223\n",
      "Train Epoch: 074 Batch: 00077/00094 | Loss: 109.3074 | CE: 0.0838 | KD: 1060.9841\n",
      "Train Epoch: 074 Batch: 00078/00094 | Loss: 109.2694 | CE: 0.0652 | KD: 1060.7958\n",
      "Train Epoch: 074 Batch: 00079/00094 | Loss: 109.3061 | CE: 0.0850 | KD: 1060.9587\n",
      "Train Epoch: 074 Batch: 00080/00094 | Loss: 109.4406 | CE: 0.1230 | KD: 1061.8967\n",
      "Train Epoch: 074 Batch: 00081/00094 | Loss: 109.2620 | CE: 0.0626 | KD: 1060.7493\n",
      "Train Epoch: 074 Batch: 00082/00094 | Loss: 109.2669 | CE: 0.0595 | KD: 1060.8270\n",
      "Train Epoch: 074 Batch: 00083/00094 | Loss: 109.3420 | CE: 0.1138 | KD: 1061.0288\n",
      "Train Epoch: 074 Batch: 00084/00094 | Loss: 109.4578 | CE: 0.2024 | KD: 1061.2926\n",
      "Train Epoch: 074 Batch: 00085/00094 | Loss: 109.3115 | CE: 0.0821 | KD: 1061.0396\n",
      "Train Epoch: 074 Batch: 00086/00094 | Loss: 109.3140 | CE: 0.0594 | KD: 1061.2845\n",
      "Train Epoch: 074 Batch: 00087/00094 | Loss: 109.3025 | CE: 0.0850 | KD: 1060.9243\n",
      "Train Epoch: 074 Batch: 00088/00094 | Loss: 109.2994 | CE: 0.0776 | KD: 1060.9657\n",
      "Train Epoch: 074 Batch: 00089/00094 | Loss: 109.3362 | CE: 0.0917 | KD: 1061.1866\n",
      "Train Epoch: 074 Batch: 00090/00094 | Loss: 109.3031 | CE: 0.0742 | KD: 1061.0360\n",
      "Train Epoch: 074 Batch: 00091/00094 | Loss: 109.3207 | CE: 0.0833 | KD: 1061.1178\n",
      "Train Epoch: 074 Batch: 00092/00094 | Loss: 109.2846 | CE: 0.0735 | KD: 1060.8621\n",
      "Train Epoch: 074 Batch: 00093/00094 | Loss: 109.4228 | CE: 0.1897 | KD: 1061.0768\n",
      "Train Epoch: 074 Batch: 00094/00094 | Loss: 109.3818 | CE: 0.1625 | KD: 1060.9415\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0752 | acc:98.7000\n",
      "[VAL Acc] Target: 98.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7892 | acc:49.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2343 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2267 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3263 | acc:55.1332\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7955 | acc:64.2329\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 64.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7893 | acc:70.4545\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9063 | acc:58.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.13%\n",
      "[VAL Acc] Avg 61.58%\n",
      "Train Epoch: 075 Batch: 00001/00094 | Loss: 109.3150 | CE: 0.0501 | KD: 1061.3855\n",
      "Train Epoch: 075 Batch: 00002/00094 | Loss: 109.3340 | CE: 0.0975 | KD: 1061.1090\n",
      "Train Epoch: 075 Batch: 00003/00094 | Loss: 109.3517 | CE: 0.1105 | KD: 1061.1544\n",
      "Train Epoch: 075 Batch: 00004/00094 | Loss: 109.4304 | CE: 0.1511 | KD: 1061.5250\n",
      "Train Epoch: 075 Batch: 00005/00094 | Loss: 109.3243 | CE: 0.0766 | KD: 1061.2184\n",
      "Train Epoch: 075 Batch: 00006/00094 | Loss: 109.3436 | CE: 0.1277 | KD: 1060.9098\n",
      "Train Epoch: 075 Batch: 00007/00094 | Loss: 109.2863 | CE: 0.0546 | KD: 1061.0627\n",
      "Train Epoch: 075 Batch: 00008/00094 | Loss: 109.2823 | CE: 0.0625 | KD: 1060.9468\n",
      "Train Epoch: 075 Batch: 00009/00094 | Loss: 109.3120 | CE: 0.0814 | KD: 1061.0524\n",
      "Train Epoch: 075 Batch: 00010/00094 | Loss: 109.2993 | CE: 0.0647 | KD: 1061.0911\n",
      "Train Epoch: 075 Batch: 00011/00094 | Loss: 109.2884 | CE: 0.0704 | KD: 1060.9299\n",
      "Train Epoch: 075 Batch: 00012/00094 | Loss: 109.3145 | CE: 0.0676 | KD: 1061.2103\n",
      "Train Epoch: 075 Batch: 00013/00094 | Loss: 109.3298 | CE: 0.1225 | KD: 1060.8263\n",
      "Train Epoch: 075 Batch: 00014/00094 | Loss: 109.3350 | CE: 0.0642 | KD: 1061.4423\n",
      "Train Epoch: 075 Batch: 00015/00094 | Loss: 109.3399 | CE: 0.1112 | KD: 1061.0332\n",
      "Train Epoch: 075 Batch: 00016/00094 | Loss: 109.2876 | CE: 0.0626 | KD: 1060.9984\n",
      "Train Epoch: 075 Batch: 00017/00094 | Loss: 109.3057 | CE: 0.0797 | KD: 1061.0082\n",
      "Train Epoch: 075 Batch: 00018/00094 | Loss: 109.2971 | CE: 0.0638 | KD: 1061.0785\n",
      "Train Epoch: 075 Batch: 00019/00094 | Loss: 109.4448 | CE: 0.1389 | KD: 1061.7826\n",
      "Train Epoch: 075 Batch: 00020/00094 | Loss: 109.2602 | CE: 0.0525 | KD: 1060.8298\n",
      "Train Epoch: 075 Batch: 00021/00094 | Loss: 109.3670 | CE: 0.1005 | KD: 1061.4012\n",
      "Train Epoch: 075 Batch: 00022/00094 | Loss: 109.2937 | CE: 0.0771 | KD: 1060.9163\n",
      "Train Epoch: 075 Batch: 00023/00094 | Loss: 109.3286 | CE: 0.0833 | KD: 1061.1945\n",
      "Train Epoch: 075 Batch: 00024/00094 | Loss: 109.3496 | CE: 0.1414 | KD: 1060.8347\n",
      "Train Epoch: 075 Batch: 00025/00094 | Loss: 109.3143 | CE: 0.0593 | KD: 1061.2887\n",
      "Train Epoch: 075 Batch: 00026/00094 | Loss: 109.3243 | CE: 0.0981 | KD: 1061.0100\n",
      "Train Epoch: 075 Batch: 00027/00094 | Loss: 109.2988 | CE: 0.0775 | KD: 1060.9620\n",
      "Train Epoch: 075 Batch: 00028/00094 | Loss: 109.2856 | CE: 0.0664 | KD: 1060.9414\n",
      "Train Epoch: 075 Batch: 00029/00094 | Loss: 109.2654 | CE: 0.0634 | KD: 1060.7739\n",
      "Train Epoch: 075 Batch: 00030/00094 | Loss: 109.2814 | CE: 0.0732 | KD: 1060.8335\n",
      "Train Epoch: 075 Batch: 00031/00094 | Loss: 109.2795 | CE: 0.0673 | KD: 1060.8727\n",
      "Train Epoch: 075 Batch: 00032/00094 | Loss: 109.2415 | CE: 0.0605 | KD: 1060.5696\n",
      "Train Epoch: 075 Batch: 00033/00094 | Loss: 109.2765 | CE: 0.0733 | KD: 1060.7859\n",
      "Train Epoch: 075 Batch: 00034/00094 | Loss: 109.3049 | CE: 0.0692 | KD: 1061.1011\n",
      "Train Epoch: 075 Batch: 00035/00094 | Loss: 109.3676 | CE: 0.1001 | KD: 1061.4104\n",
      "Train Epoch: 075 Batch: 00036/00094 | Loss: 109.3176 | CE: 0.0896 | KD: 1061.0270\n",
      "Train Epoch: 075 Batch: 00037/00094 | Loss: 109.2818 | CE: 0.0620 | KD: 1060.9470\n",
      "Train Epoch: 075 Batch: 00038/00094 | Loss: 109.3270 | CE: 0.0607 | KD: 1061.3987\n",
      "Train Epoch: 075 Batch: 00039/00094 | Loss: 109.3445 | CE: 0.0740 | KD: 1061.4393\n",
      "Train Epoch: 075 Batch: 00040/00094 | Loss: 109.2954 | CE: 0.0567 | KD: 1061.1310\n",
      "Train Epoch: 075 Batch: 00041/00094 | Loss: 109.2424 | CE: 0.0506 | KD: 1060.6750\n",
      "Train Epoch: 075 Batch: 00042/00094 | Loss: 109.3301 | CE: 0.0910 | KD: 1061.1344\n",
      "Train Epoch: 075 Batch: 00043/00094 | Loss: 109.3348 | CE: 0.0926 | KD: 1061.1646\n",
      "Train Epoch: 075 Batch: 00044/00094 | Loss: 109.2711 | CE: 0.0544 | KD: 1060.9169\n",
      "Train Epoch: 075 Batch: 00045/00094 | Loss: 109.3169 | CE: 0.0966 | KD: 1060.9517\n",
      "Train Epoch: 075 Batch: 00046/00094 | Loss: 109.2981 | CE: 0.0709 | KD: 1061.0186\n",
      "Train Epoch: 075 Batch: 00047/00094 | Loss: 109.3259 | CE: 0.0928 | KD: 1061.0763\n",
      "Train Epoch: 075 Batch: 00048/00094 | Loss: 109.3085 | CE: 0.0816 | KD: 1061.0157\n",
      "Train Epoch: 075 Batch: 00049/00094 | Loss: 109.3054 | CE: 0.0827 | KD: 1060.9747\n",
      "Train Epoch: 075 Batch: 00050/00094 | Loss: 109.4273 | CE: 0.1817 | KD: 1061.1978\n",
      "Train Epoch: 075 Batch: 00051/00094 | Loss: 109.2986 | CE: 0.0788 | KD: 1060.9471\n",
      "Train Epoch: 075 Batch: 00052/00094 | Loss: 109.2665 | CE: 0.0678 | KD: 1060.7427\n",
      "Train Epoch: 075 Batch: 00053/00094 | Loss: 109.3566 | CE: 0.1017 | KD: 1061.2877\n",
      "Train Epoch: 075 Batch: 00054/00094 | Loss: 109.4088 | CE: 0.1540 | KD: 1061.2870\n",
      "Train Epoch: 075 Batch: 00055/00094 | Loss: 109.3967 | CE: 0.1512 | KD: 1061.1974\n",
      "Train Epoch: 075 Batch: 00056/00094 | Loss: 109.4065 | CE: 0.1541 | KD: 1061.2643\n",
      "Train Epoch: 075 Batch: 00057/00094 | Loss: 109.2836 | CE: 0.0709 | KD: 1060.8777\n",
      "Train Epoch: 075 Batch: 00058/00094 | Loss: 109.2894 | CE: 0.0914 | KD: 1060.7360\n",
      "Train Epoch: 075 Batch: 00059/00094 | Loss: 109.2786 | CE: 0.0655 | KD: 1060.8816\n",
      "Train Epoch: 075 Batch: 00060/00094 | Loss: 109.2989 | CE: 0.0639 | KD: 1061.0950\n",
      "Train Epoch: 075 Batch: 00061/00094 | Loss: 109.3188 | CE: 0.0688 | KD: 1061.2405\n",
      "Train Epoch: 075 Batch: 00062/00094 | Loss: 109.3070 | CE: 0.0687 | KD: 1061.1268\n",
      "Train Epoch: 075 Batch: 00063/00094 | Loss: 109.2370 | CE: 0.0632 | KD: 1060.5006\n",
      "Train Epoch: 075 Batch: 00064/00094 | Loss: 109.4614 | CE: 0.2029 | KD: 1061.3234\n",
      "Train Epoch: 075 Batch: 00065/00094 | Loss: 109.3608 | CE: 0.1363 | KD: 1060.9922\n",
      "Train Epoch: 075 Batch: 00066/00094 | Loss: 109.3044 | CE: 0.0787 | KD: 1061.0038\n",
      "Train Epoch: 075 Batch: 00067/00094 | Loss: 109.3701 | CE: 0.1235 | KD: 1061.2075\n",
      "Train Epoch: 075 Batch: 00068/00094 | Loss: 109.2897 | CE: 0.0547 | KD: 1061.0947\n",
      "Train Epoch: 075 Batch: 00069/00094 | Loss: 109.3403 | CE: 0.0872 | KD: 1061.2700\n",
      "Train Epoch: 075 Batch: 00070/00094 | Loss: 109.2816 | CE: 0.0704 | KD: 1060.8633\n",
      "Train Epoch: 075 Batch: 00071/00094 | Loss: 109.3305 | CE: 0.0769 | KD: 1061.2760\n",
      "Train Epoch: 075 Batch: 00072/00094 | Loss: 109.3128 | CE: 0.0696 | KD: 1061.1746\n",
      "Train Epoch: 075 Batch: 00073/00094 | Loss: 109.3590 | CE: 0.1056 | KD: 1061.2736\n",
      "Train Epoch: 075 Batch: 00074/00094 | Loss: 109.2831 | CE: 0.0576 | KD: 1061.0026\n",
      "Train Epoch: 075 Batch: 00075/00094 | Loss: 109.3857 | CE: 0.1576 | KD: 1061.0275\n",
      "Train Epoch: 075 Batch: 00076/00094 | Loss: 109.3113 | CE: 0.0588 | KD: 1061.2648\n",
      "Train Epoch: 075 Batch: 00077/00094 | Loss: 109.2989 | CE: 0.0720 | KD: 1061.0167\n",
      "Train Epoch: 075 Batch: 00078/00094 | Loss: 109.3151 | CE: 0.0695 | KD: 1061.1982\n",
      "Train Epoch: 075 Batch: 00079/00094 | Loss: 109.2562 | CE: 0.0485 | KD: 1060.8287\n",
      "Train Epoch: 075 Batch: 00080/00094 | Loss: 109.3328 | CE: 0.0901 | KD: 1061.1692\n",
      "Train Epoch: 075 Batch: 00081/00094 | Loss: 109.3122 | CE: 0.0920 | KD: 1060.9513\n",
      "Train Epoch: 075 Batch: 00082/00094 | Loss: 109.2795 | CE: 0.0662 | KD: 1060.8837\n",
      "Train Epoch: 075 Batch: 00083/00094 | Loss: 109.4529 | CE: 0.1624 | KD: 1061.6339\n",
      "Train Epoch: 075 Batch: 00084/00094 | Loss: 109.3100 | CE: 0.0844 | KD: 1061.0043\n",
      "Train Epoch: 075 Batch: 00085/00094 | Loss: 109.3625 | CE: 0.1098 | KD: 1061.2667\n",
      "Train Epoch: 075 Batch: 00086/00094 | Loss: 109.2952 | CE: 0.0577 | KD: 1061.1183\n",
      "Train Epoch: 075 Batch: 00087/00094 | Loss: 109.2870 | CE: 0.0646 | KD: 1060.9727\n",
      "Train Epoch: 075 Batch: 00088/00094 | Loss: 109.3477 | CE: 0.0901 | KD: 1061.3151\n",
      "Train Epoch: 075 Batch: 00089/00094 | Loss: 109.3025 | CE: 0.0659 | KD: 1061.1096\n",
      "Train Epoch: 075 Batch: 00090/00094 | Loss: 109.3285 | CE: 0.0769 | KD: 1061.2557\n",
      "Train Epoch: 075 Batch: 00091/00094 | Loss: 109.3115 | CE: 0.0701 | KD: 1061.1576\n",
      "Train Epoch: 075 Batch: 00092/00094 | Loss: 109.3014 | CE: 0.0602 | KD: 1061.1555\n",
      "Train Epoch: 075 Batch: 00093/00094 | Loss: 109.2855 | CE: 0.0486 | KD: 1061.1133\n",
      "Train Epoch: 075 Batch: 00094/00094 | Loss: 109.2770 | CE: 0.0556 | KD: 1060.9619\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0666 | acc:99.2500\n",
      "[VAL Acc] Target: 99.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6853 | acc:49.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1977 | acc:53.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2011 | acc:46.1832\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2485 | acc:56.6614\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7879 | acc:64.0481\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 64.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7366 | acc:71.5125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9407 | acc:58.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.06%\n",
      "[VAL Acc] Avg 62.37%\n",
      "Train Epoch: 076 Batch: 00001/00094 | Loss: 109.3146 | CE: 0.0820 | KD: 1061.0718\n",
      "Train Epoch: 076 Batch: 00002/00094 | Loss: 109.2678 | CE: 0.0607 | KD: 1060.8229\n",
      "Train Epoch: 076 Batch: 00003/00094 | Loss: 109.4054 | CE: 0.1378 | KD: 1061.4117\n",
      "Train Epoch: 076 Batch: 00004/00094 | Loss: 109.2827 | CE: 0.0628 | KD: 1060.9476\n",
      "Train Epoch: 076 Batch: 00005/00094 | Loss: 109.3313 | CE: 0.0528 | KD: 1061.5173\n",
      "Train Epoch: 076 Batch: 00006/00094 | Loss: 109.3161 | CE: 0.0815 | KD: 1061.0909\n",
      "Train Epoch: 076 Batch: 00007/00094 | Loss: 109.3429 | CE: 0.0897 | KD: 1061.2711\n",
      "Train Epoch: 076 Batch: 00008/00094 | Loss: 109.3055 | CE: 0.0775 | KD: 1061.0269\n",
      "Train Epoch: 076 Batch: 00009/00094 | Loss: 109.2912 | CE: 0.0662 | KD: 1060.9982\n",
      "Train Epoch: 076 Batch: 00010/00094 | Loss: 109.3653 | CE: 0.1142 | KD: 1061.2510\n",
      "Train Epoch: 076 Batch: 00011/00094 | Loss: 109.3886 | CE: 0.1187 | KD: 1061.4340\n",
      "Train Epoch: 076 Batch: 00012/00094 | Loss: 109.2886 | CE: 0.0577 | KD: 1061.0549\n",
      "Train Epoch: 076 Batch: 00013/00094 | Loss: 109.2823 | CE: 0.0538 | KD: 1061.0320\n",
      "Train Epoch: 076 Batch: 00014/00094 | Loss: 109.2879 | CE: 0.0605 | KD: 1061.0212\n",
      "Train Epoch: 076 Batch: 00015/00094 | Loss: 109.3331 | CE: 0.1040 | KD: 1061.0371\n",
      "Train Epoch: 076 Batch: 00016/00094 | Loss: 109.2535 | CE: 0.0635 | KD: 1060.6577\n",
      "Train Epoch: 076 Batch: 00017/00094 | Loss: 109.3906 | CE: 0.1983 | KD: 1060.6799\n",
      "Train Epoch: 076 Batch: 00018/00094 | Loss: 109.2794 | CE: 0.0704 | KD: 1060.8422\n",
      "Train Epoch: 076 Batch: 00019/00094 | Loss: 109.2536 | CE: 0.0621 | KD: 1060.6722\n",
      "Train Epoch: 076 Batch: 00020/00094 | Loss: 109.3556 | CE: 0.1136 | KD: 1061.1625\n",
      "Train Epoch: 076 Batch: 00021/00094 | Loss: 109.2668 | CE: 0.0514 | KD: 1060.9038\n",
      "Train Epoch: 076 Batch: 00022/00094 | Loss: 109.2789 | CE: 0.0575 | KD: 1060.9624\n",
      "Train Epoch: 076 Batch: 00023/00094 | Loss: 109.3057 | CE: 0.1131 | KD: 1060.6831\n",
      "Train Epoch: 076 Batch: 00024/00094 | Loss: 109.3211 | CE: 0.0840 | KD: 1061.1152\n",
      "Train Epoch: 076 Batch: 00025/00094 | Loss: 109.3117 | CE: 0.0737 | KD: 1061.1245\n",
      "Train Epoch: 076 Batch: 00026/00094 | Loss: 109.2470 | CE: 0.0514 | KD: 1060.7122\n",
      "Train Epoch: 076 Batch: 00027/00094 | Loss: 109.2994 | CE: 0.0733 | KD: 1061.0083\n",
      "Train Epoch: 076 Batch: 00028/00094 | Loss: 109.2530 | CE: 0.0655 | KD: 1060.6331\n",
      "Train Epoch: 076 Batch: 00029/00094 | Loss: 109.3011 | CE: 0.0760 | KD: 1060.9988\n",
      "Train Epoch: 076 Batch: 00030/00094 | Loss: 109.2786 | CE: 0.0705 | KD: 1060.8336\n",
      "Train Epoch: 076 Batch: 00031/00094 | Loss: 109.4424 | CE: 0.2175 | KD: 1060.9960\n",
      "Train Epoch: 076 Batch: 00032/00094 | Loss: 109.3609 | CE: 0.0771 | KD: 1061.5691\n",
      "Train Epoch: 076 Batch: 00033/00094 | Loss: 109.3142 | CE: 0.0813 | KD: 1061.0751\n",
      "Train Epoch: 076 Batch: 00034/00094 | Loss: 109.2769 | CE: 0.0506 | KD: 1061.0096\n",
      "Train Epoch: 076 Batch: 00035/00094 | Loss: 109.2870 | CE: 0.0528 | KD: 1061.0870\n",
      "Train Epoch: 076 Batch: 00036/00094 | Loss: 109.3065 | CE: 0.0532 | KD: 1061.2725\n",
      "Train Epoch: 076 Batch: 00037/00094 | Loss: 109.3343 | CE: 0.0576 | KD: 1061.4998\n",
      "Train Epoch: 076 Batch: 00038/00094 | Loss: 109.2702 | CE: 0.0697 | KD: 1060.7595\n",
      "Train Epoch: 076 Batch: 00039/00094 | Loss: 109.2986 | CE: 0.0909 | KD: 1060.8293\n",
      "Train Epoch: 076 Batch: 00040/00094 | Loss: 109.2639 | CE: 0.0518 | KD: 1060.8719\n",
      "Train Epoch: 076 Batch: 00041/00094 | Loss: 109.2734 | CE: 0.0596 | KD: 1060.8892\n",
      "Train Epoch: 076 Batch: 00042/00094 | Loss: 109.2971 | CE: 0.0678 | KD: 1061.0397\n",
      "Train Epoch: 076 Batch: 00043/00094 | Loss: 109.3016 | CE: 0.0578 | KD: 1061.1807\n",
      "Train Epoch: 076 Batch: 00044/00094 | Loss: 109.3926 | CE: 0.1108 | KD: 1061.5498\n",
      "Train Epoch: 076 Batch: 00045/00094 | Loss: 109.3236 | CE: 0.0550 | KD: 1061.4214\n",
      "Train Epoch: 076 Batch: 00046/00094 | Loss: 109.2428 | CE: 0.0637 | KD: 1060.5513\n",
      "Train Epoch: 076 Batch: 00047/00094 | Loss: 109.2963 | CE: 0.0847 | KD: 1060.8683\n",
      "Train Epoch: 076 Batch: 00048/00094 | Loss: 109.3565 | CE: 0.1007 | KD: 1061.2964\n",
      "Train Epoch: 076 Batch: 00049/00094 | Loss: 109.3261 | CE: 0.0596 | KD: 1061.4009\n",
      "Train Epoch: 076 Batch: 00050/00094 | Loss: 109.3223 | CE: 0.1029 | KD: 1060.9435\n",
      "Train Epoch: 076 Batch: 00051/00094 | Loss: 109.4328 | CE: 0.1596 | KD: 1061.4658\n",
      "Train Epoch: 076 Batch: 00052/00094 | Loss: 109.3073 | CE: 0.0637 | KD: 1061.1783\n",
      "Train Epoch: 076 Batch: 00053/00094 | Loss: 109.4504 | CE: 0.2049 | KD: 1061.1963\n",
      "Train Epoch: 076 Batch: 00054/00094 | Loss: 109.3001 | CE: 0.0574 | KD: 1061.1697\n",
      "Train Epoch: 076 Batch: 00055/00094 | Loss: 109.3310 | CE: 0.0948 | KD: 1061.1062\n",
      "Train Epoch: 076 Batch: 00056/00094 | Loss: 109.3326 | CE: 0.0843 | KD: 1061.2244\n",
      "Train Epoch: 076 Batch: 00057/00094 | Loss: 109.3081 | CE: 0.0880 | KD: 1060.9502\n",
      "Train Epoch: 076 Batch: 00058/00094 | Loss: 109.3350 | CE: 0.0729 | KD: 1061.3575\n",
      "Train Epoch: 076 Batch: 00059/00094 | Loss: 109.3471 | CE: 0.0902 | KD: 1061.3077\n",
      "Train Epoch: 076 Batch: 00060/00094 | Loss: 109.3171 | CE: 0.0607 | KD: 1061.3019\n",
      "Train Epoch: 076 Batch: 00061/00094 | Loss: 109.3193 | CE: 0.0648 | KD: 1061.2841\n",
      "Train Epoch: 076 Batch: 00062/00094 | Loss: 109.3033 | CE: 0.0958 | KD: 1060.8285\n",
      "Train Epoch: 076 Batch: 00063/00094 | Loss: 109.2318 | CE: 0.0563 | KD: 1060.5167\n",
      "Train Epoch: 076 Batch: 00064/00094 | Loss: 109.2762 | CE: 0.0594 | KD: 1060.9176\n",
      "Train Epoch: 076 Batch: 00065/00094 | Loss: 109.3059 | CE: 0.0644 | KD: 1061.1582\n",
      "Train Epoch: 076 Batch: 00066/00094 | Loss: 109.2447 | CE: 0.0703 | KD: 1060.5066\n",
      "Train Epoch: 076 Batch: 00067/00094 | Loss: 109.3431 | CE: 0.1095 | KD: 1061.0820\n",
      "Train Epoch: 076 Batch: 00068/00094 | Loss: 109.3020 | CE: 0.0564 | KD: 1061.1976\n",
      "Train Epoch: 076 Batch: 00069/00094 | Loss: 109.3511 | CE: 0.1397 | KD: 1060.8661\n",
      "Train Epoch: 076 Batch: 00070/00094 | Loss: 109.3152 | CE: 0.0735 | KD: 1061.1595\n",
      "Train Epoch: 076 Batch: 00071/00094 | Loss: 109.3445 | CE: 0.0959 | KD: 1061.2268\n",
      "Train Epoch: 076 Batch: 00072/00094 | Loss: 109.3107 | CE: 0.0725 | KD: 1061.1259\n",
      "Train Epoch: 076 Batch: 00073/00094 | Loss: 109.3067 | CE: 0.0768 | KD: 1061.0458\n",
      "Train Epoch: 076 Batch: 00074/00094 | Loss: 109.3838 | CE: 0.1420 | KD: 1061.1606\n",
      "Train Epoch: 076 Batch: 00075/00094 | Loss: 109.2378 | CE: 0.0455 | KD: 1060.6792\n",
      "Train Epoch: 076 Batch: 00076/00094 | Loss: 109.2805 | CE: 0.0606 | KD: 1060.9482\n",
      "Train Epoch: 076 Batch: 00077/00094 | Loss: 109.3370 | CE: 0.0558 | KD: 1061.5436\n",
      "Train Epoch: 076 Batch: 00078/00094 | Loss: 109.3401 | CE: 0.0885 | KD: 1061.2562\n",
      "Train Epoch: 076 Batch: 00079/00094 | Loss: 109.2738 | CE: 0.0593 | KD: 1060.8955\n",
      "Train Epoch: 076 Batch: 00080/00094 | Loss: 109.3570 | CE: 0.0760 | KD: 1061.5419\n",
      "Train Epoch: 076 Batch: 00081/00094 | Loss: 109.3363 | CE: 0.1112 | KD: 1060.9988\n",
      "Train Epoch: 076 Batch: 00082/00094 | Loss: 109.3266 | CE: 0.1025 | KD: 1060.9891\n",
      "Train Epoch: 076 Batch: 00083/00094 | Loss: 109.3249 | CE: 0.0884 | KD: 1061.1095\n",
      "Train Epoch: 076 Batch: 00084/00094 | Loss: 109.2905 | CE: 0.0862 | KD: 1060.7959\n",
      "Train Epoch: 076 Batch: 00085/00094 | Loss: 109.3623 | CE: 0.1135 | KD: 1061.2290\n",
      "Train Epoch: 076 Batch: 00086/00094 | Loss: 109.2727 | CE: 0.0525 | KD: 1060.9512\n",
      "Train Epoch: 076 Batch: 00087/00094 | Loss: 109.2779 | CE: 0.0533 | KD: 1060.9944\n",
      "Train Epoch: 076 Batch: 00088/00094 | Loss: 109.2881 | CE: 0.0550 | KD: 1061.0762\n",
      "Train Epoch: 076 Batch: 00089/00094 | Loss: 109.4422 | CE: 0.1671 | KD: 1061.4841\n",
      "Train Epoch: 076 Batch: 00090/00094 | Loss: 109.3059 | CE: 0.0703 | KD: 1061.1001\n",
      "Train Epoch: 076 Batch: 00091/00094 | Loss: 109.3038 | CE: 0.0876 | KD: 1060.9125\n",
      "Train Epoch: 076 Batch: 00092/00094 | Loss: 109.2912 | CE: 0.0635 | KD: 1061.0244\n",
      "Train Epoch: 076 Batch: 00093/00094 | Loss: 109.3646 | CE: 0.1177 | KD: 1061.2108\n",
      "Train Epoch: 076 Batch: 00094/00094 | Loss: 109.3007 | CE: 0.0794 | KD: 1060.9623\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0746 | acc:98.7000\n",
      "[VAL Acc] Target: 98.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8334 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2946 | acc:51.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3047 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3773 | acc:55.1332\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8206 | acc:63.4935\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 63.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7837 | acc:70.3370\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9213 | acc:58.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.56%\n",
      "[VAL Acc] Avg 61.55%\n",
      "Train Epoch: 077 Batch: 00001/00094 | Loss: 98.3584 | CE: 0.0566 | KD: 1060.9901\n",
      "Train Epoch: 077 Batch: 00002/00094 | Loss: 98.3828 | CE: 0.0662 | KD: 1061.1495\n",
      "Train Epoch: 077 Batch: 00003/00094 | Loss: 98.3648 | CE: 0.0571 | KD: 1061.0547\n",
      "Train Epoch: 077 Batch: 00004/00094 | Loss: 98.3853 | CE: 0.1024 | KD: 1060.7863\n",
      "Train Epoch: 077 Batch: 00005/00094 | Loss: 98.4736 | CE: 0.1255 | KD: 1061.4902\n",
      "Train Epoch: 077 Batch: 00006/00094 | Loss: 98.4276 | CE: 0.1018 | KD: 1061.2490\n",
      "Train Epoch: 077 Batch: 00007/00094 | Loss: 98.3677 | CE: 0.0782 | KD: 1060.8574\n",
      "Train Epoch: 077 Batch: 00008/00094 | Loss: 98.4069 | CE: 0.1312 | KD: 1060.7090\n",
      "Train Epoch: 077 Batch: 00009/00094 | Loss: 98.3930 | CE: 0.0736 | KD: 1061.1794\n",
      "Train Epoch: 077 Batch: 00010/00094 | Loss: 98.3556 | CE: 0.0690 | KD: 1060.8260\n",
      "Train Epoch: 077 Batch: 00011/00094 | Loss: 98.4204 | CE: 0.0685 | KD: 1061.5311\n",
      "Train Epoch: 077 Batch: 00012/00094 | Loss: 98.4019 | CE: 0.1137 | KD: 1060.8438\n",
      "Train Epoch: 077 Batch: 00013/00094 | Loss: 98.3681 | CE: 0.0636 | KD: 1061.0194\n",
      "Train Epoch: 077 Batch: 00014/00094 | Loss: 98.3509 | CE: 0.0654 | KD: 1060.8141\n",
      "Train Epoch: 077 Batch: 00015/00094 | Loss: 98.3550 | CE: 0.0706 | KD: 1060.8026\n",
      "Train Epoch: 077 Batch: 00016/00094 | Loss: 98.3422 | CE: 0.0520 | KD: 1060.8649\n",
      "Train Epoch: 077 Batch: 00017/00094 | Loss: 98.2894 | CE: 0.0469 | KD: 1060.3502\n",
      "Train Epoch: 077 Batch: 00018/00094 | Loss: 98.3650 | CE: 0.0466 | KD: 1061.1688\n",
      "Train Epoch: 077 Batch: 00019/00094 | Loss: 98.3591 | CE: 0.0690 | KD: 1060.8641\n",
      "Train Epoch: 077 Batch: 00020/00094 | Loss: 98.3885 | CE: 0.0657 | KD: 1061.2159\n",
      "Train Epoch: 077 Batch: 00021/00094 | Loss: 98.3637 | CE: 0.0682 | KD: 1060.9216\n",
      "Train Epoch: 077 Batch: 00022/00094 | Loss: 98.3526 | CE: 0.0691 | KD: 1060.7924\n",
      "Train Epoch: 077 Batch: 00023/00094 | Loss: 98.5453 | CE: 0.2087 | KD: 1061.3656\n",
      "Train Epoch: 077 Batch: 00024/00094 | Loss: 98.3719 | CE: 0.0554 | KD: 1061.1488\n",
      "Train Epoch: 077 Batch: 00025/00094 | Loss: 98.3511 | CE: 0.0761 | KD: 1060.7015\n",
      "Train Epoch: 077 Batch: 00026/00094 | Loss: 98.3850 | CE: 0.1028 | KD: 1060.7786\n",
      "Train Epoch: 077 Batch: 00027/00094 | Loss: 98.3584 | CE: 0.0535 | KD: 1061.0240\n",
      "Train Epoch: 077 Batch: 00028/00094 | Loss: 98.4413 | CE: 0.1158 | KD: 1061.2451\n",
      "Train Epoch: 077 Batch: 00029/00094 | Loss: 98.4038 | CE: 0.0916 | KD: 1061.1019\n",
      "Train Epoch: 077 Batch: 00030/00094 | Loss: 98.4488 | CE: 0.1117 | KD: 1061.3701\n",
      "Train Epoch: 077 Batch: 00031/00094 | Loss: 98.4293 | CE: 0.0710 | KD: 1061.6002\n",
      "Train Epoch: 077 Batch: 00032/00094 | Loss: 98.3961 | CE: 0.0928 | KD: 1061.0068\n",
      "Train Epoch: 077 Batch: 00033/00094 | Loss: 98.4801 | CE: 0.1208 | KD: 1061.6097\n",
      "Train Epoch: 077 Batch: 00034/00094 | Loss: 98.3649 | CE: 0.0549 | KD: 1061.0792\n",
      "Train Epoch: 077 Batch: 00035/00094 | Loss: 98.3634 | CE: 0.0705 | KD: 1060.8944\n",
      "Train Epoch: 077 Batch: 00036/00094 | Loss: 98.3519 | CE: 0.0652 | KD: 1060.8270\n",
      "Train Epoch: 077 Batch: 00037/00094 | Loss: 98.4015 | CE: 0.0685 | KD: 1061.3267\n",
      "Train Epoch: 077 Batch: 00038/00094 | Loss: 98.3781 | CE: 0.0892 | KD: 1060.8505\n",
      "Train Epoch: 077 Batch: 00039/00094 | Loss: 98.4063 | CE: 0.0887 | KD: 1061.1608\n",
      "Train Epoch: 077 Batch: 00040/00094 | Loss: 98.4391 | CE: 0.1250 | KD: 1061.1221\n",
      "Train Epoch: 077 Batch: 00041/00094 | Loss: 98.3222 | CE: 0.0468 | KD: 1060.7046\n",
      "Train Epoch: 077 Batch: 00042/00094 | Loss: 98.3660 | CE: 0.0601 | KD: 1061.0347\n",
      "Train Epoch: 077 Batch: 00043/00094 | Loss: 98.3837 | CE: 0.0715 | KD: 1061.1019\n",
      "Train Epoch: 077 Batch: 00044/00094 | Loss: 98.4575 | CE: 0.1261 | KD: 1061.3096\n",
      "Train Epoch: 077 Batch: 00045/00094 | Loss: 98.3839 | CE: 0.0792 | KD: 1061.0217\n",
      "Train Epoch: 077 Batch: 00046/00094 | Loss: 98.3732 | CE: 0.0695 | KD: 1061.0110\n",
      "Train Epoch: 077 Batch: 00047/00094 | Loss: 98.3905 | CE: 0.0928 | KD: 1060.9460\n",
      "Train Epoch: 077 Batch: 00048/00094 | Loss: 98.4159 | CE: 0.0847 | KD: 1061.3080\n",
      "Train Epoch: 077 Batch: 00049/00094 | Loss: 98.4299 | CE: 0.0984 | KD: 1061.3107\n",
      "Train Epoch: 077 Batch: 00050/00094 | Loss: 98.4865 | CE: 0.1569 | KD: 1061.2905\n",
      "Train Epoch: 077 Batch: 00051/00094 | Loss: 98.3559 | CE: 0.0614 | KD: 1060.9110\n",
      "Train Epoch: 077 Batch: 00052/00094 | Loss: 98.3783 | CE: 0.0648 | KD: 1061.1166\n",
      "Train Epoch: 077 Batch: 00053/00094 | Loss: 98.4185 | CE: 0.0976 | KD: 1061.1957\n",
      "Train Epoch: 077 Batch: 00054/00094 | Loss: 98.3728 | CE: 0.0658 | KD: 1061.0464\n",
      "Train Epoch: 077 Batch: 00055/00094 | Loss: 98.3821 | CE: 0.0758 | KD: 1061.0394\n",
      "Train Epoch: 077 Batch: 00056/00094 | Loss: 98.3929 | CE: 0.0767 | KD: 1061.1458\n",
      "Train Epoch: 077 Batch: 00057/00094 | Loss: 98.4066 | CE: 0.0947 | KD: 1061.0986\n",
      "Train Epoch: 077 Batch: 00058/00094 | Loss: 98.3721 | CE: 0.0524 | KD: 1061.1827\n",
      "Train Epoch: 077 Batch: 00059/00094 | Loss: 98.3902 | CE: 0.0935 | KD: 1060.9352\n",
      "Train Epoch: 077 Batch: 00060/00094 | Loss: 98.4143 | CE: 0.0747 | KD: 1061.3984\n",
      "Train Epoch: 077 Batch: 00061/00094 | Loss: 98.3569 | CE: 0.0641 | KD: 1060.8934\n",
      "Train Epoch: 077 Batch: 00062/00094 | Loss: 98.3434 | CE: 0.0406 | KD: 1061.0012\n",
      "Train Epoch: 077 Batch: 00063/00094 | Loss: 98.3777 | CE: 0.0507 | KD: 1061.2623\n",
      "Train Epoch: 077 Batch: 00064/00094 | Loss: 98.3720 | CE: 0.0680 | KD: 1061.0138\n",
      "Train Epoch: 077 Batch: 00065/00094 | Loss: 98.4337 | CE: 0.0777 | KD: 1061.5754\n",
      "Train Epoch: 077 Batch: 00066/00094 | Loss: 98.4006 | CE: 0.0605 | KD: 1061.4038\n",
      "Train Epoch: 077 Batch: 00067/00094 | Loss: 98.3889 | CE: 0.0611 | KD: 1061.2705\n",
      "Train Epoch: 077 Batch: 00068/00094 | Loss: 98.3615 | CE: 0.0550 | KD: 1061.0403\n",
      "Train Epoch: 077 Batch: 00069/00094 | Loss: 98.4181 | CE: 0.0728 | KD: 1061.4597\n",
      "Train Epoch: 077 Batch: 00070/00094 | Loss: 98.3762 | CE: 0.0684 | KD: 1061.0553\n",
      "Train Epoch: 077 Batch: 00071/00094 | Loss: 98.4699 | CE: 0.1032 | KD: 1061.6906\n",
      "Train Epoch: 077 Batch: 00072/00094 | Loss: 98.4621 | CE: 0.0873 | KD: 1061.7784\n",
      "Train Epoch: 077 Batch: 00073/00094 | Loss: 98.4456 | CE: 0.1160 | KD: 1061.2899\n",
      "Train Epoch: 077 Batch: 00074/00094 | Loss: 98.3849 | CE: 0.1233 | KD: 1060.5558\n",
      "Train Epoch: 077 Batch: 00075/00094 | Loss: 98.3662 | CE: 0.0564 | KD: 1061.0758\n",
      "Train Epoch: 077 Batch: 00076/00094 | Loss: 98.4144 | CE: 0.0979 | KD: 1061.1483\n",
      "Train Epoch: 077 Batch: 00077/00094 | Loss: 98.3980 | CE: 0.0850 | KD: 1061.1104\n",
      "Train Epoch: 077 Batch: 00078/00094 | Loss: 98.3763 | CE: 0.0547 | KD: 1061.2036\n",
      "Train Epoch: 077 Batch: 00079/00094 | Loss: 98.3946 | CE: 0.0693 | KD: 1061.2437\n",
      "Train Epoch: 077 Batch: 00080/00094 | Loss: 98.3580 | CE: 0.0628 | KD: 1060.9183\n",
      "Train Epoch: 077 Batch: 00081/00094 | Loss: 98.4449 | CE: 0.1298 | KD: 1061.1332\n",
      "Train Epoch: 077 Batch: 00082/00094 | Loss: 98.3167 | CE: 0.0513 | KD: 1060.5969\n",
      "Train Epoch: 077 Batch: 00083/00094 | Loss: 98.3530 | CE: 0.0573 | KD: 1060.9240\n",
      "Train Epoch: 077 Batch: 00084/00094 | Loss: 98.3681 | CE: 0.0736 | KD: 1060.9104\n",
      "Train Epoch: 077 Batch: 00085/00094 | Loss: 98.3612 | CE: 0.0761 | KD: 1060.8091\n",
      "Train Epoch: 077 Batch: 00086/00094 | Loss: 98.3577 | CE: 0.0593 | KD: 1060.9539\n",
      "Train Epoch: 077 Batch: 00087/00094 | Loss: 98.4576 | CE: 0.0907 | KD: 1061.6931\n",
      "Train Epoch: 077 Batch: 00088/00094 | Loss: 98.3333 | CE: 0.0482 | KD: 1060.8099\n",
      "Train Epoch: 077 Batch: 00089/00094 | Loss: 98.3306 | CE: 0.0744 | KD: 1060.4972\n",
      "Train Epoch: 077 Batch: 00090/00094 | Loss: 98.3555 | CE: 0.0652 | KD: 1060.8661\n",
      "Train Epoch: 077 Batch: 00091/00094 | Loss: 98.3974 | CE: 0.0749 | KD: 1061.2134\n",
      "Train Epoch: 077 Batch: 00092/00094 | Loss: 98.3859 | CE: 0.0741 | KD: 1061.0979\n",
      "Train Epoch: 077 Batch: 00093/00094 | Loss: 98.3607 | CE: 0.0476 | KD: 1061.1127\n",
      "Train Epoch: 077 Batch: 00094/00094 | Loss: 98.3468 | CE: 0.0571 | KD: 1060.8595\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0696 | acc:98.7500\n",
      "[VAL Acc] Target: 98.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7611 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2269 | acc:54.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2603 | acc:43.1298\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2953 | acc:57.4451\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7376 | acc:67.2828\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 67.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7194 | acc:72.4922\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9192 | acc:58.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.19%\n",
      "[VAL Acc] Avg 62.68%\n",
      "Train Epoch: 078 Batch: 00001/00094 | Loss: 98.3609 | CE: 0.0483 | KD: 1061.1061\n",
      "Train Epoch: 078 Batch: 00002/00094 | Loss: 98.4025 | CE: 0.0563 | KD: 1061.4692\n",
      "Train Epoch: 078 Batch: 00003/00094 | Loss: 98.3569 | CE: 0.0500 | KD: 1061.0457\n",
      "Train Epoch: 078 Batch: 00004/00094 | Loss: 98.3906 | CE: 0.0778 | KD: 1061.1091\n",
      "Train Epoch: 078 Batch: 00005/00094 | Loss: 98.4040 | CE: 0.0882 | KD: 1061.1410\n",
      "Train Epoch: 078 Batch: 00006/00094 | Loss: 98.3313 | CE: 0.0424 | KD: 1060.8506\n",
      "Train Epoch: 078 Batch: 00007/00094 | Loss: 98.3256 | CE: 0.0484 | KD: 1060.7246\n",
      "Train Epoch: 078 Batch: 00008/00094 | Loss: 98.4009 | CE: 0.0851 | KD: 1061.1405\n",
      "Train Epoch: 078 Batch: 00009/00094 | Loss: 98.5440 | CE: 0.2239 | KD: 1061.1873\n",
      "Train Epoch: 078 Batch: 00010/00094 | Loss: 98.3648 | CE: 0.0533 | KD: 1061.0947\n",
      "Train Epoch: 078 Batch: 00011/00094 | Loss: 98.3975 | CE: 0.0653 | KD: 1061.3176\n",
      "Train Epoch: 078 Batch: 00012/00094 | Loss: 98.3602 | CE: 0.0520 | KD: 1061.0597\n",
      "Train Epoch: 078 Batch: 00013/00094 | Loss: 98.3764 | CE: 0.0744 | KD: 1060.9919\n",
      "Train Epoch: 078 Batch: 00014/00094 | Loss: 98.4451 | CE: 0.1409 | KD: 1061.0162\n",
      "Train Epoch: 078 Batch: 00015/00094 | Loss: 98.3671 | CE: 0.0538 | KD: 1061.1141\n",
      "Train Epoch: 078 Batch: 00016/00094 | Loss: 98.3680 | CE: 0.0517 | KD: 1061.1465\n",
      "Train Epoch: 078 Batch: 00017/00094 | Loss: 98.4035 | CE: 0.0915 | KD: 1061.1010\n",
      "Train Epoch: 078 Batch: 00018/00094 | Loss: 98.3451 | CE: 0.0526 | KD: 1060.8893\n",
      "Train Epoch: 078 Batch: 00019/00094 | Loss: 98.4538 | CE: 0.1311 | KD: 1061.2157\n",
      "Train Epoch: 078 Batch: 00020/00094 | Loss: 98.3611 | CE: 0.0524 | KD: 1061.0654\n",
      "Train Epoch: 078 Batch: 00021/00094 | Loss: 98.3371 | CE: 0.0490 | KD: 1060.8431\n",
      "Train Epoch: 078 Batch: 00022/00094 | Loss: 98.3792 | CE: 0.0708 | KD: 1061.0613\n",
      "Train Epoch: 078 Batch: 00023/00094 | Loss: 98.3506 | CE: 0.0476 | KD: 1061.0023\n",
      "Train Epoch: 078 Batch: 00024/00094 | Loss: 98.4625 | CE: 0.1116 | KD: 1061.5193\n",
      "Train Epoch: 078 Batch: 00025/00094 | Loss: 98.4104 | CE: 0.1061 | KD: 1061.0176\n",
      "Train Epoch: 078 Batch: 00026/00094 | Loss: 98.3834 | CE: 0.0525 | KD: 1061.3041\n",
      "Train Epoch: 078 Batch: 00027/00094 | Loss: 98.3921 | CE: 0.0614 | KD: 1061.3015\n",
      "Train Epoch: 078 Batch: 00028/00094 | Loss: 98.4598 | CE: 0.1643 | KD: 1060.9222\n",
      "Train Epoch: 078 Batch: 00029/00094 | Loss: 98.3830 | CE: 0.0608 | KD: 1061.2103\n",
      "Train Epoch: 078 Batch: 00030/00094 | Loss: 98.4019 | CE: 0.0548 | KD: 1061.4785\n",
      "Train Epoch: 078 Batch: 00031/00094 | Loss: 98.3553 | CE: 0.0586 | KD: 1060.9351\n",
      "Train Epoch: 078 Batch: 00032/00094 | Loss: 98.3589 | CE: 0.0442 | KD: 1061.1295\n",
      "Train Epoch: 078 Batch: 00033/00094 | Loss: 98.4153 | CE: 0.0805 | KD: 1061.3461\n",
      "Train Epoch: 078 Batch: 00034/00094 | Loss: 98.3904 | CE: 0.0783 | KD: 1061.1012\n",
      "Train Epoch: 078 Batch: 00035/00094 | Loss: 98.4759 | CE: 0.1417 | KD: 1061.3406\n",
      "Train Epoch: 078 Batch: 00036/00094 | Loss: 98.4165 | CE: 0.1056 | KD: 1061.0884\n",
      "Train Epoch: 078 Batch: 00037/00094 | Loss: 98.4221 | CE: 0.1106 | KD: 1061.0946\n",
      "Train Epoch: 078 Batch: 00038/00094 | Loss: 98.3845 | CE: 0.0725 | KD: 1061.0999\n",
      "Train Epoch: 078 Batch: 00039/00094 | Loss: 98.4091 | CE: 0.0641 | KD: 1061.4561\n",
      "Train Epoch: 078 Batch: 00040/00094 | Loss: 98.3935 | CE: 0.0967 | KD: 1060.9354\n",
      "Train Epoch: 078 Batch: 00041/00094 | Loss: 98.3693 | CE: 0.0664 | KD: 1061.0020\n",
      "Train Epoch: 078 Batch: 00042/00094 | Loss: 98.4080 | CE: 0.0807 | KD: 1061.2650\n",
      "Train Epoch: 078 Batch: 00043/00094 | Loss: 98.3529 | CE: 0.0730 | KD: 1060.7531\n",
      "Train Epoch: 078 Batch: 00044/00094 | Loss: 98.3443 | CE: 0.0635 | KD: 1060.7633\n",
      "Train Epoch: 078 Batch: 00045/00094 | Loss: 98.3967 | CE: 0.0468 | KD: 1061.5093\n",
      "Train Epoch: 078 Batch: 00046/00094 | Loss: 98.3648 | CE: 0.0499 | KD: 1061.1311\n",
      "Train Epoch: 078 Batch: 00047/00094 | Loss: 98.4077 | CE: 0.0830 | KD: 1061.2363\n",
      "Train Epoch: 078 Batch: 00048/00094 | Loss: 98.3484 | CE: 0.0488 | KD: 1060.9656\n",
      "Train Epoch: 078 Batch: 00049/00094 | Loss: 98.4001 | CE: 0.0745 | KD: 1061.2471\n",
      "Train Epoch: 078 Batch: 00050/00094 | Loss: 98.3537 | CE: 0.0513 | KD: 1060.9963\n",
      "Train Epoch: 078 Batch: 00051/00094 | Loss: 98.3554 | CE: 0.0489 | KD: 1061.0408\n",
      "Train Epoch: 078 Batch: 00052/00094 | Loss: 98.3581 | CE: 0.0540 | KD: 1061.0143\n",
      "Train Epoch: 078 Batch: 00053/00094 | Loss: 98.3864 | CE: 0.0830 | KD: 1061.0066\n",
      "Train Epoch: 078 Batch: 00054/00094 | Loss: 98.3546 | CE: 0.0610 | KD: 1060.9016\n",
      "Train Epoch: 078 Batch: 00055/00094 | Loss: 98.4012 | CE: 0.0950 | KD: 1061.0374\n",
      "Train Epoch: 078 Batch: 00056/00094 | Loss: 98.4475 | CE: 0.0690 | KD: 1061.8179\n",
      "Train Epoch: 078 Batch: 00057/00094 | Loss: 98.3972 | CE: 0.0686 | KD: 1061.2784\n",
      "Train Epoch: 078 Batch: 00058/00094 | Loss: 98.3552 | CE: 0.0548 | KD: 1060.9753\n",
      "Train Epoch: 078 Batch: 00059/00094 | Loss: 98.3571 | CE: 0.0487 | KD: 1061.0605\n",
      "Train Epoch: 078 Batch: 00060/00094 | Loss: 98.4303 | CE: 0.0875 | KD: 1061.4333\n",
      "Train Epoch: 078 Batch: 00061/00094 | Loss: 98.3787 | CE: 0.0606 | KD: 1061.1660\n",
      "Train Epoch: 078 Batch: 00062/00094 | Loss: 98.3990 | CE: 0.0694 | KD: 1061.2904\n",
      "Train Epoch: 078 Batch: 00063/00094 | Loss: 98.3832 | CE: 0.0644 | KD: 1061.1741\n",
      "Train Epoch: 078 Batch: 00064/00094 | Loss: 98.4111 | CE: 0.1164 | KD: 1060.9142\n",
      "Train Epoch: 078 Batch: 00065/00094 | Loss: 98.3552 | CE: 0.0403 | KD: 1061.1313\n",
      "Train Epoch: 078 Batch: 00066/00094 | Loss: 98.3637 | CE: 0.0423 | KD: 1061.2017\n",
      "Train Epoch: 078 Batch: 00067/00094 | Loss: 98.4059 | CE: 0.0975 | KD: 1061.0613\n",
      "Train Epoch: 078 Batch: 00068/00094 | Loss: 98.4248 | CE: 0.1406 | KD: 1060.7996\n",
      "Train Epoch: 078 Batch: 00069/00094 | Loss: 98.3698 | CE: 0.0554 | KD: 1061.1260\n",
      "Train Epoch: 078 Batch: 00070/00094 | Loss: 98.3433 | CE: 0.0584 | KD: 1060.8079\n",
      "Train Epoch: 078 Batch: 00071/00094 | Loss: 98.3984 | CE: 0.0582 | KD: 1061.4049\n",
      "Train Epoch: 078 Batch: 00072/00094 | Loss: 98.3526 | CE: 0.0627 | KD: 1060.8617\n",
      "Train Epoch: 078 Batch: 00073/00094 | Loss: 98.3782 | CE: 0.0620 | KD: 1061.1460\n",
      "Train Epoch: 078 Batch: 00074/00094 | Loss: 98.3973 | CE: 0.0698 | KD: 1061.2675\n",
      "Train Epoch: 078 Batch: 00075/00094 | Loss: 98.3480 | CE: 0.0567 | KD: 1060.8762\n",
      "Train Epoch: 078 Batch: 00076/00094 | Loss: 98.3375 | CE: 0.0483 | KD: 1060.8536\n",
      "Train Epoch: 078 Batch: 00077/00094 | Loss: 98.4111 | CE: 0.0781 | KD: 1061.3271\n",
      "Train Epoch: 078 Batch: 00078/00094 | Loss: 98.3411 | CE: 0.0536 | KD: 1060.8354\n",
      "Train Epoch: 078 Batch: 00079/00094 | Loss: 98.3222 | CE: 0.0469 | KD: 1060.7039\n",
      "Train Epoch: 078 Batch: 00080/00094 | Loss: 98.3949 | CE: 0.0479 | KD: 1061.4783\n",
      "Train Epoch: 078 Batch: 00081/00094 | Loss: 98.3804 | CE: 0.0631 | KD: 1061.1572\n",
      "Train Epoch: 078 Batch: 00082/00094 | Loss: 98.3689 | CE: 0.0839 | KD: 1060.8086\n",
      "Train Epoch: 078 Batch: 00083/00094 | Loss: 98.3732 | CE: 0.0558 | KD: 1061.1588\n",
      "Train Epoch: 078 Batch: 00084/00094 | Loss: 98.3446 | CE: 0.0439 | KD: 1060.9781\n",
      "Train Epoch: 078 Batch: 00085/00094 | Loss: 98.3859 | CE: 0.0600 | KD: 1061.2509\n",
      "Train Epoch: 078 Batch: 00086/00094 | Loss: 98.3898 | CE: 0.0624 | KD: 1061.2662\n",
      "Train Epoch: 078 Batch: 00087/00094 | Loss: 98.3995 | CE: 0.0463 | KD: 1061.5444\n",
      "Train Epoch: 078 Batch: 00088/00094 | Loss: 98.4729 | CE: 0.0901 | KD: 1061.8638\n",
      "Train Epoch: 078 Batch: 00089/00094 | Loss: 98.3731 | CE: 0.0757 | KD: 1060.9417\n",
      "Train Epoch: 078 Batch: 00090/00094 | Loss: 98.4396 | CE: 0.0751 | KD: 1061.6674\n",
      "Train Epoch: 078 Batch: 00091/00094 | Loss: 98.3186 | CE: 0.0507 | KD: 1060.6240\n",
      "Train Epoch: 078 Batch: 00092/00094 | Loss: 98.4263 | CE: 0.1105 | KD: 1061.1410\n",
      "Train Epoch: 078 Batch: 00093/00094 | Loss: 98.4086 | CE: 0.0560 | KD: 1061.5388\n",
      "Train Epoch: 078 Batch: 00094/00094 | Loss: 98.3447 | CE: 0.0662 | KD: 1060.7384\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0668 | acc:98.9500\n",
      "[VAL Acc] Target: 98.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7758 | acc:48.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2919 | acc:52.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2163 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1918 | acc:58.8950\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8174 | acc:64.2329\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 64.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6572 | acc:74.4906\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9100 | acc:59.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.81%\n",
      "[VAL Acc] Avg 62.94%\n",
      "Train Epoch: 079 Batch: 00001/00094 | Loss: 98.3926 | CE: 0.0790 | KD: 1061.1178\n",
      "Train Epoch: 079 Batch: 00002/00094 | Loss: 98.3284 | CE: 0.0459 | KD: 1060.7820\n",
      "Train Epoch: 079 Batch: 00003/00094 | Loss: 98.3414 | CE: 0.0698 | KD: 1060.6644\n",
      "Train Epoch: 079 Batch: 00004/00094 | Loss: 98.3543 | CE: 0.0416 | KD: 1061.1075\n",
      "Train Epoch: 079 Batch: 00005/00094 | Loss: 98.3503 | CE: 0.0611 | KD: 1060.8542\n",
      "Train Epoch: 079 Batch: 00006/00094 | Loss: 98.4345 | CE: 0.0970 | KD: 1061.3755\n",
      "Train Epoch: 079 Batch: 00007/00094 | Loss: 98.4238 | CE: 0.1297 | KD: 1060.9070\n",
      "Train Epoch: 079 Batch: 00008/00094 | Loss: 98.3913 | CE: 0.0606 | KD: 1061.3024\n",
      "Train Epoch: 079 Batch: 00009/00094 | Loss: 98.4135 | CE: 0.0958 | KD: 1061.1620\n",
      "Train Epoch: 079 Batch: 00010/00094 | Loss: 98.3788 | CE: 0.0836 | KD: 1060.9194\n",
      "Train Epoch: 079 Batch: 00011/00094 | Loss: 98.4042 | CE: 0.1032 | KD: 1060.9813\n",
      "Train Epoch: 079 Batch: 00012/00094 | Loss: 98.4047 | CE: 0.0891 | KD: 1061.1390\n",
      "Train Epoch: 079 Batch: 00013/00094 | Loss: 98.4300 | CE: 0.1185 | KD: 1061.0952\n",
      "Train Epoch: 079 Batch: 00014/00094 | Loss: 98.4249 | CE: 0.1043 | KD: 1061.1930\n",
      "Train Epoch: 079 Batch: 00015/00094 | Loss: 98.3396 | CE: 0.0701 | KD: 1060.6414\n",
      "Train Epoch: 079 Batch: 00016/00094 | Loss: 98.4416 | CE: 0.1166 | KD: 1061.2411\n",
      "Train Epoch: 079 Batch: 00017/00094 | Loss: 98.3677 | CE: 0.0823 | KD: 1060.8134\n",
      "Train Epoch: 079 Batch: 00018/00094 | Loss: 98.3758 | CE: 0.0635 | KD: 1061.1040\n",
      "Train Epoch: 079 Batch: 00019/00094 | Loss: 98.4051 | CE: 0.1033 | KD: 1060.9899\n",
      "Train Epoch: 079 Batch: 00020/00094 | Loss: 98.3968 | CE: 0.0992 | KD: 1060.9454\n",
      "Train Epoch: 079 Batch: 00021/00094 | Loss: 98.4572 | CE: 0.1056 | KD: 1061.5269\n",
      "Train Epoch: 079 Batch: 00022/00094 | Loss: 98.4236 | CE: 0.0670 | KD: 1061.5815\n",
      "Train Epoch: 079 Batch: 00023/00094 | Loss: 98.3659 | CE: 0.0610 | KD: 1061.0232\n",
      "Train Epoch: 079 Batch: 00024/00094 | Loss: 98.3624 | CE: 0.0555 | KD: 1061.0454\n",
      "Train Epoch: 079 Batch: 00025/00094 | Loss: 98.3780 | CE: 0.0677 | KD: 1061.0822\n",
      "Train Epoch: 079 Batch: 00026/00094 | Loss: 98.4291 | CE: 0.1125 | KD: 1061.1495\n",
      "Train Epoch: 079 Batch: 00027/00094 | Loss: 98.3543 | CE: 0.0566 | KD: 1060.9456\n",
      "Train Epoch: 079 Batch: 00028/00094 | Loss: 98.3893 | CE: 0.0521 | KD: 1061.3719\n",
      "Train Epoch: 079 Batch: 00029/00094 | Loss: 98.3878 | CE: 0.0551 | KD: 1061.3242\n",
      "Train Epoch: 079 Batch: 00030/00094 | Loss: 98.4464 | CE: 0.0990 | KD: 1061.4824\n",
      "Train Epoch: 079 Batch: 00031/00094 | Loss: 98.3838 | CE: 0.0490 | KD: 1061.3458\n",
      "Train Epoch: 079 Batch: 00032/00094 | Loss: 98.3851 | CE: 0.0439 | KD: 1061.4160\n",
      "Train Epoch: 079 Batch: 00033/00094 | Loss: 98.4272 | CE: 0.0754 | KD: 1061.5292\n",
      "Train Epoch: 079 Batch: 00034/00094 | Loss: 98.4747 | CE: 0.1138 | KD: 1061.6273\n",
      "Train Epoch: 079 Batch: 00035/00094 | Loss: 98.3912 | CE: 0.0610 | KD: 1061.2964\n",
      "Train Epoch: 079 Batch: 00036/00094 | Loss: 98.3515 | CE: 0.0424 | KD: 1061.0696\n",
      "Train Epoch: 079 Batch: 00037/00094 | Loss: 98.4210 | CE: 0.0970 | KD: 1061.2291\n",
      "Train Epoch: 079 Batch: 00038/00094 | Loss: 98.3655 | CE: 0.0777 | KD: 1060.8386\n",
      "Train Epoch: 079 Batch: 00039/00094 | Loss: 98.3548 | CE: 0.0584 | KD: 1060.9309\n",
      "Train Epoch: 079 Batch: 00040/00094 | Loss: 98.4894 | CE: 0.1692 | KD: 1061.1884\n",
      "Train Epoch: 079 Batch: 00041/00094 | Loss: 98.4067 | CE: 0.1142 | KD: 1060.8904\n",
      "Train Epoch: 079 Batch: 00042/00094 | Loss: 98.4594 | CE: 0.1200 | KD: 1061.3958\n",
      "Train Epoch: 079 Batch: 00043/00094 | Loss: 98.3844 | CE: 0.0869 | KD: 1060.9432\n",
      "Train Epoch: 079 Batch: 00044/00094 | Loss: 98.4323 | CE: 0.0915 | KD: 1061.4105\n",
      "Train Epoch: 079 Batch: 00045/00094 | Loss: 98.3750 | CE: 0.0844 | KD: 1060.8688\n",
      "Train Epoch: 079 Batch: 00046/00094 | Loss: 98.3939 | CE: 0.0788 | KD: 1061.1335\n",
      "Train Epoch: 079 Batch: 00047/00094 | Loss: 98.4081 | CE: 0.1114 | KD: 1060.9351\n",
      "Train Epoch: 079 Batch: 00048/00094 | Loss: 98.3906 | CE: 0.0846 | KD: 1061.0349\n",
      "Train Epoch: 079 Batch: 00049/00094 | Loss: 98.3224 | CE: 0.0554 | KD: 1060.6151\n",
      "Train Epoch: 079 Batch: 00050/00094 | Loss: 98.4291 | CE: 0.0967 | KD: 1061.3197\n",
      "Train Epoch: 079 Batch: 00051/00094 | Loss: 98.4003 | CE: 0.0759 | KD: 1061.2343\n",
      "Train Epoch: 079 Batch: 00052/00094 | Loss: 98.4207 | CE: 0.0829 | KD: 1061.3788\n",
      "Train Epoch: 079 Batch: 00053/00094 | Loss: 98.3572 | CE: 0.0542 | KD: 1061.0034\n",
      "Train Epoch: 079 Batch: 00054/00094 | Loss: 98.4189 | CE: 0.1263 | KD: 1060.8905\n",
      "Train Epoch: 079 Batch: 00055/00094 | Loss: 98.3444 | CE: 0.0495 | KD: 1060.9156\n",
      "Train Epoch: 079 Batch: 00056/00094 | Loss: 98.4164 | CE: 0.1053 | KD: 1061.0907\n",
      "Train Epoch: 079 Batch: 00057/00094 | Loss: 98.4340 | CE: 0.0973 | KD: 1061.3676\n",
      "Train Epoch: 079 Batch: 00058/00094 | Loss: 98.4425 | CE: 0.1125 | KD: 1061.2937\n",
      "Train Epoch: 079 Batch: 00059/00094 | Loss: 98.3715 | CE: 0.0577 | KD: 1061.1194\n",
      "Train Epoch: 079 Batch: 00060/00094 | Loss: 98.3614 | CE: 0.0486 | KD: 1061.1089\n",
      "Train Epoch: 079 Batch: 00061/00094 | Loss: 98.3538 | CE: 0.0491 | KD: 1061.0219\n",
      "Train Epoch: 079 Batch: 00062/00094 | Loss: 98.3485 | CE: 0.0453 | KD: 1061.0046\n",
      "Train Epoch: 079 Batch: 00063/00094 | Loss: 98.3771 | CE: 0.0592 | KD: 1061.1636\n",
      "Train Epoch: 079 Batch: 00064/00094 | Loss: 98.3583 | CE: 0.0528 | KD: 1061.0300\n",
      "Train Epoch: 079 Batch: 00065/00094 | Loss: 98.3953 | CE: 0.1106 | KD: 1060.8058\n",
      "Train Epoch: 079 Batch: 00066/00094 | Loss: 98.3571 | CE: 0.0463 | KD: 1061.0875\n",
      "Train Epoch: 079 Batch: 00067/00094 | Loss: 98.3689 | CE: 0.0744 | KD: 1060.9109\n",
      "Train Epoch: 079 Batch: 00068/00094 | Loss: 98.3710 | CE: 0.0557 | KD: 1061.1351\n",
      "Train Epoch: 079 Batch: 00069/00094 | Loss: 98.3732 | CE: 0.0682 | KD: 1061.0247\n",
      "Train Epoch: 079 Batch: 00070/00094 | Loss: 98.3830 | CE: 0.0802 | KD: 1061.0005\n",
      "Train Epoch: 079 Batch: 00071/00094 | Loss: 98.3371 | CE: 0.0634 | KD: 1060.6863\n",
      "Train Epoch: 079 Batch: 00072/00094 | Loss: 98.3393 | CE: 0.0618 | KD: 1060.7279\n",
      "Train Epoch: 079 Batch: 00073/00094 | Loss: 98.3861 | CE: 0.0500 | KD: 1061.3602\n",
      "Train Epoch: 079 Batch: 00074/00094 | Loss: 98.4728 | CE: 0.0846 | KD: 1061.9221\n",
      "Train Epoch: 079 Batch: 00075/00094 | Loss: 98.4293 | CE: 0.0631 | KD: 1061.6851\n",
      "Train Epoch: 079 Batch: 00076/00094 | Loss: 98.3858 | CE: 0.0913 | KD: 1060.9114\n",
      "Train Epoch: 079 Batch: 00077/00094 | Loss: 98.3425 | CE: 0.0463 | KD: 1060.9302\n",
      "Train Epoch: 079 Batch: 00078/00094 | Loss: 98.3548 | CE: 0.0623 | KD: 1060.8889\n",
      "Train Epoch: 079 Batch: 00079/00094 | Loss: 98.3400 | CE: 0.0486 | KD: 1060.8781\n",
      "Train Epoch: 079 Batch: 00080/00094 | Loss: 98.4573 | CE: 0.0962 | KD: 1061.6296\n",
      "Train Epoch: 079 Batch: 00081/00094 | Loss: 98.4206 | CE: 0.0595 | KD: 1061.6304\n",
      "Train Epoch: 079 Batch: 00082/00094 | Loss: 98.3746 | CE: 0.0642 | KD: 1061.0824\n",
      "Train Epoch: 079 Batch: 00083/00094 | Loss: 98.4106 | CE: 0.0883 | KD: 1061.2111\n",
      "Train Epoch: 079 Batch: 00084/00094 | Loss: 98.4850 | CE: 0.1684 | KD: 1061.1501\n",
      "Train Epoch: 079 Batch: 00085/00094 | Loss: 98.3502 | CE: 0.0746 | KD: 1060.7074\n",
      "Train Epoch: 079 Batch: 00086/00094 | Loss: 98.3576 | CE: 0.0468 | KD: 1061.0867\n",
      "Train Epoch: 079 Batch: 00087/00094 | Loss: 98.4056 | CE: 0.0690 | KD: 1061.3655\n",
      "Train Epoch: 079 Batch: 00088/00094 | Loss: 98.4386 | CE: 0.1461 | KD: 1060.8899\n",
      "Train Epoch: 079 Batch: 00089/00094 | Loss: 98.4172 | CE: 0.0794 | KD: 1061.3785\n",
      "Train Epoch: 079 Batch: 00090/00094 | Loss: 98.3979 | CE: 0.0540 | KD: 1061.4449\n",
      "Train Epoch: 079 Batch: 00091/00094 | Loss: 98.3898 | CE: 0.0569 | KD: 1061.3256\n",
      "Train Epoch: 079 Batch: 00092/00094 | Loss: 98.3076 | CE: 0.0485 | KD: 1060.5286\n",
      "Train Epoch: 079 Batch: 00093/00094 | Loss: 98.3472 | CE: 0.0592 | KD: 1060.8405\n",
      "Train Epoch: 079 Batch: 00094/00094 | Loss: 98.4160 | CE: 0.1169 | KD: 1060.9612\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0736 | acc:98.7500\n",
      "[VAL Acc] Target: 98.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7745 | acc:49.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2411 | acc:52.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2863 | acc:44.2748\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4421 | acc:54.5846\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6639 | acc:69.7782\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8328 | acc:69.5925\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.59%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9472 | acc:58.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.38%\n",
      "[VAL Acc] Avg 62.16%\n",
      "Train Epoch: 080 Batch: 00001/00094 | Loss: 98.3712 | CE: 0.0607 | KD: 1061.0841\n",
      "Train Epoch: 080 Batch: 00002/00094 | Loss: 98.4139 | CE: 0.1074 | KD: 1061.0406\n",
      "Train Epoch: 080 Batch: 00003/00094 | Loss: 98.3736 | CE: 0.0938 | KD: 1060.7527\n",
      "Train Epoch: 080 Batch: 00004/00094 | Loss: 98.3776 | CE: 0.0745 | KD: 1061.0045\n",
      "Train Epoch: 080 Batch: 00005/00094 | Loss: 98.3666 | CE: 0.0655 | KD: 1060.9824\n",
      "Train Epoch: 080 Batch: 00006/00094 | Loss: 98.3573 | CE: 0.0680 | KD: 1060.8553\n",
      "Train Epoch: 080 Batch: 00007/00094 | Loss: 98.4714 | CE: 0.1348 | KD: 1061.3655\n",
      "Train Epoch: 080 Batch: 00008/00094 | Loss: 98.3874 | CE: 0.0706 | KD: 1061.1526\n",
      "Train Epoch: 080 Batch: 00009/00094 | Loss: 98.3662 | CE: 0.0512 | KD: 1061.1326\n",
      "Train Epoch: 080 Batch: 00010/00094 | Loss: 98.4037 | CE: 0.0703 | KD: 1061.3311\n",
      "Train Epoch: 080 Batch: 00011/00094 | Loss: 98.4076 | CE: 0.0602 | KD: 1061.4829\n",
      "Train Epoch: 080 Batch: 00012/00094 | Loss: 98.4334 | CE: 0.1288 | KD: 1061.0201\n",
      "Train Epoch: 080 Batch: 00013/00094 | Loss: 98.4207 | CE: 0.1068 | KD: 1061.1207\n",
      "Train Epoch: 080 Batch: 00014/00094 | Loss: 98.4115 | CE: 0.1009 | KD: 1061.0856\n",
      "Train Epoch: 080 Batch: 00015/00094 | Loss: 98.3559 | CE: 0.0702 | KD: 1060.8164\n",
      "Train Epoch: 080 Batch: 00016/00094 | Loss: 98.3943 | CE: 0.0610 | KD: 1061.3304\n",
      "Train Epoch: 080 Batch: 00017/00094 | Loss: 98.3757 | CE: 0.0496 | KD: 1061.2528\n",
      "Train Epoch: 080 Batch: 00018/00094 | Loss: 98.3520 | CE: 0.0481 | KD: 1061.0129\n",
      "Train Epoch: 080 Batch: 00019/00094 | Loss: 98.3979 | CE: 0.0565 | KD: 1061.4178\n",
      "Train Epoch: 080 Batch: 00020/00094 | Loss: 98.4188 | CE: 0.0909 | KD: 1061.2719\n",
      "Train Epoch: 080 Batch: 00021/00094 | Loss: 98.3511 | CE: 0.0754 | KD: 1060.7085\n",
      "Train Epoch: 080 Batch: 00022/00094 | Loss: 98.4214 | CE: 0.0789 | KD: 1061.4297\n",
      "Train Epoch: 080 Batch: 00023/00094 | Loss: 98.3637 | CE: 0.0471 | KD: 1061.1493\n",
      "Train Epoch: 080 Batch: 00024/00094 | Loss: 98.4003 | CE: 0.0582 | KD: 1061.4253\n",
      "Train Epoch: 080 Batch: 00025/00094 | Loss: 98.3774 | CE: 0.0760 | KD: 1060.9855\n",
      "Train Epoch: 080 Batch: 00026/00094 | Loss: 98.3655 | CE: 0.0425 | KD: 1061.2190\n",
      "Train Epoch: 080 Batch: 00027/00094 | Loss: 98.3932 | CE: 0.0754 | KD: 1061.1635\n",
      "Train Epoch: 080 Batch: 00028/00094 | Loss: 98.3589 | CE: 0.0525 | KD: 1061.0404\n",
      "Train Epoch: 080 Batch: 00029/00094 | Loss: 98.3177 | CE: 0.0465 | KD: 1060.6594\n",
      "Train Epoch: 080 Batch: 00030/00094 | Loss: 98.3654 | CE: 0.0470 | KD: 1061.1694\n",
      "Train Epoch: 080 Batch: 00031/00094 | Loss: 98.4242 | CE: 0.0787 | KD: 1061.4619\n",
      "Train Epoch: 080 Batch: 00032/00094 | Loss: 98.4164 | CE: 0.0691 | KD: 1061.4811\n",
      "Train Epoch: 080 Batch: 00033/00094 | Loss: 98.3863 | CE: 0.0579 | KD: 1061.2773\n",
      "Train Epoch: 080 Batch: 00034/00094 | Loss: 98.3301 | CE: 0.0486 | KD: 1060.7715\n",
      "Train Epoch: 080 Batch: 00035/00094 | Loss: 98.3767 | CE: 0.0663 | KD: 1061.0834\n",
      "Train Epoch: 080 Batch: 00036/00094 | Loss: 98.4136 | CE: 0.0629 | KD: 1061.5183\n",
      "Train Epoch: 080 Batch: 00037/00094 | Loss: 98.4091 | CE: 0.0824 | KD: 1061.2593\n",
      "Train Epoch: 080 Batch: 00038/00094 | Loss: 98.4252 | CE: 0.0963 | KD: 1061.2827\n",
      "Train Epoch: 080 Batch: 00039/00094 | Loss: 98.3627 | CE: 0.0506 | KD: 1061.1013\n",
      "Train Epoch: 080 Batch: 00040/00094 | Loss: 98.4246 | CE: 0.0811 | KD: 1061.4409\n",
      "Train Epoch: 080 Batch: 00041/00094 | Loss: 98.3464 | CE: 0.0572 | KD: 1060.8540\n",
      "Train Epoch: 080 Batch: 00042/00094 | Loss: 98.3949 | CE: 0.0715 | KD: 1061.2227\n",
      "Train Epoch: 080 Batch: 00043/00094 | Loss: 98.4717 | CE: 0.1284 | KD: 1061.4385\n",
      "Train Epoch: 080 Batch: 00044/00094 | Loss: 98.4204 | CE: 0.0607 | KD: 1061.6144\n",
      "Train Epoch: 080 Batch: 00045/00094 | Loss: 98.4287 | CE: 0.1026 | KD: 1061.2524\n",
      "Train Epoch: 080 Batch: 00046/00094 | Loss: 98.4066 | CE: 0.0788 | KD: 1061.2701\n",
      "Train Epoch: 080 Batch: 00047/00094 | Loss: 98.3819 | CE: 0.1039 | KD: 1060.7329\n",
      "Train Epoch: 080 Batch: 00048/00094 | Loss: 98.3998 | CE: 0.0694 | KD: 1061.2983\n",
      "Train Epoch: 080 Batch: 00049/00094 | Loss: 98.3595 | CE: 0.0667 | KD: 1060.8925\n",
      "Train Epoch: 080 Batch: 00050/00094 | Loss: 98.4224 | CE: 0.0816 | KD: 1061.4102\n",
      "Train Epoch: 080 Batch: 00051/00094 | Loss: 98.3702 | CE: 0.0688 | KD: 1060.9856\n",
      "Train Epoch: 080 Batch: 00052/00094 | Loss: 98.4579 | CE: 0.1257 | KD: 1061.3182\n",
      "Train Epoch: 080 Batch: 00053/00094 | Loss: 98.3526 | CE: 0.0643 | KD: 1060.8447\n",
      "Train Epoch: 080 Batch: 00054/00094 | Loss: 98.4367 | CE: 0.1192 | KD: 1061.1589\n",
      "Train Epoch: 080 Batch: 00055/00094 | Loss: 98.3543 | CE: 0.0539 | KD: 1060.9755\n",
      "Train Epoch: 080 Batch: 00056/00094 | Loss: 98.3426 | CE: 0.0455 | KD: 1060.9401\n",
      "Train Epoch: 080 Batch: 00057/00094 | Loss: 98.3354 | CE: 0.0550 | KD: 1060.7592\n",
      "Train Epoch: 080 Batch: 00058/00094 | Loss: 98.4044 | CE: 0.0700 | KD: 1061.3420\n",
      "Train Epoch: 080 Batch: 00059/00094 | Loss: 98.3989 | CE: 0.0746 | KD: 1061.2329\n",
      "Train Epoch: 080 Batch: 00060/00094 | Loss: 98.3710 | CE: 0.0671 | KD: 1061.0121\n",
      "Train Epoch: 080 Batch: 00061/00094 | Loss: 98.3913 | CE: 0.0590 | KD: 1061.3188\n",
      "Train Epoch: 080 Batch: 00062/00094 | Loss: 98.3555 | CE: 0.0523 | KD: 1061.0056\n",
      "Train Epoch: 080 Batch: 00063/00094 | Loss: 98.4520 | CE: 0.0691 | KD: 1061.8655\n",
      "Train Epoch: 080 Batch: 00064/00094 | Loss: 98.4681 | CE: 0.1510 | KD: 1061.1549\n",
      "Train Epoch: 080 Batch: 00065/00094 | Loss: 98.4147 | CE: 0.0820 | KD: 1061.3240\n",
      "Train Epoch: 080 Batch: 00066/00094 | Loss: 98.3598 | CE: 0.0591 | KD: 1060.9773\n",
      "Train Epoch: 080 Batch: 00067/00094 | Loss: 98.3700 | CE: 0.0522 | KD: 1061.1633\n",
      "Train Epoch: 080 Batch: 00068/00094 | Loss: 98.4108 | CE: 0.0821 | KD: 1061.2805\n",
      "Train Epoch: 080 Batch: 00069/00094 | Loss: 98.4421 | CE: 0.1115 | KD: 1061.3004\n",
      "Train Epoch: 080 Batch: 00070/00094 | Loss: 98.4020 | CE: 0.0664 | KD: 1061.3550\n",
      "Train Epoch: 080 Batch: 00071/00094 | Loss: 98.3797 | CE: 0.0727 | KD: 1061.0452\n",
      "Train Epoch: 080 Batch: 00072/00094 | Loss: 98.3709 | CE: 0.0412 | KD: 1061.2914\n",
      "Train Epoch: 080 Batch: 00073/00094 | Loss: 98.3790 | CE: 0.0540 | KD: 1061.2410\n",
      "Train Epoch: 080 Batch: 00074/00094 | Loss: 98.3446 | CE: 0.0683 | KD: 1060.7147\n",
      "Train Epoch: 080 Batch: 00075/00094 | Loss: 98.4151 | CE: 0.0550 | KD: 1061.6198\n",
      "Train Epoch: 080 Batch: 00076/00094 | Loss: 98.3682 | CE: 0.0568 | KD: 1061.0944\n",
      "Train Epoch: 080 Batch: 00077/00094 | Loss: 98.3685 | CE: 0.0512 | KD: 1061.1578\n",
      "Train Epoch: 080 Batch: 00078/00094 | Loss: 98.3811 | CE: 0.0539 | KD: 1061.2643\n",
      "Train Epoch: 080 Batch: 00079/00094 | Loss: 98.4302 | CE: 0.0586 | KD: 1061.7434\n",
      "Train Epoch: 080 Batch: 00080/00094 | Loss: 98.3932 | CE: 0.1040 | KD: 1060.8542\n",
      "Train Epoch: 080 Batch: 00081/00094 | Loss: 98.4328 | CE: 0.0876 | KD: 1061.4595\n",
      "Train Epoch: 080 Batch: 00082/00094 | Loss: 98.4016 | CE: 0.0987 | KD: 1061.0023\n",
      "Train Epoch: 080 Batch: 00083/00094 | Loss: 98.4160 | CE: 0.0867 | KD: 1061.2865\n",
      "Train Epoch: 080 Batch: 00084/00094 | Loss: 98.4298 | CE: 0.0829 | KD: 1061.4773\n",
      "Train Epoch: 080 Batch: 00085/00094 | Loss: 98.3937 | CE: 0.0977 | KD: 1060.9274\n",
      "Train Epoch: 080 Batch: 00086/00094 | Loss: 98.3436 | CE: 0.0705 | KD: 1060.6803\n",
      "Train Epoch: 080 Batch: 00087/00094 | Loss: 98.3754 | CE: 0.1067 | KD: 1060.6329\n",
      "Train Epoch: 080 Batch: 00088/00094 | Loss: 98.3198 | CE: 0.0423 | KD: 1060.7274\n",
      "Train Epoch: 080 Batch: 00089/00094 | Loss: 98.3644 | CE: 0.0537 | KD: 1061.0858\n",
      "Train Epoch: 080 Batch: 00090/00094 | Loss: 98.3988 | CE: 0.0825 | KD: 1061.1458\n",
      "Train Epoch: 080 Batch: 00091/00094 | Loss: 98.4152 | CE: 0.0783 | KD: 1061.3688\n",
      "Train Epoch: 080 Batch: 00092/00094 | Loss: 98.4590 | CE: 0.0802 | KD: 1061.8218\n",
      "Train Epoch: 080 Batch: 00093/00094 | Loss: 98.6311 | CE: 0.2850 | KD: 1061.4683\n",
      "Train Epoch: 080 Batch: 00094/00094 | Loss: 98.3944 | CE: 0.0443 | KD: 1061.5117\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1105 | acc:96.3500\n",
      "[VAL Acc] Target: 96.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6047 | acc:48.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0798 | acc:53.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1531 | acc:46.7557\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1954 | acc:55.9953\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.2582 | acc:52.3105\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7273 | acc:70.2194\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0242 | acc:56.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.94%\n",
      "[VAL Acc] Avg 60.09%\n",
      "Train Epoch: 081 Batch: 00001/00094 | Loss: 88.6049 | CE: 0.1087 | KD: 1061.2850\n",
      "Train Epoch: 081 Batch: 00002/00094 | Loss: 88.5611 | CE: 0.0669 | KD: 1061.2617\n",
      "Train Epoch: 081 Batch: 00003/00094 | Loss: 88.5528 | CE: 0.0651 | KD: 1061.1836\n",
      "Train Epoch: 081 Batch: 00004/00094 | Loss: 88.5398 | CE: 0.0597 | KD: 1061.0916\n",
      "Train Epoch: 081 Batch: 00005/00094 | Loss: 88.6743 | CE: 0.1758 | KD: 1061.3130\n",
      "Train Epoch: 081 Batch: 00006/00094 | Loss: 88.6190 | CE: 0.0811 | KD: 1061.7848\n",
      "Train Epoch: 081 Batch: 00007/00094 | Loss: 88.5528 | CE: 0.0639 | KD: 1061.1980\n",
      "Train Epoch: 081 Batch: 00008/00094 | Loss: 88.5485 | CE: 0.1155 | KD: 1060.5271\n",
      "Train Epoch: 081 Batch: 00009/00094 | Loss: 88.6320 | CE: 0.1224 | KD: 1061.4449\n",
      "Train Epoch: 081 Batch: 00010/00094 | Loss: 88.5773 | CE: 0.0713 | KD: 1061.4026\n",
      "Train Epoch: 081 Batch: 00011/00094 | Loss: 88.5572 | CE: 0.0805 | KD: 1061.0513\n",
      "Train Epoch: 081 Batch: 00012/00094 | Loss: 88.5779 | CE: 0.0940 | KD: 1061.1368\n",
      "Train Epoch: 081 Batch: 00013/00094 | Loss: 88.5357 | CE: 0.0547 | KD: 1061.1017\n",
      "Train Epoch: 081 Batch: 00014/00094 | Loss: 88.5234 | CE: 0.0550 | KD: 1060.9519\n",
      "Train Epoch: 081 Batch: 00015/00094 | Loss: 88.5724 | CE: 0.1069 | KD: 1060.9170\n",
      "Train Epoch: 081 Batch: 00016/00094 | Loss: 88.6188 | CE: 0.1322 | KD: 1061.1694\n",
      "Train Epoch: 081 Batch: 00017/00094 | Loss: 88.5650 | CE: 0.0690 | KD: 1061.2831\n",
      "Train Epoch: 081 Batch: 00018/00094 | Loss: 88.6949 | CE: 0.2027 | KD: 1061.2361\n",
      "Train Epoch: 081 Batch: 00019/00094 | Loss: 88.5368 | CE: 0.0528 | KD: 1061.1379\n",
      "Train Epoch: 081 Batch: 00020/00094 | Loss: 88.5785 | CE: 0.1204 | KD: 1060.8287\n",
      "Train Epoch: 081 Batch: 00021/00094 | Loss: 88.6011 | CE: 0.1160 | KD: 1061.1519\n",
      "Train Epoch: 081 Batch: 00022/00094 | Loss: 88.5323 | CE: 0.0769 | KD: 1060.7958\n",
      "Train Epoch: 081 Batch: 00023/00094 | Loss: 88.5269 | CE: 0.0653 | KD: 1060.8699\n",
      "Train Epoch: 081 Batch: 00024/00094 | Loss: 88.5054 | CE: 0.0612 | KD: 1060.6610\n",
      "Train Epoch: 081 Batch: 00025/00094 | Loss: 88.5678 | CE: 0.0794 | KD: 1061.1906\n",
      "Train Epoch: 081 Batch: 00026/00094 | Loss: 88.6050 | CE: 0.0940 | KD: 1061.4620\n",
      "Train Epoch: 081 Batch: 00027/00094 | Loss: 88.5592 | CE: 0.0561 | KD: 1061.3674\n",
      "Train Epoch: 081 Batch: 00028/00094 | Loss: 88.5523 | CE: 0.0738 | KD: 1061.0724\n",
      "Train Epoch: 081 Batch: 00029/00094 | Loss: 88.5544 | CE: 0.0682 | KD: 1061.1646\n",
      "Train Epoch: 081 Batch: 00030/00094 | Loss: 88.5335 | CE: 0.0542 | KD: 1061.0828\n",
      "Train Epoch: 081 Batch: 00031/00094 | Loss: 88.5622 | CE: 0.0617 | KD: 1061.3367\n",
      "Train Epoch: 081 Batch: 00032/00094 | Loss: 88.5341 | CE: 0.0543 | KD: 1061.0886\n",
      "Train Epoch: 081 Batch: 00033/00094 | Loss: 88.5087 | CE: 0.0445 | KD: 1060.9016\n",
      "Train Epoch: 081 Batch: 00034/00094 | Loss: 88.5728 | CE: 0.0665 | KD: 1061.4060\n",
      "Train Epoch: 081 Batch: 00035/00094 | Loss: 88.5331 | CE: 0.0438 | KD: 1061.2019\n",
      "Train Epoch: 081 Batch: 00036/00094 | Loss: 88.5764 | CE: 0.0789 | KD: 1061.3002\n",
      "Train Epoch: 081 Batch: 00037/00094 | Loss: 88.5349 | CE: 0.0776 | KD: 1060.8174\n",
      "Train Epoch: 081 Batch: 00038/00094 | Loss: 88.5627 | CE: 0.0691 | KD: 1061.2531\n",
      "Train Epoch: 081 Batch: 00039/00094 | Loss: 88.5374 | CE: 0.0727 | KD: 1060.9064\n",
      "Train Epoch: 081 Batch: 00040/00094 | Loss: 88.5487 | CE: 0.0596 | KD: 1061.1991\n",
      "Train Epoch: 081 Batch: 00041/00094 | Loss: 88.5505 | CE: 0.0906 | KD: 1060.8494\n",
      "Train Epoch: 081 Batch: 00042/00094 | Loss: 88.5731 | CE: 0.1195 | KD: 1060.7739\n",
      "Train Epoch: 081 Batch: 00043/00094 | Loss: 88.5453 | CE: 0.0764 | KD: 1060.9578\n",
      "Train Epoch: 081 Batch: 00044/00094 | Loss: 88.5546 | CE: 0.0864 | KD: 1060.9495\n",
      "Train Epoch: 081 Batch: 00045/00094 | Loss: 88.5636 | CE: 0.0710 | KD: 1061.2418\n",
      "Train Epoch: 081 Batch: 00046/00094 | Loss: 88.5139 | CE: 0.0439 | KD: 1060.9712\n",
      "Train Epoch: 081 Batch: 00047/00094 | Loss: 88.5652 | CE: 0.0945 | KD: 1060.9794\n",
      "Train Epoch: 081 Batch: 00048/00094 | Loss: 88.5333 | CE: 0.0544 | KD: 1061.0776\n",
      "Train Epoch: 081 Batch: 00049/00094 | Loss: 88.5731 | CE: 0.0892 | KD: 1061.1376\n",
      "Train Epoch: 081 Batch: 00050/00094 | Loss: 88.6149 | CE: 0.1010 | KD: 1061.4966\n",
      "Train Epoch: 081 Batch: 00051/00094 | Loss: 88.5419 | CE: 0.0789 | KD: 1060.8871\n",
      "Train Epoch: 081 Batch: 00052/00094 | Loss: 88.5213 | CE: 0.0420 | KD: 1061.0820\n",
      "Train Epoch: 081 Batch: 00053/00094 | Loss: 88.5348 | CE: 0.0704 | KD: 1060.9045\n",
      "Train Epoch: 081 Batch: 00054/00094 | Loss: 88.5261 | CE: 0.0444 | KD: 1061.1101\n",
      "Train Epoch: 081 Batch: 00055/00094 | Loss: 88.5495 | CE: 0.0810 | KD: 1060.9534\n",
      "Train Epoch: 081 Batch: 00056/00094 | Loss: 88.5919 | CE: 0.0671 | KD: 1061.6285\n",
      "Train Epoch: 081 Batch: 00057/00094 | Loss: 88.5525 | CE: 0.0443 | KD: 1061.4293\n",
      "Train Epoch: 081 Batch: 00058/00094 | Loss: 88.5301 | CE: 0.0679 | KD: 1060.8772\n",
      "Train Epoch: 081 Batch: 00059/00094 | Loss: 88.5248 | CE: 0.0507 | KD: 1061.0198\n",
      "Train Epoch: 081 Batch: 00060/00094 | Loss: 88.5392 | CE: 0.0567 | KD: 1061.1200\n",
      "Train Epoch: 081 Batch: 00061/00094 | Loss: 88.5262 | CE: 0.0731 | KD: 1060.7676\n",
      "Train Epoch: 081 Batch: 00062/00094 | Loss: 88.5553 | CE: 0.0476 | KD: 1061.4218\n",
      "Train Epoch: 081 Batch: 00063/00094 | Loss: 88.5328 | CE: 0.0528 | KD: 1061.0908\n",
      "Train Epoch: 081 Batch: 00064/00094 | Loss: 88.5376 | CE: 0.0724 | KD: 1060.9128\n",
      "Train Epoch: 081 Batch: 00065/00094 | Loss: 88.5845 | CE: 0.0873 | KD: 1061.2959\n",
      "Train Epoch: 081 Batch: 00066/00094 | Loss: 88.5233 | CE: 0.0525 | KD: 1060.9808\n",
      "Train Epoch: 081 Batch: 00067/00094 | Loss: 88.5496 | CE: 0.0611 | KD: 1061.1923\n",
      "Train Epoch: 081 Batch: 00068/00094 | Loss: 88.6020 | CE: 0.1007 | KD: 1061.3456\n",
      "Train Epoch: 081 Batch: 00069/00094 | Loss: 88.5698 | CE: 0.0534 | KD: 1061.5260\n",
      "Train Epoch: 081 Batch: 00070/00094 | Loss: 88.5405 | CE: 0.0591 | KD: 1061.1074\n",
      "Train Epoch: 081 Batch: 00071/00094 | Loss: 88.5340 | CE: 0.0515 | KD: 1061.1207\n",
      "Train Epoch: 081 Batch: 00072/00094 | Loss: 88.5652 | CE: 0.0765 | KD: 1061.1949\n",
      "Train Epoch: 081 Batch: 00073/00094 | Loss: 88.5597 | CE: 0.0909 | KD: 1060.9569\n",
      "Train Epoch: 081 Batch: 00074/00094 | Loss: 88.5456 | CE: 0.0652 | KD: 1061.0962\n",
      "Train Epoch: 081 Batch: 00075/00094 | Loss: 88.5483 | CE: 0.0727 | KD: 1061.0385\n",
      "Train Epoch: 081 Batch: 00076/00094 | Loss: 88.5151 | CE: 0.0480 | KD: 1060.9362\n",
      "Train Epoch: 081 Batch: 00077/00094 | Loss: 88.5372 | CE: 0.0787 | KD: 1060.8330\n",
      "Train Epoch: 081 Batch: 00078/00094 | Loss: 88.5863 | CE: 0.0871 | KD: 1061.3204\n",
      "Train Epoch: 081 Batch: 00079/00094 | Loss: 88.5808 | CE: 0.0901 | KD: 1061.2191\n",
      "Train Epoch: 081 Batch: 00080/00094 | Loss: 88.5755 | CE: 0.0771 | KD: 1061.3110\n",
      "Train Epoch: 081 Batch: 00081/00094 | Loss: 88.5752 | CE: 0.1055 | KD: 1060.9673\n",
      "Train Epoch: 081 Batch: 00082/00094 | Loss: 88.5255 | CE: 0.0516 | KD: 1061.0173\n",
      "Train Epoch: 081 Batch: 00083/00094 | Loss: 88.5451 | CE: 0.0966 | KD: 1060.7131\n",
      "Train Epoch: 081 Batch: 00084/00094 | Loss: 88.5269 | CE: 0.0787 | KD: 1060.7095\n",
      "Train Epoch: 081 Batch: 00085/00094 | Loss: 88.4953 | CE: 0.0497 | KD: 1060.6783\n",
      "Train Epoch: 081 Batch: 00086/00094 | Loss: 88.4989 | CE: 0.0414 | KD: 1060.8213\n",
      "Train Epoch: 081 Batch: 00087/00094 | Loss: 88.5299 | CE: 0.0563 | KD: 1061.0128\n",
      "Train Epoch: 081 Batch: 00088/00094 | Loss: 88.5248 | CE: 0.0522 | KD: 1061.0016\n",
      "Train Epoch: 081 Batch: 00089/00094 | Loss: 88.5432 | CE: 0.0535 | KD: 1061.2062\n",
      "Train Epoch: 081 Batch: 00090/00094 | Loss: 88.5865 | CE: 0.0743 | KD: 1061.4766\n",
      "Train Epoch: 081 Batch: 00091/00094 | Loss: 88.5883 | CE: 0.0763 | KD: 1061.4739\n",
      "Train Epoch: 081 Batch: 00092/00094 | Loss: 88.5983 | CE: 0.1280 | KD: 1060.9734\n",
      "Train Epoch: 081 Batch: 00093/00094 | Loss: 88.5463 | CE: 0.0507 | KD: 1061.2781\n",
      "Train Epoch: 081 Batch: 00094/00094 | Loss: 88.5338 | CE: 0.0748 | KD: 1060.8389\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0673 | acc:98.6500\n",
      "[VAL Acc] Target: 98.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7769 | acc:49.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2576 | acc:53.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3326 | acc:44.0840\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9614 | acc:63.4796\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 63.48%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7671 | acc:64.9723\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 64.97%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5567 | acc:77.8605\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 77.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9609 | acc:58.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.63%\n",
      "[VAL Acc] Avg 63.81%\n",
      "Train Epoch: 082 Batch: 00001/00094 | Loss: 88.5102 | CE: 0.0463 | KD: 1060.8976\n",
      "Train Epoch: 082 Batch: 00002/00094 | Loss: 88.6534 | CE: 0.1122 | KD: 1061.8256\n",
      "Train Epoch: 082 Batch: 00003/00094 | Loss: 88.5390 | CE: 0.0470 | KD: 1061.2340\n",
      "Train Epoch: 082 Batch: 00004/00094 | Loss: 88.5499 | CE: 0.0643 | KD: 1061.1573\n",
      "Train Epoch: 082 Batch: 00005/00094 | Loss: 88.6028 | CE: 0.0884 | KD: 1061.5029\n",
      "Train Epoch: 082 Batch: 00006/00094 | Loss: 88.5609 | CE: 0.0760 | KD: 1061.1500\n",
      "Train Epoch: 082 Batch: 00007/00094 | Loss: 88.5170 | CE: 0.0574 | KD: 1060.8469\n",
      "Train Epoch: 082 Batch: 00008/00094 | Loss: 88.5240 | CE: 0.0380 | KD: 1061.1633\n",
      "Train Epoch: 082 Batch: 00009/00094 | Loss: 88.5296 | CE: 0.0625 | KD: 1060.9360\n",
      "Train Epoch: 082 Batch: 00010/00094 | Loss: 88.5112 | CE: 0.0461 | KD: 1060.9119\n",
      "Train Epoch: 082 Batch: 00011/00094 | Loss: 88.5405 | CE: 0.0619 | KD: 1061.0739\n",
      "Train Epoch: 082 Batch: 00012/00094 | Loss: 88.5224 | CE: 0.0587 | KD: 1060.8949\n",
      "Train Epoch: 082 Batch: 00013/00094 | Loss: 88.5198 | CE: 0.0475 | KD: 1060.9983\n",
      "Train Epoch: 082 Batch: 00014/00094 | Loss: 88.5544 | CE: 0.0534 | KD: 1061.3417\n",
      "Train Epoch: 082 Batch: 00015/00094 | Loss: 88.5268 | CE: 0.0641 | KD: 1060.8828\n",
      "Train Epoch: 082 Batch: 00016/00094 | Loss: 88.5522 | CE: 0.0819 | KD: 1060.9747\n",
      "Train Epoch: 082 Batch: 00017/00094 | Loss: 88.5222 | CE: 0.0492 | KD: 1061.0074\n",
      "Train Epoch: 082 Batch: 00018/00094 | Loss: 88.5560 | CE: 0.0974 | KD: 1060.8340\n",
      "Train Epoch: 082 Batch: 00019/00094 | Loss: 88.5061 | CE: 0.0358 | KD: 1060.9730\n",
      "Train Epoch: 082 Batch: 00020/00094 | Loss: 88.4954 | CE: 0.0423 | KD: 1060.7683\n",
      "Train Epoch: 082 Batch: 00021/00094 | Loss: 88.5705 | CE: 0.0517 | KD: 1061.5554\n",
      "Train Epoch: 082 Batch: 00022/00094 | Loss: 88.5124 | CE: 0.0516 | KD: 1060.8602\n",
      "Train Epoch: 082 Batch: 00023/00094 | Loss: 88.5346 | CE: 0.0464 | KD: 1061.1888\n",
      "Train Epoch: 082 Batch: 00024/00094 | Loss: 88.6249 | CE: 0.1339 | KD: 1061.2223\n",
      "Train Epoch: 082 Batch: 00025/00094 | Loss: 88.5533 | CE: 0.0575 | KD: 1061.2802\n",
      "Train Epoch: 082 Batch: 00026/00094 | Loss: 88.5450 | CE: 0.0759 | KD: 1060.9608\n",
      "Train Epoch: 082 Batch: 00027/00094 | Loss: 88.5648 | CE: 0.0585 | KD: 1061.4060\n",
      "Train Epoch: 082 Batch: 00028/00094 | Loss: 88.5238 | CE: 0.0383 | KD: 1061.1555\n",
      "Train Epoch: 082 Batch: 00029/00094 | Loss: 88.5676 | CE: 0.0828 | KD: 1061.1488\n",
      "Train Epoch: 082 Batch: 00030/00094 | Loss: 88.5094 | CE: 0.0452 | KD: 1060.9009\n",
      "Train Epoch: 082 Batch: 00031/00094 | Loss: 88.5380 | CE: 0.0476 | KD: 1061.2155\n",
      "Train Epoch: 082 Batch: 00032/00094 | Loss: 88.5260 | CE: 0.0633 | KD: 1060.8828\n",
      "Train Epoch: 082 Batch: 00033/00094 | Loss: 88.5334 | CE: 0.0478 | KD: 1061.1582\n",
      "Train Epoch: 082 Batch: 00034/00094 | Loss: 88.5836 | CE: 0.0773 | KD: 1061.4053\n",
      "Train Epoch: 082 Batch: 00035/00094 | Loss: 88.5663 | CE: 0.0705 | KD: 1061.2802\n",
      "Train Epoch: 082 Batch: 00036/00094 | Loss: 88.6717 | CE: 0.1875 | KD: 1061.1416\n",
      "Train Epoch: 082 Batch: 00037/00094 | Loss: 88.5870 | CE: 0.0930 | KD: 1061.2579\n",
      "Train Epoch: 082 Batch: 00038/00094 | Loss: 88.6191 | CE: 0.1175 | KD: 1061.3495\n",
      "Train Epoch: 082 Batch: 00039/00094 | Loss: 88.5799 | CE: 0.1116 | KD: 1060.9508\n",
      "Train Epoch: 082 Batch: 00040/00094 | Loss: 88.5785 | CE: 0.0629 | KD: 1061.5168\n",
      "Train Epoch: 082 Batch: 00041/00094 | Loss: 88.5248 | CE: 0.0397 | KD: 1061.1516\n",
      "Train Epoch: 082 Batch: 00042/00094 | Loss: 88.5562 | CE: 0.0425 | KD: 1061.4946\n",
      "Train Epoch: 082 Batch: 00043/00094 | Loss: 88.5177 | CE: 0.0438 | KD: 1061.0167\n",
      "Train Epoch: 082 Batch: 00044/00094 | Loss: 88.5162 | CE: 0.0539 | KD: 1060.8777\n",
      "Train Epoch: 082 Batch: 00045/00094 | Loss: 88.5527 | CE: 0.0420 | KD: 1061.4590\n",
      "Train Epoch: 082 Batch: 00046/00094 | Loss: 88.5461 | CE: 0.0744 | KD: 1060.9906\n",
      "Train Epoch: 082 Batch: 00047/00094 | Loss: 88.6648 | CE: 0.1640 | KD: 1061.3394\n",
      "Train Epoch: 082 Batch: 00048/00094 | Loss: 88.5261 | CE: 0.0505 | KD: 1061.0378\n",
      "Train Epoch: 082 Batch: 00049/00094 | Loss: 88.5916 | CE: 0.1279 | KD: 1060.8953\n",
      "Train Epoch: 082 Batch: 00050/00094 | Loss: 88.5563 | CE: 0.0721 | KD: 1061.1416\n",
      "Train Epoch: 082 Batch: 00051/00094 | Loss: 88.5642 | CE: 0.0745 | KD: 1061.2074\n",
      "Train Epoch: 082 Batch: 00052/00094 | Loss: 88.5222 | CE: 0.0459 | KD: 1061.0464\n",
      "Train Epoch: 082 Batch: 00053/00094 | Loss: 88.5572 | CE: 0.0467 | KD: 1061.4567\n",
      "Train Epoch: 082 Batch: 00054/00094 | Loss: 88.5441 | CE: 0.0604 | KD: 1061.1340\n",
      "Train Epoch: 082 Batch: 00055/00094 | Loss: 88.5255 | CE: 0.0443 | KD: 1061.1046\n",
      "Train Epoch: 082 Batch: 00056/00094 | Loss: 88.5220 | CE: 0.0762 | KD: 1060.6808\n",
      "Train Epoch: 082 Batch: 00057/00094 | Loss: 88.5523 | CE: 0.0574 | KD: 1061.2695\n",
      "Train Epoch: 082 Batch: 00058/00094 | Loss: 88.5584 | CE: 0.0506 | KD: 1061.4240\n",
      "Train Epoch: 082 Batch: 00059/00094 | Loss: 88.6497 | CE: 0.1557 | KD: 1061.2584\n",
      "Train Epoch: 082 Batch: 00060/00094 | Loss: 88.5533 | CE: 0.0783 | KD: 1061.0298\n",
      "Train Epoch: 082 Batch: 00061/00094 | Loss: 88.5772 | CE: 0.0906 | KD: 1061.1702\n",
      "Train Epoch: 082 Batch: 00062/00094 | Loss: 88.5317 | CE: 0.0535 | KD: 1061.0691\n",
      "Train Epoch: 082 Batch: 00063/00094 | Loss: 88.4867 | CE: 0.0432 | KD: 1060.6532\n",
      "Train Epoch: 082 Batch: 00064/00094 | Loss: 88.5593 | CE: 0.0519 | KD: 1061.4188\n",
      "Train Epoch: 082 Batch: 00065/00094 | Loss: 88.5974 | CE: 0.0875 | KD: 1061.4486\n",
      "Train Epoch: 082 Batch: 00066/00094 | Loss: 88.6112 | CE: 0.1073 | KD: 1061.3767\n",
      "Train Epoch: 082 Batch: 00067/00094 | Loss: 88.6059 | CE: 0.1068 | KD: 1061.3196\n",
      "Train Epoch: 082 Batch: 00068/00094 | Loss: 88.5769 | CE: 0.0832 | KD: 1061.2540\n",
      "Train Epoch: 082 Batch: 00069/00094 | Loss: 88.5693 | CE: 0.0827 | KD: 1061.1704\n",
      "Train Epoch: 082 Batch: 00070/00094 | Loss: 88.6786 | CE: 0.1726 | KD: 1061.4017\n",
      "Train Epoch: 082 Batch: 00071/00094 | Loss: 88.5111 | CE: 0.0581 | KD: 1060.7665\n",
      "Train Epoch: 082 Batch: 00072/00094 | Loss: 88.5664 | CE: 0.0993 | KD: 1060.9357\n",
      "Train Epoch: 082 Batch: 00073/00094 | Loss: 88.6442 | CE: 0.1397 | KD: 1061.3834\n",
      "Train Epoch: 082 Batch: 00074/00094 | Loss: 88.5411 | CE: 0.0538 | KD: 1061.1775\n",
      "Train Epoch: 082 Batch: 00075/00094 | Loss: 88.5301 | CE: 0.0610 | KD: 1060.9592\n",
      "Train Epoch: 082 Batch: 00076/00094 | Loss: 88.5491 | CE: 0.0561 | KD: 1061.2458\n",
      "Train Epoch: 082 Batch: 00077/00094 | Loss: 88.5857 | CE: 0.0923 | KD: 1061.2515\n",
      "Train Epoch: 082 Batch: 00078/00094 | Loss: 88.5737 | CE: 0.0699 | KD: 1061.3757\n",
      "Train Epoch: 082 Batch: 00079/00094 | Loss: 88.5205 | CE: 0.0502 | KD: 1060.9739\n",
      "Train Epoch: 082 Batch: 00080/00094 | Loss: 88.5643 | CE: 0.0921 | KD: 1060.9967\n",
      "Train Epoch: 082 Batch: 00081/00094 | Loss: 88.5481 | CE: 0.0619 | KD: 1061.1650\n",
      "Train Epoch: 082 Batch: 00082/00094 | Loss: 88.5133 | CE: 0.0446 | KD: 1060.9551\n",
      "Train Epoch: 082 Batch: 00083/00094 | Loss: 88.5379 | CE: 0.0778 | KD: 1060.8512\n",
      "Train Epoch: 082 Batch: 00084/00094 | Loss: 88.5428 | CE: 0.0569 | KD: 1061.1610\n",
      "Train Epoch: 082 Batch: 00085/00094 | Loss: 88.5709 | CE: 0.0789 | KD: 1061.2338\n",
      "Train Epoch: 082 Batch: 00086/00094 | Loss: 88.5365 | CE: 0.0503 | KD: 1061.1650\n",
      "Train Epoch: 082 Batch: 00087/00094 | Loss: 88.5366 | CE: 0.0512 | KD: 1061.1558\n",
      "Train Epoch: 082 Batch: 00088/00094 | Loss: 88.6049 | CE: 0.1082 | KD: 1061.2916\n",
      "Train Epoch: 082 Batch: 00089/00094 | Loss: 88.5454 | CE: 0.0464 | KD: 1061.3188\n",
      "Train Epoch: 082 Batch: 00090/00094 | Loss: 88.5606 | CE: 0.0431 | KD: 1061.5396\n",
      "Train Epoch: 082 Batch: 00091/00094 | Loss: 88.5266 | CE: 0.0515 | KD: 1061.0316\n",
      "Train Epoch: 082 Batch: 00092/00094 | Loss: 88.5335 | CE: 0.0446 | KD: 1061.1973\n",
      "Train Epoch: 082 Batch: 00093/00094 | Loss: 88.5177 | CE: 0.0497 | KD: 1060.9467\n",
      "Train Epoch: 082 Batch: 00094/00094 | Loss: 88.6200 | CE: 0.0814 | KD: 1061.7931\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0689 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8427 | acc:49.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.3383 | acc:50.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3529 | acc:45.2290\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3575 | acc:55.8386\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7376 | acc:67.0055\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 67.01%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6972 | acc:73.1583\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0018 | acc:58.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.38%\n",
      "[VAL Acc] Avg 62.26%\n",
      "Train Epoch: 083 Batch: 00001/00094 | Loss: 88.5374 | CE: 0.0476 | KD: 1061.2078\n",
      "Train Epoch: 083 Batch: 00002/00094 | Loss: 88.5528 | CE: 0.0617 | KD: 1061.2238\n",
      "Train Epoch: 083 Batch: 00003/00094 | Loss: 88.5758 | CE: 0.0478 | KD: 1061.6670\n",
      "Train Epoch: 083 Batch: 00004/00094 | Loss: 88.5420 | CE: 0.0470 | KD: 1061.2715\n",
      "Train Epoch: 083 Batch: 00005/00094 | Loss: 88.5169 | CE: 0.0571 | KD: 1060.8481\n",
      "Train Epoch: 083 Batch: 00006/00094 | Loss: 88.5301 | CE: 0.0323 | KD: 1061.3038\n",
      "Train Epoch: 083 Batch: 00007/00094 | Loss: 88.5581 | CE: 0.0636 | KD: 1061.2645\n",
      "Train Epoch: 083 Batch: 00008/00094 | Loss: 88.5373 | CE: 0.0469 | KD: 1061.2152\n",
      "Train Epoch: 083 Batch: 00009/00094 | Loss: 88.6028 | CE: 0.0857 | KD: 1061.5355\n",
      "Train Epoch: 083 Batch: 00010/00094 | Loss: 88.5363 | CE: 0.0487 | KD: 1061.1825\n",
      "Train Epoch: 083 Batch: 00011/00094 | Loss: 88.5563 | CE: 0.0760 | KD: 1061.0939\n",
      "Train Epoch: 083 Batch: 00012/00094 | Loss: 88.5275 | CE: 0.0414 | KD: 1061.1643\n",
      "Train Epoch: 083 Batch: 00013/00094 | Loss: 88.5283 | CE: 0.0417 | KD: 1061.1703\n",
      "Train Epoch: 083 Batch: 00014/00094 | Loss: 88.5605 | CE: 0.0481 | KD: 1061.4785\n",
      "Train Epoch: 083 Batch: 00015/00094 | Loss: 88.5510 | CE: 0.0835 | KD: 1060.9397\n",
      "Train Epoch: 083 Batch: 00016/00094 | Loss: 88.5667 | CE: 0.0900 | KD: 1061.0507\n",
      "Train Epoch: 083 Batch: 00017/00094 | Loss: 88.6159 | CE: 0.1004 | KD: 1061.5168\n",
      "Train Epoch: 083 Batch: 00018/00094 | Loss: 88.5210 | CE: 0.0601 | KD: 1060.8606\n",
      "Train Epoch: 083 Batch: 00019/00094 | Loss: 88.5545 | CE: 0.0596 | KD: 1061.2695\n",
      "Train Epoch: 083 Batch: 00020/00094 | Loss: 88.5140 | CE: 0.0414 | KD: 1061.0023\n",
      "Train Epoch: 083 Batch: 00021/00094 | Loss: 88.5550 | CE: 0.0723 | KD: 1061.1230\n",
      "Train Epoch: 083 Batch: 00022/00094 | Loss: 88.5218 | CE: 0.0531 | KD: 1060.9548\n",
      "Train Epoch: 083 Batch: 00023/00094 | Loss: 88.5086 | CE: 0.0623 | KD: 1060.6870\n",
      "Train Epoch: 083 Batch: 00024/00094 | Loss: 88.5782 | CE: 0.0516 | KD: 1061.6494\n",
      "Train Epoch: 083 Batch: 00025/00094 | Loss: 88.6034 | CE: 0.1059 | KD: 1061.3002\n",
      "Train Epoch: 083 Batch: 00026/00094 | Loss: 88.6143 | CE: 0.1422 | KD: 1060.9958\n",
      "Train Epoch: 083 Batch: 00027/00094 | Loss: 88.5183 | CE: 0.0632 | KD: 1060.7919\n",
      "Train Epoch: 083 Batch: 00028/00094 | Loss: 88.5748 | CE: 0.0842 | KD: 1061.2174\n",
      "Train Epoch: 083 Batch: 00029/00094 | Loss: 88.6201 | CE: 0.1135 | KD: 1061.4092\n",
      "Train Epoch: 083 Batch: 00030/00094 | Loss: 88.5658 | CE: 0.0610 | KD: 1061.3878\n",
      "Train Epoch: 083 Batch: 00031/00094 | Loss: 88.5564 | CE: 0.0446 | KD: 1061.4714\n",
      "Train Epoch: 083 Batch: 00032/00094 | Loss: 88.5457 | CE: 0.0892 | KD: 1060.8081\n",
      "Train Epoch: 083 Batch: 00033/00094 | Loss: 88.5382 | CE: 0.0482 | KD: 1061.2106\n",
      "Train Epoch: 083 Batch: 00034/00094 | Loss: 88.5313 | CE: 0.0593 | KD: 1060.9943\n",
      "Train Epoch: 083 Batch: 00035/00094 | Loss: 88.6040 | CE: 0.1037 | KD: 1061.3341\n",
      "Train Epoch: 083 Batch: 00036/00094 | Loss: 88.5231 | CE: 0.0650 | KD: 1060.8271\n",
      "Train Epoch: 083 Batch: 00037/00094 | Loss: 88.5572 | CE: 0.0584 | KD: 1061.3158\n",
      "Train Epoch: 083 Batch: 00038/00094 | Loss: 88.5943 | CE: 0.0870 | KD: 1061.4182\n",
      "Train Epoch: 083 Batch: 00039/00094 | Loss: 88.4914 | CE: 0.0431 | KD: 1060.7102\n",
      "Train Epoch: 083 Batch: 00040/00094 | Loss: 88.5943 | CE: 0.0744 | KD: 1061.5690\n",
      "Train Epoch: 083 Batch: 00041/00094 | Loss: 88.5532 | CE: 0.0463 | KD: 1061.4128\n",
      "Train Epoch: 083 Batch: 00042/00094 | Loss: 88.5639 | CE: 0.0577 | KD: 1061.4050\n",
      "Train Epoch: 083 Batch: 00043/00094 | Loss: 88.5384 | CE: 0.0488 | KD: 1061.2053\n",
      "Train Epoch: 083 Batch: 00044/00094 | Loss: 88.5638 | CE: 0.0988 | KD: 1060.9100\n",
      "Train Epoch: 083 Batch: 00045/00094 | Loss: 88.5511 | CE: 0.0736 | KD: 1061.0612\n",
      "Train Epoch: 083 Batch: 00046/00094 | Loss: 88.5326 | CE: 0.0488 | KD: 1061.1365\n",
      "Train Epoch: 083 Batch: 00047/00094 | Loss: 88.5540 | CE: 0.0534 | KD: 1061.3381\n",
      "Train Epoch: 083 Batch: 00048/00094 | Loss: 88.5796 | CE: 0.1147 | KD: 1060.9086\n",
      "Train Epoch: 083 Batch: 00049/00094 | Loss: 88.6308 | CE: 0.1154 | KD: 1061.5150\n",
      "Train Epoch: 083 Batch: 00050/00094 | Loss: 88.6124 | CE: 0.0591 | KD: 1061.9700\n",
      "Train Epoch: 083 Batch: 00051/00094 | Loss: 88.6089 | CE: 0.0883 | KD: 1061.5769\n",
      "Train Epoch: 083 Batch: 00052/00094 | Loss: 88.6197 | CE: 0.0825 | KD: 1061.7776\n",
      "Train Epoch: 083 Batch: 00053/00094 | Loss: 88.5078 | CE: 0.0553 | KD: 1060.7607\n",
      "Train Epoch: 083 Batch: 00054/00094 | Loss: 88.5753 | CE: 0.0844 | KD: 1061.2209\n",
      "Train Epoch: 083 Batch: 00055/00094 | Loss: 88.5481 | CE: 0.0637 | KD: 1061.1442\n",
      "Train Epoch: 083 Batch: 00056/00094 | Loss: 88.5782 | CE: 0.0914 | KD: 1061.1720\n",
      "Train Epoch: 083 Batch: 00057/00094 | Loss: 88.5404 | CE: 0.0516 | KD: 1061.1962\n",
      "Train Epoch: 083 Batch: 00058/00094 | Loss: 88.5020 | CE: 0.0411 | KD: 1060.8623\n",
      "Train Epoch: 083 Batch: 00059/00094 | Loss: 88.5627 | CE: 0.0577 | KD: 1061.3898\n",
      "Train Epoch: 083 Batch: 00060/00094 | Loss: 88.5215 | CE: 0.0486 | KD: 1061.0049\n",
      "Train Epoch: 083 Batch: 00061/00094 | Loss: 88.5199 | CE: 0.0466 | KD: 1061.0104\n",
      "Train Epoch: 083 Batch: 00062/00094 | Loss: 88.5523 | CE: 0.1035 | KD: 1060.7170\n",
      "Train Epoch: 083 Batch: 00063/00094 | Loss: 88.5532 | CE: 0.0674 | KD: 1061.1600\n",
      "Train Epoch: 083 Batch: 00064/00094 | Loss: 88.5549 | CE: 0.0763 | KD: 1061.0739\n",
      "Train Epoch: 083 Batch: 00065/00094 | Loss: 88.7277 | CE: 0.1860 | KD: 1061.8306\n",
      "Train Epoch: 083 Batch: 00066/00094 | Loss: 88.5450 | CE: 0.0395 | KD: 1061.3961\n",
      "Train Epoch: 083 Batch: 00067/00094 | Loss: 88.5028 | CE: 0.0463 | KD: 1060.8080\n",
      "Train Epoch: 083 Batch: 00068/00094 | Loss: 88.5738 | CE: 0.0760 | KD: 1061.3046\n",
      "Train Epoch: 083 Batch: 00069/00094 | Loss: 88.5293 | CE: 0.0455 | KD: 1061.1362\n",
      "Train Epoch: 083 Batch: 00070/00094 | Loss: 88.5653 | CE: 0.1025 | KD: 1060.8846\n",
      "Train Epoch: 083 Batch: 00071/00094 | Loss: 88.6708 | CE: 0.1675 | KD: 1061.3694\n",
      "Train Epoch: 083 Batch: 00072/00094 | Loss: 88.5580 | CE: 0.0433 | KD: 1061.5067\n",
      "Train Epoch: 083 Batch: 00073/00094 | Loss: 88.4988 | CE: 0.0533 | KD: 1060.6768\n",
      "Train Epoch: 083 Batch: 00074/00094 | Loss: 88.5419 | CE: 0.0729 | KD: 1060.9584\n",
      "Train Epoch: 083 Batch: 00075/00094 | Loss: 88.5359 | CE: 0.0761 | KD: 1060.8477\n",
      "Train Epoch: 083 Batch: 00076/00094 | Loss: 88.5607 | CE: 0.0755 | KD: 1061.1534\n",
      "Train Epoch: 083 Batch: 00077/00094 | Loss: 88.5625 | CE: 0.0834 | KD: 1061.0807\n",
      "Train Epoch: 083 Batch: 00078/00094 | Loss: 88.5534 | CE: 0.0626 | KD: 1061.2194\n",
      "Train Epoch: 083 Batch: 00079/00094 | Loss: 88.6119 | CE: 0.1098 | KD: 1061.3555\n",
      "Train Epoch: 083 Batch: 00080/00094 | Loss: 88.5638 | CE: 0.0481 | KD: 1061.5186\n",
      "Train Epoch: 083 Batch: 00081/00094 | Loss: 88.5171 | CE: 0.0428 | KD: 1061.0219\n",
      "Train Epoch: 083 Batch: 00082/00094 | Loss: 88.5250 | CE: 0.0463 | KD: 1061.0750\n",
      "Train Epoch: 083 Batch: 00083/00094 | Loss: 88.6120 | CE: 0.1322 | KD: 1061.0885\n",
      "Train Epoch: 083 Batch: 00084/00094 | Loss: 88.5223 | CE: 0.0403 | KD: 1061.1141\n",
      "Train Epoch: 083 Batch: 00085/00094 | Loss: 88.5125 | CE: 0.0443 | KD: 1060.9490\n",
      "Train Epoch: 083 Batch: 00086/00094 | Loss: 88.5432 | CE: 0.0668 | KD: 1061.0464\n",
      "Train Epoch: 083 Batch: 00087/00094 | Loss: 88.5567 | CE: 0.0553 | KD: 1061.3470\n",
      "Train Epoch: 083 Batch: 00088/00094 | Loss: 88.5184 | CE: 0.0506 | KD: 1060.9449\n",
      "Train Epoch: 083 Batch: 00089/00094 | Loss: 88.5449 | CE: 0.0620 | KD: 1061.1249\n",
      "Train Epoch: 083 Batch: 00090/00094 | Loss: 88.5113 | CE: 0.0488 | KD: 1060.8811\n",
      "Train Epoch: 083 Batch: 00091/00094 | Loss: 88.5572 | CE: 0.0661 | KD: 1061.2235\n",
      "Train Epoch: 083 Batch: 00092/00094 | Loss: 88.5844 | CE: 0.0792 | KD: 1061.3922\n",
      "Train Epoch: 083 Batch: 00093/00094 | Loss: 88.5712 | CE: 0.0738 | KD: 1061.2991\n",
      "Train Epoch: 083 Batch: 00094/00094 | Loss: 88.5422 | CE: 0.0497 | KD: 1061.2396\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0693 | acc:98.8500\n",
      "[VAL Acc] Target: 98.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8339 | acc:49.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2831 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3993 | acc:44.0840\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4076 | acc:55.0157\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7280 | acc:66.8207\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7543 | acc:71.5125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9424 | acc:58.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.63%\n",
      "[VAL Acc] Avg 62.31%\n",
      "Train Epoch: 084 Batch: 00001/00094 | Loss: 88.5292 | CE: 0.0467 | KD: 1061.1207\n",
      "Train Epoch: 084 Batch: 00002/00094 | Loss: 88.6219 | CE: 0.1013 | KD: 1061.5775\n",
      "Train Epoch: 084 Batch: 00003/00094 | Loss: 88.5561 | CE: 0.0687 | KD: 1061.1786\n",
      "Train Epoch: 084 Batch: 00004/00094 | Loss: 88.5915 | CE: 0.0947 | KD: 1061.2917\n",
      "Train Epoch: 084 Batch: 00005/00094 | Loss: 88.5649 | CE: 0.0542 | KD: 1061.4595\n",
      "Train Epoch: 084 Batch: 00006/00094 | Loss: 88.5298 | CE: 0.0773 | KD: 1060.7606\n",
      "Train Epoch: 084 Batch: 00007/00094 | Loss: 88.5500 | CE: 0.0459 | KD: 1061.3804\n",
      "Train Epoch: 084 Batch: 00008/00094 | Loss: 88.5030 | CE: 0.0551 | KD: 1060.7063\n",
      "Train Epoch: 084 Batch: 00009/00094 | Loss: 88.5108 | CE: 0.0387 | KD: 1060.9962\n",
      "Train Epoch: 084 Batch: 00010/00094 | Loss: 88.5615 | CE: 0.0873 | KD: 1061.0200\n",
      "Train Epoch: 084 Batch: 00011/00094 | Loss: 88.5639 | CE: 0.0598 | KD: 1061.3799\n",
      "Train Epoch: 084 Batch: 00012/00094 | Loss: 88.5493 | CE: 0.0574 | KD: 1061.2328\n",
      "Train Epoch: 084 Batch: 00013/00094 | Loss: 88.5600 | CE: 0.0909 | KD: 1060.9594\n",
      "Train Epoch: 084 Batch: 00014/00094 | Loss: 88.5616 | CE: 0.0438 | KD: 1061.5447\n",
      "Train Epoch: 084 Batch: 00015/00094 | Loss: 88.5219 | CE: 0.0667 | KD: 1060.7921\n",
      "Train Epoch: 084 Batch: 00016/00094 | Loss: 88.5132 | CE: 0.0545 | KD: 1060.8353\n",
      "Train Epoch: 084 Batch: 00017/00094 | Loss: 88.6265 | CE: 0.1123 | KD: 1061.5006\n",
      "Train Epoch: 084 Batch: 00018/00094 | Loss: 88.6917 | CE: 0.1620 | KD: 1061.6865\n",
      "Train Epoch: 084 Batch: 00019/00094 | Loss: 88.5461 | CE: 0.0873 | KD: 1060.8369\n",
      "Train Epoch: 084 Batch: 00020/00094 | Loss: 88.5610 | CE: 0.0651 | KD: 1061.2804\n",
      "Train Epoch: 084 Batch: 00021/00094 | Loss: 88.5893 | CE: 0.0857 | KD: 1061.3734\n",
      "Train Epoch: 084 Batch: 00022/00094 | Loss: 88.5580 | CE: 0.0597 | KD: 1061.3102\n",
      "Train Epoch: 084 Batch: 00023/00094 | Loss: 88.5185 | CE: 0.0707 | KD: 1060.7045\n",
      "Train Epoch: 084 Batch: 00024/00094 | Loss: 88.5656 | CE: 0.0629 | KD: 1061.3624\n",
      "Train Epoch: 084 Batch: 00025/00094 | Loss: 88.5987 | CE: 0.0919 | KD: 1061.4117\n",
      "Train Epoch: 084 Batch: 00026/00094 | Loss: 88.4939 | CE: 0.0486 | KD: 1060.6737\n",
      "Train Epoch: 084 Batch: 00027/00094 | Loss: 88.5466 | CE: 0.0555 | KD: 1061.2234\n",
      "Train Epoch: 084 Batch: 00028/00094 | Loss: 88.5733 | CE: 0.0523 | KD: 1061.5812\n",
      "Train Epoch: 084 Batch: 00029/00094 | Loss: 88.5478 | CE: 0.0618 | KD: 1061.1624\n",
      "Train Epoch: 084 Batch: 00030/00094 | Loss: 88.5038 | CE: 0.0464 | KD: 1060.8196\n",
      "Train Epoch: 084 Batch: 00031/00094 | Loss: 88.5350 | CE: 0.0668 | KD: 1060.9487\n",
      "Train Epoch: 084 Batch: 00032/00094 | Loss: 88.5706 | CE: 0.0884 | KD: 1061.1172\n",
      "Train Epoch: 084 Batch: 00033/00094 | Loss: 88.5681 | CE: 0.0693 | KD: 1061.3159\n",
      "Train Epoch: 084 Batch: 00034/00094 | Loss: 88.5368 | CE: 0.0572 | KD: 1061.0859\n",
      "Train Epoch: 084 Batch: 00035/00094 | Loss: 88.6296 | CE: 0.1083 | KD: 1061.5863\n",
      "Train Epoch: 084 Batch: 00036/00094 | Loss: 88.5669 | CE: 0.0693 | KD: 1061.3020\n",
      "Train Epoch: 084 Batch: 00037/00094 | Loss: 88.5773 | CE: 0.0527 | KD: 1061.6248\n",
      "Train Epoch: 084 Batch: 00038/00094 | Loss: 88.5364 | CE: 0.0468 | KD: 1061.2048\n",
      "Train Epoch: 084 Batch: 00039/00094 | Loss: 88.5280 | CE: 0.0914 | KD: 1060.5701\n",
      "Train Epoch: 084 Batch: 00040/00094 | Loss: 88.5459 | CE: 0.0605 | KD: 1061.1555\n",
      "Train Epoch: 084 Batch: 00041/00094 | Loss: 88.5692 | CE: 0.0495 | KD: 1061.5664\n",
      "Train Epoch: 084 Batch: 00042/00094 | Loss: 88.5283 | CE: 0.0507 | KD: 1061.0620\n",
      "Train Epoch: 084 Batch: 00043/00094 | Loss: 88.5634 | CE: 0.0491 | KD: 1061.5016\n",
      "Train Epoch: 084 Batch: 00044/00094 | Loss: 88.5507 | CE: 0.0517 | KD: 1061.3186\n",
      "Train Epoch: 084 Batch: 00045/00094 | Loss: 88.5667 | CE: 0.1198 | KD: 1060.6938\n",
      "Train Epoch: 084 Batch: 00046/00094 | Loss: 88.5640 | CE: 0.0409 | KD: 1061.6077\n",
      "Train Epoch: 084 Batch: 00047/00094 | Loss: 88.5037 | CE: 0.0447 | KD: 1060.8383\n",
      "Train Epoch: 084 Batch: 00048/00094 | Loss: 88.5177 | CE: 0.0386 | KD: 1061.0797\n",
      "Train Epoch: 084 Batch: 00049/00094 | Loss: 88.6159 | CE: 0.0924 | KD: 1061.6125\n",
      "Train Epoch: 084 Batch: 00050/00094 | Loss: 88.5399 | CE: 0.0715 | KD: 1060.9510\n",
      "Train Epoch: 084 Batch: 00051/00094 | Loss: 88.4912 | CE: 0.0444 | KD: 1060.6924\n",
      "Train Epoch: 084 Batch: 00052/00094 | Loss: 88.5728 | CE: 0.0647 | KD: 1061.4275\n",
      "Train Epoch: 084 Batch: 00053/00094 | Loss: 88.5476 | CE: 0.0818 | KD: 1060.9207\n",
      "Train Epoch: 084 Batch: 00054/00094 | Loss: 88.5787 | CE: 0.0822 | KD: 1061.2887\n",
      "Train Epoch: 084 Batch: 00055/00094 | Loss: 88.5976 | CE: 0.1042 | KD: 1061.2511\n",
      "Train Epoch: 084 Batch: 00056/00094 | Loss: 88.5361 | CE: 0.0533 | KD: 1061.1245\n",
      "Train Epoch: 084 Batch: 00057/00094 | Loss: 88.5172 | CE: 0.0539 | KD: 1060.8904\n",
      "Train Epoch: 084 Batch: 00058/00094 | Loss: 88.5464 | CE: 0.0567 | KD: 1061.2070\n",
      "Train Epoch: 084 Batch: 00059/00094 | Loss: 88.5344 | CE: 0.0607 | KD: 1061.0155\n",
      "Train Epoch: 084 Batch: 00060/00094 | Loss: 88.5750 | CE: 0.0666 | KD: 1061.4318\n",
      "Train Epoch: 084 Batch: 00061/00094 | Loss: 88.5344 | CE: 0.0482 | KD: 1061.1652\n",
      "Train Epoch: 084 Batch: 00062/00094 | Loss: 88.5101 | CE: 0.0624 | KD: 1060.7026\n",
      "Train Epoch: 084 Batch: 00063/00094 | Loss: 88.5400 | CE: 0.0613 | KD: 1061.0748\n",
      "Train Epoch: 084 Batch: 00064/00094 | Loss: 88.5845 | CE: 0.0726 | KD: 1061.4731\n",
      "Train Epoch: 084 Batch: 00065/00094 | Loss: 88.5136 | CE: 0.0416 | KD: 1060.9946\n",
      "Train Epoch: 084 Batch: 00066/00094 | Loss: 88.5483 | CE: 0.0682 | KD: 1061.0924\n",
      "Train Epoch: 084 Batch: 00067/00094 | Loss: 88.5312 | CE: 0.0546 | KD: 1061.0504\n",
      "Train Epoch: 084 Batch: 00068/00094 | Loss: 88.5600 | CE: 0.0826 | KD: 1061.0588\n",
      "Train Epoch: 084 Batch: 00069/00094 | Loss: 88.5345 | CE: 0.0589 | KD: 1061.0375\n",
      "Train Epoch: 084 Batch: 00070/00094 | Loss: 88.5441 | CE: 0.0919 | KD: 1060.7563\n",
      "Train Epoch: 084 Batch: 00071/00094 | Loss: 88.5239 | CE: 0.0486 | KD: 1061.0338\n",
      "Train Epoch: 084 Batch: 00072/00094 | Loss: 88.5160 | CE: 0.0368 | KD: 1061.0812\n",
      "Train Epoch: 084 Batch: 00073/00094 | Loss: 88.5339 | CE: 0.0577 | KD: 1061.0446\n",
      "Train Epoch: 084 Batch: 00074/00094 | Loss: 88.4971 | CE: 0.0440 | KD: 1060.7682\n",
      "Train Epoch: 084 Batch: 00075/00094 | Loss: 88.5332 | CE: 0.0507 | KD: 1061.1208\n",
      "Train Epoch: 084 Batch: 00076/00094 | Loss: 88.5565 | CE: 0.0762 | KD: 1061.0947\n",
      "Train Epoch: 084 Batch: 00077/00094 | Loss: 88.5147 | CE: 0.0654 | KD: 1060.7222\n",
      "Train Epoch: 084 Batch: 00078/00094 | Loss: 88.5788 | CE: 0.0752 | KD: 1061.3735\n",
      "Train Epoch: 084 Batch: 00079/00094 | Loss: 88.5193 | CE: 0.0425 | KD: 1061.0527\n",
      "Train Epoch: 084 Batch: 00080/00094 | Loss: 88.5535 | CE: 0.0483 | KD: 1061.3929\n",
      "Train Epoch: 084 Batch: 00081/00094 | Loss: 88.5726 | CE: 0.0899 | KD: 1061.1218\n",
      "Train Epoch: 084 Batch: 00082/00094 | Loss: 88.5099 | CE: 0.0426 | KD: 1060.9373\n",
      "Train Epoch: 084 Batch: 00083/00094 | Loss: 88.6280 | CE: 0.1054 | KD: 1061.6013\n",
      "Train Epoch: 084 Batch: 00084/00094 | Loss: 88.5619 | CE: 0.0626 | KD: 1061.3219\n",
      "Train Epoch: 084 Batch: 00085/00094 | Loss: 88.5417 | CE: 0.0663 | KD: 1061.0353\n",
      "Train Epoch: 084 Batch: 00086/00094 | Loss: 88.5701 | CE: 0.0647 | KD: 1061.3944\n",
      "Train Epoch: 084 Batch: 00087/00094 | Loss: 88.5231 | CE: 0.0458 | KD: 1061.0579\n",
      "Train Epoch: 084 Batch: 00088/00094 | Loss: 88.5863 | CE: 0.0978 | KD: 1061.1920\n",
      "Train Epoch: 084 Batch: 00089/00094 | Loss: 88.5890 | CE: 0.0963 | KD: 1061.2427\n",
      "Train Epoch: 084 Batch: 00090/00094 | Loss: 88.5842 | CE: 0.1261 | KD: 1060.8284\n",
      "Train Epoch: 084 Batch: 00091/00094 | Loss: 88.4732 | CE: 0.0373 | KD: 1060.5621\n",
      "Train Epoch: 084 Batch: 00092/00094 | Loss: 88.5594 | CE: 0.0747 | KD: 1061.1471\n",
      "Train Epoch: 084 Batch: 00093/00094 | Loss: 88.5425 | CE: 0.0799 | KD: 1060.8822\n",
      "Train Epoch: 084 Batch: 00094/00094 | Loss: 88.5535 | CE: 0.0432 | KD: 1061.4547\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0561 | acc:99.2000\n",
      "[VAL Acc] Target: 99.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7929 | acc:49.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2714 | acc:52.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.4184 | acc:43.7023\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2503 | acc:58.1897\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7334 | acc:66.6359\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6133 | acc:75.7445\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9077 | acc:59.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.69%\n",
      "[VAL Acc] Avg 63.10%\n",
      "Train Epoch: 085 Batch: 00001/00094 | Loss: 79.7348 | CE: 0.0738 | KD: 1061.4763\n",
      "Train Epoch: 085 Batch: 00002/00094 | Loss: 79.7390 | CE: 0.0871 | KD: 1061.3561\n",
      "Train Epoch: 085 Batch: 00003/00094 | Loss: 79.6585 | CE: 0.0387 | KD: 1060.9276\n",
      "Train Epoch: 085 Batch: 00004/00094 | Loss: 79.6842 | CE: 0.0438 | KD: 1061.2023\n",
      "Train Epoch: 085 Batch: 00005/00094 | Loss: 79.7146 | CE: 0.0680 | KD: 1061.2850\n",
      "Train Epoch: 085 Batch: 00006/00094 | Loss: 79.6700 | CE: 0.0539 | KD: 1060.8783\n",
      "Train Epoch: 085 Batch: 00007/00094 | Loss: 79.7377 | CE: 0.0580 | KD: 1061.7267\n",
      "Train Epoch: 085 Batch: 00008/00094 | Loss: 79.7026 | CE: 0.0543 | KD: 1061.3074\n",
      "Train Epoch: 085 Batch: 00009/00094 | Loss: 79.7093 | CE: 0.0791 | KD: 1061.0663\n",
      "Train Epoch: 085 Batch: 00010/00094 | Loss: 79.6617 | CE: 0.0407 | KD: 1060.9442\n",
      "Train Epoch: 085 Batch: 00011/00094 | Loss: 79.6807 | CE: 0.0450 | KD: 1061.1400\n",
      "Train Epoch: 085 Batch: 00012/00094 | Loss: 79.6641 | CE: 0.0370 | KD: 1061.0251\n",
      "Train Epoch: 085 Batch: 00013/00094 | Loss: 79.6761 | CE: 0.0466 | KD: 1061.0575\n",
      "Train Epoch: 085 Batch: 00014/00094 | Loss: 79.7395 | CE: 0.0856 | KD: 1061.3817\n",
      "Train Epoch: 085 Batch: 00015/00094 | Loss: 79.6809 | CE: 0.0668 | KD: 1060.8527\n",
      "Train Epoch: 085 Batch: 00016/00094 | Loss: 79.6901 | CE: 0.0675 | KD: 1060.9658\n",
      "Train Epoch: 085 Batch: 00017/00094 | Loss: 79.7537 | CE: 0.0950 | KD: 1061.4464\n",
      "Train Epoch: 085 Batch: 00018/00094 | Loss: 79.6793 | CE: 0.0520 | KD: 1061.0280\n",
      "Train Epoch: 085 Batch: 00019/00094 | Loss: 79.7104 | CE: 0.0593 | KD: 1061.3456\n",
      "Train Epoch: 085 Batch: 00020/00094 | Loss: 79.6841 | CE: 0.0395 | KD: 1061.2573\n",
      "Train Epoch: 085 Batch: 00021/00094 | Loss: 79.7275 | CE: 0.0790 | KD: 1061.3097\n",
      "Train Epoch: 085 Batch: 00022/00094 | Loss: 79.7440 | CE: 0.1046 | KD: 1061.1886\n",
      "Train Epoch: 085 Batch: 00023/00094 | Loss: 79.6756 | CE: 0.0382 | KD: 1061.1622\n",
      "Train Epoch: 085 Batch: 00024/00094 | Loss: 79.6834 | CE: 0.0615 | KD: 1060.9570\n",
      "Train Epoch: 085 Batch: 00025/00094 | Loss: 79.7114 | CE: 0.0707 | KD: 1061.2058\n",
      "Train Epoch: 085 Batch: 00026/00094 | Loss: 79.6891 | CE: 0.0482 | KD: 1061.2091\n",
      "Train Epoch: 085 Batch: 00027/00094 | Loss: 79.6773 | CE: 0.0693 | KD: 1060.7714\n",
      "Train Epoch: 085 Batch: 00028/00094 | Loss: 79.7273 | CE: 0.0951 | KD: 1061.0934\n",
      "Train Epoch: 085 Batch: 00029/00094 | Loss: 79.7153 | CE: 0.0732 | KD: 1061.2251\n",
      "Train Epoch: 085 Batch: 00030/00094 | Loss: 79.6660 | CE: 0.0493 | KD: 1060.8865\n",
      "Train Epoch: 085 Batch: 00031/00094 | Loss: 79.7822 | CE: 0.1211 | KD: 1061.4783\n",
      "Train Epoch: 085 Batch: 00032/00094 | Loss: 79.7443 | CE: 0.1002 | KD: 1061.2522\n",
      "Train Epoch: 085 Batch: 00033/00094 | Loss: 79.7141 | CE: 0.0675 | KD: 1061.2855\n",
      "Train Epoch: 085 Batch: 00034/00094 | Loss: 79.6826 | CE: 0.0646 | KD: 1060.9036\n",
      "Train Epoch: 085 Batch: 00035/00094 | Loss: 79.6576 | CE: 0.0477 | KD: 1060.7954\n",
      "Train Epoch: 085 Batch: 00036/00094 | Loss: 79.6986 | CE: 0.0835 | KD: 1060.8650\n",
      "Train Epoch: 085 Batch: 00037/00094 | Loss: 79.7077 | CE: 0.0620 | KD: 1061.2729\n",
      "Train Epoch: 085 Batch: 00038/00094 | Loss: 79.6944 | CE: 0.0781 | KD: 1060.8822\n",
      "Train Epoch: 085 Batch: 00039/00094 | Loss: 79.7947 | CE: 0.1602 | KD: 1061.1245\n",
      "Train Epoch: 085 Batch: 00040/00094 | Loss: 79.7049 | CE: 0.0499 | KD: 1061.3970\n",
      "Train Epoch: 085 Batch: 00041/00094 | Loss: 79.6981 | CE: 0.0615 | KD: 1061.1509\n",
      "Train Epoch: 085 Batch: 00042/00094 | Loss: 79.7106 | CE: 0.0817 | KD: 1061.0490\n",
      "Train Epoch: 085 Batch: 00043/00094 | Loss: 79.7258 | CE: 0.0708 | KD: 1061.3962\n",
      "Train Epoch: 085 Batch: 00044/00094 | Loss: 79.6772 | CE: 0.0456 | KD: 1061.0851\n",
      "Train Epoch: 085 Batch: 00045/00094 | Loss: 79.7284 | CE: 0.0695 | KD: 1061.4498\n",
      "Train Epoch: 085 Batch: 00046/00094 | Loss: 79.6765 | CE: 0.0618 | KD: 1060.8607\n",
      "Train Epoch: 085 Batch: 00047/00094 | Loss: 79.6633 | CE: 0.0694 | KD: 1060.5830\n",
      "Train Epoch: 085 Batch: 00048/00094 | Loss: 79.6784 | CE: 0.0588 | KD: 1060.9259\n",
      "Train Epoch: 085 Batch: 00049/00094 | Loss: 79.7685 | CE: 0.1012 | KD: 1061.5612\n",
      "Train Epoch: 085 Batch: 00050/00094 | Loss: 79.7303 | CE: 0.0551 | KD: 1061.6653\n",
      "Train Epoch: 085 Batch: 00051/00094 | Loss: 79.7077 | CE: 0.0640 | KD: 1061.2456\n",
      "Train Epoch: 085 Batch: 00052/00094 | Loss: 79.7177 | CE: 0.0891 | KD: 1061.0460\n",
      "Train Epoch: 085 Batch: 00053/00094 | Loss: 79.7315 | CE: 0.0916 | KD: 1061.1958\n",
      "Train Epoch: 085 Batch: 00054/00094 | Loss: 79.7037 | CE: 0.0445 | KD: 1061.4529\n",
      "Train Epoch: 085 Batch: 00055/00094 | Loss: 79.7721 | CE: 0.1075 | KD: 1061.5240\n",
      "Train Epoch: 085 Batch: 00056/00094 | Loss: 79.6912 | CE: 0.0517 | KD: 1061.1910\n",
      "Train Epoch: 085 Batch: 00057/00094 | Loss: 79.7038 | CE: 0.0559 | KD: 1061.3029\n",
      "Train Epoch: 085 Batch: 00058/00094 | Loss: 79.7731 | CE: 0.1165 | KD: 1061.4186\n",
      "Train Epoch: 085 Batch: 00059/00094 | Loss: 79.6726 | CE: 0.0465 | KD: 1061.0123\n",
      "Train Epoch: 085 Batch: 00060/00094 | Loss: 79.7600 | CE: 0.0838 | KD: 1061.6791\n",
      "Train Epoch: 085 Batch: 00061/00094 | Loss: 79.6562 | CE: 0.0538 | KD: 1060.6971\n",
      "Train Epoch: 085 Batch: 00062/00094 | Loss: 79.7072 | CE: 0.0510 | KD: 1061.4138\n",
      "Train Epoch: 085 Batch: 00063/00094 | Loss: 79.6911 | CE: 0.0490 | KD: 1061.2257\n",
      "Train Epoch: 085 Batch: 00064/00094 | Loss: 79.6901 | CE: 0.0666 | KD: 1060.9779\n",
      "Train Epoch: 085 Batch: 00065/00094 | Loss: 79.6814 | CE: 0.0438 | KD: 1061.1666\n",
      "Train Epoch: 085 Batch: 00066/00094 | Loss: 79.7611 | CE: 0.0754 | KD: 1061.8057\n",
      "Train Epoch: 085 Batch: 00067/00094 | Loss: 79.6795 | CE: 0.0645 | KD: 1060.8641\n",
      "Train Epoch: 085 Batch: 00068/00094 | Loss: 79.6948 | CE: 0.0517 | KD: 1061.2396\n",
      "Train Epoch: 085 Batch: 00069/00094 | Loss: 79.6559 | CE: 0.0553 | KD: 1060.6716\n",
      "Train Epoch: 085 Batch: 00070/00094 | Loss: 79.7516 | CE: 0.0975 | KD: 1061.3856\n",
      "Train Epoch: 085 Batch: 00071/00094 | Loss: 79.6905 | CE: 0.0426 | KD: 1061.3014\n",
      "Train Epoch: 085 Batch: 00072/00094 | Loss: 79.7167 | CE: 0.0573 | KD: 1061.4554\n",
      "Train Epoch: 085 Batch: 00073/00094 | Loss: 79.6854 | CE: 0.0414 | KD: 1061.2510\n",
      "Train Epoch: 085 Batch: 00074/00094 | Loss: 79.6808 | CE: 0.0382 | KD: 1061.2321\n",
      "Train Epoch: 085 Batch: 00075/00094 | Loss: 79.7439 | CE: 0.0805 | KD: 1061.5082\n",
      "Train Epoch: 085 Batch: 00076/00094 | Loss: 79.6927 | CE: 0.0563 | KD: 1061.1494\n",
      "Train Epoch: 085 Batch: 00077/00094 | Loss: 79.6748 | CE: 0.0557 | KD: 1060.9187\n",
      "Train Epoch: 085 Batch: 00078/00094 | Loss: 79.6800 | CE: 0.0599 | KD: 1060.9325\n",
      "Train Epoch: 085 Batch: 00079/00094 | Loss: 79.6765 | CE: 0.0540 | KD: 1060.9645\n",
      "Train Epoch: 085 Batch: 00080/00094 | Loss: 79.6440 | CE: 0.0526 | KD: 1060.5504\n",
      "Train Epoch: 085 Batch: 00081/00094 | Loss: 79.6894 | CE: 0.0659 | KD: 1060.9772\n",
      "Train Epoch: 085 Batch: 00082/00094 | Loss: 79.7145 | CE: 0.0692 | KD: 1061.2688\n",
      "Train Epoch: 085 Batch: 00083/00094 | Loss: 79.6810 | CE: 0.0462 | KD: 1061.1281\n",
      "Train Epoch: 085 Batch: 00084/00094 | Loss: 79.6937 | CE: 0.0543 | KD: 1061.1896\n",
      "Train Epoch: 085 Batch: 00085/00094 | Loss: 79.7529 | CE: 0.0731 | KD: 1061.7273\n",
      "Train Epoch: 085 Batch: 00086/00094 | Loss: 79.7172 | CE: 0.0763 | KD: 1061.2089\n",
      "Train Epoch: 085 Batch: 00087/00094 | Loss: 79.7308 | CE: 0.0954 | KD: 1061.1351\n",
      "Train Epoch: 085 Batch: 00088/00094 | Loss: 79.6955 | CE: 0.0470 | KD: 1061.3097\n",
      "Train Epoch: 085 Batch: 00089/00094 | Loss: 79.6978 | CE: 0.0439 | KD: 1061.3828\n",
      "Train Epoch: 085 Batch: 00090/00094 | Loss: 79.7123 | CE: 0.0747 | KD: 1061.1658\n",
      "Train Epoch: 085 Batch: 00091/00094 | Loss: 79.6900 | CE: 0.0542 | KD: 1061.1406\n",
      "Train Epoch: 085 Batch: 00092/00094 | Loss: 79.6606 | CE: 0.0483 | KD: 1060.8286\n",
      "Train Epoch: 085 Batch: 00093/00094 | Loss: 79.7263 | CE: 0.0751 | KD: 1061.3472\n",
      "Train Epoch: 085 Batch: 00094/00094 | Loss: 79.8343 | CE: 0.1631 | KD: 1061.6130\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0602 | acc:98.9000\n",
      "[VAL Acc] Target: 98.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7956 | acc:49.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.3113 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3771 | acc:42.3664\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 42.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3093 | acc:56.5831\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7654 | acc:66.3586\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 66.36%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7362 | acc:71.9828\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.98%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9616 | acc:58.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.31%\n",
      "[VAL Acc] Avg 62.08%\n",
      "Train Epoch: 086 Batch: 00001/00094 | Loss: 79.6699 | CE: 0.0592 | KD: 1060.8068\n",
      "Train Epoch: 086 Batch: 00002/00094 | Loss: 79.7124 | CE: 0.0812 | KD: 1061.0807\n",
      "Train Epoch: 086 Batch: 00003/00094 | Loss: 79.7166 | CE: 0.0883 | KD: 1061.0410\n",
      "Train Epoch: 086 Batch: 00004/00094 | Loss: 79.6758 | CE: 0.0365 | KD: 1061.1884\n",
      "Train Epoch: 086 Batch: 00005/00094 | Loss: 79.7056 | CE: 0.0791 | KD: 1061.0179\n",
      "Train Epoch: 086 Batch: 00006/00094 | Loss: 79.7020 | CE: 0.0593 | KD: 1061.2323\n",
      "Train Epoch: 086 Batch: 00007/00094 | Loss: 79.6616 | CE: 0.0567 | KD: 1060.7294\n",
      "Train Epoch: 086 Batch: 00008/00094 | Loss: 79.6820 | CE: 0.0490 | KD: 1061.1036\n",
      "Train Epoch: 086 Batch: 00009/00094 | Loss: 79.7650 | CE: 0.0849 | KD: 1061.7314\n",
      "Train Epoch: 086 Batch: 00010/00094 | Loss: 79.7000 | CE: 0.0475 | KD: 1061.3633\n",
      "Train Epoch: 086 Batch: 00011/00094 | Loss: 79.6675 | CE: 0.0410 | KD: 1061.0176\n",
      "Train Epoch: 086 Batch: 00012/00094 | Loss: 79.8258 | CE: 0.1706 | KD: 1061.3992\n",
      "Train Epoch: 086 Batch: 00013/00094 | Loss: 79.7201 | CE: 0.0776 | KD: 1061.2305\n",
      "Train Epoch: 086 Batch: 00014/00094 | Loss: 79.6742 | CE: 0.0407 | KD: 1061.1102\n",
      "Train Epoch: 086 Batch: 00015/00094 | Loss: 79.7343 | CE: 0.0616 | KD: 1061.6335\n",
      "Train Epoch: 086 Batch: 00016/00094 | Loss: 79.7159 | CE: 0.0868 | KD: 1061.0515\n",
      "Train Epoch: 086 Batch: 00017/00094 | Loss: 79.6782 | CE: 0.0510 | KD: 1061.0258\n",
      "Train Epoch: 086 Batch: 00018/00094 | Loss: 79.6744 | CE: 0.0428 | KD: 1061.0852\n",
      "Train Epoch: 086 Batch: 00019/00094 | Loss: 79.6441 | CE: 0.0397 | KD: 1060.7228\n",
      "Train Epoch: 086 Batch: 00020/00094 | Loss: 79.7200 | CE: 0.0515 | KD: 1061.5764\n",
      "Train Epoch: 086 Batch: 00021/00094 | Loss: 79.7022 | CE: 0.0660 | KD: 1061.1471\n",
      "Train Epoch: 086 Batch: 00022/00094 | Loss: 79.6788 | CE: 0.0436 | KD: 1061.1329\n",
      "Train Epoch: 086 Batch: 00023/00094 | Loss: 79.7687 | CE: 0.1134 | KD: 1061.4009\n",
      "Train Epoch: 086 Batch: 00024/00094 | Loss: 79.6674 | CE: 0.0439 | KD: 1060.9764\n",
      "Train Epoch: 086 Batch: 00025/00094 | Loss: 79.7135 | CE: 0.0772 | KD: 1061.1482\n",
      "Train Epoch: 086 Batch: 00026/00094 | Loss: 79.6860 | CE: 0.0402 | KD: 1061.2737\n",
      "Train Epoch: 086 Batch: 00027/00094 | Loss: 79.6986 | CE: 0.0791 | KD: 1060.9240\n",
      "Train Epoch: 086 Batch: 00028/00094 | Loss: 79.6767 | CE: 0.0600 | KD: 1060.8861\n",
      "Train Epoch: 086 Batch: 00029/00094 | Loss: 79.6944 | CE: 0.0529 | KD: 1061.2167\n",
      "Train Epoch: 086 Batch: 00030/00094 | Loss: 79.7216 | CE: 0.0621 | KD: 1061.4575\n",
      "Train Epoch: 086 Batch: 00031/00094 | Loss: 79.6929 | CE: 0.0459 | KD: 1061.2897\n",
      "Train Epoch: 086 Batch: 00032/00094 | Loss: 79.6682 | CE: 0.0392 | KD: 1061.0507\n",
      "Train Epoch: 086 Batch: 00033/00094 | Loss: 79.7362 | CE: 0.0493 | KD: 1061.8218\n",
      "Train Epoch: 086 Batch: 00034/00094 | Loss: 79.8128 | CE: 0.1376 | KD: 1061.6656\n",
      "Train Epoch: 086 Batch: 00035/00094 | Loss: 79.7021 | CE: 0.0772 | KD: 1060.9960\n",
      "Train Epoch: 086 Batch: 00036/00094 | Loss: 79.6749 | CE: 0.0486 | KD: 1061.0138\n",
      "Train Epoch: 086 Batch: 00037/00094 | Loss: 79.6884 | CE: 0.0478 | KD: 1061.2053\n",
      "Train Epoch: 086 Batch: 00038/00094 | Loss: 79.7362 | CE: 0.0868 | KD: 1061.3218\n",
      "Train Epoch: 086 Batch: 00039/00094 | Loss: 79.6556 | CE: 0.0459 | KD: 1060.7930\n",
      "Train Epoch: 086 Batch: 00040/00094 | Loss: 79.6654 | CE: 0.0434 | KD: 1060.9578\n",
      "Train Epoch: 086 Batch: 00041/00094 | Loss: 79.6940 | CE: 0.0457 | KD: 1061.3086\n",
      "Train Epoch: 086 Batch: 00042/00094 | Loss: 79.7167 | CE: 0.0750 | KD: 1061.2197\n",
      "Train Epoch: 086 Batch: 00043/00094 | Loss: 79.7116 | CE: 0.0464 | KD: 1061.5339\n",
      "Train Epoch: 086 Batch: 00044/00094 | Loss: 79.7045 | CE: 0.0886 | KD: 1060.8761\n",
      "Train Epoch: 086 Batch: 00045/00094 | Loss: 79.7042 | CE: 0.0613 | KD: 1061.2363\n",
      "Train Epoch: 086 Batch: 00046/00094 | Loss: 79.7137 | CE: 0.0774 | KD: 1061.1481\n",
      "Train Epoch: 086 Batch: 00047/00094 | Loss: 79.7144 | CE: 0.0618 | KD: 1061.3656\n",
      "Train Epoch: 086 Batch: 00048/00094 | Loss: 79.6989 | CE: 0.0573 | KD: 1061.2194\n",
      "Train Epoch: 086 Batch: 00049/00094 | Loss: 79.6946 | CE: 0.0617 | KD: 1061.1025\n",
      "Train Epoch: 086 Batch: 00050/00094 | Loss: 79.6906 | CE: 0.0788 | KD: 1060.8209\n",
      "Train Epoch: 086 Batch: 00051/00094 | Loss: 79.6868 | CE: 0.0484 | KD: 1061.1763\n",
      "Train Epoch: 086 Batch: 00052/00094 | Loss: 79.7026 | CE: 0.0508 | KD: 1061.3541\n",
      "Train Epoch: 086 Batch: 00053/00094 | Loss: 79.6893 | CE: 0.0446 | KD: 1061.2599\n",
      "Train Epoch: 086 Batch: 00054/00094 | Loss: 79.7468 | CE: 0.1069 | KD: 1061.1954\n",
      "Train Epoch: 086 Batch: 00055/00094 | Loss: 79.7055 | CE: 0.0734 | KD: 1061.0920\n",
      "Train Epoch: 086 Batch: 00056/00094 | Loss: 79.6909 | CE: 0.0483 | KD: 1061.2317\n",
      "Train Epoch: 086 Batch: 00057/00094 | Loss: 79.7159 | CE: 0.0627 | KD: 1061.3717\n",
      "Train Epoch: 086 Batch: 00058/00094 | Loss: 79.7553 | CE: 0.1262 | KD: 1061.0515\n",
      "Train Epoch: 086 Batch: 00059/00094 | Loss: 79.6575 | CE: 0.0406 | KD: 1060.8885\n",
      "Train Epoch: 086 Batch: 00060/00094 | Loss: 79.7119 | CE: 0.0538 | KD: 1061.4384\n",
      "Train Epoch: 086 Batch: 00061/00094 | Loss: 79.6975 | CE: 0.0604 | KD: 1061.1591\n",
      "Train Epoch: 086 Batch: 00062/00094 | Loss: 79.6940 | CE: 0.0718 | KD: 1060.9597\n",
      "Train Epoch: 086 Batch: 00063/00094 | Loss: 79.6702 | CE: 0.0385 | KD: 1061.0872\n",
      "Train Epoch: 086 Batch: 00064/00094 | Loss: 79.7206 | CE: 0.0596 | KD: 1061.4763\n",
      "Train Epoch: 086 Batch: 00065/00094 | Loss: 79.7136 | CE: 0.0709 | KD: 1061.2340\n",
      "Train Epoch: 086 Batch: 00066/00094 | Loss: 79.6937 | CE: 0.0567 | KD: 1061.1575\n",
      "Train Epoch: 086 Batch: 00067/00094 | Loss: 79.7130 | CE: 0.0792 | KD: 1061.1147\n",
      "Train Epoch: 086 Batch: 00068/00094 | Loss: 79.7557 | CE: 0.0892 | KD: 1061.5507\n",
      "Train Epoch: 086 Batch: 00069/00094 | Loss: 79.6755 | CE: 0.0411 | KD: 1061.1228\n",
      "Train Epoch: 086 Batch: 00070/00094 | Loss: 79.6955 | CE: 0.0430 | KD: 1061.3639\n",
      "Train Epoch: 086 Batch: 00071/00094 | Loss: 79.7116 | CE: 0.0500 | KD: 1061.4847\n",
      "Train Epoch: 086 Batch: 00072/00094 | Loss: 79.6839 | CE: 0.0496 | KD: 1061.1211\n",
      "Train Epoch: 086 Batch: 00073/00094 | Loss: 79.6928 | CE: 0.0587 | KD: 1061.1191\n",
      "Train Epoch: 086 Batch: 00074/00094 | Loss: 79.6970 | CE: 0.0629 | KD: 1061.1180\n",
      "Train Epoch: 086 Batch: 00075/00094 | Loss: 79.6803 | CE: 0.0504 | KD: 1061.0627\n",
      "Train Epoch: 086 Batch: 00076/00094 | Loss: 79.6863 | CE: 0.0390 | KD: 1061.2947\n",
      "Train Epoch: 086 Batch: 00077/00094 | Loss: 79.6995 | CE: 0.0518 | KD: 1061.3002\n",
      "Train Epoch: 086 Batch: 00078/00094 | Loss: 79.6980 | CE: 0.0563 | KD: 1061.2200\n",
      "Train Epoch: 086 Batch: 00079/00094 | Loss: 79.7312 | CE: 0.1170 | KD: 1060.8546\n",
      "Train Epoch: 086 Batch: 00080/00094 | Loss: 79.6927 | CE: 0.0443 | KD: 1061.3101\n",
      "Train Epoch: 086 Batch: 00081/00094 | Loss: 79.7475 | CE: 0.0929 | KD: 1061.3921\n",
      "Train Epoch: 086 Batch: 00082/00094 | Loss: 79.6752 | CE: 0.0423 | KD: 1061.1018\n",
      "Train Epoch: 086 Batch: 00083/00094 | Loss: 79.7063 | CE: 0.0691 | KD: 1061.1599\n",
      "Train Epoch: 086 Batch: 00084/00094 | Loss: 79.6911 | CE: 0.0459 | KD: 1061.2673\n",
      "Train Epoch: 086 Batch: 00085/00094 | Loss: 79.7096 | CE: 0.0411 | KD: 1061.5764\n",
      "Train Epoch: 086 Batch: 00086/00094 | Loss: 79.7026 | CE: 0.0589 | KD: 1061.2462\n",
      "Train Epoch: 086 Batch: 00087/00094 | Loss: 79.7289 | CE: 0.0833 | KD: 1061.2723\n",
      "Train Epoch: 086 Batch: 00088/00094 | Loss: 79.6953 | CE: 0.0478 | KD: 1061.2977\n",
      "Train Epoch: 086 Batch: 00089/00094 | Loss: 79.7103 | CE: 0.0611 | KD: 1061.3198\n",
      "Train Epoch: 086 Batch: 00090/00094 | Loss: 79.6818 | CE: 0.0415 | KD: 1061.2013\n",
      "Train Epoch: 086 Batch: 00091/00094 | Loss: 79.6998 | CE: 0.0770 | KD: 1060.9681\n",
      "Train Epoch: 086 Batch: 00092/00094 | Loss: 79.7261 | CE: 0.0710 | KD: 1061.3978\n",
      "Train Epoch: 086 Batch: 00093/00094 | Loss: 79.6800 | CE: 0.0449 | KD: 1061.1323\n",
      "Train Epoch: 086 Batch: 00094/00094 | Loss: 79.7399 | CE: 0.1459 | KD: 1060.5842\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0691 | acc:98.3000\n",
      "[VAL Acc] Target: 98.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7647 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2610 | acc:51.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3474 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2832 | acc:57.2492\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0643 | acc:57.4861\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 57.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6446 | acc:74.9216\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9979 | acc:58.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.13%\n",
      "[VAL Acc] Avg 61.54%\n",
      "Train Epoch: 087 Batch: 00001/00094 | Loss: 79.6600 | CE: 0.0451 | KD: 1060.8623\n",
      "Train Epoch: 087 Batch: 00002/00094 | Loss: 79.7862 | CE: 0.1274 | KD: 1061.4487\n",
      "Train Epoch: 087 Batch: 00003/00094 | Loss: 79.6671 | CE: 0.0537 | KD: 1060.8428\n",
      "Train Epoch: 087 Batch: 00004/00094 | Loss: 79.7089 | CE: 0.0727 | KD: 1061.1458\n",
      "Train Epoch: 087 Batch: 00005/00094 | Loss: 79.7153 | CE: 0.0802 | KD: 1061.1326\n",
      "Train Epoch: 087 Batch: 00006/00094 | Loss: 79.6929 | CE: 0.0402 | KD: 1061.3672\n",
      "Train Epoch: 087 Batch: 00007/00094 | Loss: 79.6717 | CE: 0.0418 | KD: 1061.0630\n",
      "Train Epoch: 087 Batch: 00008/00094 | Loss: 79.7257 | CE: 0.0802 | KD: 1061.2710\n",
      "Train Epoch: 087 Batch: 00009/00094 | Loss: 79.7095 | CE: 0.0530 | KD: 1061.4171\n",
      "Train Epoch: 087 Batch: 00010/00094 | Loss: 79.6892 | CE: 0.0582 | KD: 1061.0773\n",
      "Train Epoch: 087 Batch: 00011/00094 | Loss: 79.6854 | CE: 0.0557 | KD: 1061.0608\n",
      "Train Epoch: 087 Batch: 00012/00094 | Loss: 79.7023 | CE: 0.0488 | KD: 1061.3777\n",
      "Train Epoch: 087 Batch: 00013/00094 | Loss: 79.7312 | CE: 0.0641 | KD: 1061.5579\n",
      "Train Epoch: 087 Batch: 00014/00094 | Loss: 79.7172 | CE: 0.0796 | KD: 1061.1665\n",
      "Train Epoch: 087 Batch: 00015/00094 | Loss: 79.7034 | CE: 0.0563 | KD: 1061.2921\n",
      "Train Epoch: 087 Batch: 00016/00094 | Loss: 79.7086 | CE: 0.0518 | KD: 1061.4210\n",
      "Train Epoch: 087 Batch: 00017/00094 | Loss: 79.6906 | CE: 0.0525 | KD: 1061.1727\n",
      "Train Epoch: 087 Batch: 00018/00094 | Loss: 79.6421 | CE: 0.0328 | KD: 1060.7881\n",
      "Train Epoch: 087 Batch: 00019/00094 | Loss: 79.6938 | CE: 0.0491 | KD: 1061.2587\n",
      "Train Epoch: 087 Batch: 00020/00094 | Loss: 79.6961 | CE: 0.0817 | KD: 1060.8562\n",
      "Train Epoch: 087 Batch: 00021/00094 | Loss: 79.6839 | CE: 0.0662 | KD: 1060.9004\n",
      "Train Epoch: 087 Batch: 00022/00094 | Loss: 79.7015 | CE: 0.0546 | KD: 1061.2893\n",
      "Train Epoch: 087 Batch: 00023/00094 | Loss: 79.7131 | CE: 0.0702 | KD: 1061.2349\n",
      "Train Epoch: 087 Batch: 00024/00094 | Loss: 79.6981 | CE: 0.0673 | KD: 1061.0740\n",
      "Train Epoch: 087 Batch: 00025/00094 | Loss: 79.7204 | CE: 0.0579 | KD: 1061.4966\n",
      "Train Epoch: 087 Batch: 00026/00094 | Loss: 79.6643 | CE: 0.0324 | KD: 1061.0897\n",
      "Train Epoch: 087 Batch: 00027/00094 | Loss: 79.6997 | CE: 0.0513 | KD: 1061.3085\n",
      "Train Epoch: 087 Batch: 00028/00094 | Loss: 79.8657 | CE: 0.1497 | KD: 1062.2100\n",
      "Train Epoch: 087 Batch: 00029/00094 | Loss: 79.6758 | CE: 0.0379 | KD: 1061.1696\n",
      "Train Epoch: 087 Batch: 00030/00094 | Loss: 79.6841 | CE: 0.0554 | KD: 1061.0463\n",
      "Train Epoch: 087 Batch: 00031/00094 | Loss: 79.6955 | CE: 0.0454 | KD: 1061.3315\n",
      "Train Epoch: 087 Batch: 00032/00094 | Loss: 79.6538 | CE: 0.0394 | KD: 1060.8561\n",
      "Train Epoch: 087 Batch: 00033/00094 | Loss: 79.7258 | CE: 0.0907 | KD: 1061.1324\n",
      "Train Epoch: 087 Batch: 00034/00094 | Loss: 79.6695 | CE: 0.0488 | KD: 1060.9397\n",
      "Train Epoch: 087 Batch: 00035/00094 | Loss: 79.7563 | CE: 0.0968 | KD: 1061.4575\n",
      "Train Epoch: 087 Batch: 00036/00094 | Loss: 79.7089 | CE: 0.0475 | KD: 1061.4817\n",
      "Train Epoch: 087 Batch: 00037/00094 | Loss: 79.6691 | CE: 0.0394 | KD: 1061.0604\n",
      "Train Epoch: 087 Batch: 00038/00094 | Loss: 79.6947 | CE: 0.0648 | KD: 1061.0634\n",
      "Train Epoch: 087 Batch: 00039/00094 | Loss: 79.6767 | CE: 0.0616 | KD: 1060.8656\n",
      "Train Epoch: 087 Batch: 00040/00094 | Loss: 79.7059 | CE: 0.0478 | KD: 1061.4379\n",
      "Train Epoch: 087 Batch: 00041/00094 | Loss: 79.6615 | CE: 0.0446 | KD: 1060.8888\n",
      "Train Epoch: 087 Batch: 00042/00094 | Loss: 79.6886 | CE: 0.0434 | KD: 1061.2668\n",
      "Train Epoch: 087 Batch: 00043/00094 | Loss: 79.6831 | CE: 0.0493 | KD: 1061.1149\n",
      "Train Epoch: 087 Batch: 00044/00094 | Loss: 79.7151 | CE: 0.0858 | KD: 1061.0542\n",
      "Train Epoch: 087 Batch: 00045/00094 | Loss: 79.6750 | CE: 0.0449 | KD: 1061.0659\n",
      "Train Epoch: 087 Batch: 00046/00094 | Loss: 79.7244 | CE: 0.0841 | KD: 1061.2004\n",
      "Train Epoch: 087 Batch: 00047/00094 | Loss: 79.7104 | CE: 0.0873 | KD: 1060.9724\n",
      "Train Epoch: 087 Batch: 00048/00094 | Loss: 79.7123 | CE: 0.0473 | KD: 1061.5306\n",
      "Train Epoch: 087 Batch: 00049/00094 | Loss: 79.7401 | CE: 0.0755 | KD: 1061.5250\n",
      "Train Epoch: 087 Batch: 00050/00094 | Loss: 79.6942 | CE: 0.0492 | KD: 1061.2648\n",
      "Train Epoch: 087 Batch: 00051/00094 | Loss: 79.7358 | CE: 0.0714 | KD: 1061.5232\n",
      "Train Epoch: 087 Batch: 00052/00094 | Loss: 79.6689 | CE: 0.0508 | KD: 1060.9044\n",
      "Train Epoch: 087 Batch: 00053/00094 | Loss: 79.6764 | CE: 0.0384 | KD: 1061.1705\n",
      "Train Epoch: 087 Batch: 00054/00094 | Loss: 79.6833 | CE: 0.0376 | KD: 1061.2725\n",
      "Train Epoch: 087 Batch: 00055/00094 | Loss: 79.7207 | CE: 0.0637 | KD: 1061.4235\n",
      "Train Epoch: 087 Batch: 00056/00094 | Loss: 79.6441 | CE: 0.0344 | KD: 1060.7941\n",
      "Train Epoch: 087 Batch: 00057/00094 | Loss: 79.7174 | CE: 0.0501 | KD: 1061.5599\n",
      "Train Epoch: 087 Batch: 00058/00094 | Loss: 79.7011 | CE: 0.0508 | KD: 1061.3350\n",
      "Train Epoch: 087 Batch: 00059/00094 | Loss: 79.7339 | CE: 0.1016 | KD: 1061.0953\n",
      "Train Epoch: 087 Batch: 00060/00094 | Loss: 79.6646 | CE: 0.0370 | KD: 1061.0317\n",
      "Train Epoch: 087 Batch: 00061/00094 | Loss: 79.6756 | CE: 0.0593 | KD: 1060.8807\n",
      "Train Epoch: 087 Batch: 00062/00094 | Loss: 79.6859 | CE: 0.0593 | KD: 1061.0186\n",
      "Train Epoch: 087 Batch: 00063/00094 | Loss: 79.7355 | CE: 0.1015 | KD: 1061.1179\n",
      "Train Epoch: 087 Batch: 00064/00094 | Loss: 79.7955 | CE: 0.1434 | KD: 1061.3593\n",
      "Train Epoch: 087 Batch: 00065/00094 | Loss: 79.6875 | CE: 0.0533 | KD: 1061.1199\n",
      "Train Epoch: 087 Batch: 00066/00094 | Loss: 79.6987 | CE: 0.0501 | KD: 1061.3119\n",
      "Train Epoch: 087 Batch: 00067/00094 | Loss: 79.6838 | CE: 0.0575 | KD: 1061.0149\n",
      "Train Epoch: 087 Batch: 00068/00094 | Loss: 79.7023 | CE: 0.0410 | KD: 1061.4807\n",
      "Train Epoch: 087 Batch: 00069/00094 | Loss: 79.7515 | CE: 0.0985 | KD: 1061.3705\n",
      "Train Epoch: 087 Batch: 00070/00094 | Loss: 79.6550 | CE: 0.0385 | KD: 1060.8843\n",
      "Train Epoch: 087 Batch: 00071/00094 | Loss: 79.7086 | CE: 0.0797 | KD: 1061.0504\n",
      "Train Epoch: 087 Batch: 00072/00094 | Loss: 79.6938 | CE: 0.0536 | KD: 1061.2001\n",
      "Train Epoch: 087 Batch: 00073/00094 | Loss: 79.6831 | CE: 0.0423 | KD: 1061.2080\n",
      "Train Epoch: 087 Batch: 00074/00094 | Loss: 79.7466 | CE: 0.0575 | KD: 1061.8523\n",
      "Train Epoch: 087 Batch: 00075/00094 | Loss: 79.6732 | CE: 0.0574 | KD: 1060.8745\n",
      "Train Epoch: 087 Batch: 00076/00094 | Loss: 79.6648 | CE: 0.0511 | KD: 1060.8470\n",
      "Train Epoch: 087 Batch: 00077/00094 | Loss: 79.6625 | CE: 0.0377 | KD: 1060.9946\n",
      "Train Epoch: 087 Batch: 00078/00094 | Loss: 79.6826 | CE: 0.0457 | KD: 1061.1555\n",
      "Train Epoch: 087 Batch: 00079/00094 | Loss: 79.7080 | CE: 0.0625 | KD: 1061.2708\n",
      "Train Epoch: 087 Batch: 00080/00094 | Loss: 79.6696 | CE: 0.0552 | KD: 1060.8564\n",
      "Train Epoch: 087 Batch: 00081/00094 | Loss: 79.6579 | CE: 0.0373 | KD: 1060.9392\n",
      "Train Epoch: 087 Batch: 00082/00094 | Loss: 79.6914 | CE: 0.0540 | KD: 1061.1624\n",
      "Train Epoch: 087 Batch: 00083/00094 | Loss: 79.6909 | CE: 0.0405 | KD: 1061.3365\n",
      "Train Epoch: 087 Batch: 00084/00094 | Loss: 79.7054 | CE: 0.0442 | KD: 1061.4791\n",
      "Train Epoch: 087 Batch: 00085/00094 | Loss: 79.7102 | CE: 0.0460 | KD: 1061.5203\n",
      "Train Epoch: 087 Batch: 00086/00094 | Loss: 79.6954 | CE: 0.0562 | KD: 1061.1855\n",
      "Train Epoch: 087 Batch: 00087/00094 | Loss: 79.6939 | CE: 0.0479 | KD: 1061.2764\n",
      "Train Epoch: 087 Batch: 00088/00094 | Loss: 79.7203 | CE: 0.0475 | KD: 1061.6339\n",
      "Train Epoch: 087 Batch: 00089/00094 | Loss: 79.7190 | CE: 0.0735 | KD: 1061.2706\n",
      "Train Epoch: 087 Batch: 00090/00094 | Loss: 79.7530 | CE: 0.0955 | KD: 1061.4308\n",
      "Train Epoch: 087 Batch: 00091/00094 | Loss: 79.6812 | CE: 0.0478 | KD: 1061.1099\n",
      "Train Epoch: 087 Batch: 00092/00094 | Loss: 79.6740 | CE: 0.0538 | KD: 1060.9336\n",
      "Train Epoch: 087 Batch: 00093/00094 | Loss: 79.6710 | CE: 0.0640 | KD: 1060.7567\n",
      "Train Epoch: 087 Batch: 00094/00094 | Loss: 79.7316 | CE: 0.0646 | KD: 1061.5570\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0576 | acc:99.1000\n",
      "[VAL Acc] Target: 99.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7515 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2982 | acc:50.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3338 | acc:42.9389\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 42.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3316 | acc:56.0737\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9281 | acc:61.0906\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 61.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6910 | acc:72.3746\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9782 | acc:58.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.44%\n",
      "[VAL Acc] Avg 61.21%\n",
      "Train Epoch: 088 Batch: 00001/00094 | Loss: 79.6977 | CE: 0.0482 | KD: 1061.3236\n",
      "Train Epoch: 088 Batch: 00002/00094 | Loss: 79.7370 | CE: 0.0574 | KD: 1061.7247\n",
      "Train Epoch: 088 Batch: 00003/00094 | Loss: 79.6737 | CE: 0.0614 | KD: 1060.8271\n",
      "Train Epoch: 088 Batch: 00004/00094 | Loss: 79.6865 | CE: 0.0479 | KD: 1061.1791\n",
      "Train Epoch: 088 Batch: 00005/00094 | Loss: 79.7440 | CE: 0.1160 | KD: 1061.0377\n",
      "Train Epoch: 088 Batch: 00006/00094 | Loss: 79.7659 | CE: 0.1215 | KD: 1061.2557\n",
      "Train Epoch: 088 Batch: 00007/00094 | Loss: 79.6698 | CE: 0.0455 | KD: 1060.9883\n",
      "Train Epoch: 088 Batch: 00008/00094 | Loss: 79.7266 | CE: 0.0764 | KD: 1061.3328\n",
      "Train Epoch: 088 Batch: 00009/00094 | Loss: 79.6961 | CE: 0.0379 | KD: 1061.4392\n",
      "Train Epoch: 088 Batch: 00010/00094 | Loss: 79.6595 | CE: 0.0431 | KD: 1060.8838\n",
      "Train Epoch: 088 Batch: 00011/00094 | Loss: 79.7091 | CE: 0.0668 | KD: 1061.2269\n",
      "Train Epoch: 088 Batch: 00012/00094 | Loss: 79.6902 | CE: 0.0788 | KD: 1060.8151\n",
      "Train Epoch: 088 Batch: 00013/00094 | Loss: 79.6691 | CE: 0.0485 | KD: 1060.9388\n",
      "Train Epoch: 088 Batch: 00014/00094 | Loss: 79.6831 | CE: 0.0391 | KD: 1061.2515\n",
      "Train Epoch: 088 Batch: 00015/00094 | Loss: 79.7062 | CE: 0.0749 | KD: 1061.0814\n",
      "Train Epoch: 088 Batch: 00016/00094 | Loss: 79.6761 | CE: 0.0388 | KD: 1061.1619\n",
      "Train Epoch: 088 Batch: 00017/00094 | Loss: 79.7023 | CE: 0.0675 | KD: 1061.1288\n",
      "Train Epoch: 088 Batch: 00018/00094 | Loss: 79.6924 | CE: 0.0472 | KD: 1061.2662\n",
      "Train Epoch: 088 Batch: 00019/00094 | Loss: 79.6709 | CE: 0.0458 | KD: 1060.9994\n",
      "Train Epoch: 088 Batch: 00020/00094 | Loss: 79.7174 | CE: 0.0919 | KD: 1061.0044\n",
      "Train Epoch: 088 Batch: 00021/00094 | Loss: 79.6814 | CE: 0.0408 | KD: 1061.2052\n",
      "Train Epoch: 088 Batch: 00022/00094 | Loss: 79.6971 | CE: 0.0512 | KD: 1061.2756\n",
      "Train Epoch: 088 Batch: 00023/00094 | Loss: 79.6586 | CE: 0.0413 | KD: 1060.8951\n",
      "Train Epoch: 088 Batch: 00024/00094 | Loss: 79.6994 | CE: 0.0521 | KD: 1061.2948\n",
      "Train Epoch: 088 Batch: 00025/00094 | Loss: 79.7232 | CE: 0.0937 | KD: 1061.0571\n",
      "Train Epoch: 088 Batch: 00026/00094 | Loss: 79.6766 | CE: 0.0399 | KD: 1061.1531\n",
      "Train Epoch: 088 Batch: 00027/00094 | Loss: 79.7293 | CE: 0.0783 | KD: 1061.3444\n",
      "Train Epoch: 088 Batch: 00028/00094 | Loss: 79.7121 | CE: 0.0737 | KD: 1061.1758\n",
      "Train Epoch: 088 Batch: 00029/00094 | Loss: 79.6920 | CE: 0.0764 | KD: 1060.8717\n",
      "Train Epoch: 088 Batch: 00030/00094 | Loss: 79.7059 | CE: 0.0473 | KD: 1061.4458\n",
      "Train Epoch: 088 Batch: 00031/00094 | Loss: 79.6751 | CE: 0.0610 | KD: 1060.8518\n",
      "Train Epoch: 088 Batch: 00032/00094 | Loss: 79.7241 | CE: 0.0519 | KD: 1061.6270\n",
      "Train Epoch: 088 Batch: 00033/00094 | Loss: 79.7309 | CE: 0.0574 | KD: 1061.6433\n",
      "Train Epoch: 088 Batch: 00034/00094 | Loss: 79.6904 | CE: 0.0467 | KD: 1061.2472\n",
      "Train Epoch: 088 Batch: 00035/00094 | Loss: 79.6898 | CE: 0.0437 | KD: 1061.2782\n",
      "Train Epoch: 088 Batch: 00036/00094 | Loss: 79.7057 | CE: 0.0551 | KD: 1061.3387\n",
      "Train Epoch: 088 Batch: 00037/00094 | Loss: 79.6787 | CE: 0.0604 | KD: 1060.9083\n",
      "Train Epoch: 088 Batch: 00038/00094 | Loss: 79.6691 | CE: 0.0393 | KD: 1061.0604\n",
      "Train Epoch: 088 Batch: 00039/00094 | Loss: 79.7085 | CE: 0.0732 | KD: 1061.1345\n",
      "Train Epoch: 088 Batch: 00040/00094 | Loss: 79.7060 | CE: 0.0658 | KD: 1061.2006\n",
      "Train Epoch: 088 Batch: 00041/00094 | Loss: 79.6770 | CE: 0.0451 | KD: 1061.0903\n",
      "Train Epoch: 088 Batch: 00042/00094 | Loss: 79.6749 | CE: 0.0508 | KD: 1060.9849\n",
      "Train Epoch: 088 Batch: 00043/00094 | Loss: 79.6628 | CE: 0.0486 | KD: 1060.8541\n",
      "Train Epoch: 088 Batch: 00044/00094 | Loss: 79.6919 | CE: 0.0482 | KD: 1061.2462\n",
      "Train Epoch: 088 Batch: 00045/00094 | Loss: 79.7017 | CE: 0.0472 | KD: 1061.3903\n",
      "Train Epoch: 088 Batch: 00046/00094 | Loss: 79.7149 | CE: 0.0560 | KD: 1061.4490\n",
      "Train Epoch: 088 Batch: 00047/00094 | Loss: 79.6935 | CE: 0.0445 | KD: 1061.3169\n",
      "Train Epoch: 088 Batch: 00048/00094 | Loss: 79.7210 | CE: 0.0711 | KD: 1061.3290\n",
      "Train Epoch: 088 Batch: 00049/00094 | Loss: 79.6956 | CE: 0.0357 | KD: 1061.4614\n",
      "Train Epoch: 088 Batch: 00050/00094 | Loss: 79.6792 | CE: 0.0374 | KD: 1061.2211\n",
      "Train Epoch: 088 Batch: 00051/00094 | Loss: 79.7424 | CE: 0.1144 | KD: 1061.0374\n",
      "Train Epoch: 088 Batch: 00052/00094 | Loss: 79.6758 | CE: 0.0508 | KD: 1060.9980\n",
      "Train Epoch: 088 Batch: 00053/00094 | Loss: 79.7007 | CE: 0.0664 | KD: 1061.1204\n",
      "Train Epoch: 088 Batch: 00054/00094 | Loss: 79.6812 | CE: 0.0620 | KD: 1060.9200\n",
      "Train Epoch: 088 Batch: 00055/00094 | Loss: 79.7162 | CE: 0.0524 | KD: 1061.5131\n",
      "Train Epoch: 088 Batch: 00056/00094 | Loss: 79.6641 | CE: 0.0470 | KD: 1060.8917\n",
      "Train Epoch: 088 Batch: 00057/00094 | Loss: 79.8311 | CE: 0.1742 | KD: 1061.4225\n",
      "Train Epoch: 088 Batch: 00058/00094 | Loss: 79.6872 | CE: 0.0417 | KD: 1061.2704\n",
      "Train Epoch: 088 Batch: 00059/00094 | Loss: 79.6852 | CE: 0.0499 | KD: 1061.1346\n",
      "Train Epoch: 088 Batch: 00060/00094 | Loss: 79.6972 | CE: 0.0511 | KD: 1061.2782\n",
      "Train Epoch: 088 Batch: 00061/00094 | Loss: 79.7789 | CE: 0.1615 | KD: 1060.8954\n",
      "Train Epoch: 088 Batch: 00062/00094 | Loss: 79.7101 | CE: 0.0391 | KD: 1061.6106\n",
      "Train Epoch: 088 Batch: 00063/00094 | Loss: 79.6669 | CE: 0.0465 | KD: 1060.9363\n",
      "Train Epoch: 088 Batch: 00064/00094 | Loss: 79.6924 | CE: 0.0445 | KD: 1061.3014\n",
      "Train Epoch: 088 Batch: 00065/00094 | Loss: 79.6769 | CE: 0.0613 | KD: 1060.8723\n",
      "Train Epoch: 088 Batch: 00066/00094 | Loss: 79.7136 | CE: 0.0368 | KD: 1061.6882\n",
      "Train Epoch: 088 Batch: 00067/00094 | Loss: 79.7006 | CE: 0.0608 | KD: 1061.1942\n",
      "Train Epoch: 088 Batch: 00068/00094 | Loss: 79.7191 | CE: 0.0651 | KD: 1061.3842\n",
      "Train Epoch: 088 Batch: 00069/00094 | Loss: 79.6899 | CE: 0.0469 | KD: 1061.2375\n",
      "Train Epoch: 088 Batch: 00070/00094 | Loss: 79.6670 | CE: 0.0440 | KD: 1060.9711\n",
      "Train Epoch: 088 Batch: 00071/00094 | Loss: 79.6873 | CE: 0.0726 | KD: 1060.8604\n",
      "Train Epoch: 088 Batch: 00072/00094 | Loss: 79.7256 | CE: 0.0829 | KD: 1061.2329\n",
      "Train Epoch: 088 Batch: 00073/00094 | Loss: 79.7390 | CE: 0.0429 | KD: 1061.9447\n",
      "Train Epoch: 088 Batch: 00074/00094 | Loss: 79.6868 | CE: 0.0425 | KD: 1061.2549\n",
      "Train Epoch: 088 Batch: 00075/00094 | Loss: 79.6761 | CE: 0.0510 | KD: 1060.9983\n",
      "Train Epoch: 088 Batch: 00076/00094 | Loss: 79.6738 | CE: 0.0499 | KD: 1060.9819\n",
      "Train Epoch: 088 Batch: 00077/00094 | Loss: 79.6825 | CE: 0.0455 | KD: 1061.1572\n",
      "Train Epoch: 088 Batch: 00078/00094 | Loss: 79.7054 | CE: 0.0555 | KD: 1061.3297\n",
      "Train Epoch: 088 Batch: 00079/00094 | Loss: 79.6716 | CE: 0.0340 | KD: 1061.1653\n",
      "Train Epoch: 088 Batch: 00080/00094 | Loss: 79.6864 | CE: 0.0599 | KD: 1061.0172\n",
      "Train Epoch: 088 Batch: 00081/00094 | Loss: 79.7916 | CE: 0.1145 | KD: 1061.6908\n",
      "Train Epoch: 088 Batch: 00082/00094 | Loss: 79.6704 | CE: 0.0429 | KD: 1061.0298\n",
      "Train Epoch: 088 Batch: 00083/00094 | Loss: 79.7141 | CE: 0.0681 | KD: 1061.2770\n",
      "Train Epoch: 088 Batch: 00084/00094 | Loss: 79.7119 | CE: 0.0692 | KD: 1061.2330\n",
      "Train Epoch: 088 Batch: 00085/00094 | Loss: 79.7377 | CE: 0.0971 | KD: 1061.2050\n",
      "Train Epoch: 088 Batch: 00086/00094 | Loss: 79.7334 | CE: 0.0926 | KD: 1061.2079\n",
      "Train Epoch: 088 Batch: 00087/00094 | Loss: 79.6984 | CE: 0.0559 | KD: 1061.2303\n",
      "Train Epoch: 088 Batch: 00088/00094 | Loss: 79.6470 | CE: 0.0350 | KD: 1060.8235\n",
      "Train Epoch: 088 Batch: 00089/00094 | Loss: 79.6939 | CE: 0.0480 | KD: 1061.2766\n",
      "Train Epoch: 088 Batch: 00090/00094 | Loss: 79.6880 | CE: 0.0611 | KD: 1061.0220\n",
      "Train Epoch: 088 Batch: 00091/00094 | Loss: 79.7077 | CE: 0.0401 | KD: 1061.5654\n",
      "Train Epoch: 088 Batch: 00092/00094 | Loss: 79.6776 | CE: 0.0325 | KD: 1061.2655\n",
      "Train Epoch: 088 Batch: 00093/00094 | Loss: 79.6872 | CE: 0.0473 | KD: 1061.1963\n",
      "Train Epoch: 088 Batch: 00094/00094 | Loss: 79.7223 | CE: 0.0423 | KD: 1061.7295\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0526 | acc:99.3500\n",
      "[VAL Acc] Target: 99.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7387 | acc:48.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1967 | acc:53.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3164 | acc:42.9389\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 42.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3145 | acc:55.9169\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8986 | acc:62.4769\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 62.48%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6642 | acc:73.7461\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9743 | acc:58.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.31%\n",
      "[VAL Acc] Avg 61.83%\n",
      "Train Epoch: 089 Batch: 00001/00094 | Loss: 71.7173 | CE: 0.0396 | KD: 1061.2227\n",
      "Train Epoch: 089 Batch: 00002/00094 | Loss: 71.6980 | CE: 0.0361 | KD: 1060.9882\n",
      "Train Epoch: 089 Batch: 00003/00094 | Loss: 71.7704 | CE: 0.0700 | KD: 1061.5577\n",
      "Train Epoch: 089 Batch: 00004/00094 | Loss: 71.7697 | CE: 0.0695 | KD: 1061.5557\n",
      "Train Epoch: 089 Batch: 00005/00094 | Loss: 71.7472 | CE: 0.0418 | KD: 1061.6327\n",
      "Train Epoch: 089 Batch: 00006/00094 | Loss: 71.7232 | CE: 0.0541 | KD: 1061.0956\n",
      "Train Epoch: 089 Batch: 00007/00094 | Loss: 71.7409 | CE: 0.0441 | KD: 1061.5056\n",
      "Train Epoch: 089 Batch: 00008/00094 | Loss: 71.7095 | CE: 0.0419 | KD: 1061.0725\n",
      "Train Epoch: 089 Batch: 00009/00094 | Loss: 71.7589 | CE: 0.0503 | KD: 1061.6797\n",
      "Train Epoch: 089 Batch: 00010/00094 | Loss: 71.7154 | CE: 0.0667 | KD: 1060.7933\n",
      "Train Epoch: 089 Batch: 00011/00094 | Loss: 71.7284 | CE: 0.0443 | KD: 1061.3174\n",
      "Train Epoch: 089 Batch: 00012/00094 | Loss: 71.7247 | CE: 0.0662 | KD: 1060.9393\n",
      "Train Epoch: 089 Batch: 00013/00094 | Loss: 71.7509 | CE: 0.0909 | KD: 1060.9612\n",
      "Train Epoch: 089 Batch: 00014/00094 | Loss: 71.7067 | CE: 0.0470 | KD: 1060.9562\n",
      "Train Epoch: 089 Batch: 00015/00094 | Loss: 71.7786 | CE: 0.0824 | KD: 1061.4957\n",
      "Train Epoch: 089 Batch: 00016/00094 | Loss: 71.7692 | CE: 0.0905 | KD: 1061.2362\n",
      "Train Epoch: 089 Batch: 00017/00094 | Loss: 71.7089 | CE: 0.0393 | KD: 1061.1031\n",
      "Train Epoch: 089 Batch: 00018/00094 | Loss: 71.7347 | CE: 0.0692 | KD: 1061.0428\n",
      "Train Epoch: 089 Batch: 00019/00094 | Loss: 71.7633 | CE: 0.0693 | KD: 1061.4645\n",
      "Train Epoch: 089 Batch: 00020/00094 | Loss: 71.7094 | CE: 0.0401 | KD: 1061.0973\n",
      "Train Epoch: 089 Batch: 00021/00094 | Loss: 71.7265 | CE: 0.0451 | KD: 1061.2772\n",
      "Train Epoch: 089 Batch: 00022/00094 | Loss: 71.7758 | CE: 0.0820 | KD: 1061.4612\n",
      "Train Epoch: 089 Batch: 00023/00094 | Loss: 71.8132 | CE: 0.1270 | KD: 1061.3479\n",
      "Train Epoch: 089 Batch: 00024/00094 | Loss: 71.7526 | CE: 0.0759 | KD: 1061.2076\n",
      "Train Epoch: 089 Batch: 00025/00094 | Loss: 71.7288 | CE: 0.0515 | KD: 1061.2172\n",
      "Train Epoch: 089 Batch: 00026/00094 | Loss: 71.7717 | CE: 0.0714 | KD: 1061.5571\n",
      "Train Epoch: 089 Batch: 00027/00094 | Loss: 71.7998 | CE: 0.0858 | KD: 1061.7595\n",
      "Train Epoch: 089 Batch: 00028/00094 | Loss: 71.7260 | CE: 0.0503 | KD: 1061.1930\n",
      "Train Epoch: 089 Batch: 00029/00094 | Loss: 71.7108 | CE: 0.0440 | KD: 1061.0605\n",
      "Train Epoch: 089 Batch: 00030/00094 | Loss: 71.7482 | CE: 0.0671 | KD: 1061.2721\n",
      "Train Epoch: 089 Batch: 00031/00094 | Loss: 71.7406 | CE: 0.0577 | KD: 1061.2997\n",
      "Train Epoch: 089 Batch: 00032/00094 | Loss: 71.8334 | CE: 0.1265 | KD: 1061.6545\n",
      "Train Epoch: 089 Batch: 00033/00094 | Loss: 71.7569 | CE: 0.0790 | KD: 1061.2256\n",
      "Train Epoch: 089 Batch: 00034/00094 | Loss: 71.7503 | CE: 0.0678 | KD: 1061.2927\n",
      "Train Epoch: 089 Batch: 00035/00094 | Loss: 71.7153 | CE: 0.0432 | KD: 1061.1404\n",
      "Train Epoch: 089 Batch: 00036/00094 | Loss: 71.7299 | CE: 0.0376 | KD: 1061.4374\n",
      "Train Epoch: 089 Batch: 00037/00094 | Loss: 71.7217 | CE: 0.0524 | KD: 1061.0983\n",
      "Train Epoch: 089 Batch: 00038/00094 | Loss: 71.7112 | CE: 0.0418 | KD: 1061.0995\n",
      "Train Epoch: 089 Batch: 00039/00094 | Loss: 71.7018 | CE: 0.0428 | KD: 1060.9454\n",
      "Train Epoch: 089 Batch: 00040/00094 | Loss: 71.8380 | CE: 0.1303 | KD: 1061.6659\n",
      "Train Epoch: 089 Batch: 00041/00094 | Loss: 71.7330 | CE: 0.0623 | KD: 1061.1200\n",
      "Train Epoch: 089 Batch: 00042/00094 | Loss: 71.7481 | CE: 0.0629 | KD: 1061.3337\n",
      "Train Epoch: 089 Batch: 00043/00094 | Loss: 71.7248 | CE: 0.0315 | KD: 1061.4532\n",
      "Train Epoch: 089 Batch: 00044/00094 | Loss: 71.7812 | CE: 0.1174 | KD: 1061.0171\n",
      "Train Epoch: 089 Batch: 00045/00094 | Loss: 71.7643 | CE: 0.0833 | KD: 1061.2714\n",
      "Train Epoch: 089 Batch: 00046/00094 | Loss: 71.7098 | CE: 0.0536 | KD: 1060.9041\n",
      "Train Epoch: 089 Batch: 00047/00094 | Loss: 71.7217 | CE: 0.0503 | KD: 1061.1294\n",
      "Train Epoch: 089 Batch: 00048/00094 | Loss: 71.7109 | CE: 0.0585 | KD: 1060.8473\n",
      "Train Epoch: 089 Batch: 00049/00094 | Loss: 71.6960 | CE: 0.0408 | KD: 1060.8895\n",
      "Train Epoch: 089 Batch: 00050/00094 | Loss: 71.6909 | CE: 0.0368 | KD: 1060.8741\n",
      "Train Epoch: 089 Batch: 00051/00094 | Loss: 71.7821 | CE: 0.1104 | KD: 1061.1335\n",
      "Train Epoch: 089 Batch: 00052/00094 | Loss: 71.7032 | CE: 0.0378 | KD: 1061.0403\n",
      "Train Epoch: 089 Batch: 00053/00094 | Loss: 71.7873 | CE: 0.0917 | KD: 1061.4873\n",
      "Train Epoch: 089 Batch: 00054/00094 | Loss: 71.6844 | CE: 0.0321 | KD: 1060.8473\n",
      "Train Epoch: 089 Batch: 00055/00094 | Loss: 71.7539 | CE: 0.0712 | KD: 1061.2957\n",
      "Train Epoch: 089 Batch: 00056/00094 | Loss: 71.7529 | CE: 0.0779 | KD: 1061.1818\n",
      "Train Epoch: 089 Batch: 00057/00094 | Loss: 71.7419 | CE: 0.0443 | KD: 1061.5170\n",
      "Train Epoch: 089 Batch: 00058/00094 | Loss: 71.7407 | CE: 0.0747 | KD: 1061.0493\n",
      "Train Epoch: 089 Batch: 00059/00094 | Loss: 71.9297 | CE: 0.2231 | KD: 1061.6501\n",
      "Train Epoch: 089 Batch: 00060/00094 | Loss: 71.7562 | CE: 0.0618 | KD: 1061.4703\n",
      "Train Epoch: 089 Batch: 00061/00094 | Loss: 71.7058 | CE: 0.0544 | KD: 1060.8333\n",
      "Train Epoch: 089 Batch: 00062/00094 | Loss: 71.7705 | CE: 0.0831 | KD: 1061.3668\n",
      "Train Epoch: 089 Batch: 00063/00094 | Loss: 71.7255 | CE: 0.0361 | KD: 1061.3947\n",
      "Train Epoch: 089 Batch: 00064/00094 | Loss: 71.7771 | CE: 0.1057 | KD: 1061.1295\n",
      "Train Epoch: 089 Batch: 00065/00094 | Loss: 71.7841 | CE: 0.0717 | KD: 1061.7354\n",
      "Train Epoch: 089 Batch: 00066/00094 | Loss: 71.7120 | CE: 0.0378 | KD: 1061.1699\n",
      "Train Epoch: 089 Batch: 00067/00094 | Loss: 71.7174 | CE: 0.0573 | KD: 1060.9629\n",
      "Train Epoch: 089 Batch: 00068/00094 | Loss: 71.6954 | CE: 0.0316 | KD: 1061.0156\n",
      "Train Epoch: 089 Batch: 00069/00094 | Loss: 71.7347 | CE: 0.0478 | KD: 1061.3589\n",
      "Train Epoch: 089 Batch: 00070/00094 | Loss: 71.7057 | CE: 0.0455 | KD: 1060.9634\n",
      "Train Epoch: 089 Batch: 00071/00094 | Loss: 71.7578 | CE: 0.0636 | KD: 1061.4656\n",
      "Train Epoch: 089 Batch: 00072/00094 | Loss: 71.7088 | CE: 0.0635 | KD: 1060.7422\n",
      "Train Epoch: 089 Batch: 00073/00094 | Loss: 71.7092 | CE: 0.0475 | KD: 1060.9857\n",
      "Train Epoch: 089 Batch: 00074/00094 | Loss: 71.7416 | CE: 0.0507 | KD: 1061.4181\n",
      "Train Epoch: 089 Batch: 00075/00094 | Loss: 71.7220 | CE: 0.0633 | KD: 1060.9413\n",
      "Train Epoch: 089 Batch: 00076/00094 | Loss: 71.7589 | CE: 0.0730 | KD: 1061.3429\n",
      "Train Epoch: 089 Batch: 00077/00094 | Loss: 71.7381 | CE: 0.0390 | KD: 1061.5405\n",
      "Train Epoch: 089 Batch: 00078/00094 | Loss: 71.7359 | CE: 0.0526 | KD: 1061.3055\n",
      "Train Epoch: 089 Batch: 00079/00094 | Loss: 71.7006 | CE: 0.0489 | KD: 1060.8367\n",
      "Train Epoch: 089 Batch: 00080/00094 | Loss: 71.7297 | CE: 0.0401 | KD: 1061.3986\n",
      "Train Epoch: 089 Batch: 00081/00094 | Loss: 71.7499 | CE: 0.0444 | KD: 1061.6335\n",
      "Train Epoch: 089 Batch: 00082/00094 | Loss: 71.7176 | CE: 0.0465 | KD: 1061.1235\n",
      "Train Epoch: 089 Batch: 00083/00094 | Loss: 71.7368 | CE: 0.0458 | KD: 1061.4182\n",
      "Train Epoch: 089 Batch: 00084/00094 | Loss: 71.7123 | CE: 0.0462 | KD: 1061.0519\n",
      "Train Epoch: 089 Batch: 00085/00094 | Loss: 71.7559 | CE: 0.0516 | KD: 1061.6163\n",
      "Train Epoch: 089 Batch: 00086/00094 | Loss: 71.7759 | CE: 0.0600 | KD: 1061.7875\n",
      "Train Epoch: 089 Batch: 00087/00094 | Loss: 71.7306 | CE: 0.0430 | KD: 1061.3694\n",
      "Train Epoch: 089 Batch: 00088/00094 | Loss: 71.7206 | CE: 0.0558 | KD: 1061.0321\n",
      "Train Epoch: 089 Batch: 00089/00094 | Loss: 71.7241 | CE: 0.0452 | KD: 1061.2412\n",
      "Train Epoch: 089 Batch: 00090/00094 | Loss: 71.7587 | CE: 0.0727 | KD: 1061.3450\n",
      "Train Epoch: 089 Batch: 00091/00094 | Loss: 71.7348 | CE: 0.0343 | KD: 1061.5601\n",
      "Train Epoch: 089 Batch: 00092/00094 | Loss: 71.7292 | CE: 0.0613 | KD: 1061.0778\n",
      "Train Epoch: 089 Batch: 00093/00094 | Loss: 71.7189 | CE: 0.0562 | KD: 1061.0009\n",
      "Train Epoch: 089 Batch: 00094/00094 | Loss: 71.7438 | CE: 0.0398 | KD: 1061.6117\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0558 | acc:99.2000\n",
      "[VAL Acc] Target: 99.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8150 | acc:48.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2557 | acc:52.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3884 | acc:42.9389\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 42.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3357 | acc:56.0345\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9187 | acc:62.9390\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 62.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7091 | acc:72.4138\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9764 | acc:57.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.88%\n",
      "[VAL Acc] Avg 61.61%\n",
      "Train Epoch: 090 Batch: 00001/00094 | Loss: 71.7315 | CE: 0.0376 | KD: 1061.4614\n",
      "Train Epoch: 090 Batch: 00002/00094 | Loss: 71.7318 | CE: 0.0704 | KD: 1060.9801\n",
      "Train Epoch: 090 Batch: 00003/00094 | Loss: 71.7039 | CE: 0.0430 | KD: 1060.9738\n",
      "Train Epoch: 090 Batch: 00004/00094 | Loss: 71.8397 | CE: 0.1002 | KD: 1062.1378\n",
      "Train Epoch: 090 Batch: 00005/00094 | Loss: 71.7094 | CE: 0.0353 | KD: 1061.1691\n",
      "Train Epoch: 090 Batch: 00006/00094 | Loss: 71.7217 | CE: 0.0425 | KD: 1061.2441\n",
      "Train Epoch: 090 Batch: 00007/00094 | Loss: 71.7079 | CE: 0.0435 | KD: 1061.0251\n",
      "Train Epoch: 090 Batch: 00008/00094 | Loss: 71.7613 | CE: 0.0704 | KD: 1061.4187\n",
      "Train Epoch: 090 Batch: 00009/00094 | Loss: 71.7629 | CE: 0.0788 | KD: 1061.3182\n",
      "Train Epoch: 090 Batch: 00010/00094 | Loss: 71.7763 | CE: 0.0762 | KD: 1061.5549\n",
      "Train Epoch: 090 Batch: 00011/00094 | Loss: 71.7735 | CE: 0.0866 | KD: 1061.3583\n",
      "Train Epoch: 090 Batch: 00012/00094 | Loss: 71.7966 | CE: 0.0818 | KD: 1061.7721\n",
      "Train Epoch: 090 Batch: 00013/00094 | Loss: 71.7570 | CE: 0.0610 | KD: 1061.4939\n",
      "Train Epoch: 090 Batch: 00014/00094 | Loss: 71.7762 | CE: 0.0910 | KD: 1061.3334\n",
      "Train Epoch: 090 Batch: 00015/00094 | Loss: 71.7258 | CE: 0.0439 | KD: 1061.2841\n",
      "Train Epoch: 090 Batch: 00016/00094 | Loss: 71.7103 | CE: 0.0447 | KD: 1061.0431\n",
      "Train Epoch: 090 Batch: 00017/00094 | Loss: 71.7648 | CE: 0.0487 | KD: 1061.7908\n",
      "Train Epoch: 090 Batch: 00018/00094 | Loss: 71.7935 | CE: 0.1471 | KD: 1060.7596\n",
      "Train Epoch: 090 Batch: 00019/00094 | Loss: 71.7016 | CE: 0.0562 | KD: 1060.7444\n",
      "Train Epoch: 090 Batch: 00020/00094 | Loss: 71.7261 | CE: 0.0493 | KD: 1061.2092\n",
      "Train Epoch: 090 Batch: 00021/00094 | Loss: 71.7274 | CE: 0.0519 | KD: 1061.1896\n",
      "Train Epoch: 090 Batch: 00022/00094 | Loss: 71.7578 | CE: 0.0869 | KD: 1061.1224\n",
      "Train Epoch: 090 Batch: 00023/00094 | Loss: 71.7066 | CE: 0.0414 | KD: 1061.0372\n",
      "Train Epoch: 090 Batch: 00024/00094 | Loss: 71.7413 | CE: 0.0633 | KD: 1061.2267\n",
      "Train Epoch: 090 Batch: 00025/00094 | Loss: 71.7228 | CE: 0.0370 | KD: 1061.3435\n",
      "Train Epoch: 090 Batch: 00026/00094 | Loss: 71.7541 | CE: 0.0757 | KD: 1061.2324\n",
      "Train Epoch: 090 Batch: 00027/00094 | Loss: 71.7188 | CE: 0.0419 | KD: 1061.2111\n",
      "Train Epoch: 090 Batch: 00028/00094 | Loss: 71.7202 | CE: 0.0438 | KD: 1061.2021\n",
      "Train Epoch: 090 Batch: 00029/00094 | Loss: 71.7292 | CE: 0.0407 | KD: 1061.3829\n",
      "Train Epoch: 090 Batch: 00030/00094 | Loss: 71.7229 | CE: 0.0656 | KD: 1060.9202\n",
      "Train Epoch: 090 Batch: 00031/00094 | Loss: 71.7457 | CE: 0.0621 | KD: 1061.3098\n",
      "Train Epoch: 090 Batch: 00032/00094 | Loss: 71.7328 | CE: 0.0571 | KD: 1061.1926\n",
      "Train Epoch: 090 Batch: 00033/00094 | Loss: 71.7294 | CE: 0.0429 | KD: 1061.3530\n",
      "Train Epoch: 090 Batch: 00034/00094 | Loss: 71.7932 | CE: 0.0947 | KD: 1061.5305\n",
      "Train Epoch: 090 Batch: 00035/00094 | Loss: 71.7155 | CE: 0.0460 | KD: 1061.1008\n",
      "Train Epoch: 090 Batch: 00036/00094 | Loss: 71.7696 | CE: 0.1184 | KD: 1060.8311\n",
      "Train Epoch: 090 Batch: 00037/00094 | Loss: 71.7717 | CE: 0.0836 | KD: 1061.3757\n",
      "Train Epoch: 090 Batch: 00038/00094 | Loss: 71.7954 | CE: 0.0736 | KD: 1061.8756\n",
      "Train Epoch: 090 Batch: 00039/00094 | Loss: 71.7065 | CE: 0.0409 | KD: 1061.0432\n",
      "Train Epoch: 090 Batch: 00040/00094 | Loss: 71.7333 | CE: 0.0588 | KD: 1061.1746\n",
      "Train Epoch: 090 Batch: 00041/00094 | Loss: 71.7624 | CE: 0.0671 | KD: 1061.4835\n",
      "Train Epoch: 090 Batch: 00042/00094 | Loss: 71.7227 | CE: 0.0506 | KD: 1061.1405\n",
      "Train Epoch: 090 Batch: 00043/00094 | Loss: 71.8075 | CE: 0.1303 | KD: 1061.2151\n",
      "Train Epoch: 090 Batch: 00044/00094 | Loss: 71.6970 | CE: 0.0348 | KD: 1060.9927\n",
      "Train Epoch: 090 Batch: 00045/00094 | Loss: 71.8858 | CE: 0.1741 | KD: 1061.7247\n",
      "Train Epoch: 090 Batch: 00046/00094 | Loss: 71.7384 | CE: 0.0541 | KD: 1061.3202\n",
      "Train Epoch: 090 Batch: 00047/00094 | Loss: 71.7581 | CE: 0.0442 | KD: 1061.7571\n",
      "Train Epoch: 090 Batch: 00048/00094 | Loss: 71.7230 | CE: 0.0489 | KD: 1061.1699\n",
      "Train Epoch: 090 Batch: 00049/00094 | Loss: 71.7410 | CE: 0.0765 | KD: 1061.0269\n",
      "Train Epoch: 090 Batch: 00050/00094 | Loss: 71.7440 | CE: 0.0440 | KD: 1061.5522\n",
      "Train Epoch: 090 Batch: 00051/00094 | Loss: 71.7927 | CE: 0.1288 | KD: 1061.0178\n",
      "Train Epoch: 090 Batch: 00052/00094 | Loss: 71.8443 | CE: 0.1456 | KD: 1061.5322\n",
      "Train Epoch: 090 Batch: 00053/00094 | Loss: 71.7286 | CE: 0.0471 | KD: 1061.2787\n",
      "Train Epoch: 090 Batch: 00054/00094 | Loss: 71.7109 | CE: 0.0453 | KD: 1061.0437\n",
      "Train Epoch: 090 Batch: 00055/00094 | Loss: 71.7490 | CE: 0.0631 | KD: 1061.3435\n",
      "Train Epoch: 090 Batch: 00056/00094 | Loss: 71.7471 | CE: 0.0410 | KD: 1061.6426\n",
      "Train Epoch: 090 Batch: 00057/00094 | Loss: 71.7025 | CE: 0.0514 | KD: 1060.8280\n",
      "Train Epoch: 090 Batch: 00058/00094 | Loss: 71.7370 | CE: 0.0589 | KD: 1061.2284\n",
      "Train Epoch: 090 Batch: 00059/00094 | Loss: 71.7509 | CE: 0.0457 | KD: 1061.6302\n",
      "Train Epoch: 090 Batch: 00060/00094 | Loss: 71.7106 | CE: 0.0364 | KD: 1061.1700\n",
      "Train Epoch: 090 Batch: 00061/00094 | Loss: 71.7056 | CE: 0.0437 | KD: 1060.9884\n",
      "Train Epoch: 090 Batch: 00062/00094 | Loss: 71.7972 | CE: 0.0911 | KD: 1061.6439\n",
      "Train Epoch: 090 Batch: 00063/00094 | Loss: 71.7197 | CE: 0.0675 | KD: 1060.8439\n",
      "Train Epoch: 090 Batch: 00064/00094 | Loss: 71.6836 | CE: 0.0366 | KD: 1060.7671\n",
      "Train Epoch: 090 Batch: 00065/00094 | Loss: 71.7730 | CE: 0.1138 | KD: 1060.9490\n",
      "Train Epoch: 090 Batch: 00066/00094 | Loss: 71.7347 | CE: 0.0535 | KD: 1061.2750\n",
      "Train Epoch: 090 Batch: 00067/00094 | Loss: 71.7366 | CE: 0.0751 | KD: 1060.9839\n",
      "Train Epoch: 090 Batch: 00068/00094 | Loss: 71.7492 | CE: 0.0735 | KD: 1061.1929\n",
      "Train Epoch: 090 Batch: 00069/00094 | Loss: 71.7414 | CE: 0.0720 | KD: 1061.0997\n",
      "Train Epoch: 090 Batch: 00070/00094 | Loss: 71.7148 | CE: 0.0355 | KD: 1061.2454\n",
      "Train Epoch: 090 Batch: 00071/00094 | Loss: 71.7384 | CE: 0.0770 | KD: 1060.9808\n",
      "Train Epoch: 090 Batch: 00072/00094 | Loss: 71.7560 | CE: 0.0634 | KD: 1061.4438\n",
      "Train Epoch: 090 Batch: 00073/00094 | Loss: 71.7621 | CE: 0.0358 | KD: 1061.9413\n",
      "Train Epoch: 090 Batch: 00074/00094 | Loss: 71.7689 | CE: 0.0611 | KD: 1061.6678\n",
      "Train Epoch: 090 Batch: 00075/00094 | Loss: 71.7379 | CE: 0.0725 | KD: 1061.0402\n",
      "Train Epoch: 090 Batch: 00076/00094 | Loss: 71.7076 | CE: 0.0387 | KD: 1061.0914\n",
      "Train Epoch: 090 Batch: 00077/00094 | Loss: 71.7893 | CE: 0.0826 | KD: 1061.6521\n",
      "Train Epoch: 090 Batch: 00078/00094 | Loss: 71.7433 | CE: 0.0602 | KD: 1061.3024\n",
      "Train Epoch: 090 Batch: 00079/00094 | Loss: 71.7942 | CE: 0.1205 | KD: 1061.1620\n",
      "Train Epoch: 090 Batch: 00080/00094 | Loss: 71.7660 | CE: 0.0419 | KD: 1061.9100\n",
      "Train Epoch: 090 Batch: 00081/00094 | Loss: 71.7055 | CE: 0.0531 | KD: 1060.8467\n",
      "Train Epoch: 090 Batch: 00082/00094 | Loss: 71.7597 | CE: 0.0810 | KD: 1061.2377\n",
      "Train Epoch: 090 Batch: 00083/00094 | Loss: 71.7164 | CE: 0.0456 | KD: 1061.1200\n",
      "Train Epoch: 090 Batch: 00084/00094 | Loss: 71.7577 | CE: 0.0735 | KD: 1061.3185\n",
      "Train Epoch: 090 Batch: 00085/00094 | Loss: 71.6769 | CE: 0.0313 | KD: 1060.7471\n",
      "Train Epoch: 090 Batch: 00086/00094 | Loss: 71.7755 | CE: 0.0760 | KD: 1061.5441\n",
      "Train Epoch: 090 Batch: 00087/00094 | Loss: 71.7037 | CE: 0.0418 | KD: 1060.9889\n",
      "Train Epoch: 090 Batch: 00088/00094 | Loss: 71.6930 | CE: 0.0359 | KD: 1060.9181\n",
      "Train Epoch: 090 Batch: 00089/00094 | Loss: 71.7260 | CE: 0.0392 | KD: 1061.3566\n",
      "Train Epoch: 090 Batch: 00090/00094 | Loss: 71.8071 | CE: 0.1218 | KD: 1061.3344\n",
      "Train Epoch: 090 Batch: 00091/00094 | Loss: 71.7425 | CE: 0.0754 | KD: 1061.0646\n",
      "Train Epoch: 090 Batch: 00092/00094 | Loss: 71.7583 | CE: 0.0865 | KD: 1061.1350\n",
      "Train Epoch: 090 Batch: 00093/00094 | Loss: 71.7225 | CE: 0.0532 | KD: 1061.0979\n",
      "Train Epoch: 090 Batch: 00094/00094 | Loss: 71.7201 | CE: 0.0368 | KD: 1061.3063\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0547 | acc:99.0500\n",
      "[VAL Acc] Target: 99.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8484 | acc:48.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.3047 | acc:53.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3787 | acc:44.4656\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3441 | acc:56.4655\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9060 | acc:62.9390\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 62.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6740 | acc:75.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9694 | acc:58.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.25%\n",
      "[VAL Acc] Avg 62.29%\n",
      "Train Epoch: 091 Batch: 00001/00094 | Loss: 71.7303 | CE: 0.0540 | KD: 1061.2006\n",
      "Train Epoch: 091 Batch: 00002/00094 | Loss: 71.7077 | CE: 0.0643 | KD: 1060.7139\n",
      "Train Epoch: 091 Batch: 00003/00094 | Loss: 71.7390 | CE: 0.0627 | KD: 1061.2019\n",
      "Train Epoch: 091 Batch: 00004/00094 | Loss: 71.7001 | CE: 0.0588 | KD: 1060.6832\n",
      "Train Epoch: 091 Batch: 00005/00094 | Loss: 71.7281 | CE: 0.0566 | KD: 1061.1316\n",
      "Train Epoch: 091 Batch: 00006/00094 | Loss: 71.7037 | CE: 0.0531 | KD: 1060.8217\n",
      "Train Epoch: 091 Batch: 00007/00094 | Loss: 71.7520 | CE: 0.0756 | KD: 1061.2045\n",
      "Train Epoch: 091 Batch: 00008/00094 | Loss: 71.7070 | CE: 0.0510 | KD: 1060.9000\n",
      "Train Epoch: 091 Batch: 00009/00094 | Loss: 71.7554 | CE: 0.1029 | KD: 1060.8491\n",
      "Train Epoch: 091 Batch: 00010/00094 | Loss: 71.6899 | CE: 0.0435 | KD: 1060.7592\n",
      "Train Epoch: 091 Batch: 00011/00094 | Loss: 71.7123 | CE: 0.0413 | KD: 1061.1228\n",
      "Train Epoch: 091 Batch: 00012/00094 | Loss: 71.7223 | CE: 0.0522 | KD: 1061.1097\n",
      "Train Epoch: 091 Batch: 00013/00094 | Loss: 71.7780 | CE: 0.0691 | KD: 1061.6846\n",
      "Train Epoch: 091 Batch: 00014/00094 | Loss: 71.7155 | CE: 0.0432 | KD: 1061.1418\n",
      "Train Epoch: 091 Batch: 00015/00094 | Loss: 71.7247 | CE: 0.0499 | KD: 1061.1793\n",
      "Train Epoch: 091 Batch: 00016/00094 | Loss: 71.7104 | CE: 0.0533 | KD: 1060.9172\n",
      "Train Epoch: 091 Batch: 00017/00094 | Loss: 71.7121 | CE: 0.0388 | KD: 1061.1581\n",
      "Train Epoch: 091 Batch: 00018/00094 | Loss: 71.7013 | CE: 0.0369 | KD: 1061.0245\n",
      "Train Epoch: 091 Batch: 00019/00094 | Loss: 71.7546 | CE: 0.0704 | KD: 1061.3193\n",
      "Train Epoch: 091 Batch: 00020/00094 | Loss: 71.6845 | CE: 0.0428 | KD: 1060.6893\n",
      "Train Epoch: 091 Batch: 00021/00094 | Loss: 71.7998 | CE: 0.0928 | KD: 1061.6555\n",
      "Train Epoch: 091 Batch: 00022/00094 | Loss: 71.7183 | CE: 0.0533 | KD: 1061.0349\n",
      "Train Epoch: 091 Batch: 00023/00094 | Loss: 71.7642 | CE: 0.0813 | KD: 1061.2983\n",
      "Train Epoch: 091 Batch: 00024/00094 | Loss: 71.7157 | CE: 0.0475 | KD: 1061.0812\n",
      "Train Epoch: 091 Batch: 00025/00094 | Loss: 71.7453 | CE: 0.0473 | KD: 1061.5223\n",
      "Train Epoch: 091 Batch: 00026/00094 | Loss: 71.7385 | CE: 0.0753 | KD: 1061.0090\n",
      "Train Epoch: 091 Batch: 00027/00094 | Loss: 71.7514 | CE: 0.0516 | KD: 1061.5497\n",
      "Train Epoch: 091 Batch: 00028/00094 | Loss: 71.6991 | CE: 0.0481 | KD: 1060.8260\n",
      "Train Epoch: 091 Batch: 00029/00094 | Loss: 71.7664 | CE: 0.0763 | KD: 1061.4070\n",
      "Train Epoch: 091 Batch: 00030/00094 | Loss: 71.7366 | CE: 0.0451 | KD: 1061.4264\n",
      "Train Epoch: 091 Batch: 00031/00094 | Loss: 71.7224 | CE: 0.0409 | KD: 1061.2798\n",
      "Train Epoch: 091 Batch: 00032/00094 | Loss: 71.7236 | CE: 0.0459 | KD: 1061.2223\n",
      "Train Epoch: 091 Batch: 00033/00094 | Loss: 71.7988 | CE: 0.1217 | KD: 1061.2134\n",
      "Train Epoch: 091 Batch: 00034/00094 | Loss: 71.7449 | CE: 0.0557 | KD: 1061.3928\n",
      "Train Epoch: 091 Batch: 00035/00094 | Loss: 71.7260 | CE: 0.0516 | KD: 1061.1732\n",
      "Train Epoch: 091 Batch: 00036/00094 | Loss: 71.7021 | CE: 0.0559 | KD: 1060.7568\n",
      "Train Epoch: 091 Batch: 00037/00094 | Loss: 71.7182 | CE: 0.0366 | KD: 1061.2799\n",
      "Train Epoch: 091 Batch: 00038/00094 | Loss: 71.7501 | CE: 0.0625 | KD: 1061.3682\n",
      "Train Epoch: 091 Batch: 00039/00094 | Loss: 71.7947 | CE: 0.1161 | KD: 1061.2361\n",
      "Train Epoch: 091 Batch: 00040/00094 | Loss: 71.7505 | CE: 0.0365 | KD: 1061.7606\n",
      "Train Epoch: 091 Batch: 00041/00094 | Loss: 71.9250 | CE: 0.2020 | KD: 1061.8933\n",
      "Train Epoch: 091 Batch: 00042/00094 | Loss: 71.7262 | CE: 0.0610 | KD: 1061.0359\n",
      "Train Epoch: 091 Batch: 00043/00094 | Loss: 71.7407 | CE: 0.0459 | KD: 1061.4749\n",
      "Train Epoch: 091 Batch: 00044/00094 | Loss: 71.8023 | CE: 0.1206 | KD: 1061.2816\n",
      "Train Epoch: 091 Batch: 00045/00094 | Loss: 71.7379 | CE: 0.0458 | KD: 1061.4363\n",
      "Train Epoch: 091 Batch: 00046/00094 | Loss: 71.7045 | CE: 0.0427 | KD: 1060.9880\n",
      "Train Epoch: 091 Batch: 00047/00094 | Loss: 71.7511 | CE: 0.0542 | KD: 1061.5065\n",
      "Train Epoch: 091 Batch: 00048/00094 | Loss: 71.8147 | CE: 0.1060 | KD: 1061.6819\n",
      "Train Epoch: 091 Batch: 00049/00094 | Loss: 71.7841 | CE: 0.0656 | KD: 1061.8276\n",
      "Train Epoch: 091 Batch: 00050/00094 | Loss: 71.7083 | CE: 0.0411 | KD: 1061.0662\n",
      "Train Epoch: 091 Batch: 00051/00094 | Loss: 71.7405 | CE: 0.0486 | KD: 1061.4325\n",
      "Train Epoch: 091 Batch: 00052/00094 | Loss: 71.7757 | CE: 0.0655 | KD: 1061.7031\n",
      "Train Epoch: 091 Batch: 00053/00094 | Loss: 71.7378 | CE: 0.0455 | KD: 1061.4391\n",
      "Train Epoch: 091 Batch: 00054/00094 | Loss: 71.7439 | CE: 0.0560 | KD: 1061.3729\n",
      "Train Epoch: 091 Batch: 00055/00094 | Loss: 71.7741 | CE: 0.0960 | KD: 1061.2286\n",
      "Train Epoch: 091 Batch: 00056/00094 | Loss: 71.7702 | CE: 0.0874 | KD: 1061.2979\n",
      "Train Epoch: 091 Batch: 00057/00094 | Loss: 71.6999 | CE: 0.0376 | KD: 1060.9941\n",
      "Train Epoch: 091 Batch: 00058/00094 | Loss: 71.7414 | CE: 0.0799 | KD: 1060.9825\n",
      "Train Epoch: 091 Batch: 00059/00094 | Loss: 71.7389 | CE: 0.0502 | KD: 1061.3860\n",
      "Train Epoch: 091 Batch: 00060/00094 | Loss: 71.7086 | CE: 0.0462 | KD: 1060.9969\n",
      "Train Epoch: 091 Batch: 00061/00094 | Loss: 71.7394 | CE: 0.0612 | KD: 1061.2299\n",
      "Train Epoch: 091 Batch: 00062/00094 | Loss: 71.7522 | CE: 0.0489 | KD: 1061.6013\n",
      "Train Epoch: 091 Batch: 00063/00094 | Loss: 71.7499 | CE: 0.0479 | KD: 1061.5822\n",
      "Train Epoch: 091 Batch: 00064/00094 | Loss: 71.7206 | CE: 0.0455 | KD: 1061.1848\n",
      "Train Epoch: 091 Batch: 00065/00094 | Loss: 71.7378 | CE: 0.0722 | KD: 1061.0435\n",
      "Train Epoch: 091 Batch: 00066/00094 | Loss: 71.7260 | CE: 0.0722 | KD: 1060.8677\n",
      "Train Epoch: 091 Batch: 00067/00094 | Loss: 71.7563 | CE: 0.0680 | KD: 1061.3781\n",
      "Train Epoch: 091 Batch: 00068/00094 | Loss: 71.7352 | CE: 0.0385 | KD: 1061.5029\n",
      "Train Epoch: 091 Batch: 00069/00094 | Loss: 71.7062 | CE: 0.0453 | KD: 1060.9735\n",
      "Train Epoch: 091 Batch: 00070/00094 | Loss: 71.7634 | CE: 0.0446 | KD: 1061.8302\n",
      "Train Epoch: 091 Batch: 00071/00094 | Loss: 71.7411 | CE: 0.0559 | KD: 1061.3334\n",
      "Train Epoch: 091 Batch: 00072/00094 | Loss: 71.7561 | CE: 0.0621 | KD: 1061.4629\n",
      "Train Epoch: 091 Batch: 00073/00094 | Loss: 71.7234 | CE: 0.0603 | KD: 1061.0074\n",
      "Train Epoch: 091 Batch: 00074/00094 | Loss: 71.7171 | CE: 0.0418 | KD: 1061.1877\n",
      "Train Epoch: 091 Batch: 00075/00094 | Loss: 71.7013 | CE: 0.0411 | KD: 1060.9634\n",
      "Train Epoch: 091 Batch: 00076/00094 | Loss: 71.7151 | CE: 0.0578 | KD: 1060.9219\n",
      "Train Epoch: 091 Batch: 00077/00094 | Loss: 71.7024 | CE: 0.0435 | KD: 1060.9435\n",
      "Train Epoch: 091 Batch: 00078/00094 | Loss: 71.7276 | CE: 0.0551 | KD: 1061.1458\n",
      "Train Epoch: 091 Batch: 00079/00094 | Loss: 71.7911 | CE: 0.1217 | KD: 1061.0991\n",
      "Train Epoch: 091 Batch: 00080/00094 | Loss: 71.7544 | CE: 0.0794 | KD: 1061.1830\n",
      "Train Epoch: 091 Batch: 00081/00094 | Loss: 71.7595 | CE: 0.0649 | KD: 1061.4734\n",
      "Train Epoch: 091 Batch: 00082/00094 | Loss: 71.7762 | CE: 0.1007 | KD: 1061.1897\n",
      "Train Epoch: 091 Batch: 00083/00094 | Loss: 71.6729 | CE: 0.0360 | KD: 1060.6172\n",
      "Train Epoch: 091 Batch: 00084/00094 | Loss: 71.7612 | CE: 0.0598 | KD: 1061.5735\n",
      "Train Epoch: 091 Batch: 00085/00094 | Loss: 71.7373 | CE: 0.0419 | KD: 1061.4836\n",
      "Train Epoch: 091 Batch: 00086/00094 | Loss: 71.8501 | CE: 0.1673 | KD: 1061.2982\n",
      "Train Epoch: 091 Batch: 00087/00094 | Loss: 71.7516 | CE: 0.0815 | KD: 1061.1101\n",
      "Train Epoch: 091 Batch: 00088/00094 | Loss: 71.7072 | CE: 0.0411 | KD: 1061.0508\n",
      "Train Epoch: 091 Batch: 00089/00094 | Loss: 71.6952 | CE: 0.0371 | KD: 1060.9325\n",
      "Train Epoch: 091 Batch: 00090/00094 | Loss: 71.7600 | CE: 0.0955 | KD: 1061.0266\n",
      "Train Epoch: 091 Batch: 00091/00094 | Loss: 71.7015 | CE: 0.0427 | KD: 1060.9427\n",
      "Train Epoch: 091 Batch: 00092/00094 | Loss: 71.7585 | CE: 0.0898 | KD: 1061.0879\n",
      "Train Epoch: 091 Batch: 00093/00094 | Loss: 71.7909 | CE: 0.1060 | KD: 1061.3293\n",
      "Train Epoch: 091 Batch: 00094/00094 | Loss: 71.7465 | CE: 0.0464 | KD: 1061.5543\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0541 | acc:99.2000\n",
      "[VAL Acc] Target: 99.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6263 | acc:48.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1592 | acc:52.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2633 | acc:42.9389\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 42.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2201 | acc:57.9545\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9827 | acc:59.6118\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 59.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5921 | acc:76.4890\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9968 | acc:58.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.00%\n",
      "[VAL Acc] Avg 61.82%\n",
      "Early stopping ...\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : t18_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/28f50f2b2b104d2fba71d4af73f78b99\u001b[0m\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/biggan_val_acc [91]        : (85.7, 99.35000000000001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/crn_val_acc [91]           : (85.7, 99.35000000000001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/cyclegan_val_acc [91]      : (85.7, 99.35000000000001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/faceforensics_val_acc [91] : (85.7, 99.35000000000001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/gaugan_val_acc [91]        : (85.7, 99.35000000000001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/imle_val_acc [91]          : (85.7, 99.35000000000001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/wild_val_acc [91]          : (85.7, 99.35000000000001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/target_val_acc [91]                                             : (85.7, 99.35000000000001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/val_acc [91]                                                    : (60.08981184719021, 63.84240729576098)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [2250]                                                         : (71.67694854736328, 531.2557373046875)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss [8554]                                                  : (71.6728744506836, 533.070556640625)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_kd [8554]                                               : (1060.1962890625, 1062.36279296875)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_main [8554]                                             : (0.031347401440143585, 1.8891607522964478)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start_acc                                                           : 52.849999999999994\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : t18_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size           : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config_file          : model_config.conf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decay_factor         : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stop           : 35\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs               : 250\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flip                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kd_alpha             : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr                   : 0.005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_schedule          : cosine\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_gpu              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     output_model_version : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resolution           : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (8.36 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1 (7.34 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\", \"biggan\", \"cyclegan\", \"imle\", \"faceforensics\", \"crn\", \"wild\"] \n",
    "TARGET_DATASET = \"diffusionshort\"\n",
    "NETWORK = \"ResNet18\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/KD_r18/cddb_easy\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/KD_r18/diff\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "COMET_NAME = \"t18_diff\"\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "if NETWORK == \"ViT\": input_model = f\"{CHECKPOINT_DIR_TARGET}/{source_string}/model_best_accuracy.pth\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='model_config.conf', network='MobileNet2', input_model='../KD-AIGC-Detection/checkpoints/KD_mn/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild', output_dir='../KD-AIGC-Detection/checkpoints/KD_mn/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild', target_dataset='../KD-AIGC-Detection/datasets/custom/diffusionshort', use_comet=True, comet_name='tmn_diff', num_gpu='0', output_model_version='3', epochs='250', early_stop='35', batch_size='64', resolution='128', lr_schedule='cosine', lr='0.005', kd_alpha='0.5', decay_factor='0.9', flip='False')\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/c667a4373aa74fbab3625be034854214\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/imle\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/crn\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/wild\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/train\n",
      "Dataset available in train_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan / ../KD-AIGC-Detection/datasets/custom/biggan / ../KD-AIGC-Detection/datasets/custom/cyclegan / ../KD-AIGC-Detection/datasets/custom/imle / ../KD-AIGC-Detection/datasets/custom/faceforensics / ../KD-AIGC-Detection/datasets/custom/crn / ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Loading MobileNet2 from ../KD-AIGC-Detection/checkpoints/KD_mn/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild/model_best_accuracy.pth\n",
      "Apply Cosine learning rate schedule\n",
      "/home/fra/miniconda3/envs/paper/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.6949 | acc:59.9500\n",
      "Start training in 250 epochs\n",
      "Train Epoch: 001 Batch: 00001/00094 | Loss: 530.8846 | CE: 0.7790 | KD: 1060.2113\n",
      "Train Epoch: 001 Batch: 00002/00094 | Loss: 531.0128 | CE: 0.8875 | KD: 1060.2506\n",
      "Train Epoch: 001 Batch: 00003/00094 | Loss: 530.8724 | CE: 0.7727 | KD: 1060.1996\n",
      "Train Epoch: 001 Batch: 00004/00094 | Loss: 530.7594 | CE: 0.6736 | KD: 1060.1716\n",
      "Train Epoch: 001 Batch: 00005/00094 | Loss: 530.6432 | CE: 0.5909 | KD: 1060.1046\n",
      "Train Epoch: 001 Batch: 00006/00094 | Loss: 530.8110 | CE: 0.7145 | KD: 1060.1929\n",
      "Train Epoch: 001 Batch: 00007/00094 | Loss: 530.7515 | CE: 0.6946 | KD: 1060.1136\n",
      "Train Epoch: 001 Batch: 00008/00094 | Loss: 530.7842 | CE: 0.7190 | KD: 1060.1305\n",
      "Train Epoch: 001 Batch: 00009/00094 | Loss: 530.6561 | CE: 0.5939 | KD: 1060.1243\n",
      "Train Epoch: 001 Batch: 00010/00094 | Loss: 530.6890 | CE: 0.6164 | KD: 1060.1451\n",
      "Train Epoch: 001 Batch: 00011/00094 | Loss: 530.7028 | CE: 0.6539 | KD: 1060.0979\n",
      "Train Epoch: 001 Batch: 00012/00094 | Loss: 530.7183 | CE: 0.6650 | KD: 1060.1064\n",
      "Train Epoch: 001 Batch: 00013/00094 | Loss: 530.6937 | CE: 0.6400 | KD: 1060.1074\n",
      "Train Epoch: 001 Batch: 00014/00094 | Loss: 530.6522 | CE: 0.5775 | KD: 1060.1494\n",
      "Train Epoch: 001 Batch: 00015/00094 | Loss: 530.6224 | CE: 0.5646 | KD: 1060.1157\n",
      "Train Epoch: 001 Batch: 00016/00094 | Loss: 530.6743 | CE: 0.6121 | KD: 1060.1244\n",
      "Train Epoch: 001 Batch: 00017/00094 | Loss: 530.7177 | CE: 0.6749 | KD: 1060.0856\n",
      "Train Epoch: 001 Batch: 00018/00094 | Loss: 530.5766 | CE: 0.5576 | KD: 1060.0381\n",
      "Train Epoch: 001 Batch: 00019/00094 | Loss: 530.7235 | CE: 0.6652 | KD: 1060.1167\n",
      "Train Epoch: 001 Batch: 00020/00094 | Loss: 530.6024 | CE: 0.5856 | KD: 1060.0336\n",
      "Train Epoch: 001 Batch: 00021/00094 | Loss: 530.6457 | CE: 0.6106 | KD: 1060.0702\n",
      "Train Epoch: 001 Batch: 00022/00094 | Loss: 530.5523 | CE: 0.5300 | KD: 1060.0446\n",
      "Train Epoch: 001 Batch: 00023/00094 | Loss: 530.5393 | CE: 0.5316 | KD: 1060.0155\n",
      "Train Epoch: 001 Batch: 00024/00094 | Loss: 530.4851 | CE: 0.4671 | KD: 1060.0360\n",
      "Train Epoch: 001 Batch: 00025/00094 | Loss: 530.5663 | CE: 0.5385 | KD: 1060.0557\n",
      "Train Epoch: 001 Batch: 00026/00094 | Loss: 530.5501 | CE: 0.5191 | KD: 1060.0620\n",
      "Train Epoch: 001 Batch: 00027/00094 | Loss: 530.4955 | CE: 0.4970 | KD: 1059.9972\n",
      "Train Epoch: 001 Batch: 00028/00094 | Loss: 530.5244 | CE: 0.5178 | KD: 1060.0132\n",
      "Train Epoch: 001 Batch: 00029/00094 | Loss: 530.5125 | CE: 0.4928 | KD: 1060.0394\n",
      "Train Epoch: 001 Batch: 00030/00094 | Loss: 530.5962 | CE: 0.5732 | KD: 1060.0460\n",
      "Train Epoch: 001 Batch: 00031/00094 | Loss: 530.6074 | CE: 0.5681 | KD: 1060.0785\n",
      "Train Epoch: 001 Batch: 00032/00094 | Loss: 530.5001 | CE: 0.4845 | KD: 1060.0311\n",
      "Train Epoch: 001 Batch: 00033/00094 | Loss: 530.4641 | CE: 0.4635 | KD: 1060.0013\n",
      "Train Epoch: 001 Batch: 00034/00094 | Loss: 530.5082 | CE: 0.4890 | KD: 1060.0385\n",
      "Train Epoch: 001 Batch: 00035/00094 | Loss: 530.5217 | CE: 0.5057 | KD: 1060.0321\n",
      "Train Epoch: 001 Batch: 00036/00094 | Loss: 530.4792 | CE: 0.4766 | KD: 1060.0052\n",
      "Train Epoch: 001 Batch: 00037/00094 | Loss: 530.5159 | CE: 0.4997 | KD: 1060.0323\n",
      "Train Epoch: 001 Batch: 00038/00094 | Loss: 530.4842 | CE: 0.4756 | KD: 1060.0171\n",
      "Train Epoch: 001 Batch: 00039/00094 | Loss: 530.4202 | CE: 0.4260 | KD: 1059.9883\n",
      "Train Epoch: 001 Batch: 00040/00094 | Loss: 530.4973 | CE: 0.4895 | KD: 1060.0155\n",
      "Train Epoch: 001 Batch: 00041/00094 | Loss: 530.5145 | CE: 0.5037 | KD: 1060.0215\n",
      "Train Epoch: 001 Batch: 00042/00094 | Loss: 530.5158 | CE: 0.5219 | KD: 1059.9877\n",
      "Train Epoch: 001 Batch: 00043/00094 | Loss: 530.4822 | CE: 0.4708 | KD: 1060.0229\n",
      "Train Epoch: 001 Batch: 00044/00094 | Loss: 530.5179 | CE: 0.5093 | KD: 1060.0171\n",
      "Train Epoch: 001 Batch: 00045/00094 | Loss: 530.3828 | CE: 0.3918 | KD: 1059.9821\n",
      "Train Epoch: 001 Batch: 00046/00094 | Loss: 530.5152 | CE: 0.4910 | KD: 1060.0485\n",
      "Train Epoch: 001 Batch: 00047/00094 | Loss: 530.4716 | CE: 0.4685 | KD: 1060.0061\n",
      "Train Epoch: 001 Batch: 00048/00094 | Loss: 530.3520 | CE: 0.3658 | KD: 1059.9724\n",
      "Train Epoch: 001 Batch: 00049/00094 | Loss: 530.4905 | CE: 0.4704 | KD: 1060.0402\n",
      "Train Epoch: 001 Batch: 00050/00094 | Loss: 530.4807 | CE: 0.4788 | KD: 1060.0038\n",
      "Train Epoch: 001 Batch: 00051/00094 | Loss: 530.3831 | CE: 0.3873 | KD: 1059.9915\n",
      "Train Epoch: 001 Batch: 00052/00094 | Loss: 530.5287 | CE: 0.5091 | KD: 1060.0393\n",
      "Train Epoch: 001 Batch: 00053/00094 | Loss: 530.4501 | CE: 0.4462 | KD: 1060.0077\n",
      "Train Epoch: 001 Batch: 00054/00094 | Loss: 530.4516 | CE: 0.4521 | KD: 1059.9990\n",
      "Train Epoch: 001 Batch: 00055/00094 | Loss: 530.4290 | CE: 0.4271 | KD: 1060.0037\n",
      "Train Epoch: 001 Batch: 00056/00094 | Loss: 530.5588 | CE: 0.5305 | KD: 1060.0566\n",
      "Train Epoch: 001 Batch: 00057/00094 | Loss: 530.4322 | CE: 0.4280 | KD: 1060.0084\n",
      "Train Epoch: 001 Batch: 00058/00094 | Loss: 530.5219 | CE: 0.5127 | KD: 1060.0184\n",
      "Train Epoch: 001 Batch: 00059/00094 | Loss: 530.4305 | CE: 0.4114 | KD: 1060.0383\n",
      "Train Epoch: 001 Batch: 00060/00094 | Loss: 530.4731 | CE: 0.4674 | KD: 1060.0115\n",
      "Train Epoch: 001 Batch: 00061/00094 | Loss: 530.4541 | CE: 0.4661 | KD: 1059.9760\n",
      "Train Epoch: 001 Batch: 00062/00094 | Loss: 530.3558 | CE: 0.3672 | KD: 1059.9773\n",
      "Train Epoch: 001 Batch: 00063/00094 | Loss: 530.3721 | CE: 0.3797 | KD: 1059.9846\n",
      "Train Epoch: 001 Batch: 00064/00094 | Loss: 530.4191 | CE: 0.4178 | KD: 1060.0026\n",
      "Train Epoch: 001 Batch: 00065/00094 | Loss: 530.4036 | CE: 0.4147 | KD: 1059.9778\n",
      "Train Epoch: 001 Batch: 00066/00094 | Loss: 530.4150 | CE: 0.4133 | KD: 1060.0035\n",
      "Train Epoch: 001 Batch: 00067/00094 | Loss: 530.4634 | CE: 0.4539 | KD: 1060.0189\n",
      "Train Epoch: 001 Batch: 00068/00094 | Loss: 530.4402 | CE: 0.4492 | KD: 1059.9821\n",
      "Train Epoch: 001 Batch: 00069/00094 | Loss: 530.4520 | CE: 0.4420 | KD: 1060.0201\n",
      "Train Epoch: 001 Batch: 00070/00094 | Loss: 530.4028 | CE: 0.3941 | KD: 1060.0175\n",
      "Train Epoch: 001 Batch: 00071/00094 | Loss: 530.4042 | CE: 0.4010 | KD: 1060.0062\n",
      "Train Epoch: 001 Batch: 00072/00094 | Loss: 530.3935 | CE: 0.4030 | KD: 1059.9811\n",
      "Train Epoch: 001 Batch: 00073/00094 | Loss: 530.3490 | CE: 0.3508 | KD: 1059.9965\n",
      "Train Epoch: 001 Batch: 00074/00094 | Loss: 530.3436 | CE: 0.3580 | KD: 1059.9713\n",
      "Train Epoch: 001 Batch: 00075/00094 | Loss: 530.3681 | CE: 0.3709 | KD: 1059.9943\n",
      "Train Epoch: 001 Batch: 00076/00094 | Loss: 530.4009 | CE: 0.4094 | KD: 1059.9829\n",
      "Train Epoch: 001 Batch: 00077/00094 | Loss: 530.4459 | CE: 0.4456 | KD: 1060.0006\n",
      "Train Epoch: 001 Batch: 00078/00094 | Loss: 530.3902 | CE: 0.3889 | KD: 1060.0026\n",
      "Train Epoch: 001 Batch: 00079/00094 | Loss: 530.4161 | CE: 0.4243 | KD: 1059.9836\n",
      "Train Epoch: 001 Batch: 00080/00094 | Loss: 530.4224 | CE: 0.4116 | KD: 1060.0217\n",
      "Train Epoch: 001 Batch: 00081/00094 | Loss: 530.4509 | CE: 0.4503 | KD: 1060.0012\n",
      "Train Epoch: 001 Batch: 00082/00094 | Loss: 530.4081 | CE: 0.4046 | KD: 1060.0071\n",
      "Train Epoch: 001 Batch: 00083/00094 | Loss: 530.3217 | CE: 0.3320 | KD: 1059.9792\n",
      "Train Epoch: 001 Batch: 00084/00094 | Loss: 530.3531 | CE: 0.3476 | KD: 1060.0110\n",
      "Train Epoch: 001 Batch: 00085/00094 | Loss: 530.3671 | CE: 0.3751 | KD: 1059.9839\n",
      "Train Epoch: 001 Batch: 00086/00094 | Loss: 530.3471 | CE: 0.3587 | KD: 1059.9769\n",
      "Train Epoch: 001 Batch: 00087/00094 | Loss: 530.2862 | CE: 0.3146 | KD: 1059.9431\n",
      "Train Epoch: 001 Batch: 00088/00094 | Loss: 530.4241 | CE: 0.4094 | KD: 1060.0293\n",
      "Train Epoch: 001 Batch: 00089/00094 | Loss: 530.3190 | CE: 0.3435 | KD: 1059.9510\n",
      "Train Epoch: 001 Batch: 00090/00094 | Loss: 530.4000 | CE: 0.3901 | KD: 1060.0198\n",
      "Train Epoch: 001 Batch: 00091/00094 | Loss: 530.3384 | CE: 0.3379 | KD: 1060.0009\n",
      "Train Epoch: 001 Batch: 00092/00094 | Loss: 530.3501 | CE: 0.3493 | KD: 1060.0016\n",
      "Train Epoch: 001 Batch: 00093/00094 | Loss: 530.2644 | CE: 0.2980 | KD: 1059.9329\n",
      "Train Epoch: 001 Batch: 00094/00094 | Loss: 530.3884 | CE: 0.3937 | KD: 1059.9894\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3583 | acc:88.0000\n",
      "[VAL Acc] Target: 88.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:0.7973 | acc:57.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 57.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5893 | acc:69.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 69.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.7372 | acc:58.3969\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 58.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.5580 | acc:69.0439\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 69.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4574 | acc:81.1460\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 81.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5109 | acc:76.6850\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7238 | acc:60.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.44%\n",
      "[VAL Acc] Avg 70.01%\n",
      "VAL Acc improve from 0.00% to 70.01%\n",
      "Save best model\n",
      "Train Epoch: 002 Batch: 00001/00094 | Loss: 530.3605 | CE: 0.3515 | KD: 1060.0178\n",
      "Train Epoch: 002 Batch: 00002/00094 | Loss: 530.3105 | CE: 0.3261 | KD: 1059.9690\n",
      "Train Epoch: 002 Batch: 00003/00094 | Loss: 530.3517 | CE: 0.3545 | KD: 1059.9944\n",
      "Train Epoch: 002 Batch: 00004/00094 | Loss: 530.3404 | CE: 0.3390 | KD: 1060.0029\n",
      "Train Epoch: 002 Batch: 00005/00094 | Loss: 530.3910 | CE: 0.3848 | KD: 1060.0125\n",
      "Train Epoch: 002 Batch: 00006/00094 | Loss: 530.3449 | CE: 0.3453 | KD: 1059.9993\n",
      "Train Epoch: 002 Batch: 00007/00094 | Loss: 530.4193 | CE: 0.4203 | KD: 1059.9978\n",
      "Train Epoch: 002 Batch: 00008/00094 | Loss: 530.3595 | CE: 0.3600 | KD: 1059.9990\n",
      "Train Epoch: 002 Batch: 00009/00094 | Loss: 530.3386 | CE: 0.3320 | KD: 1060.0132\n",
      "Train Epoch: 002 Batch: 00010/00094 | Loss: 530.3053 | CE: 0.3247 | KD: 1059.9612\n",
      "Train Epoch: 002 Batch: 00011/00094 | Loss: 530.3393 | CE: 0.3404 | KD: 1059.9978\n",
      "Train Epoch: 002 Batch: 00012/00094 | Loss: 530.3103 | CE: 0.3133 | KD: 1059.9940\n",
      "Train Epoch: 002 Batch: 00013/00094 | Loss: 530.3442 | CE: 0.3384 | KD: 1060.0117\n",
      "Train Epoch: 002 Batch: 00014/00094 | Loss: 530.3604 | CE: 0.3615 | KD: 1059.9978\n",
      "Train Epoch: 002 Batch: 00015/00094 | Loss: 530.4231 | CE: 0.4161 | KD: 1060.0139\n",
      "Train Epoch: 002 Batch: 00016/00094 | Loss: 530.4073 | CE: 0.3828 | KD: 1060.0491\n",
      "Train Epoch: 002 Batch: 00017/00094 | Loss: 530.3482 | CE: 0.3482 | KD: 1060.0001\n",
      "Train Epoch: 002 Batch: 00018/00094 | Loss: 530.2706 | CE: 0.2702 | KD: 1060.0009\n",
      "Train Epoch: 002 Batch: 00019/00094 | Loss: 530.3375 | CE: 0.3324 | KD: 1060.0100\n",
      "Train Epoch: 002 Batch: 00020/00094 | Loss: 530.2611 | CE: 0.2766 | KD: 1059.9691\n",
      "Train Epoch: 002 Batch: 00021/00094 | Loss: 530.3901 | CE: 0.3641 | KD: 1060.0521\n",
      "Train Epoch: 002 Batch: 00022/00094 | Loss: 530.4152 | CE: 0.4195 | KD: 1059.9915\n",
      "Train Epoch: 002 Batch: 00023/00094 | Loss: 530.2585 | CE: 0.2850 | KD: 1059.9469\n",
      "Train Epoch: 002 Batch: 00024/00094 | Loss: 530.4921 | CE: 0.4560 | KD: 1060.0720\n",
      "Train Epoch: 002 Batch: 00025/00094 | Loss: 530.2847 | CE: 0.2843 | KD: 1060.0007\n",
      "Train Epoch: 002 Batch: 00026/00094 | Loss: 530.3063 | CE: 0.3016 | KD: 1060.0094\n",
      "Train Epoch: 002 Batch: 00027/00094 | Loss: 530.3301 | CE: 0.3229 | KD: 1060.0144\n",
      "Train Epoch: 002 Batch: 00028/00094 | Loss: 530.3116 | CE: 0.3274 | KD: 1059.9684\n",
      "Train Epoch: 002 Batch: 00029/00094 | Loss: 530.2297 | CE: 0.2572 | KD: 1059.9449\n",
      "Train Epoch: 002 Batch: 00030/00094 | Loss: 530.3503 | CE: 0.3160 | KD: 1060.0686\n",
      "Train Epoch: 002 Batch: 00031/00094 | Loss: 530.3318 | CE: 0.3430 | KD: 1059.9777\n",
      "Train Epoch: 002 Batch: 00032/00094 | Loss: 530.2537 | CE: 0.2746 | KD: 1059.9581\n",
      "Train Epoch: 002 Batch: 00033/00094 | Loss: 530.3125 | CE: 0.3030 | KD: 1060.0190\n",
      "Train Epoch: 002 Batch: 00034/00094 | Loss: 530.3668 | CE: 0.3544 | KD: 1060.0248\n",
      "Train Epoch: 002 Batch: 00035/00094 | Loss: 530.3180 | CE: 0.3263 | KD: 1059.9833\n",
      "Train Epoch: 002 Batch: 00036/00094 | Loss: 530.4003 | CE: 0.3855 | KD: 1060.0297\n",
      "Train Epoch: 002 Batch: 00037/00094 | Loss: 530.3011 | CE: 0.3061 | KD: 1059.9901\n",
      "Train Epoch: 002 Batch: 00038/00094 | Loss: 530.2707 | CE: 0.2838 | KD: 1059.9738\n",
      "Train Epoch: 002 Batch: 00039/00094 | Loss: 530.3420 | CE: 0.3376 | KD: 1060.0088\n",
      "Train Epoch: 002 Batch: 00040/00094 | Loss: 530.2927 | CE: 0.2855 | KD: 1060.0144\n",
      "Train Epoch: 002 Batch: 00041/00094 | Loss: 530.3008 | CE: 0.2947 | KD: 1060.0121\n",
      "Train Epoch: 002 Batch: 00042/00094 | Loss: 530.3682 | CE: 0.3633 | KD: 1060.0098\n",
      "Train Epoch: 002 Batch: 00043/00094 | Loss: 530.3112 | CE: 0.3196 | KD: 1059.9833\n",
      "Train Epoch: 002 Batch: 00044/00094 | Loss: 530.3376 | CE: 0.3286 | KD: 1060.0181\n",
      "Train Epoch: 002 Batch: 00045/00094 | Loss: 530.3021 | CE: 0.3034 | KD: 1059.9974\n",
      "Train Epoch: 002 Batch: 00046/00094 | Loss: 530.2894 | CE: 0.2985 | KD: 1059.9818\n",
      "Train Epoch: 002 Batch: 00047/00094 | Loss: 530.2953 | CE: 0.2921 | KD: 1060.0065\n",
      "Train Epoch: 002 Batch: 00048/00094 | Loss: 530.2779 | CE: 0.2879 | KD: 1059.9801\n",
      "Train Epoch: 002 Batch: 00049/00094 | Loss: 530.3246 | CE: 0.3342 | KD: 1059.9807\n",
      "Train Epoch: 002 Batch: 00050/00094 | Loss: 530.2961 | CE: 0.3165 | KD: 1059.9592\n",
      "Train Epoch: 002 Batch: 00051/00094 | Loss: 530.2842 | CE: 0.2956 | KD: 1059.9772\n",
      "Train Epoch: 002 Batch: 00052/00094 | Loss: 530.2812 | CE: 0.2812 | KD: 1059.9999\n",
      "Train Epoch: 002 Batch: 00053/00094 | Loss: 530.2626 | CE: 0.2691 | KD: 1059.9872\n",
      "Train Epoch: 002 Batch: 00054/00094 | Loss: 530.3500 | CE: 0.3386 | KD: 1060.0228\n",
      "Train Epoch: 002 Batch: 00055/00094 | Loss: 530.2779 | CE: 0.2787 | KD: 1059.9983\n",
      "Train Epoch: 002 Batch: 00056/00094 | Loss: 530.3303 | CE: 0.3398 | KD: 1059.9808\n",
      "Train Epoch: 002 Batch: 00057/00094 | Loss: 530.2962 | CE: 0.2923 | KD: 1060.0078\n",
      "Train Epoch: 002 Batch: 00058/00094 | Loss: 530.2953 | CE: 0.3109 | KD: 1059.9689\n",
      "Train Epoch: 002 Batch: 00059/00094 | Loss: 530.2516 | CE: 0.2711 | KD: 1059.9612\n",
      "Train Epoch: 002 Batch: 00060/00094 | Loss: 530.3131 | CE: 0.3092 | KD: 1060.0077\n",
      "Train Epoch: 002 Batch: 00061/00094 | Loss: 530.2475 | CE: 0.2494 | KD: 1059.9961\n",
      "Train Epoch: 002 Batch: 00062/00094 | Loss: 530.2792 | CE: 0.2963 | KD: 1059.9658\n",
      "Train Epoch: 002 Batch: 00063/00094 | Loss: 530.3436 | CE: 0.3362 | KD: 1060.0148\n",
      "Train Epoch: 002 Batch: 00064/00094 | Loss: 530.2313 | CE: 0.2516 | KD: 1059.9595\n",
      "Train Epoch: 002 Batch: 00065/00094 | Loss: 530.2512 | CE: 0.2721 | KD: 1059.9584\n",
      "Train Epoch: 002 Batch: 00066/00094 | Loss: 530.3831 | CE: 0.3512 | KD: 1060.0638\n",
      "Train Epoch: 002 Batch: 00067/00094 | Loss: 530.2487 | CE: 0.2596 | KD: 1059.9781\n",
      "Train Epoch: 002 Batch: 00068/00094 | Loss: 530.3666 | CE: 0.3465 | KD: 1060.0404\n",
      "Train Epoch: 002 Batch: 00069/00094 | Loss: 530.2720 | CE: 0.2689 | KD: 1060.0062\n",
      "Train Epoch: 002 Batch: 00070/00094 | Loss: 530.3093 | CE: 0.3170 | KD: 1059.9847\n",
      "Train Epoch: 002 Batch: 00071/00094 | Loss: 530.2957 | CE: 0.2811 | KD: 1060.0293\n",
      "Train Epoch: 002 Batch: 00072/00094 | Loss: 530.3036 | CE: 0.2856 | KD: 1060.0361\n",
      "Train Epoch: 002 Batch: 00073/00094 | Loss: 530.1977 | CE: 0.2318 | KD: 1059.9319\n",
      "Train Epoch: 002 Batch: 00074/00094 | Loss: 530.2861 | CE: 0.2616 | KD: 1060.0488\n",
      "Train Epoch: 002 Batch: 00075/00094 | Loss: 530.2359 | CE: 0.2482 | KD: 1059.9753\n",
      "Train Epoch: 002 Batch: 00076/00094 | Loss: 530.3043 | CE: 0.3104 | KD: 1059.9878\n",
      "Train Epoch: 002 Batch: 00077/00094 | Loss: 530.2845 | CE: 0.2723 | KD: 1060.0245\n",
      "Train Epoch: 002 Batch: 00078/00094 | Loss: 530.3241 | CE: 0.2970 | KD: 1060.0542\n",
      "Train Epoch: 002 Batch: 00079/00094 | Loss: 530.3181 | CE: 0.3106 | KD: 1060.0148\n",
      "Train Epoch: 002 Batch: 00080/00094 | Loss: 530.3754 | CE: 0.3388 | KD: 1060.0731\n",
      "Train Epoch: 002 Batch: 00081/00094 | Loss: 530.2863 | CE: 0.2533 | KD: 1060.0659\n",
      "Train Epoch: 002 Batch: 00082/00094 | Loss: 530.3022 | CE: 0.2961 | KD: 1060.0123\n",
      "Train Epoch: 002 Batch: 00083/00094 | Loss: 530.3161 | CE: 0.3191 | KD: 1059.9940\n",
      "Train Epoch: 002 Batch: 00084/00094 | Loss: 530.2250 | CE: 0.2354 | KD: 1059.9792\n",
      "Train Epoch: 002 Batch: 00085/00094 | Loss: 530.2597 | CE: 0.2608 | KD: 1059.9978\n",
      "Train Epoch: 002 Batch: 00086/00094 | Loss: 530.2926 | CE: 0.2916 | KD: 1060.0020\n",
      "Train Epoch: 002 Batch: 00087/00094 | Loss: 530.3166 | CE: 0.2732 | KD: 1060.0867\n",
      "Train Epoch: 002 Batch: 00088/00094 | Loss: 530.3605 | CE: 0.3195 | KD: 1060.0820\n",
      "Train Epoch: 002 Batch: 00089/00094 | Loss: 530.2715 | CE: 0.2755 | KD: 1059.9918\n",
      "Train Epoch: 002 Batch: 00090/00094 | Loss: 530.2877 | CE: 0.2809 | KD: 1060.0137\n",
      "Train Epoch: 002 Batch: 00091/00094 | Loss: 530.2743 | CE: 0.2845 | KD: 1059.9796\n",
      "Train Epoch: 002 Batch: 00092/00094 | Loss: 530.2532 | CE: 0.2566 | KD: 1059.9930\n",
      "Train Epoch: 002 Batch: 00093/00094 | Loss: 530.2640 | CE: 0.2531 | KD: 1060.0219\n",
      "Train Epoch: 002 Batch: 00094/00094 | Loss: 530.2614 | CE: 0.2706 | KD: 1059.9814\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2651 | acc:92.8000\n",
      "[VAL Acc] Target: 92.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:0.9575 | acc:54.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 54.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5991 | acc:68.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.7784 | acc:56.2977\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 56.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6403 | acc:62.7743\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.77%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4440 | acc:81.4233\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 81.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5875 | acc:71.8652\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7525 | acc:59.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.25%\n",
      "[VAL Acc] Avg 68.49%\n",
      "Train Epoch: 003 Batch: 00001/00094 | Loss: 530.2460 | CE: 0.2301 | KD: 1060.0319\n",
      "Train Epoch: 003 Batch: 00002/00094 | Loss: 530.2348 | CE: 0.2268 | KD: 1060.0160\n",
      "Train Epoch: 003 Batch: 00003/00094 | Loss: 530.2657 | CE: 0.2564 | KD: 1060.0186\n",
      "Train Epoch: 003 Batch: 00004/00094 | Loss: 530.2510 | CE: 0.2453 | KD: 1060.0116\n",
      "Train Epoch: 003 Batch: 00005/00094 | Loss: 530.3296 | CE: 0.3135 | KD: 1060.0322\n",
      "Train Epoch: 003 Batch: 00006/00094 | Loss: 530.2747 | CE: 0.2728 | KD: 1060.0038\n",
      "Train Epoch: 003 Batch: 00007/00094 | Loss: 530.2933 | CE: 0.2874 | KD: 1060.0118\n",
      "Train Epoch: 003 Batch: 00008/00094 | Loss: 530.2167 | CE: 0.2472 | KD: 1059.9388\n",
      "Train Epoch: 003 Batch: 00009/00094 | Loss: 530.2832 | CE: 0.2757 | KD: 1060.0150\n",
      "Train Epoch: 003 Batch: 00010/00094 | Loss: 530.2421 | CE: 0.2377 | KD: 1060.0088\n",
      "Train Epoch: 003 Batch: 00011/00094 | Loss: 530.3224 | CE: 0.3034 | KD: 1060.0380\n",
      "Train Epoch: 003 Batch: 00012/00094 | Loss: 530.2727 | CE: 0.2748 | KD: 1059.9957\n",
      "Train Epoch: 003 Batch: 00013/00094 | Loss: 530.2850 | CE: 0.2946 | KD: 1059.9808\n",
      "Train Epoch: 003 Batch: 00014/00094 | Loss: 530.3108 | CE: 0.2886 | KD: 1060.0444\n",
      "Train Epoch: 003 Batch: 00015/00094 | Loss: 530.2900 | CE: 0.2885 | KD: 1060.0029\n",
      "Train Epoch: 003 Batch: 00016/00094 | Loss: 530.2468 | CE: 0.2411 | KD: 1060.0115\n",
      "Train Epoch: 003 Batch: 00017/00094 | Loss: 530.3178 | CE: 0.3096 | KD: 1060.0165\n",
      "Train Epoch: 003 Batch: 00018/00094 | Loss: 530.2258 | CE: 0.2445 | KD: 1059.9626\n",
      "Train Epoch: 003 Batch: 00019/00094 | Loss: 530.3098 | CE: 0.2998 | KD: 1060.0198\n",
      "Train Epoch: 003 Batch: 00020/00094 | Loss: 530.2694 | CE: 0.2626 | KD: 1060.0137\n",
      "Train Epoch: 003 Batch: 00021/00094 | Loss: 530.2868 | CE: 0.2796 | KD: 1060.0143\n",
      "Train Epoch: 003 Batch: 00022/00094 | Loss: 530.2820 | CE: 0.2697 | KD: 1060.0247\n",
      "Train Epoch: 003 Batch: 00023/00094 | Loss: 530.2529 | CE: 0.2539 | KD: 1059.9978\n",
      "Train Epoch: 003 Batch: 00024/00094 | Loss: 530.2771 | CE: 0.2801 | KD: 1059.9940\n",
      "Train Epoch: 003 Batch: 00025/00094 | Loss: 530.3333 | CE: 0.3056 | KD: 1060.0554\n",
      "Train Epoch: 003 Batch: 00026/00094 | Loss: 530.3638 | CE: 0.3695 | KD: 1059.9885\n",
      "Train Epoch: 003 Batch: 00027/00094 | Loss: 530.2548 | CE: 0.2551 | KD: 1059.9994\n",
      "Train Epoch: 003 Batch: 00028/00094 | Loss: 530.3531 | CE: 0.3231 | KD: 1060.0601\n",
      "Train Epoch: 003 Batch: 00029/00094 | Loss: 530.2036 | CE: 0.2181 | KD: 1059.9711\n",
      "Train Epoch: 003 Batch: 00030/00094 | Loss: 530.2560 | CE: 0.2497 | KD: 1060.0126\n",
      "Train Epoch: 003 Batch: 00031/00094 | Loss: 530.3067 | CE: 0.2949 | KD: 1060.0236\n",
      "Train Epoch: 003 Batch: 00032/00094 | Loss: 530.2453 | CE: 0.2655 | KD: 1059.9596\n",
      "Train Epoch: 003 Batch: 00033/00094 | Loss: 530.2136 | CE: 0.2340 | KD: 1059.9591\n",
      "Train Epoch: 003 Batch: 00034/00094 | Loss: 530.2550 | CE: 0.2484 | KD: 1060.0132\n",
      "Train Epoch: 003 Batch: 00035/00094 | Loss: 530.2196 | CE: 0.2150 | KD: 1060.0092\n",
      "Train Epoch: 003 Batch: 00036/00094 | Loss: 530.2619 | CE: 0.2491 | KD: 1060.0256\n",
      "Train Epoch: 003 Batch: 00037/00094 | Loss: 530.3156 | CE: 0.2909 | KD: 1060.0492\n",
      "Train Epoch: 003 Batch: 00038/00094 | Loss: 530.3356 | CE: 0.3308 | KD: 1060.0098\n",
      "Train Epoch: 003 Batch: 00039/00094 | Loss: 530.2891 | CE: 0.2899 | KD: 1059.9985\n",
      "Train Epoch: 003 Batch: 00040/00094 | Loss: 530.2468 | CE: 0.2559 | KD: 1059.9819\n",
      "Train Epoch: 003 Batch: 00041/00094 | Loss: 530.1956 | CE: 0.2123 | KD: 1059.9666\n",
      "Train Epoch: 003 Batch: 00042/00094 | Loss: 530.2349 | CE: 0.2420 | KD: 1059.9857\n",
      "Train Epoch: 003 Batch: 00043/00094 | Loss: 530.2393 | CE: 0.2352 | KD: 1060.0082\n",
      "Train Epoch: 003 Batch: 00044/00094 | Loss: 530.2306 | CE: 0.2311 | KD: 1059.9990\n",
      "Train Epoch: 003 Batch: 00045/00094 | Loss: 530.2053 | CE: 0.2038 | KD: 1060.0029\n",
      "Train Epoch: 003 Batch: 00046/00094 | Loss: 530.2580 | CE: 0.2631 | KD: 1059.9899\n",
      "Train Epoch: 003 Batch: 00047/00094 | Loss: 530.2357 | CE: 0.2452 | KD: 1059.9810\n",
      "Train Epoch: 003 Batch: 00048/00094 | Loss: 530.2335 | CE: 0.2285 | KD: 1060.0101\n",
      "Train Epoch: 003 Batch: 00049/00094 | Loss: 530.2950 | CE: 0.2609 | KD: 1060.0682\n",
      "Train Epoch: 003 Batch: 00050/00094 | Loss: 530.3196 | CE: 0.2910 | KD: 1060.0571\n",
      "Train Epoch: 003 Batch: 00051/00094 | Loss: 530.3449 | CE: 0.3306 | KD: 1060.0286\n",
      "Train Epoch: 003 Batch: 00052/00094 | Loss: 530.2475 | CE: 0.2644 | KD: 1059.9662\n",
      "Train Epoch: 003 Batch: 00053/00094 | Loss: 530.2673 | CE: 0.2623 | KD: 1060.0100\n",
      "Train Epoch: 003 Batch: 00054/00094 | Loss: 530.2737 | CE: 0.2686 | KD: 1060.0103\n",
      "Train Epoch: 003 Batch: 00055/00094 | Loss: 530.2203 | CE: 0.2220 | KD: 1059.9966\n",
      "Train Epoch: 003 Batch: 00056/00094 | Loss: 530.2985 | CE: 0.2659 | KD: 1060.0652\n",
      "Train Epoch: 003 Batch: 00057/00094 | Loss: 530.2456 | CE: 0.2207 | KD: 1060.0498\n",
      "Train Epoch: 003 Batch: 00058/00094 | Loss: 530.2862 | CE: 0.2587 | KD: 1060.0549\n",
      "Train Epoch: 003 Batch: 00059/00094 | Loss: 530.2018 | CE: 0.2212 | KD: 1059.9611\n",
      "Train Epoch: 003 Batch: 00060/00094 | Loss: 530.2042 | CE: 0.2286 | KD: 1059.9512\n",
      "Train Epoch: 003 Batch: 00061/00094 | Loss: 530.3868 | CE: 0.3478 | KD: 1060.0781\n",
      "Train Epoch: 003 Batch: 00062/00094 | Loss: 530.2465 | CE: 0.2447 | KD: 1060.0035\n",
      "Train Epoch: 003 Batch: 00063/00094 | Loss: 530.2513 | CE: 0.2337 | KD: 1060.0353\n",
      "Train Epoch: 003 Batch: 00064/00094 | Loss: 530.3829 | CE: 0.3259 | KD: 1060.1141\n",
      "Train Epoch: 003 Batch: 00065/00094 | Loss: 530.2438 | CE: 0.2546 | KD: 1059.9784\n",
      "Train Epoch: 003 Batch: 00066/00094 | Loss: 530.2422 | CE: 0.2559 | KD: 1059.9727\n",
      "Train Epoch: 003 Batch: 00067/00094 | Loss: 530.2339 | CE: 0.2292 | KD: 1060.0094\n",
      "Train Epoch: 003 Batch: 00068/00094 | Loss: 530.2521 | CE: 0.2506 | KD: 1060.0032\n",
      "Train Epoch: 003 Batch: 00069/00094 | Loss: 530.2545 | CE: 0.2627 | KD: 1059.9836\n",
      "Train Epoch: 003 Batch: 00070/00094 | Loss: 530.2438 | CE: 0.2413 | KD: 1060.0051\n",
      "Train Epoch: 003 Batch: 00071/00094 | Loss: 530.2260 | CE: 0.2270 | KD: 1059.9980\n",
      "Train Epoch: 003 Batch: 00072/00094 | Loss: 530.2466 | CE: 0.2401 | KD: 1060.0131\n",
      "Train Epoch: 003 Batch: 00073/00094 | Loss: 530.2241 | CE: 0.2126 | KD: 1060.0231\n",
      "Train Epoch: 003 Batch: 00074/00094 | Loss: 530.2251 | CE: 0.2348 | KD: 1059.9807\n",
      "Train Epoch: 003 Batch: 00075/00094 | Loss: 530.2369 | CE: 0.2366 | KD: 1060.0006\n",
      "Train Epoch: 003 Batch: 00076/00094 | Loss: 530.3003 | CE: 0.2731 | KD: 1060.0544\n",
      "Train Epoch: 003 Batch: 00077/00094 | Loss: 530.2593 | CE: 0.2623 | KD: 1059.9939\n",
      "Train Epoch: 003 Batch: 00078/00094 | Loss: 530.2428 | CE: 0.2447 | KD: 1059.9963\n",
      "Train Epoch: 003 Batch: 00079/00094 | Loss: 530.1796 | CE: 0.1746 | KD: 1060.0099\n",
      "Train Epoch: 003 Batch: 00080/00094 | Loss: 530.2604 | CE: 0.2589 | KD: 1060.0028\n",
      "Train Epoch: 003 Batch: 00081/00094 | Loss: 530.3372 | CE: 0.2932 | KD: 1060.0879\n",
      "Train Epoch: 003 Batch: 00082/00094 | Loss: 530.2080 | CE: 0.2189 | KD: 1059.9783\n",
      "Train Epoch: 003 Batch: 00083/00094 | Loss: 530.1946 | CE: 0.1832 | KD: 1060.0228\n",
      "Train Epoch: 003 Batch: 00084/00094 | Loss: 530.2764 | CE: 0.2695 | KD: 1060.0138\n",
      "Train Epoch: 003 Batch: 00085/00094 | Loss: 530.2283 | CE: 0.2385 | KD: 1059.9797\n",
      "Train Epoch: 003 Batch: 00086/00094 | Loss: 530.2385 | CE: 0.2321 | KD: 1060.0128\n",
      "Train Epoch: 003 Batch: 00087/00094 | Loss: 530.1794 | CE: 0.1793 | KD: 1060.0002\n",
      "Train Epoch: 003 Batch: 00088/00094 | Loss: 530.2880 | CE: 0.2725 | KD: 1060.0310\n",
      "Train Epoch: 003 Batch: 00089/00094 | Loss: 530.2789 | CE: 0.2605 | KD: 1060.0367\n",
      "Train Epoch: 003 Batch: 00090/00094 | Loss: 530.2969 | CE: 0.2940 | KD: 1060.0059\n",
      "Train Epoch: 003 Batch: 00091/00094 | Loss: 530.2630 | CE: 0.2809 | KD: 1059.9642\n",
      "Train Epoch: 003 Batch: 00092/00094 | Loss: 530.2584 | CE: 0.2493 | KD: 1060.0181\n",
      "Train Epoch: 003 Batch: 00093/00094 | Loss: 530.2573 | CE: 0.2405 | KD: 1060.0336\n",
      "Train Epoch: 003 Batch: 00094/00094 | Loss: 530.2115 | CE: 0.2210 | KD: 1059.9811\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2319 | acc:94.1500\n",
      "[VAL Acc] Target: 94.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0696 | acc:53.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 53.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6252 | acc:66.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 66.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8289 | acc:54.0076\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 54.01%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7203 | acc:59.6787\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4550 | acc:78.0961\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6363 | acc:70.4154\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7027 | acc:61.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.50%\n",
      "[VAL Acc] Avg 67.29%\n",
      "Train Epoch: 004 Batch: 00001/00094 | Loss: 530.2408 | CE: 0.2400 | KD: 1060.0018\n",
      "Train Epoch: 004 Batch: 00002/00094 | Loss: 530.2074 | CE: 0.2149 | KD: 1059.9851\n",
      "Train Epoch: 004 Batch: 00003/00094 | Loss: 530.2311 | CE: 0.2341 | KD: 1059.9941\n",
      "Train Epoch: 004 Batch: 00004/00094 | Loss: 530.2065 | CE: 0.1992 | KD: 1060.0146\n",
      "Train Epoch: 004 Batch: 00005/00094 | Loss: 530.3181 | CE: 0.3023 | KD: 1060.0316\n",
      "Train Epoch: 004 Batch: 00006/00094 | Loss: 530.2739 | CE: 0.2593 | KD: 1060.0292\n",
      "Train Epoch: 004 Batch: 00007/00094 | Loss: 530.2332 | CE: 0.2114 | KD: 1060.0437\n",
      "Train Epoch: 004 Batch: 00008/00094 | Loss: 530.2318 | CE: 0.2318 | KD: 1059.9999\n",
      "Train Epoch: 004 Batch: 00009/00094 | Loss: 530.1602 | CE: 0.1783 | KD: 1059.9637\n",
      "Train Epoch: 004 Batch: 00010/00094 | Loss: 530.2489 | CE: 0.2403 | KD: 1060.0173\n",
      "Train Epoch: 004 Batch: 00011/00094 | Loss: 530.2549 | CE: 0.2515 | KD: 1060.0068\n",
      "Train Epoch: 004 Batch: 00012/00094 | Loss: 530.2401 | CE: 0.2450 | KD: 1059.9902\n",
      "Train Epoch: 004 Batch: 00013/00094 | Loss: 530.2578 | CE: 0.2323 | KD: 1060.0509\n",
      "Train Epoch: 004 Batch: 00014/00094 | Loss: 530.2601 | CE: 0.2336 | KD: 1060.0529\n",
      "Train Epoch: 004 Batch: 00015/00094 | Loss: 530.2053 | CE: 0.2074 | KD: 1059.9958\n",
      "Train Epoch: 004 Batch: 00016/00094 | Loss: 530.2636 | CE: 0.2469 | KD: 1060.0334\n",
      "Train Epoch: 004 Batch: 00017/00094 | Loss: 530.1982 | CE: 0.1957 | KD: 1060.0049\n",
      "Train Epoch: 004 Batch: 00018/00094 | Loss: 530.3093 | CE: 0.2733 | KD: 1060.0720\n",
      "Train Epoch: 004 Batch: 00019/00094 | Loss: 530.2767 | CE: 0.2849 | KD: 1059.9835\n",
      "Train Epoch: 004 Batch: 00020/00094 | Loss: 530.2512 | CE: 0.2494 | KD: 1060.0037\n",
      "Train Epoch: 004 Batch: 00021/00094 | Loss: 530.2229 | CE: 0.2098 | KD: 1060.0262\n",
      "Train Epoch: 004 Batch: 00022/00094 | Loss: 530.2610 | CE: 0.2298 | KD: 1060.0624\n",
      "Train Epoch: 004 Batch: 00023/00094 | Loss: 530.2385 | CE: 0.2239 | KD: 1060.0292\n",
      "Train Epoch: 004 Batch: 00024/00094 | Loss: 530.1942 | CE: 0.1974 | KD: 1059.9937\n",
      "Train Epoch: 004 Batch: 00025/00094 | Loss: 530.2271 | CE: 0.2221 | KD: 1060.0100\n",
      "Train Epoch: 004 Batch: 00026/00094 | Loss: 530.2033 | CE: 0.2157 | KD: 1059.9751\n",
      "Train Epoch: 004 Batch: 00027/00094 | Loss: 530.1740 | CE: 0.1949 | KD: 1059.9583\n",
      "Train Epoch: 004 Batch: 00028/00094 | Loss: 530.2406 | CE: 0.2186 | KD: 1060.0439\n",
      "Train Epoch: 004 Batch: 00029/00094 | Loss: 530.2280 | CE: 0.2360 | KD: 1059.9840\n",
      "Train Epoch: 004 Batch: 00030/00094 | Loss: 530.2708 | CE: 0.2557 | KD: 1060.0300\n",
      "Train Epoch: 004 Batch: 00031/00094 | Loss: 530.1704 | CE: 0.1779 | KD: 1059.9850\n",
      "Train Epoch: 004 Batch: 00032/00094 | Loss: 530.2328 | CE: 0.2115 | KD: 1060.0425\n",
      "Train Epoch: 004 Batch: 00033/00094 | Loss: 530.2510 | CE: 0.2201 | KD: 1060.0618\n",
      "Train Epoch: 004 Batch: 00034/00094 | Loss: 530.3254 | CE: 0.2916 | KD: 1060.0676\n",
      "Train Epoch: 004 Batch: 00035/00094 | Loss: 530.2642 | CE: 0.2372 | KD: 1060.0538\n",
      "Train Epoch: 004 Batch: 00036/00094 | Loss: 530.2655 | CE: 0.2383 | KD: 1060.0544\n",
      "Train Epoch: 004 Batch: 00037/00094 | Loss: 530.2051 | CE: 0.2178 | KD: 1059.9746\n",
      "Train Epoch: 004 Batch: 00038/00094 | Loss: 530.2490 | CE: 0.2443 | KD: 1060.0094\n",
      "Train Epoch: 004 Batch: 00039/00094 | Loss: 530.2498 | CE: 0.2160 | KD: 1060.0677\n",
      "Train Epoch: 004 Batch: 00040/00094 | Loss: 530.2938 | CE: 0.3016 | KD: 1059.9845\n",
      "Train Epoch: 004 Batch: 00041/00094 | Loss: 530.3358 | CE: 0.2828 | KD: 1060.1060\n",
      "Train Epoch: 004 Batch: 00042/00094 | Loss: 530.2778 | CE: 0.2494 | KD: 1060.0568\n",
      "Train Epoch: 004 Batch: 00043/00094 | Loss: 530.3108 | CE: 0.2934 | KD: 1060.0348\n",
      "Train Epoch: 004 Batch: 00044/00094 | Loss: 530.2823 | CE: 0.2677 | KD: 1060.0293\n",
      "Train Epoch: 004 Batch: 00045/00094 | Loss: 530.2378 | CE: 0.2180 | KD: 1060.0397\n",
      "Train Epoch: 004 Batch: 00046/00094 | Loss: 530.2089 | CE: 0.2009 | KD: 1060.0160\n",
      "Train Epoch: 004 Batch: 00047/00094 | Loss: 530.1868 | CE: 0.1867 | KD: 1060.0001\n",
      "Train Epoch: 004 Batch: 00048/00094 | Loss: 530.2238 | CE: 0.2144 | KD: 1060.0188\n",
      "Train Epoch: 004 Batch: 00049/00094 | Loss: 530.3029 | CE: 0.2479 | KD: 1060.1100\n",
      "Train Epoch: 004 Batch: 00050/00094 | Loss: 530.2607 | CE: 0.2362 | KD: 1060.0490\n",
      "Train Epoch: 004 Batch: 00051/00094 | Loss: 530.2096 | CE: 0.2118 | KD: 1059.9957\n",
      "Train Epoch: 004 Batch: 00052/00094 | Loss: 530.1891 | CE: 0.1873 | KD: 1060.0038\n",
      "Train Epoch: 004 Batch: 00053/00094 | Loss: 530.2115 | CE: 0.2222 | KD: 1059.9786\n",
      "Train Epoch: 004 Batch: 00054/00094 | Loss: 530.2739 | CE: 0.2648 | KD: 1060.0181\n",
      "Train Epoch: 004 Batch: 00055/00094 | Loss: 530.2240 | CE: 0.2352 | KD: 1059.9777\n",
      "Train Epoch: 004 Batch: 00056/00094 | Loss: 530.2302 | CE: 0.2115 | KD: 1060.0374\n",
      "Train Epoch: 004 Batch: 00057/00094 | Loss: 530.2799 | CE: 0.2794 | KD: 1060.0010\n",
      "Train Epoch: 004 Batch: 00058/00094 | Loss: 530.2897 | CE: 0.2814 | KD: 1060.0166\n",
      "Train Epoch: 004 Batch: 00059/00094 | Loss: 530.2288 | CE: 0.2370 | KD: 1059.9835\n",
      "Train Epoch: 004 Batch: 00060/00094 | Loss: 530.1599 | CE: 0.1606 | KD: 1059.9985\n",
      "Train Epoch: 004 Batch: 00061/00094 | Loss: 530.2599 | CE: 0.2203 | KD: 1060.0793\n",
      "Train Epoch: 004 Batch: 00062/00094 | Loss: 530.2911 | CE: 0.2449 | KD: 1060.0924\n",
      "Train Epoch: 004 Batch: 00063/00094 | Loss: 530.2416 | CE: 0.2483 | KD: 1059.9865\n",
      "Train Epoch: 004 Batch: 00064/00094 | Loss: 530.2104 | CE: 0.1916 | KD: 1060.0376\n",
      "Train Epoch: 004 Batch: 00065/00094 | Loss: 530.2065 | CE: 0.2026 | KD: 1060.0079\n",
      "Train Epoch: 004 Batch: 00066/00094 | Loss: 530.2923 | CE: 0.2886 | KD: 1060.0073\n",
      "Train Epoch: 004 Batch: 00067/00094 | Loss: 530.2154 | CE: 0.1946 | KD: 1060.0416\n",
      "Train Epoch: 004 Batch: 00068/00094 | Loss: 530.2242 | CE: 0.2274 | KD: 1059.9935\n",
      "Train Epoch: 004 Batch: 00069/00094 | Loss: 530.3308 | CE: 0.2913 | KD: 1060.0790\n",
      "Train Epoch: 004 Batch: 00070/00094 | Loss: 530.2073 | CE: 0.2184 | KD: 1059.9779\n",
      "Train Epoch: 004 Batch: 00071/00094 | Loss: 530.2988 | CE: 0.2814 | KD: 1060.0348\n",
      "Train Epoch: 004 Batch: 00072/00094 | Loss: 530.2474 | CE: 0.2235 | KD: 1060.0477\n",
      "Train Epoch: 004 Batch: 00073/00094 | Loss: 530.2063 | CE: 0.1955 | KD: 1060.0216\n",
      "Train Epoch: 004 Batch: 00074/00094 | Loss: 530.2308 | CE: 0.2333 | KD: 1059.9950\n",
      "Train Epoch: 004 Batch: 00075/00094 | Loss: 530.3523 | CE: 0.3297 | KD: 1060.0452\n",
      "Train Epoch: 004 Batch: 00076/00094 | Loss: 530.2402 | CE: 0.2360 | KD: 1060.0085\n",
      "Train Epoch: 004 Batch: 00077/00094 | Loss: 530.2267 | CE: 0.2109 | KD: 1060.0316\n",
      "Train Epoch: 004 Batch: 00078/00094 | Loss: 530.2087 | CE: 0.2278 | KD: 1059.9617\n",
      "Train Epoch: 004 Batch: 00079/00094 | Loss: 530.1852 | CE: 0.1957 | KD: 1059.9790\n",
      "Train Epoch: 004 Batch: 00080/00094 | Loss: 530.2382 | CE: 0.2391 | KD: 1059.9980\n",
      "Train Epoch: 004 Batch: 00081/00094 | Loss: 530.3191 | CE: 0.3112 | KD: 1060.0157\n",
      "Train Epoch: 004 Batch: 00082/00094 | Loss: 530.2192 | CE: 0.2066 | KD: 1060.0251\n",
      "Train Epoch: 004 Batch: 00083/00094 | Loss: 530.1664 | CE: 0.1888 | KD: 1059.9553\n",
      "Train Epoch: 004 Batch: 00084/00094 | Loss: 530.2260 | CE: 0.2247 | KD: 1060.0027\n",
      "Train Epoch: 004 Batch: 00085/00094 | Loss: 530.2069 | CE: 0.1918 | KD: 1060.0302\n",
      "Train Epoch: 004 Batch: 00086/00094 | Loss: 530.3268 | CE: 0.3236 | KD: 1060.0065\n",
      "Train Epoch: 004 Batch: 00087/00094 | Loss: 530.2232 | CE: 0.2260 | KD: 1059.9944\n",
      "Train Epoch: 004 Batch: 00088/00094 | Loss: 530.2204 | CE: 0.2203 | KD: 1060.0001\n",
      "Train Epoch: 004 Batch: 00089/00094 | Loss: 530.2833 | CE: 0.2636 | KD: 1060.0396\n",
      "Train Epoch: 004 Batch: 00090/00094 | Loss: 530.1633 | CE: 0.1721 | KD: 1059.9824\n",
      "Train Epoch: 004 Batch: 00091/00094 | Loss: 530.2493 | CE: 0.2453 | KD: 1060.0079\n",
      "Train Epoch: 004 Batch: 00092/00094 | Loss: 530.1933 | CE: 0.1829 | KD: 1060.0208\n",
      "Train Epoch: 004 Batch: 00093/00094 | Loss: 530.2992 | CE: 0.2707 | KD: 1060.0569\n",
      "Train Epoch: 004 Batch: 00094/00094 | Loss: 530.3336 | CE: 0.2972 | KD: 1060.0729\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2154 | acc:95.0000\n",
      "[VAL Acc] Target: 95.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0837 | acc:52.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 52.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6332 | acc:66.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 66.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8547 | acc:52.0992\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 52.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7758 | acc:57.2100\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.21%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4805 | acc:78.6506\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6265 | acc:70.0627\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7045 | acc:61.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.88%\n",
      "[VAL Acc] Avg 66.67%\n",
      "Train Epoch: 005 Batch: 00001/00094 | Loss: 530.2634 | CE: 0.2499 | KD: 1060.0271\n",
      "Train Epoch: 005 Batch: 00002/00094 | Loss: 530.2439 | CE: 0.2186 | KD: 1060.0507\n",
      "Train Epoch: 005 Batch: 00003/00094 | Loss: 530.1952 | CE: 0.1940 | KD: 1060.0023\n",
      "Train Epoch: 005 Batch: 00004/00094 | Loss: 530.2573 | CE: 0.2367 | KD: 1060.0413\n",
      "Train Epoch: 005 Batch: 00005/00094 | Loss: 530.3284 | CE: 0.3189 | KD: 1060.0190\n",
      "Train Epoch: 005 Batch: 00006/00094 | Loss: 530.2477 | CE: 0.2342 | KD: 1060.0270\n",
      "Train Epoch: 005 Batch: 00007/00094 | Loss: 530.2306 | CE: 0.2438 | KD: 1059.9735\n",
      "Train Epoch: 005 Batch: 00008/00094 | Loss: 530.2710 | CE: 0.2735 | KD: 1059.9951\n",
      "Train Epoch: 005 Batch: 00009/00094 | Loss: 530.2095 | CE: 0.2152 | KD: 1059.9885\n",
      "Train Epoch: 005 Batch: 00010/00094 | Loss: 530.1730 | CE: 0.1886 | KD: 1059.9689\n",
      "Train Epoch: 005 Batch: 00011/00094 | Loss: 530.2226 | CE: 0.2187 | KD: 1060.0078\n",
      "Train Epoch: 005 Batch: 00012/00094 | Loss: 530.1707 | CE: 0.1914 | KD: 1059.9584\n",
      "Train Epoch: 005 Batch: 00013/00094 | Loss: 530.1913 | CE: 0.2028 | KD: 1059.9771\n",
      "Train Epoch: 005 Batch: 00014/00094 | Loss: 530.1940 | CE: 0.1864 | KD: 1060.0151\n",
      "Train Epoch: 005 Batch: 00015/00094 | Loss: 530.2203 | CE: 0.2028 | KD: 1060.0350\n",
      "Train Epoch: 005 Batch: 00016/00094 | Loss: 530.3041 | CE: 0.2683 | KD: 1060.0717\n",
      "Train Epoch: 005 Batch: 00017/00094 | Loss: 530.1733 | CE: 0.1981 | KD: 1059.9506\n",
      "Train Epoch: 005 Batch: 00018/00094 | Loss: 530.3468 | CE: 0.3280 | KD: 1060.0377\n",
      "Train Epoch: 005 Batch: 00019/00094 | Loss: 530.1801 | CE: 0.1869 | KD: 1059.9865\n",
      "Train Epoch: 005 Batch: 00020/00094 | Loss: 530.2219 | CE: 0.2429 | KD: 1059.9580\n",
      "Train Epoch: 005 Batch: 00021/00094 | Loss: 530.2346 | CE: 0.2297 | KD: 1060.0096\n",
      "Train Epoch: 005 Batch: 00022/00094 | Loss: 530.2068 | CE: 0.1890 | KD: 1060.0355\n",
      "Train Epoch: 005 Batch: 00023/00094 | Loss: 530.2131 | CE: 0.2077 | KD: 1060.0109\n",
      "Train Epoch: 005 Batch: 00024/00094 | Loss: 530.2308 | CE: 0.2304 | KD: 1060.0007\n",
      "Train Epoch: 005 Batch: 00025/00094 | Loss: 530.2258 | CE: 0.2178 | KD: 1060.0161\n",
      "Train Epoch: 005 Batch: 00026/00094 | Loss: 530.2617 | CE: 0.2295 | KD: 1060.0645\n",
      "Train Epoch: 005 Batch: 00027/00094 | Loss: 530.1987 | CE: 0.2120 | KD: 1059.9733\n",
      "Train Epoch: 005 Batch: 00028/00094 | Loss: 530.2032 | CE: 0.2091 | KD: 1059.9883\n",
      "Train Epoch: 005 Batch: 00029/00094 | Loss: 530.2037 | CE: 0.2000 | KD: 1060.0073\n",
      "Train Epoch: 005 Batch: 00030/00094 | Loss: 530.2363 | CE: 0.2395 | KD: 1059.9935\n",
      "Train Epoch: 005 Batch: 00031/00094 | Loss: 530.2001 | CE: 0.1996 | KD: 1060.0010\n",
      "Train Epoch: 005 Batch: 00032/00094 | Loss: 530.2672 | CE: 0.2314 | KD: 1060.0715\n",
      "Train Epoch: 005 Batch: 00033/00094 | Loss: 530.2329 | CE: 0.2343 | KD: 1059.9973\n",
      "Train Epoch: 005 Batch: 00034/00094 | Loss: 530.2518 | CE: 0.2300 | KD: 1060.0436\n",
      "Train Epoch: 005 Batch: 00035/00094 | Loss: 530.2576 | CE: 0.2658 | KD: 1059.9838\n",
      "Train Epoch: 005 Batch: 00036/00094 | Loss: 530.1795 | CE: 0.1837 | KD: 1059.9916\n",
      "Train Epoch: 005 Batch: 00037/00094 | Loss: 530.2891 | CE: 0.2510 | KD: 1060.0762\n",
      "Train Epoch: 005 Batch: 00038/00094 | Loss: 530.2548 | CE: 0.2266 | KD: 1060.0564\n",
      "Train Epoch: 005 Batch: 00039/00094 | Loss: 530.2145 | CE: 0.2066 | KD: 1060.0159\n",
      "Train Epoch: 005 Batch: 00040/00094 | Loss: 530.2383 | CE: 0.2082 | KD: 1060.0603\n",
      "Train Epoch: 005 Batch: 00041/00094 | Loss: 530.2797 | CE: 0.2682 | KD: 1060.0231\n",
      "Train Epoch: 005 Batch: 00042/00094 | Loss: 530.1710 | CE: 0.1716 | KD: 1059.9989\n",
      "Train Epoch: 005 Batch: 00043/00094 | Loss: 530.2307 | CE: 0.2307 | KD: 1060.0001\n",
      "Train Epoch: 005 Batch: 00044/00094 | Loss: 530.2090 | CE: 0.2073 | KD: 1060.0035\n",
      "Train Epoch: 005 Batch: 00045/00094 | Loss: 530.2692 | CE: 0.2608 | KD: 1060.0167\n",
      "Train Epoch: 005 Batch: 00046/00094 | Loss: 530.2778 | CE: 0.2578 | KD: 1060.0399\n",
      "Train Epoch: 005 Batch: 00047/00094 | Loss: 530.1610 | CE: 0.1579 | KD: 1060.0063\n",
      "Train Epoch: 005 Batch: 00048/00094 | Loss: 530.2261 | CE: 0.2063 | KD: 1060.0397\n",
      "Train Epoch: 005 Batch: 00049/00094 | Loss: 530.2488 | CE: 0.2410 | KD: 1060.0156\n",
      "Train Epoch: 005 Batch: 00050/00094 | Loss: 530.1786 | CE: 0.1900 | KD: 1059.9773\n",
      "Train Epoch: 005 Batch: 00051/00094 | Loss: 530.2264 | CE: 0.2303 | KD: 1059.9923\n",
      "Train Epoch: 005 Batch: 00052/00094 | Loss: 530.2330 | CE: 0.2061 | KD: 1060.0537\n",
      "Train Epoch: 005 Batch: 00053/00094 | Loss: 530.1644 | CE: 0.1950 | KD: 1059.9388\n",
      "Train Epoch: 005 Batch: 00054/00094 | Loss: 530.2155 | CE: 0.2428 | KD: 1059.9454\n",
      "Train Epoch: 005 Batch: 00055/00094 | Loss: 530.1854 | CE: 0.1688 | KD: 1060.0331\n",
      "Train Epoch: 005 Batch: 00056/00094 | Loss: 530.2179 | CE: 0.2142 | KD: 1060.0073\n",
      "Train Epoch: 005 Batch: 00057/00094 | Loss: 530.1677 | CE: 0.1742 | KD: 1059.9871\n",
      "Train Epoch: 005 Batch: 00058/00094 | Loss: 530.2476 | CE: 0.2531 | KD: 1059.9889\n",
      "Train Epoch: 005 Batch: 00059/00094 | Loss: 530.2156 | CE: 0.2038 | KD: 1060.0238\n",
      "Train Epoch: 005 Batch: 00060/00094 | Loss: 530.1769 | CE: 0.1778 | KD: 1059.9983\n",
      "Train Epoch: 005 Batch: 00061/00094 | Loss: 530.2433 | CE: 0.2268 | KD: 1060.0331\n",
      "Train Epoch: 005 Batch: 00062/00094 | Loss: 530.2574 | CE: 0.2403 | KD: 1060.0342\n",
      "Train Epoch: 005 Batch: 00063/00094 | Loss: 530.1705 | CE: 0.1850 | KD: 1059.9711\n",
      "Train Epoch: 005 Batch: 00064/00094 | Loss: 530.2370 | CE: 0.2234 | KD: 1060.0272\n",
      "Train Epoch: 005 Batch: 00065/00094 | Loss: 530.2458 | CE: 0.2354 | KD: 1060.0208\n",
      "Train Epoch: 005 Batch: 00066/00094 | Loss: 530.2640 | CE: 0.2373 | KD: 1060.0533\n",
      "Train Epoch: 005 Batch: 00067/00094 | Loss: 530.2235 | CE: 0.2050 | KD: 1060.0369\n",
      "Train Epoch: 005 Batch: 00068/00094 | Loss: 530.2419 | CE: 0.2376 | KD: 1060.0087\n",
      "Train Epoch: 005 Batch: 00069/00094 | Loss: 530.2267 | CE: 0.2121 | KD: 1060.0292\n",
      "Train Epoch: 005 Batch: 00070/00094 | Loss: 530.1961 | CE: 0.1958 | KD: 1060.0006\n",
      "Train Epoch: 005 Batch: 00071/00094 | Loss: 530.2717 | CE: 0.2584 | KD: 1060.0265\n",
      "Train Epoch: 005 Batch: 00072/00094 | Loss: 530.2429 | CE: 0.2198 | KD: 1060.0461\n",
      "Train Epoch: 005 Batch: 00073/00094 | Loss: 530.2339 | CE: 0.2147 | KD: 1060.0385\n",
      "Train Epoch: 005 Batch: 00074/00094 | Loss: 530.2216 | CE: 0.2032 | KD: 1060.0369\n",
      "Train Epoch: 005 Batch: 00075/00094 | Loss: 530.2819 | CE: 0.2621 | KD: 1060.0396\n",
      "Train Epoch: 005 Batch: 00076/00094 | Loss: 530.2651 | CE: 0.2493 | KD: 1060.0315\n",
      "Train Epoch: 005 Batch: 00077/00094 | Loss: 530.1717 | CE: 0.1765 | KD: 1059.9904\n",
      "Train Epoch: 005 Batch: 00078/00094 | Loss: 530.2535 | CE: 0.2144 | KD: 1060.0784\n",
      "Train Epoch: 005 Batch: 00079/00094 | Loss: 530.3080 | CE: 0.2852 | KD: 1060.0457\n",
      "Train Epoch: 005 Batch: 00080/00094 | Loss: 530.2492 | CE: 0.2313 | KD: 1060.0359\n",
      "Train Epoch: 005 Batch: 00081/00094 | Loss: 530.1801 | CE: 0.1765 | KD: 1060.0071\n",
      "Train Epoch: 005 Batch: 00082/00094 | Loss: 530.1713 | CE: 0.1680 | KD: 1060.0066\n",
      "Train Epoch: 005 Batch: 00083/00094 | Loss: 530.2039 | CE: 0.2132 | KD: 1059.9813\n",
      "Train Epoch: 005 Batch: 00084/00094 | Loss: 530.1309 | CE: 0.1518 | KD: 1059.9583\n",
      "Train Epoch: 005 Batch: 00085/00094 | Loss: 530.2117 | CE: 0.1829 | KD: 1060.0575\n",
      "Train Epoch: 005 Batch: 00086/00094 | Loss: 530.2280 | CE: 0.2222 | KD: 1060.0116\n",
      "Train Epoch: 005 Batch: 00087/00094 | Loss: 530.1522 | CE: 0.1393 | KD: 1060.0258\n",
      "Train Epoch: 005 Batch: 00088/00094 | Loss: 530.1974 | CE: 0.1854 | KD: 1060.0242\n",
      "Train Epoch: 005 Batch: 00089/00094 | Loss: 530.2316 | CE: 0.2026 | KD: 1060.0579\n",
      "Train Epoch: 005 Batch: 00090/00094 | Loss: 530.2445 | CE: 0.2500 | KD: 1059.9890\n",
      "Train Epoch: 005 Batch: 00091/00094 | Loss: 530.1904 | CE: 0.2082 | KD: 1059.9644\n",
      "Train Epoch: 005 Batch: 00092/00094 | Loss: 530.1771 | CE: 0.1847 | KD: 1059.9847\n",
      "Train Epoch: 005 Batch: 00093/00094 | Loss: 530.2264 | CE: 0.1982 | KD: 1060.0565\n",
      "Train Epoch: 005 Batch: 00094/00094 | Loss: 530.2887 | CE: 0.2704 | KD: 1060.0365\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2043 | acc:95.7500\n",
      "[VAL Acc] Target: 95.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0709 | acc:52.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 52.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5878 | acc:68.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8733 | acc:50.9542\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7433 | acc:58.7774\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5308 | acc:74.3068\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6066 | acc:72.8840\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7041 | acc:61.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.12%\n",
      "[VAL Acc] Avg 66.84%\n",
      "Train Epoch: 006 Batch: 00001/00094 | Loss: 477.1937 | CE: 0.1897 | KD: 1060.0089\n",
      "Train Epoch: 006 Batch: 00002/00094 | Loss: 477.2063 | CE: 0.1980 | KD: 1060.0184\n",
      "Train Epoch: 006 Batch: 00003/00094 | Loss: 477.1993 | CE: 0.1893 | KD: 1060.0222\n",
      "Train Epoch: 006 Batch: 00004/00094 | Loss: 477.1671 | CE: 0.1884 | KD: 1059.9526\n",
      "Train Epoch: 006 Batch: 00005/00094 | Loss: 477.1761 | CE: 0.1848 | KD: 1059.9806\n",
      "Train Epoch: 006 Batch: 00006/00094 | Loss: 477.2242 | CE: 0.2155 | KD: 1060.0194\n",
      "Train Epoch: 006 Batch: 00007/00094 | Loss: 477.2292 | CE: 0.2183 | KD: 1060.0243\n",
      "Train Epoch: 006 Batch: 00008/00094 | Loss: 477.2146 | CE: 0.2292 | KD: 1059.9677\n",
      "Train Epoch: 006 Batch: 00009/00094 | Loss: 477.2555 | CE: 0.2409 | KD: 1060.0326\n",
      "Train Epoch: 006 Batch: 00010/00094 | Loss: 477.2652 | CE: 0.2447 | KD: 1060.0454\n",
      "Train Epoch: 006 Batch: 00011/00094 | Loss: 477.2689 | CE: 0.2499 | KD: 1060.0422\n",
      "Train Epoch: 006 Batch: 00012/00094 | Loss: 477.2294 | CE: 0.2270 | KD: 1060.0054\n",
      "Train Epoch: 006 Batch: 00013/00094 | Loss: 477.2252 | CE: 0.2179 | KD: 1060.0164\n",
      "Train Epoch: 006 Batch: 00014/00094 | Loss: 477.1959 | CE: 0.1763 | KD: 1060.0436\n",
      "Train Epoch: 006 Batch: 00015/00094 | Loss: 477.1736 | CE: 0.1702 | KD: 1060.0076\n",
      "Train Epoch: 006 Batch: 00016/00094 | Loss: 477.1652 | CE: 0.1857 | KD: 1059.9545\n",
      "Train Epoch: 006 Batch: 00017/00094 | Loss: 477.1837 | CE: 0.1900 | KD: 1059.9860\n",
      "Train Epoch: 006 Batch: 00018/00094 | Loss: 477.2425 | CE: 0.2135 | KD: 1060.0643\n",
      "Train Epoch: 006 Batch: 00019/00094 | Loss: 477.1838 | CE: 0.1849 | KD: 1059.9977\n",
      "Train Epoch: 006 Batch: 00020/00094 | Loss: 477.2106 | CE: 0.2195 | KD: 1059.9801\n",
      "Train Epoch: 006 Batch: 00021/00094 | Loss: 477.1821 | CE: 0.1911 | KD: 1059.9801\n",
      "Train Epoch: 006 Batch: 00022/00094 | Loss: 477.2452 | CE: 0.2267 | KD: 1060.0411\n",
      "Train Epoch: 006 Batch: 00023/00094 | Loss: 477.2154 | CE: 0.1955 | KD: 1060.0443\n",
      "Train Epoch: 006 Batch: 00024/00094 | Loss: 477.2502 | CE: 0.2445 | KD: 1060.0126\n",
      "Train Epoch: 006 Batch: 00025/00094 | Loss: 477.1752 | CE: 0.1686 | KD: 1060.0148\n",
      "Train Epoch: 006 Batch: 00026/00094 | Loss: 477.2012 | CE: 0.1923 | KD: 1060.0198\n",
      "Train Epoch: 006 Batch: 00027/00094 | Loss: 477.2565 | CE: 0.2485 | KD: 1060.0178\n",
      "Train Epoch: 006 Batch: 00028/00094 | Loss: 477.2565 | CE: 0.2513 | KD: 1060.0116\n",
      "Train Epoch: 006 Batch: 00029/00094 | Loss: 477.2333 | CE: 0.2154 | KD: 1060.0399\n",
      "Train Epoch: 006 Batch: 00030/00094 | Loss: 477.1933 | CE: 0.2077 | KD: 1059.9680\n",
      "Train Epoch: 006 Batch: 00031/00094 | Loss: 477.2099 | CE: 0.1899 | KD: 1060.0443\n",
      "Train Epoch: 006 Batch: 00032/00094 | Loss: 477.1417 | CE: 0.1522 | KD: 1059.9767\n",
      "Train Epoch: 006 Batch: 00033/00094 | Loss: 477.2699 | CE: 0.2483 | KD: 1060.0480\n",
      "Train Epoch: 006 Batch: 00034/00094 | Loss: 477.2591 | CE: 0.2405 | KD: 1060.0414\n",
      "Train Epoch: 006 Batch: 00035/00094 | Loss: 477.1866 | CE: 0.1843 | KD: 1060.0051\n",
      "Train Epoch: 006 Batch: 00036/00094 | Loss: 477.1407 | CE: 0.1334 | KD: 1060.0162\n",
      "Train Epoch: 006 Batch: 00037/00094 | Loss: 477.1906 | CE: 0.1853 | KD: 1060.0118\n",
      "Train Epoch: 006 Batch: 00038/00094 | Loss: 477.1526 | CE: 0.1585 | KD: 1059.9869\n",
      "Train Epoch: 006 Batch: 00039/00094 | Loss: 477.4037 | CE: 0.3482 | KD: 1060.1233\n",
      "Train Epoch: 006 Batch: 00040/00094 | Loss: 477.2708 | CE: 0.2322 | KD: 1060.0858\n",
      "Train Epoch: 006 Batch: 00041/00094 | Loss: 477.2264 | CE: 0.2317 | KD: 1059.9883\n",
      "Train Epoch: 006 Batch: 00042/00094 | Loss: 477.1616 | CE: 0.1704 | KD: 1059.9805\n",
      "Train Epoch: 006 Batch: 00043/00094 | Loss: 477.2545 | CE: 0.2458 | KD: 1060.0193\n",
      "Train Epoch: 006 Batch: 00044/00094 | Loss: 477.2312 | CE: 0.2299 | KD: 1060.0028\n",
      "Train Epoch: 006 Batch: 00045/00094 | Loss: 477.2474 | CE: 0.2300 | KD: 1060.0387\n",
      "Train Epoch: 006 Batch: 00046/00094 | Loss: 477.2009 | CE: 0.1773 | KD: 1060.0526\n",
      "Train Epoch: 006 Batch: 00047/00094 | Loss: 477.1903 | CE: 0.2103 | KD: 1059.9556\n",
      "Train Epoch: 006 Batch: 00048/00094 | Loss: 477.3139 | CE: 0.2842 | KD: 1060.0659\n",
      "Train Epoch: 006 Batch: 00049/00094 | Loss: 477.2158 | CE: 0.1990 | KD: 1060.0374\n",
      "Train Epoch: 006 Batch: 00050/00094 | Loss: 477.1887 | CE: 0.1896 | KD: 1059.9979\n",
      "Train Epoch: 006 Batch: 00051/00094 | Loss: 477.3220 | CE: 0.2960 | KD: 1060.0579\n",
      "Train Epoch: 006 Batch: 00052/00094 | Loss: 477.2051 | CE: 0.1949 | KD: 1060.0227\n",
      "Train Epoch: 006 Batch: 00053/00094 | Loss: 477.1971 | CE: 0.1943 | KD: 1060.0061\n",
      "Train Epoch: 006 Batch: 00054/00094 | Loss: 477.1979 | CE: 0.1921 | KD: 1060.0129\n",
      "Train Epoch: 006 Batch: 00055/00094 | Loss: 477.1697 | CE: 0.1841 | KD: 1059.9679\n",
      "Train Epoch: 006 Batch: 00056/00094 | Loss: 477.2280 | CE: 0.2095 | KD: 1060.0413\n",
      "Train Epoch: 006 Batch: 00057/00094 | Loss: 477.2037 | CE: 0.1888 | KD: 1060.0330\n",
      "Train Epoch: 006 Batch: 00058/00094 | Loss: 477.2660 | CE: 0.2515 | KD: 1060.0322\n",
      "Train Epoch: 006 Batch: 00059/00094 | Loss: 477.2238 | CE: 0.2309 | KD: 1059.9841\n",
      "Train Epoch: 006 Batch: 00060/00094 | Loss: 477.1349 | CE: 0.1486 | KD: 1059.9696\n",
      "Train Epoch: 006 Batch: 00061/00094 | Loss: 477.1488 | CE: 0.1778 | KD: 1059.9355\n",
      "Train Epoch: 006 Batch: 00062/00094 | Loss: 477.1921 | CE: 0.1944 | KD: 1059.9949\n",
      "Train Epoch: 006 Batch: 00063/00094 | Loss: 477.1990 | CE: 0.2076 | KD: 1059.9810\n",
      "Train Epoch: 006 Batch: 00064/00094 | Loss: 477.2056 | CE: 0.2093 | KD: 1059.9917\n",
      "Train Epoch: 006 Batch: 00065/00094 | Loss: 477.2987 | CE: 0.2875 | KD: 1060.0249\n",
      "Train Epoch: 006 Batch: 00066/00094 | Loss: 477.1999 | CE: 0.1629 | KD: 1060.0822\n",
      "Train Epoch: 006 Batch: 00067/00094 | Loss: 477.2010 | CE: 0.1846 | KD: 1060.0364\n",
      "Train Epoch: 006 Batch: 00068/00094 | Loss: 477.2456 | CE: 0.2209 | KD: 1060.0551\n",
      "Train Epoch: 006 Batch: 00069/00094 | Loss: 477.2088 | CE: 0.1751 | KD: 1060.0751\n",
      "Train Epoch: 006 Batch: 00070/00094 | Loss: 477.3025 | CE: 0.2858 | KD: 1060.0372\n",
      "Train Epoch: 006 Batch: 00071/00094 | Loss: 477.1900 | CE: 0.1861 | KD: 1060.0088\n",
      "Train Epoch: 006 Batch: 00072/00094 | Loss: 477.1412 | CE: 0.1489 | KD: 1059.9828\n",
      "Train Epoch: 006 Batch: 00073/00094 | Loss: 477.2290 | CE: 0.2255 | KD: 1060.0078\n",
      "Train Epoch: 006 Batch: 00074/00094 | Loss: 477.2365 | CE: 0.2361 | KD: 1060.0009\n",
      "Train Epoch: 006 Batch: 00075/00094 | Loss: 477.1913 | CE: 0.1720 | KD: 1060.0430\n",
      "Train Epoch: 006 Batch: 00076/00094 | Loss: 477.2402 | CE: 0.2204 | KD: 1060.0441\n",
      "Train Epoch: 006 Batch: 00077/00094 | Loss: 477.2295 | CE: 0.2105 | KD: 1060.0422\n",
      "Train Epoch: 006 Batch: 00078/00094 | Loss: 477.2143 | CE: 0.2063 | KD: 1060.0177\n",
      "Train Epoch: 006 Batch: 00079/00094 | Loss: 477.1937 | CE: 0.1926 | KD: 1060.0023\n",
      "Train Epoch: 006 Batch: 00080/00094 | Loss: 477.1854 | CE: 0.1717 | KD: 1060.0304\n",
      "Train Epoch: 006 Batch: 00081/00094 | Loss: 477.3034 | CE: 0.2840 | KD: 1060.0432\n",
      "Train Epoch: 006 Batch: 00082/00094 | Loss: 477.1972 | CE: 0.1983 | KD: 1059.9976\n",
      "Train Epoch: 006 Batch: 00083/00094 | Loss: 477.1573 | CE: 0.1701 | KD: 1059.9716\n",
      "Train Epoch: 006 Batch: 00084/00094 | Loss: 477.1619 | CE: 0.1550 | KD: 1060.0153\n",
      "Train Epoch: 006 Batch: 00085/00094 | Loss: 477.2450 | CE: 0.2305 | KD: 1060.0322\n",
      "Train Epoch: 006 Batch: 00086/00094 | Loss: 477.1638 | CE: 0.1691 | KD: 1059.9883\n",
      "Train Epoch: 006 Batch: 00087/00094 | Loss: 477.2778 | CE: 0.2576 | KD: 1060.0449\n",
      "Train Epoch: 006 Batch: 00088/00094 | Loss: 477.1863 | CE: 0.1770 | KD: 1060.0206\n",
      "Train Epoch: 006 Batch: 00089/00094 | Loss: 477.2071 | CE: 0.1895 | KD: 1060.0392\n",
      "Train Epoch: 006 Batch: 00090/00094 | Loss: 477.2123 | CE: 0.1990 | KD: 1060.0297\n",
      "Train Epoch: 006 Batch: 00091/00094 | Loss: 477.2565 | CE: 0.2100 | KD: 1060.1031\n",
      "Train Epoch: 006 Batch: 00092/00094 | Loss: 477.1668 | CE: 0.1604 | KD: 1060.0144\n",
      "Train Epoch: 006 Batch: 00093/00094 | Loss: 477.1746 | CE: 0.1613 | KD: 1060.0297\n",
      "Train Epoch: 006 Batch: 00094/00094 | Loss: 477.2231 | CE: 0.2001 | KD: 1060.0511\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1919 | acc:96.2500\n",
      "[VAL Acc] Target: 96.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1376 | acc:51.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6582 | acc:65.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 65.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8511 | acc:52.0992\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 52.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7980 | acc:57.6019\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5011 | acc:75.0462\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6641 | acc:68.8480\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7210 | acc:59.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.19%\n",
      "[VAL Acc] Avg 65.83%\n",
      "Train Epoch: 007 Batch: 00001/00094 | Loss: 477.2784 | CE: 0.2503 | KD: 1060.0623\n",
      "Train Epoch: 007 Batch: 00002/00094 | Loss: 477.2267 | CE: 0.2260 | KD: 1060.0017\n",
      "Train Epoch: 007 Batch: 00003/00094 | Loss: 477.1955 | CE: 0.1913 | KD: 1060.0093\n",
      "Train Epoch: 007 Batch: 00004/00094 | Loss: 477.1853 | CE: 0.1781 | KD: 1060.0160\n",
      "Train Epoch: 007 Batch: 00005/00094 | Loss: 477.1696 | CE: 0.1697 | KD: 1059.9998\n",
      "Train Epoch: 007 Batch: 00006/00094 | Loss: 477.1947 | CE: 0.1602 | KD: 1060.0765\n",
      "Train Epoch: 007 Batch: 00007/00094 | Loss: 477.2927 | CE: 0.2713 | KD: 1060.0475\n",
      "Train Epoch: 007 Batch: 00008/00094 | Loss: 477.2327 | CE: 0.2148 | KD: 1060.0398\n",
      "Train Epoch: 007 Batch: 00009/00094 | Loss: 477.1577 | CE: 0.1591 | KD: 1059.9968\n",
      "Train Epoch: 007 Batch: 00010/00094 | Loss: 477.2029 | CE: 0.1859 | KD: 1060.0380\n",
      "Train Epoch: 007 Batch: 00011/00094 | Loss: 477.2177 | CE: 0.2067 | KD: 1060.0243\n",
      "Train Epoch: 007 Batch: 00012/00094 | Loss: 477.1972 | CE: 0.1641 | KD: 1060.0737\n",
      "Train Epoch: 007 Batch: 00013/00094 | Loss: 477.3378 | CE: 0.3084 | KD: 1060.0653\n",
      "Train Epoch: 007 Batch: 00014/00094 | Loss: 477.1884 | CE: 0.1762 | KD: 1060.0271\n",
      "Train Epoch: 007 Batch: 00015/00094 | Loss: 477.3016 | CE: 0.2690 | KD: 1060.0725\n",
      "Train Epoch: 007 Batch: 00016/00094 | Loss: 477.2389 | CE: 0.2195 | KD: 1060.0432\n",
      "Train Epoch: 007 Batch: 00017/00094 | Loss: 477.1897 | CE: 0.2006 | KD: 1059.9758\n",
      "Train Epoch: 007 Batch: 00018/00094 | Loss: 477.2382 | CE: 0.2134 | KD: 1060.0551\n",
      "Train Epoch: 007 Batch: 00019/00094 | Loss: 477.1902 | CE: 0.1879 | KD: 1060.0051\n",
      "Train Epoch: 007 Batch: 00020/00094 | Loss: 477.1975 | CE: 0.1923 | KD: 1060.0115\n",
      "Train Epoch: 007 Batch: 00021/00094 | Loss: 477.1973 | CE: 0.1749 | KD: 1060.0497\n",
      "Train Epoch: 007 Batch: 00022/00094 | Loss: 477.2035 | CE: 0.1759 | KD: 1060.0613\n",
      "Train Epoch: 007 Batch: 00023/00094 | Loss: 477.1819 | CE: 0.1735 | KD: 1060.0187\n",
      "Train Epoch: 007 Batch: 00024/00094 | Loss: 477.2001 | CE: 0.1827 | KD: 1060.0388\n",
      "Train Epoch: 007 Batch: 00025/00094 | Loss: 477.1698 | CE: 0.1902 | KD: 1059.9548\n",
      "Train Epoch: 007 Batch: 00026/00094 | Loss: 477.1724 | CE: 0.1751 | KD: 1059.9940\n",
      "Train Epoch: 007 Batch: 00027/00094 | Loss: 477.2476 | CE: 0.2195 | KD: 1060.0625\n",
      "Train Epoch: 007 Batch: 00028/00094 | Loss: 477.1973 | CE: 0.1803 | KD: 1060.0378\n",
      "Train Epoch: 007 Batch: 00029/00094 | Loss: 477.2656 | CE: 0.2553 | KD: 1060.0229\n",
      "Train Epoch: 007 Batch: 00030/00094 | Loss: 477.1850 | CE: 0.1982 | KD: 1059.9708\n",
      "Train Epoch: 007 Batch: 00031/00094 | Loss: 477.2051 | CE: 0.1946 | KD: 1060.0234\n",
      "Train Epoch: 007 Batch: 00032/00094 | Loss: 477.1696 | CE: 0.1715 | KD: 1059.9958\n",
      "Train Epoch: 007 Batch: 00033/00094 | Loss: 477.1672 | CE: 0.1684 | KD: 1059.9973\n",
      "Train Epoch: 007 Batch: 00034/00094 | Loss: 477.1866 | CE: 0.1876 | KD: 1059.9978\n",
      "Train Epoch: 007 Batch: 00035/00094 | Loss: 477.2397 | CE: 0.2275 | KD: 1060.0271\n",
      "Train Epoch: 007 Batch: 00036/00094 | Loss: 477.1867 | CE: 0.1952 | KD: 1059.9812\n",
      "Train Epoch: 007 Batch: 00037/00094 | Loss: 477.1809 | CE: 0.1784 | KD: 1060.0055\n",
      "Train Epoch: 007 Batch: 00038/00094 | Loss: 477.1684 | CE: 0.1843 | KD: 1059.9646\n",
      "Train Epoch: 007 Batch: 00039/00094 | Loss: 477.2437 | CE: 0.2360 | KD: 1060.0173\n",
      "Train Epoch: 007 Batch: 00040/00094 | Loss: 477.2397 | CE: 0.2249 | KD: 1060.0327\n",
      "Train Epoch: 007 Batch: 00041/00094 | Loss: 477.1974 | CE: 0.1940 | KD: 1060.0077\n",
      "Train Epoch: 007 Batch: 00042/00094 | Loss: 477.2010 | CE: 0.1883 | KD: 1060.0284\n",
      "Train Epoch: 007 Batch: 00043/00094 | Loss: 477.2056 | CE: 0.1883 | KD: 1060.0383\n",
      "Train Epoch: 007 Batch: 00044/00094 | Loss: 477.1934 | CE: 0.1851 | KD: 1060.0186\n",
      "Train Epoch: 007 Batch: 00045/00094 | Loss: 477.2036 | CE: 0.1930 | KD: 1060.0237\n",
      "Train Epoch: 007 Batch: 00046/00094 | Loss: 477.1807 | CE: 0.1751 | KD: 1060.0126\n",
      "Train Epoch: 007 Batch: 00047/00094 | Loss: 477.2711 | CE: 0.2562 | KD: 1060.0331\n",
      "Train Epoch: 007 Batch: 00048/00094 | Loss: 477.2172 | CE: 0.2148 | KD: 1060.0054\n",
      "Train Epoch: 007 Batch: 00049/00094 | Loss: 477.1753 | CE: 0.1812 | KD: 1059.9869\n",
      "Train Epoch: 007 Batch: 00050/00094 | Loss: 477.1360 | CE: 0.1556 | KD: 1059.9565\n",
      "Train Epoch: 007 Batch: 00051/00094 | Loss: 477.2247 | CE: 0.2008 | KD: 1060.0531\n",
      "Train Epoch: 007 Batch: 00052/00094 | Loss: 477.1835 | CE: 0.1908 | KD: 1059.9839\n",
      "Train Epoch: 007 Batch: 00053/00094 | Loss: 477.2160 | CE: 0.2042 | KD: 1060.0264\n",
      "Train Epoch: 007 Batch: 00054/00094 | Loss: 477.1969 | CE: 0.1873 | KD: 1060.0215\n",
      "Train Epoch: 007 Batch: 00055/00094 | Loss: 477.1813 | CE: 0.1796 | KD: 1060.0039\n",
      "Train Epoch: 007 Batch: 00056/00094 | Loss: 477.2216 | CE: 0.2135 | KD: 1060.0181\n",
      "Train Epoch: 007 Batch: 00057/00094 | Loss: 477.2527 | CE: 0.2268 | KD: 1060.0576\n",
      "Train Epoch: 007 Batch: 00058/00094 | Loss: 477.2404 | CE: 0.2248 | KD: 1060.0345\n",
      "Train Epoch: 007 Batch: 00059/00094 | Loss: 477.2559 | CE: 0.2157 | KD: 1060.0894\n",
      "Train Epoch: 007 Batch: 00060/00094 | Loss: 477.2380 | CE: 0.2272 | KD: 1060.0240\n",
      "Train Epoch: 007 Batch: 00061/00094 | Loss: 477.2044 | CE: 0.1897 | KD: 1060.0328\n",
      "Train Epoch: 007 Batch: 00062/00094 | Loss: 477.2851 | CE: 0.2453 | KD: 1060.0885\n",
      "Train Epoch: 007 Batch: 00063/00094 | Loss: 477.1641 | CE: 0.1825 | KD: 1059.9590\n",
      "Train Epoch: 007 Batch: 00064/00094 | Loss: 477.1913 | CE: 0.1817 | KD: 1060.0214\n",
      "Train Epoch: 007 Batch: 00065/00094 | Loss: 477.2657 | CE: 0.2477 | KD: 1060.0402\n",
      "Train Epoch: 007 Batch: 00066/00094 | Loss: 477.1842 | CE: 0.1694 | KD: 1060.0330\n",
      "Train Epoch: 007 Batch: 00067/00094 | Loss: 477.1615 | CE: 0.1793 | KD: 1059.9603\n",
      "Train Epoch: 007 Batch: 00068/00094 | Loss: 477.1839 | CE: 0.1597 | KD: 1060.0538\n",
      "Train Epoch: 007 Batch: 00069/00094 | Loss: 477.1902 | CE: 0.1888 | KD: 1060.0031\n",
      "Train Epoch: 007 Batch: 00070/00094 | Loss: 477.1640 | CE: 0.1585 | KD: 1060.0123\n",
      "Train Epoch: 007 Batch: 00071/00094 | Loss: 477.2679 | CE: 0.2472 | KD: 1060.0461\n",
      "Train Epoch: 007 Batch: 00072/00094 | Loss: 477.2501 | CE: 0.2038 | KD: 1060.1029\n",
      "Train Epoch: 007 Batch: 00073/00094 | Loss: 477.1930 | CE: 0.1836 | KD: 1060.0209\n",
      "Train Epoch: 007 Batch: 00074/00094 | Loss: 477.1935 | CE: 0.2002 | KD: 1059.9851\n",
      "Train Epoch: 007 Batch: 00075/00094 | Loss: 477.1837 | CE: 0.1705 | KD: 1060.0293\n",
      "Train Epoch: 007 Batch: 00076/00094 | Loss: 477.2341 | CE: 0.2155 | KD: 1060.0414\n",
      "Train Epoch: 007 Batch: 00077/00094 | Loss: 477.1917 | CE: 0.2010 | KD: 1059.9794\n",
      "Train Epoch: 007 Batch: 00078/00094 | Loss: 477.1972 | CE: 0.1759 | KD: 1060.0472\n",
      "Train Epoch: 007 Batch: 00079/00094 | Loss: 477.3042 | CE: 0.2728 | KD: 1060.0699\n",
      "Train Epoch: 007 Batch: 00080/00094 | Loss: 477.2367 | CE: 0.2424 | KD: 1059.9873\n",
      "Train Epoch: 007 Batch: 00081/00094 | Loss: 477.1806 | CE: 0.1724 | KD: 1060.0183\n",
      "Train Epoch: 007 Batch: 00082/00094 | Loss: 477.1593 | CE: 0.1387 | KD: 1060.0457\n",
      "Train Epoch: 007 Batch: 00083/00094 | Loss: 477.1870 | CE: 0.1885 | KD: 1059.9966\n",
      "Train Epoch: 007 Batch: 00084/00094 | Loss: 477.2166 | CE: 0.1824 | KD: 1060.0759\n",
      "Train Epoch: 007 Batch: 00085/00094 | Loss: 477.1758 | CE: 0.1746 | KD: 1060.0026\n",
      "Train Epoch: 007 Batch: 00086/00094 | Loss: 477.2654 | CE: 0.2361 | KD: 1060.0652\n",
      "Train Epoch: 007 Batch: 00087/00094 | Loss: 477.2283 | CE: 0.1989 | KD: 1060.0653\n",
      "Train Epoch: 007 Batch: 00088/00094 | Loss: 477.1482 | CE: 0.1527 | KD: 1059.9901\n",
      "Train Epoch: 007 Batch: 00089/00094 | Loss: 477.2064 | CE: 0.1885 | KD: 1060.0399\n",
      "Train Epoch: 007 Batch: 00090/00094 | Loss: 477.1752 | CE: 0.1901 | KD: 1059.9669\n",
      "Train Epoch: 007 Batch: 00091/00094 | Loss: 477.1781 | CE: 0.1924 | KD: 1059.9681\n",
      "Train Epoch: 007 Batch: 00092/00094 | Loss: 477.1415 | CE: 0.1566 | KD: 1059.9663\n",
      "Train Epoch: 007 Batch: 00093/00094 | Loss: 477.2290 | CE: 0.2119 | KD: 1060.0381\n",
      "Train Epoch: 007 Batch: 00094/00094 | Loss: 477.1624 | CE: 0.1690 | KD: 1059.9854\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1825 | acc:96.8000\n",
      "[VAL Acc] Target: 96.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1086 | acc:51.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6559 | acc:65.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 65.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8498 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7805 | acc:57.4843\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.48%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5597 | acc:71.9039\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6429 | acc:70.1019\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.6863 | acc:62.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.62%\n",
      "[VAL Acc] Avg 65.69%\n",
      "Train Epoch: 008 Batch: 00001/00094 | Loss: 477.2286 | CE: 0.1986 | KD: 1060.0667\n",
      "Train Epoch: 008 Batch: 00002/00094 | Loss: 477.2683 | CE: 0.2402 | KD: 1060.0624\n",
      "Train Epoch: 008 Batch: 00003/00094 | Loss: 477.1772 | CE: 0.1587 | KD: 1060.0413\n",
      "Train Epoch: 008 Batch: 00004/00094 | Loss: 477.1914 | CE: 0.2050 | KD: 1059.9698\n",
      "Train Epoch: 008 Batch: 00005/00094 | Loss: 477.1557 | CE: 0.1598 | KD: 1059.9908\n",
      "Train Epoch: 008 Batch: 00006/00094 | Loss: 477.1884 | CE: 0.1646 | KD: 1060.0527\n",
      "Train Epoch: 008 Batch: 00007/00094 | Loss: 477.2474 | CE: 0.2485 | KD: 1059.9976\n",
      "Train Epoch: 008 Batch: 00008/00094 | Loss: 477.2036 | CE: 0.1906 | KD: 1060.0289\n",
      "Train Epoch: 008 Batch: 00009/00094 | Loss: 477.2020 | CE: 0.1654 | KD: 1060.0813\n",
      "Train Epoch: 008 Batch: 00010/00094 | Loss: 477.2370 | CE: 0.2218 | KD: 1060.0338\n",
      "Train Epoch: 008 Batch: 00011/00094 | Loss: 477.1540 | CE: 0.1534 | KD: 1060.0015\n",
      "Train Epoch: 008 Batch: 00012/00094 | Loss: 477.2263 | CE: 0.2235 | KD: 1060.0062\n",
      "Train Epoch: 008 Batch: 00013/00094 | Loss: 477.1751 | CE: 0.1726 | KD: 1060.0055\n",
      "Train Epoch: 008 Batch: 00014/00094 | Loss: 477.1779 | CE: 0.1928 | KD: 1059.9668\n",
      "Train Epoch: 008 Batch: 00015/00094 | Loss: 477.2136 | CE: 0.2095 | KD: 1060.0092\n",
      "Train Epoch: 008 Batch: 00016/00094 | Loss: 477.2181 | CE: 0.2001 | KD: 1060.0400\n",
      "Train Epoch: 008 Batch: 00017/00094 | Loss: 477.2024 | CE: 0.2034 | KD: 1059.9979\n",
      "Train Epoch: 008 Batch: 00018/00094 | Loss: 477.1786 | CE: 0.1431 | KD: 1060.0790\n",
      "Train Epoch: 008 Batch: 00019/00094 | Loss: 477.2365 | CE: 0.2100 | KD: 1060.0588\n",
      "Train Epoch: 008 Batch: 00020/00094 | Loss: 477.2034 | CE: 0.1906 | KD: 1060.0284\n",
      "Train Epoch: 008 Batch: 00021/00094 | Loss: 477.1538 | CE: 0.1582 | KD: 1059.9902\n",
      "Train Epoch: 008 Batch: 00022/00094 | Loss: 477.1729 | CE: 0.1696 | KD: 1060.0072\n",
      "Train Epoch: 008 Batch: 00023/00094 | Loss: 477.1932 | CE: 0.1735 | KD: 1060.0439\n",
      "Train Epoch: 008 Batch: 00024/00094 | Loss: 477.1812 | CE: 0.1817 | KD: 1059.9988\n",
      "Train Epoch: 008 Batch: 00025/00094 | Loss: 477.1495 | CE: 0.1667 | KD: 1059.9618\n",
      "Train Epoch: 008 Batch: 00026/00094 | Loss: 477.2311 | CE: 0.2293 | KD: 1060.0040\n",
      "Train Epoch: 008 Batch: 00027/00094 | Loss: 477.1805 | CE: 0.1651 | KD: 1060.0344\n",
      "Train Epoch: 008 Batch: 00028/00094 | Loss: 477.1593 | CE: 0.1665 | KD: 1059.9840\n",
      "Train Epoch: 008 Batch: 00029/00094 | Loss: 477.2091 | CE: 0.1934 | KD: 1060.0349\n",
      "Train Epoch: 008 Batch: 00030/00094 | Loss: 477.2276 | CE: 0.2140 | KD: 1060.0303\n",
      "Train Epoch: 008 Batch: 00031/00094 | Loss: 477.2105 | CE: 0.1998 | KD: 1060.0239\n",
      "Train Epoch: 008 Batch: 00032/00094 | Loss: 477.2283 | CE: 0.2140 | KD: 1060.0317\n",
      "Train Epoch: 008 Batch: 00033/00094 | Loss: 477.2367 | CE: 0.2035 | KD: 1060.0737\n",
      "Train Epoch: 008 Batch: 00034/00094 | Loss: 477.1526 | CE: 0.1698 | KD: 1059.9619\n",
      "Train Epoch: 008 Batch: 00035/00094 | Loss: 477.2439 | CE: 0.2178 | KD: 1060.0580\n",
      "Train Epoch: 008 Batch: 00036/00094 | Loss: 477.2043 | CE: 0.1816 | KD: 1060.0504\n",
      "Train Epoch: 008 Batch: 00037/00094 | Loss: 477.2466 | CE: 0.2256 | KD: 1060.0466\n",
      "Train Epoch: 008 Batch: 00038/00094 | Loss: 477.2024 | CE: 0.1990 | KD: 1060.0076\n",
      "Train Epoch: 008 Batch: 00039/00094 | Loss: 477.1969 | CE: 0.1691 | KD: 1060.0619\n",
      "Train Epoch: 008 Batch: 00040/00094 | Loss: 477.1955 | CE: 0.1980 | KD: 1059.9945\n",
      "Train Epoch: 008 Batch: 00041/00094 | Loss: 477.1796 | CE: 0.1759 | KD: 1060.0082\n",
      "Train Epoch: 008 Batch: 00042/00094 | Loss: 477.3031 | CE: 0.2843 | KD: 1060.0419\n",
      "Train Epoch: 008 Batch: 00043/00094 | Loss: 477.2059 | CE: 0.1930 | KD: 1060.0287\n",
      "Train Epoch: 008 Batch: 00044/00094 | Loss: 477.1914 | CE: 0.1937 | KD: 1059.9949\n",
      "Train Epoch: 008 Batch: 00045/00094 | Loss: 477.2250 | CE: 0.2210 | KD: 1060.0089\n",
      "Train Epoch: 008 Batch: 00046/00094 | Loss: 477.1769 | CE: 0.1628 | KD: 1060.0314\n",
      "Train Epoch: 008 Batch: 00047/00094 | Loss: 477.2143 | CE: 0.2142 | KD: 1060.0002\n",
      "Train Epoch: 008 Batch: 00048/00094 | Loss: 477.1709 | CE: 0.1804 | KD: 1059.9789\n",
      "Train Epoch: 008 Batch: 00049/00094 | Loss: 477.1677 | CE: 0.1732 | KD: 1059.9879\n",
      "Train Epoch: 008 Batch: 00050/00094 | Loss: 477.2370 | CE: 0.2190 | KD: 1060.0400\n",
      "Train Epoch: 008 Batch: 00051/00094 | Loss: 477.1638 | CE: 0.1473 | KD: 1060.0365\n",
      "Train Epoch: 008 Batch: 00052/00094 | Loss: 477.2036 | CE: 0.1852 | KD: 1060.0409\n",
      "Train Epoch: 008 Batch: 00053/00094 | Loss: 477.2050 | CE: 0.1869 | KD: 1060.0403\n",
      "Train Epoch: 008 Batch: 00054/00094 | Loss: 477.2042 | CE: 0.2130 | KD: 1059.9803\n",
      "Train Epoch: 008 Batch: 00055/00094 | Loss: 477.2227 | CE: 0.1920 | KD: 1060.0684\n",
      "Train Epoch: 008 Batch: 00056/00094 | Loss: 477.1721 | CE: 0.1682 | KD: 1060.0087\n",
      "Train Epoch: 008 Batch: 00057/00094 | Loss: 477.2065 | CE: 0.1757 | KD: 1060.0684\n",
      "Train Epoch: 008 Batch: 00058/00094 | Loss: 477.1761 | CE: 0.1846 | KD: 1059.9812\n",
      "Train Epoch: 008 Batch: 00059/00094 | Loss: 477.2499 | CE: 0.2377 | KD: 1060.0271\n",
      "Train Epoch: 008 Batch: 00060/00094 | Loss: 477.2657 | CE: 0.2359 | KD: 1060.0662\n",
      "Train Epoch: 008 Batch: 00061/00094 | Loss: 477.2322 | CE: 0.2092 | KD: 1060.0511\n",
      "Train Epoch: 008 Batch: 00062/00094 | Loss: 477.2019 | CE: 0.2002 | KD: 1060.0037\n",
      "Train Epoch: 008 Batch: 00063/00094 | Loss: 477.2035 | CE: 0.1920 | KD: 1060.0256\n",
      "Train Epoch: 008 Batch: 00064/00094 | Loss: 477.2415 | CE: 0.2424 | KD: 1059.9980\n",
      "Train Epoch: 008 Batch: 00065/00094 | Loss: 477.2130 | CE: 0.1978 | KD: 1060.0336\n",
      "Train Epoch: 008 Batch: 00066/00094 | Loss: 477.2313 | CE: 0.2081 | KD: 1060.0516\n",
      "Train Epoch: 008 Batch: 00067/00094 | Loss: 477.1806 | CE: 0.1711 | KD: 1060.0211\n",
      "Train Epoch: 008 Batch: 00068/00094 | Loss: 477.1737 | CE: 0.1575 | KD: 1060.0360\n",
      "Train Epoch: 008 Batch: 00069/00094 | Loss: 477.2246 | CE: 0.1956 | KD: 1060.0646\n",
      "Train Epoch: 008 Batch: 00070/00094 | Loss: 477.1537 | CE: 0.1417 | KD: 1060.0267\n",
      "Train Epoch: 008 Batch: 00071/00094 | Loss: 477.1506 | CE: 0.1492 | KD: 1060.0031\n",
      "Train Epoch: 008 Batch: 00072/00094 | Loss: 477.1647 | CE: 0.1546 | KD: 1060.0225\n",
      "Train Epoch: 008 Batch: 00073/00094 | Loss: 477.1978 | CE: 0.1875 | KD: 1060.0231\n",
      "Train Epoch: 008 Batch: 00074/00094 | Loss: 477.1977 | CE: 0.1855 | KD: 1060.0271\n",
      "Train Epoch: 008 Batch: 00075/00094 | Loss: 477.2076 | CE: 0.1866 | KD: 1060.0466\n",
      "Train Epoch: 008 Batch: 00076/00094 | Loss: 477.1564 | CE: 0.1667 | KD: 1059.9772\n",
      "Train Epoch: 008 Batch: 00077/00094 | Loss: 477.1933 | CE: 0.1882 | KD: 1060.0114\n",
      "Train Epoch: 008 Batch: 00078/00094 | Loss: 477.2444 | CE: 0.2105 | KD: 1060.0753\n",
      "Train Epoch: 008 Batch: 00079/00094 | Loss: 477.2090 | CE: 0.1852 | KD: 1060.0529\n",
      "Train Epoch: 008 Batch: 00080/00094 | Loss: 477.2014 | CE: 0.1767 | KD: 1060.0549\n",
      "Train Epoch: 008 Batch: 00081/00094 | Loss: 477.2833 | CE: 0.2547 | KD: 1060.0636\n",
      "Train Epoch: 008 Batch: 00082/00094 | Loss: 477.2300 | CE: 0.2205 | KD: 1060.0211\n",
      "Train Epoch: 008 Batch: 00083/00094 | Loss: 477.1919 | CE: 0.1768 | KD: 1060.0337\n",
      "Train Epoch: 008 Batch: 00084/00094 | Loss: 477.1876 | CE: 0.1737 | KD: 1060.0308\n",
      "Train Epoch: 008 Batch: 00085/00094 | Loss: 477.2589 | CE: 0.2314 | KD: 1060.0610\n",
      "Train Epoch: 008 Batch: 00086/00094 | Loss: 477.1757 | CE: 0.1810 | KD: 1059.9882\n",
      "Train Epoch: 008 Batch: 00087/00094 | Loss: 477.2565 | CE: 0.2298 | KD: 1060.0592\n",
      "Train Epoch: 008 Batch: 00088/00094 | Loss: 477.2437 | CE: 0.2545 | KD: 1059.9760\n",
      "Train Epoch: 008 Batch: 00089/00094 | Loss: 477.2202 | CE: 0.2035 | KD: 1060.0371\n",
      "Train Epoch: 008 Batch: 00090/00094 | Loss: 477.1757 | CE: 0.1774 | KD: 1059.9963\n",
      "Train Epoch: 008 Batch: 00091/00094 | Loss: 477.2019 | CE: 0.2008 | KD: 1060.0024\n",
      "Train Epoch: 008 Batch: 00092/00094 | Loss: 477.1821 | CE: 0.1734 | KD: 1060.0193\n",
      "Train Epoch: 008 Batch: 00093/00094 | Loss: 477.1946 | CE: 0.1757 | KD: 1060.0420\n",
      "Train Epoch: 008 Batch: 00094/00094 | Loss: 477.1910 | CE: 0.1621 | KD: 1060.0641\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1864 | acc:95.9500\n",
      "[VAL Acc] Target: 95.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1150 | acc:51.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6590 | acc:66.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 66.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8782 | acc:50.1908\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7761 | acc:57.8762\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5217 | acc:74.3068\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6260 | acc:71.1599\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7028 | acc:61.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.69%\n",
      "[VAL Acc] Avg 66.08%\n",
      "Train Epoch: 009 Batch: 00001/00094 | Loss: 477.2107 | CE: 0.2007 | KD: 1060.0222\n",
      "Train Epoch: 009 Batch: 00002/00094 | Loss: 477.2107 | CE: 0.1763 | KD: 1060.0764\n",
      "Train Epoch: 009 Batch: 00003/00094 | Loss: 477.2567 | CE: 0.2176 | KD: 1060.0870\n",
      "Train Epoch: 009 Batch: 00004/00094 | Loss: 477.2058 | CE: 0.1838 | KD: 1060.0490\n",
      "Train Epoch: 009 Batch: 00005/00094 | Loss: 477.1870 | CE: 0.1643 | KD: 1060.0505\n",
      "Train Epoch: 009 Batch: 00006/00094 | Loss: 477.1711 | CE: 0.1755 | KD: 1059.9904\n",
      "Train Epoch: 009 Batch: 00007/00094 | Loss: 477.1744 | CE: 0.1716 | KD: 1060.0063\n",
      "Train Epoch: 009 Batch: 00008/00094 | Loss: 477.1900 | CE: 0.1740 | KD: 1060.0355\n",
      "Train Epoch: 009 Batch: 00009/00094 | Loss: 477.1677 | CE: 0.1601 | KD: 1060.0168\n",
      "Train Epoch: 009 Batch: 00010/00094 | Loss: 477.2093 | CE: 0.1847 | KD: 1060.0547\n",
      "Train Epoch: 009 Batch: 00011/00094 | Loss: 477.1747 | CE: 0.1680 | KD: 1060.0149\n",
      "Train Epoch: 009 Batch: 00012/00094 | Loss: 477.1909 | CE: 0.1892 | KD: 1060.0039\n",
      "Train Epoch: 009 Batch: 00013/00094 | Loss: 477.1459 | CE: 0.1603 | KD: 1059.9681\n",
      "Train Epoch: 009 Batch: 00014/00094 | Loss: 477.1978 | CE: 0.1818 | KD: 1060.0355\n",
      "Train Epoch: 009 Batch: 00015/00094 | Loss: 477.1798 | CE: 0.1762 | KD: 1060.0082\n",
      "Train Epoch: 009 Batch: 00016/00094 | Loss: 477.2234 | CE: 0.2036 | KD: 1060.0439\n",
      "Train Epoch: 009 Batch: 00017/00094 | Loss: 477.2785 | CE: 0.2355 | KD: 1060.0957\n",
      "Train Epoch: 009 Batch: 00018/00094 | Loss: 477.2068 | CE: 0.1790 | KD: 1060.0619\n",
      "Train Epoch: 009 Batch: 00019/00094 | Loss: 477.2116 | CE: 0.2032 | KD: 1060.0187\n",
      "Train Epoch: 009 Batch: 00020/00094 | Loss: 477.2013 | CE: 0.1816 | KD: 1060.0437\n",
      "Train Epoch: 009 Batch: 00021/00094 | Loss: 477.1737 | CE: 0.1781 | KD: 1059.9904\n",
      "Train Epoch: 009 Batch: 00022/00094 | Loss: 477.2367 | CE: 0.2058 | KD: 1060.0687\n",
      "Train Epoch: 009 Batch: 00023/00094 | Loss: 477.2229 | CE: 0.1952 | KD: 1060.0615\n",
      "Train Epoch: 009 Batch: 00024/00094 | Loss: 477.2108 | CE: 0.1874 | KD: 1060.0521\n",
      "Train Epoch: 009 Batch: 00025/00094 | Loss: 477.1705 | CE: 0.1706 | KD: 1059.9999\n",
      "Train Epoch: 009 Batch: 00026/00094 | Loss: 477.1913 | CE: 0.2002 | KD: 1059.9803\n",
      "Train Epoch: 009 Batch: 00027/00094 | Loss: 477.1832 | CE: 0.1933 | KD: 1059.9774\n",
      "Train Epoch: 009 Batch: 00028/00094 | Loss: 477.1622 | CE: 0.1508 | KD: 1060.0253\n",
      "Train Epoch: 009 Batch: 00029/00094 | Loss: 477.1483 | CE: 0.1409 | KD: 1060.0164\n",
      "Train Epoch: 009 Batch: 00030/00094 | Loss: 477.2119 | CE: 0.1930 | KD: 1060.0419\n",
      "Train Epoch: 009 Batch: 00031/00094 | Loss: 477.2468 | CE: 0.2291 | KD: 1060.0393\n",
      "Train Epoch: 009 Batch: 00032/00094 | Loss: 477.1978 | CE: 0.1882 | KD: 1060.0214\n",
      "Train Epoch: 009 Batch: 00033/00094 | Loss: 477.1852 | CE: 0.1785 | KD: 1060.0149\n",
      "Train Epoch: 009 Batch: 00034/00094 | Loss: 477.1823 | CE: 0.1794 | KD: 1060.0065\n",
      "Train Epoch: 009 Batch: 00035/00094 | Loss: 477.2218 | CE: 0.2179 | KD: 1060.0087\n",
      "Train Epoch: 009 Batch: 00036/00094 | Loss: 477.1711 | CE: 0.1456 | KD: 1060.0566\n",
      "Train Epoch: 009 Batch: 00037/00094 | Loss: 477.1951 | CE: 0.1967 | KD: 1059.9963\n",
      "Train Epoch: 009 Batch: 00038/00094 | Loss: 477.1758 | CE: 0.1659 | KD: 1060.0221\n",
      "Train Epoch: 009 Batch: 00039/00094 | Loss: 477.2101 | CE: 0.2049 | KD: 1060.0116\n",
      "Train Epoch: 009 Batch: 00040/00094 | Loss: 477.2009 | CE: 0.1894 | KD: 1060.0255\n",
      "Train Epoch: 009 Batch: 00041/00094 | Loss: 477.1647 | CE: 0.1609 | KD: 1060.0084\n",
      "Train Epoch: 009 Batch: 00042/00094 | Loss: 477.1951 | CE: 0.2041 | KD: 1059.9800\n",
      "Train Epoch: 009 Batch: 00043/00094 | Loss: 477.2638 | CE: 0.2252 | KD: 1060.0858\n",
      "Train Epoch: 009 Batch: 00044/00094 | Loss: 477.1779 | CE: 0.1678 | KD: 1060.0226\n",
      "Train Epoch: 009 Batch: 00045/00094 | Loss: 477.1653 | CE: 0.1747 | KD: 1059.9790\n",
      "Train Epoch: 009 Batch: 00046/00094 | Loss: 477.1658 | CE: 0.1516 | KD: 1060.0316\n",
      "Train Epoch: 009 Batch: 00047/00094 | Loss: 477.1856 | CE: 0.1851 | KD: 1060.0012\n",
      "Train Epoch: 009 Batch: 00048/00094 | Loss: 477.1805 | CE: 0.1718 | KD: 1060.0193\n",
      "Train Epoch: 009 Batch: 00049/00094 | Loss: 477.2169 | CE: 0.1988 | KD: 1060.0403\n",
      "Train Epoch: 009 Batch: 00050/00094 | Loss: 477.1692 | CE: 0.1519 | KD: 1060.0383\n",
      "Train Epoch: 009 Batch: 00051/00094 | Loss: 477.1748 | CE: 0.1649 | KD: 1060.0222\n",
      "Train Epoch: 009 Batch: 00052/00094 | Loss: 477.1973 | CE: 0.1815 | KD: 1060.0352\n",
      "Train Epoch: 009 Batch: 00053/00094 | Loss: 477.2136 | CE: 0.2025 | KD: 1060.0245\n",
      "Train Epoch: 009 Batch: 00054/00094 | Loss: 477.1715 | CE: 0.1626 | KD: 1060.0199\n",
      "Train Epoch: 009 Batch: 00055/00094 | Loss: 477.2135 | CE: 0.1954 | KD: 1060.0403\n",
      "Train Epoch: 009 Batch: 00056/00094 | Loss: 477.2159 | CE: 0.1874 | KD: 1060.0634\n",
      "Train Epoch: 009 Batch: 00057/00094 | Loss: 477.2189 | CE: 0.2122 | KD: 1060.0150\n",
      "Train Epoch: 009 Batch: 00058/00094 | Loss: 477.2458 | CE: 0.2374 | KD: 1060.0187\n",
      "Train Epoch: 009 Batch: 00059/00094 | Loss: 477.2250 | CE: 0.1923 | KD: 1060.0728\n",
      "Train Epoch: 009 Batch: 00060/00094 | Loss: 477.2049 | CE: 0.1843 | KD: 1060.0458\n",
      "Train Epoch: 009 Batch: 00061/00094 | Loss: 477.1850 | CE: 0.1867 | KD: 1059.9962\n",
      "Train Epoch: 009 Batch: 00062/00094 | Loss: 477.1853 | CE: 0.1703 | KD: 1060.0332\n",
      "Train Epoch: 009 Batch: 00063/00094 | Loss: 477.2180 | CE: 0.2120 | KD: 1060.0133\n",
      "Train Epoch: 009 Batch: 00064/00094 | Loss: 477.2339 | CE: 0.1959 | KD: 1060.0845\n",
      "Train Epoch: 009 Batch: 00065/00094 | Loss: 477.1567 | CE: 0.1512 | KD: 1060.0123\n",
      "Train Epoch: 009 Batch: 00066/00094 | Loss: 477.2318 | CE: 0.2218 | KD: 1060.0225\n",
      "Train Epoch: 009 Batch: 00067/00094 | Loss: 477.2299 | CE: 0.2215 | KD: 1060.0187\n",
      "Train Epoch: 009 Batch: 00068/00094 | Loss: 477.2410 | CE: 0.2086 | KD: 1060.0720\n",
      "Train Epoch: 009 Batch: 00069/00094 | Loss: 477.2160 | CE: 0.1913 | KD: 1060.0551\n",
      "Train Epoch: 009 Batch: 00070/00094 | Loss: 477.1689 | CE: 0.1759 | KD: 1059.9845\n",
      "Train Epoch: 009 Batch: 00071/00094 | Loss: 477.2096 | CE: 0.1533 | KD: 1060.1252\n",
      "Train Epoch: 009 Batch: 00072/00094 | Loss: 477.1903 | CE: 0.2034 | KD: 1059.9708\n",
      "Train Epoch: 009 Batch: 00073/00094 | Loss: 477.2485 | CE: 0.2153 | KD: 1060.0737\n",
      "Train Epoch: 009 Batch: 00074/00094 | Loss: 477.2375 | CE: 0.2189 | KD: 1060.0415\n",
      "Train Epoch: 009 Batch: 00075/00094 | Loss: 477.2016 | CE: 0.1768 | KD: 1060.0552\n",
      "Train Epoch: 009 Batch: 00076/00094 | Loss: 477.1588 | CE: 0.1596 | KD: 1059.9982\n",
      "Train Epoch: 009 Batch: 00077/00094 | Loss: 477.1596 | CE: 0.1750 | KD: 1059.9657\n",
      "Train Epoch: 009 Batch: 00078/00094 | Loss: 477.2090 | CE: 0.1716 | KD: 1060.0830\n",
      "Train Epoch: 009 Batch: 00079/00094 | Loss: 477.2985 | CE: 0.2633 | KD: 1060.0782\n",
      "Train Epoch: 009 Batch: 00080/00094 | Loss: 477.2155 | CE: 0.2224 | KD: 1059.9847\n",
      "Train Epoch: 009 Batch: 00081/00094 | Loss: 477.2536 | CE: 0.2297 | KD: 1060.0530\n",
      "Train Epoch: 009 Batch: 00082/00094 | Loss: 477.2523 | CE: 0.2510 | KD: 1060.0029\n",
      "Train Epoch: 009 Batch: 00083/00094 | Loss: 477.2270 | CE: 0.1905 | KD: 1060.0809\n",
      "Train Epoch: 009 Batch: 00084/00094 | Loss: 477.2382 | CE: 0.2266 | KD: 1060.0256\n",
      "Train Epoch: 009 Batch: 00085/00094 | Loss: 477.2166 | CE: 0.2049 | KD: 1060.0260\n",
      "Train Epoch: 009 Batch: 00086/00094 | Loss: 477.2274 | CE: 0.2041 | KD: 1060.0516\n",
      "Train Epoch: 009 Batch: 00087/00094 | Loss: 477.2018 | CE: 0.1962 | KD: 1060.0123\n",
      "Train Epoch: 009 Batch: 00088/00094 | Loss: 477.2177 | CE: 0.1871 | KD: 1060.0681\n",
      "Train Epoch: 009 Batch: 00089/00094 | Loss: 477.1718 | CE: 0.1671 | KD: 1060.0104\n",
      "Train Epoch: 009 Batch: 00090/00094 | Loss: 477.1536 | CE: 0.1532 | KD: 1060.0010\n",
      "Train Epoch: 009 Batch: 00091/00094 | Loss: 477.2383 | CE: 0.2341 | KD: 1060.0093\n",
      "Train Epoch: 009 Batch: 00092/00094 | Loss: 477.1892 | CE: 0.1776 | KD: 1060.0259\n",
      "Train Epoch: 009 Batch: 00093/00094 | Loss: 477.1630 | CE: 0.1754 | KD: 1059.9724\n",
      "Train Epoch: 009 Batch: 00094/00094 | Loss: 477.2240 | CE: 0.2197 | KD: 1060.0095\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1795 | acc:96.4000\n",
      "[VAL Acc] Target: 96.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1162 | acc:51.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6571 | acc:65.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 65.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8947 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7880 | acc:57.9154\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5182 | acc:75.4159\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6273 | acc:71.4734\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7311 | acc:58.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.38%\n",
      "[VAL Acc] Avg 65.58%\n",
      "Train Epoch: 010 Batch: 00001/00094 | Loss: 429.4918 | CE: 0.1822 | KD: 1060.0237\n",
      "Train Epoch: 010 Batch: 00002/00094 | Loss: 429.4841 | CE: 0.1744 | KD: 1060.0238\n",
      "Train Epoch: 010 Batch: 00003/00094 | Loss: 429.4710 | CE: 0.1763 | KD: 1059.9868\n",
      "Train Epoch: 010 Batch: 00004/00094 | Loss: 429.4565 | CE: 0.1512 | KD: 1060.0131\n",
      "Train Epoch: 010 Batch: 00005/00094 | Loss: 429.4660 | CE: 0.1512 | KD: 1060.0365\n",
      "Train Epoch: 010 Batch: 00006/00094 | Loss: 429.5026 | CE: 0.1957 | KD: 1060.0170\n",
      "Train Epoch: 010 Batch: 00007/00094 | Loss: 429.4964 | CE: 0.1804 | KD: 1060.0396\n",
      "Train Epoch: 010 Batch: 00008/00094 | Loss: 429.5174 | CE: 0.2027 | KD: 1060.0363\n",
      "Train Epoch: 010 Batch: 00009/00094 | Loss: 429.5293 | CE: 0.2172 | KD: 1060.0298\n",
      "Train Epoch: 010 Batch: 00010/00094 | Loss: 429.4493 | CE: 0.1471 | KD: 1060.0056\n",
      "Train Epoch: 010 Batch: 00011/00094 | Loss: 429.5347 | CE: 0.2093 | KD: 1060.0626\n",
      "Train Epoch: 010 Batch: 00012/00094 | Loss: 429.4849 | CE: 0.1783 | KD: 1060.0164\n",
      "Train Epoch: 010 Batch: 00013/00094 | Loss: 429.5073 | CE: 0.1845 | KD: 1060.0562\n",
      "Train Epoch: 010 Batch: 00014/00094 | Loss: 429.4906 | CE: 0.1933 | KD: 1059.9932\n",
      "Train Epoch: 010 Batch: 00015/00094 | Loss: 429.5313 | CE: 0.2137 | KD: 1060.0433\n",
      "Train Epoch: 010 Batch: 00016/00094 | Loss: 429.5047 | CE: 0.1892 | KD: 1060.0382\n",
      "Train Epoch: 010 Batch: 00017/00094 | Loss: 429.5370 | CE: 0.1994 | KD: 1060.0928\n",
      "Train Epoch: 010 Batch: 00018/00094 | Loss: 429.4328 | CE: 0.1359 | KD: 1059.9924\n",
      "Train Epoch: 010 Batch: 00019/00094 | Loss: 429.5046 | CE: 0.1936 | KD: 1060.0273\n",
      "Train Epoch: 010 Batch: 00020/00094 | Loss: 429.5092 | CE: 0.1910 | KD: 1060.0449\n",
      "Train Epoch: 010 Batch: 00021/00094 | Loss: 429.4788 | CE: 0.1704 | KD: 1060.0209\n",
      "Train Epoch: 010 Batch: 00022/00094 | Loss: 429.4505 | CE: 0.1485 | KD: 1060.0049\n",
      "Train Epoch: 010 Batch: 00023/00094 | Loss: 429.5132 | CE: 0.1920 | KD: 1060.0524\n",
      "Train Epoch: 010 Batch: 00024/00094 | Loss: 429.5973 | CE: 0.2460 | KD: 1060.1266\n",
      "Train Epoch: 010 Batch: 00025/00094 | Loss: 429.4928 | CE: 0.1716 | KD: 1060.0525\n",
      "Train Epoch: 010 Batch: 00026/00094 | Loss: 429.5998 | CE: 0.2793 | KD: 1060.0507\n",
      "Train Epoch: 010 Batch: 00027/00094 | Loss: 429.4655 | CE: 0.1648 | KD: 1060.0017\n",
      "Train Epoch: 010 Batch: 00028/00094 | Loss: 429.5158 | CE: 0.2019 | KD: 1060.0343\n",
      "Train Epoch: 010 Batch: 00029/00094 | Loss: 429.4896 | CE: 0.1999 | KD: 1059.9745\n",
      "Train Epoch: 010 Batch: 00030/00094 | Loss: 429.5184 | CE: 0.1944 | KD: 1060.0593\n",
      "Train Epoch: 010 Batch: 00031/00094 | Loss: 429.4682 | CE: 0.1367 | KD: 1060.0779\n",
      "Train Epoch: 010 Batch: 00032/00094 | Loss: 429.4639 | CE: 0.1632 | KD: 1060.0018\n",
      "Train Epoch: 010 Batch: 00033/00094 | Loss: 429.5299 | CE: 0.2085 | KD: 1060.0529\n",
      "Train Epoch: 010 Batch: 00034/00094 | Loss: 429.4636 | CE: 0.1667 | KD: 1059.9923\n",
      "Train Epoch: 010 Batch: 00035/00094 | Loss: 429.4768 | CE: 0.1783 | KD: 1059.9963\n",
      "Train Epoch: 010 Batch: 00036/00094 | Loss: 429.4523 | CE: 0.1556 | KD: 1059.9918\n",
      "Train Epoch: 010 Batch: 00037/00094 | Loss: 429.5063 | CE: 0.2001 | KD: 1060.0154\n",
      "Train Epoch: 010 Batch: 00038/00094 | Loss: 429.5262 | CE: 0.2168 | KD: 1060.0234\n",
      "Train Epoch: 010 Batch: 00039/00094 | Loss: 429.4901 | CE: 0.1892 | KD: 1060.0022\n",
      "Train Epoch: 010 Batch: 00040/00094 | Loss: 429.4799 | CE: 0.1653 | KD: 1060.0363\n",
      "Train Epoch: 010 Batch: 00041/00094 | Loss: 429.5310 | CE: 0.2155 | KD: 1060.0382\n",
      "Train Epoch: 010 Batch: 00042/00094 | Loss: 429.4575 | CE: 0.1625 | KD: 1059.9878\n",
      "Train Epoch: 010 Batch: 00043/00094 | Loss: 429.4264 | CE: 0.1317 | KD: 1059.9869\n",
      "Train Epoch: 010 Batch: 00044/00094 | Loss: 429.4788 | CE: 0.1654 | KD: 1060.0330\n",
      "Train Epoch: 010 Batch: 00045/00094 | Loss: 429.4615 | CE: 0.1610 | KD: 1060.0011\n",
      "Train Epoch: 010 Batch: 00046/00094 | Loss: 429.5013 | CE: 0.1946 | KD: 1060.0165\n",
      "Train Epoch: 010 Batch: 00047/00094 | Loss: 429.4977 | CE: 0.1843 | KD: 1060.0332\n",
      "Train Epoch: 010 Batch: 00048/00094 | Loss: 429.4774 | CE: 0.1750 | KD: 1060.0059\n",
      "Train Epoch: 010 Batch: 00049/00094 | Loss: 429.4648 | CE: 0.1614 | KD: 1060.0084\n",
      "Train Epoch: 010 Batch: 00050/00094 | Loss: 429.4821 | CE: 0.1927 | KD: 1059.9740\n",
      "Train Epoch: 010 Batch: 00051/00094 | Loss: 429.5015 | CE: 0.1907 | KD: 1060.0267\n",
      "Train Epoch: 010 Batch: 00052/00094 | Loss: 429.5504 | CE: 0.2169 | KD: 1060.0825\n",
      "Train Epoch: 010 Batch: 00053/00094 | Loss: 429.4618 | CE: 0.1535 | KD: 1060.0204\n",
      "Train Epoch: 010 Batch: 00054/00094 | Loss: 429.5323 | CE: 0.2022 | KD: 1060.0742\n",
      "Train Epoch: 010 Batch: 00055/00094 | Loss: 429.4555 | CE: 0.1485 | KD: 1060.0173\n",
      "Train Epoch: 010 Batch: 00056/00094 | Loss: 429.4572 | CE: 0.1378 | KD: 1060.0481\n",
      "Train Epoch: 010 Batch: 00057/00094 | Loss: 429.5470 | CE: 0.2387 | KD: 1060.0205\n",
      "Train Epoch: 010 Batch: 00058/00094 | Loss: 429.4693 | CE: 0.1622 | KD: 1060.0175\n",
      "Train Epoch: 010 Batch: 00059/00094 | Loss: 429.5170 | CE: 0.2202 | KD: 1059.9923\n",
      "Train Epoch: 010 Batch: 00060/00094 | Loss: 429.4361 | CE: 0.1528 | KD: 1059.9586\n",
      "Train Epoch: 010 Batch: 00061/00094 | Loss: 429.4948 | CE: 0.1765 | KD: 1060.0453\n",
      "Train Epoch: 010 Batch: 00062/00094 | Loss: 429.5230 | CE: 0.2096 | KD: 1060.0332\n",
      "Train Epoch: 010 Batch: 00063/00094 | Loss: 429.5261 | CE: 0.2176 | KD: 1060.0210\n",
      "Train Epoch: 010 Batch: 00064/00094 | Loss: 429.4747 | CE: 0.1824 | KD: 1059.9808\n",
      "Train Epoch: 010 Batch: 00065/00094 | Loss: 429.5076 | CE: 0.1809 | KD: 1060.0659\n",
      "Train Epoch: 010 Batch: 00066/00094 | Loss: 429.5776 | CE: 0.2581 | KD: 1060.0481\n",
      "Train Epoch: 010 Batch: 00067/00094 | Loss: 429.4601 | CE: 0.1429 | KD: 1060.0425\n",
      "Train Epoch: 010 Batch: 00068/00094 | Loss: 429.5060 | CE: 0.1976 | KD: 1060.0205\n",
      "Train Epoch: 010 Batch: 00069/00094 | Loss: 429.5235 | CE: 0.1972 | KD: 1060.0648\n",
      "Train Epoch: 010 Batch: 00070/00094 | Loss: 429.4264 | CE: 0.1314 | KD: 1059.9874\n",
      "Train Epoch: 010 Batch: 00071/00094 | Loss: 429.5425 | CE: 0.1959 | KD: 1060.1150\n",
      "Train Epoch: 010 Batch: 00072/00094 | Loss: 429.4486 | CE: 0.1433 | KD: 1060.0131\n",
      "Train Epoch: 010 Batch: 00073/00094 | Loss: 429.5028 | CE: 0.1858 | KD: 1060.0419\n",
      "Train Epoch: 010 Batch: 00074/00094 | Loss: 429.4844 | CE: 0.1883 | KD: 1059.9905\n",
      "Train Epoch: 010 Batch: 00075/00094 | Loss: 429.4705 | CE: 0.1656 | KD: 1060.0122\n",
      "Train Epoch: 010 Batch: 00076/00094 | Loss: 429.5259 | CE: 0.2252 | KD: 1060.0020\n",
      "Train Epoch: 010 Batch: 00077/00094 | Loss: 429.5018 | CE: 0.1920 | KD: 1060.0243\n",
      "Train Epoch: 010 Batch: 00078/00094 | Loss: 429.4885 | CE: 0.1722 | KD: 1060.0403\n",
      "Train Epoch: 010 Batch: 00079/00094 | Loss: 429.4742 | CE: 0.1716 | KD: 1060.0063\n",
      "Train Epoch: 010 Batch: 00080/00094 | Loss: 429.5252 | CE: 0.2268 | KD: 1059.9962\n",
      "Train Epoch: 010 Batch: 00081/00094 | Loss: 429.5224 | CE: 0.1970 | KD: 1060.0626\n",
      "Train Epoch: 010 Batch: 00082/00094 | Loss: 429.4720 | CE: 0.1507 | KD: 1060.0526\n",
      "Train Epoch: 010 Batch: 00083/00094 | Loss: 429.5060 | CE: 0.1917 | KD: 1060.0352\n",
      "Train Epoch: 010 Batch: 00084/00094 | Loss: 429.5279 | CE: 0.2074 | KD: 1060.0505\n",
      "Train Epoch: 010 Batch: 00085/00094 | Loss: 429.4996 | CE: 0.1805 | KD: 1060.0470\n",
      "Train Epoch: 010 Batch: 00086/00094 | Loss: 429.5260 | CE: 0.2368 | KD: 1059.9734\n",
      "Train Epoch: 010 Batch: 00087/00094 | Loss: 429.4718 | CE: 0.1639 | KD: 1060.0197\n",
      "Train Epoch: 010 Batch: 00088/00094 | Loss: 429.4583 | CE: 0.1529 | KD: 1060.0134\n",
      "Train Epoch: 010 Batch: 00089/00094 | Loss: 429.4982 | CE: 0.1857 | KD: 1060.0309\n",
      "Train Epoch: 010 Batch: 00090/00094 | Loss: 429.5270 | CE: 0.1780 | KD: 1060.1210\n",
      "Train Epoch: 010 Batch: 00091/00094 | Loss: 429.4863 | CE: 0.1531 | KD: 1060.0818\n",
      "Train Epoch: 010 Batch: 00092/00094 | Loss: 429.5074 | CE: 0.2034 | KD: 1060.0099\n",
      "Train Epoch: 010 Batch: 00093/00094 | Loss: 429.4938 | CE: 0.1784 | KD: 1060.0380\n",
      "Train Epoch: 010 Batch: 00094/00094 | Loss: 429.4639 | CE: 0.1659 | KD: 1059.9951\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1844 | acc:96.7500\n",
      "[VAL Acc] Target: 96.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1092 | acc:51.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6258 | acc:68.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8924 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7486 | acc:60.1097\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5901 | acc:69.2237\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6092 | acc:71.9044\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.6976 | acc:63.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 63.00%\n",
      "[VAL Acc] Avg 66.24%\n",
      "Train Epoch: 011 Batch: 00001/00094 | Loss: 429.4822 | CE: 0.1584 | KD: 1060.0587\n",
      "Train Epoch: 011 Batch: 00002/00094 | Loss: 429.5195 | CE: 0.2001 | KD: 1060.0479\n",
      "Train Epoch: 011 Batch: 00003/00094 | Loss: 429.5396 | CE: 0.2180 | KD: 1060.0533\n",
      "Train Epoch: 011 Batch: 00004/00094 | Loss: 429.5192 | CE: 0.1959 | KD: 1060.0575\n",
      "Train Epoch: 011 Batch: 00005/00094 | Loss: 429.6357 | CE: 0.3102 | KD: 1060.0629\n",
      "Train Epoch: 011 Batch: 00006/00094 | Loss: 429.4785 | CE: 0.1717 | KD: 1060.0168\n",
      "Train Epoch: 011 Batch: 00007/00094 | Loss: 429.5304 | CE: 0.2122 | KD: 1060.0449\n",
      "Train Epoch: 011 Batch: 00008/00094 | Loss: 429.5175 | CE: 0.1933 | KD: 1060.0596\n",
      "Train Epoch: 011 Batch: 00009/00094 | Loss: 429.5679 | CE: 0.2410 | KD: 1060.0663\n",
      "Train Epoch: 011 Batch: 00010/00094 | Loss: 429.4637 | CE: 0.1437 | KD: 1060.0494\n",
      "Train Epoch: 011 Batch: 00011/00094 | Loss: 429.6219 | CE: 0.2926 | KD: 1060.0724\n",
      "Train Epoch: 011 Batch: 00012/00094 | Loss: 429.4798 | CE: 0.1569 | KD: 1060.0565\n",
      "Train Epoch: 011 Batch: 00013/00094 | Loss: 429.5333 | CE: 0.2212 | KD: 1060.0300\n",
      "Train Epoch: 011 Batch: 00014/00094 | Loss: 429.5299 | CE: 0.2039 | KD: 1060.0641\n",
      "Train Epoch: 011 Batch: 00015/00094 | Loss: 429.5052 | CE: 0.1728 | KD: 1060.0801\n",
      "Train Epoch: 011 Batch: 00016/00094 | Loss: 429.4924 | CE: 0.1942 | KD: 1059.9955\n",
      "Train Epoch: 011 Batch: 00017/00094 | Loss: 429.4762 | CE: 0.1735 | KD: 1060.0066\n",
      "Train Epoch: 011 Batch: 00018/00094 | Loss: 429.4968 | CE: 0.1925 | KD: 1060.0107\n",
      "Train Epoch: 011 Batch: 00019/00094 | Loss: 429.5154 | CE: 0.2139 | KD: 1060.0037\n",
      "Train Epoch: 011 Batch: 00020/00094 | Loss: 429.5032 | CE: 0.1916 | KD: 1060.0287\n",
      "Train Epoch: 011 Batch: 00021/00094 | Loss: 429.5023 | CE: 0.2098 | KD: 1059.9816\n",
      "Train Epoch: 011 Batch: 00022/00094 | Loss: 429.5378 | CE: 0.2027 | KD: 1060.0865\n",
      "Train Epoch: 011 Batch: 00023/00094 | Loss: 429.4708 | CE: 0.1337 | KD: 1060.0917\n",
      "Train Epoch: 011 Batch: 00024/00094 | Loss: 429.4977 | CE: 0.1822 | KD: 1060.0385\n",
      "Train Epoch: 011 Batch: 00025/00094 | Loss: 429.4618 | CE: 0.1587 | KD: 1060.0076\n",
      "Train Epoch: 011 Batch: 00026/00094 | Loss: 429.4766 | CE: 0.1684 | KD: 1060.0203\n",
      "Train Epoch: 011 Batch: 00027/00094 | Loss: 429.4927 | CE: 0.1746 | KD: 1060.0448\n",
      "Train Epoch: 011 Batch: 00028/00094 | Loss: 429.6244 | CE: 0.3078 | KD: 1060.0410\n",
      "Train Epoch: 011 Batch: 00029/00094 | Loss: 429.4579 | CE: 0.1507 | KD: 1060.0179\n",
      "Train Epoch: 011 Batch: 00030/00094 | Loss: 429.5419 | CE: 0.2363 | KD: 1060.0137\n",
      "Train Epoch: 011 Batch: 00031/00094 | Loss: 429.5189 | CE: 0.2129 | KD: 1060.0146\n",
      "Train Epoch: 011 Batch: 00032/00094 | Loss: 429.4610 | CE: 0.1566 | KD: 1060.0107\n",
      "Train Epoch: 011 Batch: 00033/00094 | Loss: 429.5821 | CE: 0.2582 | KD: 1060.0590\n",
      "Train Epoch: 011 Batch: 00034/00094 | Loss: 429.4852 | CE: 0.1801 | KD: 1060.0127\n",
      "Train Epoch: 011 Batch: 00035/00094 | Loss: 429.5621 | CE: 0.2342 | KD: 1060.0690\n",
      "Train Epoch: 011 Batch: 00036/00094 | Loss: 429.5426 | CE: 0.2365 | KD: 1060.0149\n",
      "Train Epoch: 011 Batch: 00037/00094 | Loss: 429.4703 | CE: 0.1725 | KD: 1059.9946\n",
      "Train Epoch: 011 Batch: 00038/00094 | Loss: 429.5455 | CE: 0.2389 | KD: 1060.0162\n",
      "Train Epoch: 011 Batch: 00039/00094 | Loss: 429.4664 | CE: 0.1671 | KD: 1059.9982\n",
      "Train Epoch: 011 Batch: 00040/00094 | Loss: 429.4740 | CE: 0.1601 | KD: 1060.0344\n",
      "Train Epoch: 011 Batch: 00041/00094 | Loss: 429.4999 | CE: 0.1953 | KD: 1060.0115\n",
      "Train Epoch: 011 Batch: 00042/00094 | Loss: 429.5132 | CE: 0.2030 | KD: 1060.0253\n",
      "Train Epoch: 011 Batch: 00043/00094 | Loss: 429.5250 | CE: 0.1996 | KD: 1060.0626\n",
      "Train Epoch: 011 Batch: 00044/00094 | Loss: 429.5317 | CE: 0.2249 | KD: 1060.0166\n",
      "Train Epoch: 011 Batch: 00045/00094 | Loss: 429.4720 | CE: 0.1614 | KD: 1060.0262\n",
      "Train Epoch: 011 Batch: 00046/00094 | Loss: 429.4738 | CE: 0.1687 | KD: 1060.0127\n",
      "Train Epoch: 011 Batch: 00047/00094 | Loss: 429.5246 | CE: 0.2222 | KD: 1060.0061\n",
      "Train Epoch: 011 Batch: 00048/00094 | Loss: 429.4967 | CE: 0.2001 | KD: 1059.9917\n",
      "Train Epoch: 011 Batch: 00049/00094 | Loss: 429.4617 | CE: 0.1491 | KD: 1060.0311\n",
      "Train Epoch: 011 Batch: 00050/00094 | Loss: 429.4590 | CE: 0.1529 | KD: 1060.0151\n",
      "Train Epoch: 011 Batch: 00051/00094 | Loss: 429.4964 | CE: 0.1925 | KD: 1060.0098\n",
      "Train Epoch: 011 Batch: 00052/00094 | Loss: 429.4863 | CE: 0.1746 | KD: 1060.0289\n",
      "Train Epoch: 011 Batch: 00053/00094 | Loss: 429.5192 | CE: 0.2006 | KD: 1060.0460\n",
      "Train Epoch: 011 Batch: 00054/00094 | Loss: 429.5260 | CE: 0.2222 | KD: 1060.0093\n",
      "Train Epoch: 011 Batch: 00055/00094 | Loss: 429.4749 | CE: 0.1763 | KD: 1059.9967\n",
      "Train Epoch: 011 Batch: 00056/00094 | Loss: 429.5120 | CE: 0.2055 | KD: 1060.0160\n",
      "Train Epoch: 011 Batch: 00057/00094 | Loss: 429.4971 | CE: 0.1920 | KD: 1060.0126\n",
      "Train Epoch: 011 Batch: 00058/00094 | Loss: 429.4858 | CE: 0.1519 | KD: 1060.0837\n",
      "Train Epoch: 011 Batch: 00059/00094 | Loss: 429.5193 | CE: 0.1829 | KD: 1060.0900\n",
      "Train Epoch: 011 Batch: 00060/00094 | Loss: 429.5202 | CE: 0.2150 | KD: 1060.0128\n",
      "Train Epoch: 011 Batch: 00061/00094 | Loss: 429.4880 | CE: 0.1738 | KD: 1060.0350\n",
      "Train Epoch: 011 Batch: 00062/00094 | Loss: 429.5274 | CE: 0.2161 | KD: 1060.0280\n",
      "Train Epoch: 011 Batch: 00063/00094 | Loss: 429.4854 | CE: 0.1766 | KD: 1060.0217\n",
      "Train Epoch: 011 Batch: 00064/00094 | Loss: 429.5047 | CE: 0.1953 | KD: 1060.0232\n",
      "Train Epoch: 011 Batch: 00065/00094 | Loss: 429.4835 | CE: 0.1883 | KD: 1059.9883\n",
      "Train Epoch: 011 Batch: 00066/00094 | Loss: 429.4345 | CE: 0.1459 | KD: 1059.9718\n",
      "Train Epoch: 011 Batch: 00067/00094 | Loss: 429.4826 | CE: 0.1759 | KD: 1060.0166\n",
      "Train Epoch: 011 Batch: 00068/00094 | Loss: 429.5185 | CE: 0.2026 | KD: 1060.0393\n",
      "Train Epoch: 011 Batch: 00069/00094 | Loss: 429.5643 | CE: 0.2410 | KD: 1060.0574\n",
      "Train Epoch: 011 Batch: 00070/00094 | Loss: 429.4643 | CE: 0.1566 | KD: 1060.0190\n",
      "Train Epoch: 011 Batch: 00071/00094 | Loss: 429.5262 | CE: 0.1993 | KD: 1060.0665\n",
      "Train Epoch: 011 Batch: 00072/00094 | Loss: 429.4689 | CE: 0.1688 | KD: 1060.0004\n",
      "Train Epoch: 011 Batch: 00073/00094 | Loss: 429.5423 | CE: 0.2079 | KD: 1060.0848\n",
      "Train Epoch: 011 Batch: 00074/00094 | Loss: 429.5461 | CE: 0.2346 | KD: 1060.0283\n",
      "Train Epoch: 011 Batch: 00075/00094 | Loss: 429.4960 | CE: 0.1930 | KD: 1060.0074\n",
      "Train Epoch: 011 Batch: 00076/00094 | Loss: 429.4652 | CE: 0.1614 | KD: 1060.0095\n",
      "Train Epoch: 011 Batch: 00077/00094 | Loss: 429.4944 | CE: 0.1985 | KD: 1059.9901\n",
      "Train Epoch: 011 Batch: 00078/00094 | Loss: 429.4542 | CE: 0.1427 | KD: 1060.0284\n",
      "Train Epoch: 011 Batch: 00079/00094 | Loss: 429.5104 | CE: 0.2196 | KD: 1059.9773\n",
      "Train Epoch: 011 Batch: 00080/00094 | Loss: 429.4931 | CE: 0.1795 | KD: 1060.0336\n",
      "Train Epoch: 011 Batch: 00081/00094 | Loss: 429.4902 | CE: 0.1884 | KD: 1060.0045\n",
      "Train Epoch: 011 Batch: 00082/00094 | Loss: 429.4626 | CE: 0.1510 | KD: 1060.0286\n",
      "Train Epoch: 011 Batch: 00083/00094 | Loss: 429.5139 | CE: 0.2061 | KD: 1060.0193\n",
      "Train Epoch: 011 Batch: 00084/00094 | Loss: 429.5240 | CE: 0.2006 | KD: 1060.0576\n",
      "Train Epoch: 011 Batch: 00085/00094 | Loss: 429.4762 | CE: 0.1760 | KD: 1060.0004\n",
      "Train Epoch: 011 Batch: 00086/00094 | Loss: 429.5278 | CE: 0.2106 | KD: 1060.0426\n",
      "Train Epoch: 011 Batch: 00087/00094 | Loss: 429.4843 | CE: 0.1609 | KD: 1060.0580\n",
      "Train Epoch: 011 Batch: 00088/00094 | Loss: 429.5011 | CE: 0.2004 | KD: 1060.0018\n",
      "Train Epoch: 011 Batch: 00089/00094 | Loss: 429.5755 | CE: 0.2700 | KD: 1060.0135\n",
      "Train Epoch: 011 Batch: 00090/00094 | Loss: 429.4939 | CE: 0.1796 | KD: 1060.0353\n",
      "Train Epoch: 011 Batch: 00091/00094 | Loss: 429.5064 | CE: 0.1859 | KD: 1060.0507\n",
      "Train Epoch: 011 Batch: 00092/00094 | Loss: 429.4755 | CE: 0.1597 | KD: 1060.0392\n",
      "Train Epoch: 011 Batch: 00093/00094 | Loss: 429.4693 | CE: 0.1934 | KD: 1059.9404\n",
      "Train Epoch: 011 Batch: 00094/00094 | Loss: 429.4697 | CE: 0.1639 | KD: 1060.0143\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1743 | acc:97.4500\n",
      "[VAL Acc] Target: 97.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1672 | acc:50.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6306 | acc:67.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 67.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8981 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8394 | acc:55.2508\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5237 | acc:74.0296\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6982 | acc:68.5345\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7088 | acc:61.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.19%\n",
      "[VAL Acc] Avg 65.50%\n",
      "Train Epoch: 012 Batch: 00001/00094 | Loss: 429.6194 | CE: 0.2752 | KD: 1060.1090\n",
      "Train Epoch: 012 Batch: 00002/00094 | Loss: 429.4583 | CE: 0.1599 | KD: 1059.9960\n",
      "Train Epoch: 012 Batch: 00003/00094 | Loss: 429.4911 | CE: 0.1879 | KD: 1060.0081\n",
      "Train Epoch: 012 Batch: 00004/00094 | Loss: 429.5039 | CE: 0.1847 | KD: 1060.0476\n",
      "Train Epoch: 012 Batch: 00005/00094 | Loss: 429.4849 | CE: 0.1890 | KD: 1059.9900\n",
      "Train Epoch: 012 Batch: 00006/00094 | Loss: 429.4538 | CE: 0.1632 | KD: 1059.9767\n",
      "Train Epoch: 012 Batch: 00007/00094 | Loss: 429.4756 | CE: 0.1649 | KD: 1060.0262\n",
      "Train Epoch: 012 Batch: 00008/00094 | Loss: 429.4730 | CE: 0.1679 | KD: 1060.0127\n",
      "Train Epoch: 012 Batch: 00009/00094 | Loss: 429.4942 | CE: 0.1911 | KD: 1060.0077\n",
      "Train Epoch: 012 Batch: 00010/00094 | Loss: 429.4721 | CE: 0.1896 | KD: 1059.9569\n",
      "Train Epoch: 012 Batch: 00011/00094 | Loss: 429.4644 | CE: 0.1607 | KD: 1060.0089\n",
      "Train Epoch: 012 Batch: 00012/00094 | Loss: 429.4792 | CE: 0.1617 | KD: 1060.0432\n",
      "Train Epoch: 012 Batch: 00013/00094 | Loss: 429.5029 | CE: 0.1769 | KD: 1060.0642\n",
      "Train Epoch: 012 Batch: 00014/00094 | Loss: 429.5520 | CE: 0.2304 | KD: 1060.0532\n",
      "Train Epoch: 012 Batch: 00015/00094 | Loss: 429.5642 | CE: 0.2425 | KD: 1060.0537\n",
      "Train Epoch: 012 Batch: 00016/00094 | Loss: 429.4949 | CE: 0.1914 | KD: 1060.0085\n",
      "Train Epoch: 012 Batch: 00017/00094 | Loss: 429.4764 | CE: 0.1790 | KD: 1059.9935\n",
      "Train Epoch: 012 Batch: 00018/00094 | Loss: 429.5165 | CE: 0.1958 | KD: 1060.0510\n",
      "Train Epoch: 012 Batch: 00019/00094 | Loss: 429.5285 | CE: 0.2118 | KD: 1060.0413\n",
      "Train Epoch: 012 Batch: 00020/00094 | Loss: 429.5345 | CE: 0.2217 | KD: 1060.0315\n",
      "Train Epoch: 012 Batch: 00021/00094 | Loss: 429.5033 | CE: 0.2002 | KD: 1060.0076\n",
      "Train Epoch: 012 Batch: 00022/00094 | Loss: 429.4641 | CE: 0.1602 | KD: 1060.0098\n",
      "Train Epoch: 012 Batch: 00023/00094 | Loss: 429.5363 | CE: 0.2156 | KD: 1060.0511\n",
      "Train Epoch: 012 Batch: 00024/00094 | Loss: 429.5213 | CE: 0.1893 | KD: 1060.0790\n",
      "Train Epoch: 012 Batch: 00025/00094 | Loss: 429.4610 | CE: 0.1674 | KD: 1059.9841\n",
      "Train Epoch: 012 Batch: 00026/00094 | Loss: 429.5304 | CE: 0.2064 | KD: 1060.0592\n",
      "Train Epoch: 012 Batch: 00027/00094 | Loss: 429.4698 | CE: 0.1839 | KD: 1059.9652\n",
      "Train Epoch: 012 Batch: 00028/00094 | Loss: 429.6932 | CE: 0.3505 | KD: 1060.1056\n",
      "Train Epoch: 012 Batch: 00029/00094 | Loss: 429.4663 | CE: 0.1715 | KD: 1059.9873\n",
      "Train Epoch: 012 Batch: 00030/00094 | Loss: 429.5310 | CE: 0.2171 | KD: 1060.0343\n",
      "Train Epoch: 012 Batch: 00031/00094 | Loss: 429.5070 | CE: 0.1873 | KD: 1060.0486\n",
      "Train Epoch: 012 Batch: 00032/00094 | Loss: 429.4986 | CE: 0.1847 | KD: 1060.0343\n",
      "Train Epoch: 012 Batch: 00033/00094 | Loss: 429.5215 | CE: 0.2189 | KD: 1060.0065\n",
      "Train Epoch: 012 Batch: 00034/00094 | Loss: 429.5172 | CE: 0.1948 | KD: 1060.0552\n",
      "Train Epoch: 012 Batch: 00035/00094 | Loss: 429.5215 | CE: 0.1846 | KD: 1060.0911\n",
      "Train Epoch: 012 Batch: 00036/00094 | Loss: 429.4937 | CE: 0.1916 | KD: 1060.0051\n",
      "Train Epoch: 012 Batch: 00037/00094 | Loss: 429.5182 | CE: 0.1765 | KD: 1060.1031\n",
      "Train Epoch: 012 Batch: 00038/00094 | Loss: 429.5029 | CE: 0.1791 | KD: 1060.0588\n",
      "Train Epoch: 012 Batch: 00039/00094 | Loss: 429.5354 | CE: 0.2161 | KD: 1060.0477\n",
      "Train Epoch: 012 Batch: 00040/00094 | Loss: 429.4906 | CE: 0.1828 | KD: 1060.0190\n",
      "Train Epoch: 012 Batch: 00041/00094 | Loss: 429.5265 | CE: 0.2078 | KD: 1060.0463\n",
      "Train Epoch: 012 Batch: 00042/00094 | Loss: 429.4940 | CE: 0.1802 | KD: 1060.0342\n",
      "Train Epoch: 012 Batch: 00043/00094 | Loss: 429.6369 | CE: 0.3023 | KD: 1060.0856\n",
      "Train Epoch: 012 Batch: 00044/00094 | Loss: 429.4400 | CE: 0.1558 | KD: 1059.9609\n",
      "Train Epoch: 012 Batch: 00045/00094 | Loss: 429.4841 | CE: 0.1964 | KD: 1059.9697\n",
      "Train Epoch: 012 Batch: 00046/00094 | Loss: 429.4901 | CE: 0.1666 | KD: 1060.0579\n",
      "Train Epoch: 012 Batch: 00047/00094 | Loss: 429.5001 | CE: 0.1906 | KD: 1060.0234\n",
      "Train Epoch: 012 Batch: 00048/00094 | Loss: 429.5295 | CE: 0.2113 | KD: 1060.0449\n",
      "Train Epoch: 012 Batch: 00049/00094 | Loss: 429.5073 | CE: 0.1802 | KD: 1060.0669\n",
      "Train Epoch: 012 Batch: 00050/00094 | Loss: 429.4873 | CE: 0.1896 | KD: 1059.9943\n",
      "Train Epoch: 012 Batch: 00051/00094 | Loss: 429.5570 | CE: 0.2339 | KD: 1060.0571\n",
      "Train Epoch: 012 Batch: 00052/00094 | Loss: 429.4743 | CE: 0.1593 | KD: 1060.0370\n",
      "Train Epoch: 012 Batch: 00053/00094 | Loss: 429.4669 | CE: 0.1684 | KD: 1059.9961\n",
      "Train Epoch: 012 Batch: 00054/00094 | Loss: 429.6018 | CE: 0.2884 | KD: 1060.0331\n",
      "Train Epoch: 012 Batch: 00055/00094 | Loss: 429.4826 | CE: 0.1845 | KD: 1059.9954\n",
      "Train Epoch: 012 Batch: 00056/00094 | Loss: 429.5306 | CE: 0.2134 | KD: 1060.0424\n",
      "Train Epoch: 012 Batch: 00057/00094 | Loss: 429.4938 | CE: 0.1857 | KD: 1060.0199\n",
      "Train Epoch: 012 Batch: 00058/00094 | Loss: 429.5298 | CE: 0.2113 | KD: 1060.0458\n",
      "Train Epoch: 012 Batch: 00059/00094 | Loss: 429.5568 | CE: 0.2317 | KD: 1060.0620\n",
      "Train Epoch: 012 Batch: 00060/00094 | Loss: 429.4637 | CE: 0.1697 | KD: 1059.9852\n",
      "Train Epoch: 012 Batch: 00061/00094 | Loss: 429.4482 | CE: 0.1657 | KD: 1059.9567\n",
      "Train Epoch: 012 Batch: 00062/00094 | Loss: 429.4722 | CE: 0.1730 | KD: 1059.9982\n",
      "Train Epoch: 012 Batch: 00063/00094 | Loss: 429.4704 | CE: 0.1657 | KD: 1060.0115\n",
      "Train Epoch: 012 Batch: 00064/00094 | Loss: 429.4859 | CE: 0.1737 | KD: 1060.0303\n",
      "Train Epoch: 012 Batch: 00065/00094 | Loss: 429.4672 | CE: 0.1748 | KD: 1059.9812\n",
      "Train Epoch: 012 Batch: 00066/00094 | Loss: 429.5233 | CE: 0.2063 | KD: 1060.0419\n",
      "Train Epoch: 012 Batch: 00067/00094 | Loss: 429.4509 | CE: 0.1646 | KD: 1059.9662\n",
      "Train Epoch: 012 Batch: 00068/00094 | Loss: 429.4961 | CE: 0.1703 | KD: 1060.0637\n",
      "Train Epoch: 012 Batch: 00069/00094 | Loss: 429.4839 | CE: 0.1673 | KD: 1060.0411\n",
      "Train Epoch: 012 Batch: 00070/00094 | Loss: 429.4944 | CE: 0.1950 | KD: 1059.9984\n",
      "Train Epoch: 012 Batch: 00071/00094 | Loss: 429.5566 | CE: 0.2255 | KD: 1060.0769\n",
      "Train Epoch: 012 Batch: 00072/00094 | Loss: 429.4869 | CE: 0.1627 | KD: 1060.0598\n",
      "Train Epoch: 012 Batch: 00073/00094 | Loss: 429.4915 | CE: 0.1738 | KD: 1060.0437\n",
      "Train Epoch: 012 Batch: 00074/00094 | Loss: 429.5020 | CE: 0.1868 | KD: 1060.0377\n",
      "Train Epoch: 012 Batch: 00075/00094 | Loss: 429.4884 | CE: 0.1793 | KD: 1060.0225\n",
      "Train Epoch: 012 Batch: 00076/00094 | Loss: 429.4877 | CE: 0.1715 | KD: 1060.0399\n",
      "Train Epoch: 012 Batch: 00077/00094 | Loss: 429.4931 | CE: 0.2030 | KD: 1059.9755\n",
      "Train Epoch: 012 Batch: 00078/00094 | Loss: 429.6180 | CE: 0.2777 | KD: 1060.0995\n",
      "Train Epoch: 012 Batch: 00079/00094 | Loss: 429.5643 | CE: 0.2258 | KD: 1060.0951\n",
      "Train Epoch: 012 Batch: 00080/00094 | Loss: 429.5371 | CE: 0.2279 | KD: 1060.0227\n",
      "Train Epoch: 012 Batch: 00081/00094 | Loss: 429.5893 | CE: 0.2520 | KD: 1060.0923\n",
      "Train Epoch: 012 Batch: 00082/00094 | Loss: 429.4969 | CE: 0.1915 | KD: 1060.0132\n",
      "Train Epoch: 012 Batch: 00083/00094 | Loss: 429.5071 | CE: 0.1669 | KD: 1060.0992\n",
      "Train Epoch: 012 Batch: 00084/00094 | Loss: 429.5445 | CE: 0.2231 | KD: 1060.0526\n",
      "Train Epoch: 012 Batch: 00085/00094 | Loss: 429.5019 | CE: 0.1978 | KD: 1060.0100\n",
      "Train Epoch: 012 Batch: 00086/00094 | Loss: 429.4990 | CE: 0.1878 | KD: 1060.0276\n",
      "Train Epoch: 012 Batch: 00087/00094 | Loss: 429.5035 | CE: 0.1764 | KD: 1060.0669\n",
      "Train Epoch: 012 Batch: 00088/00094 | Loss: 429.5431 | CE: 0.2201 | KD: 1060.0569\n",
      "Train Epoch: 012 Batch: 00089/00094 | Loss: 429.5013 | CE: 0.1979 | KD: 1060.0084\n",
      "Train Epoch: 012 Batch: 00090/00094 | Loss: 429.4596 | CE: 0.1550 | KD: 1060.0114\n",
      "Train Epoch: 012 Batch: 00091/00094 | Loss: 429.5284 | CE: 0.2184 | KD: 1060.0245\n",
      "Train Epoch: 012 Batch: 00092/00094 | Loss: 429.4726 | CE: 0.1638 | KD: 1060.0219\n",
      "Train Epoch: 012 Batch: 00093/00094 | Loss: 429.4925 | CE: 0.1845 | KD: 1060.0198\n",
      "Train Epoch: 012 Batch: 00094/00094 | Loss: 429.4878 | CE: 0.1675 | KD: 1060.0500\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1778 | acc:96.5000\n",
      "[VAL Acc] Target: 96.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1468 | acc:51.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6409 | acc:67.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 67.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8468 | acc:54.1985\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 54.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8614 | acc:55.8777\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5486 | acc:72.4584\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.46%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6510 | acc:70.2194\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7028 | acc:61.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.75%\n",
      "[VAL Acc] Avg 66.23%\n",
      "Train Epoch: 013 Batch: 00001/00094 | Loss: 429.5188 | CE: 0.1919 | KD: 1060.0663\n",
      "Train Epoch: 013 Batch: 00002/00094 | Loss: 429.4832 | CE: 0.1711 | KD: 1060.0299\n",
      "Train Epoch: 013 Batch: 00003/00094 | Loss: 429.4951 | CE: 0.1862 | KD: 1060.0219\n",
      "Train Epoch: 013 Batch: 00004/00094 | Loss: 429.5089 | CE: 0.1886 | KD: 1060.0500\n",
      "Train Epoch: 013 Batch: 00005/00094 | Loss: 429.4781 | CE: 0.1583 | KD: 1060.0491\n",
      "Train Epoch: 013 Batch: 00006/00094 | Loss: 429.4996 | CE: 0.2004 | KD: 1059.9980\n",
      "Train Epoch: 013 Batch: 00007/00094 | Loss: 429.4676 | CE: 0.1580 | KD: 1060.0237\n",
      "Train Epoch: 013 Batch: 00008/00094 | Loss: 429.5078 | CE: 0.2023 | KD: 1060.0137\n",
      "Train Epoch: 013 Batch: 00009/00094 | Loss: 429.5039 | CE: 0.1946 | KD: 1060.0229\n",
      "Train Epoch: 013 Batch: 00010/00094 | Loss: 429.5151 | CE: 0.1929 | KD: 1060.0549\n",
      "Train Epoch: 013 Batch: 00011/00094 | Loss: 429.4382 | CE: 0.1463 | KD: 1059.9800\n",
      "Train Epoch: 013 Batch: 00012/00094 | Loss: 429.5393 | CE: 0.2234 | KD: 1060.0392\n",
      "Train Epoch: 013 Batch: 00013/00094 | Loss: 429.4385 | CE: 0.1433 | KD: 1059.9882\n",
      "Train Epoch: 013 Batch: 00014/00094 | Loss: 429.5043 | CE: 0.1870 | KD: 1060.0427\n",
      "Train Epoch: 013 Batch: 00015/00094 | Loss: 429.4917 | CE: 0.1996 | KD: 1059.9805\n",
      "Train Epoch: 013 Batch: 00016/00094 | Loss: 429.5657 | CE: 0.2396 | KD: 1060.0643\n",
      "Train Epoch: 013 Batch: 00017/00094 | Loss: 429.4630 | CE: 0.1374 | KD: 1060.0634\n",
      "Train Epoch: 013 Batch: 00018/00094 | Loss: 429.4703 | CE: 0.1792 | KD: 1059.9780\n",
      "Train Epoch: 013 Batch: 00019/00094 | Loss: 429.4611 | CE: 0.1742 | KD: 1059.9677\n",
      "Train Epoch: 013 Batch: 00020/00094 | Loss: 429.5221 | CE: 0.1984 | KD: 1060.0585\n",
      "Train Epoch: 013 Batch: 00021/00094 | Loss: 429.4631 | CE: 0.1431 | KD: 1060.0493\n",
      "Train Epoch: 013 Batch: 00022/00094 | Loss: 429.4839 | CE: 0.1719 | KD: 1060.0298\n",
      "Train Epoch: 013 Batch: 00023/00094 | Loss: 429.4540 | CE: 0.1634 | KD: 1059.9768\n",
      "Train Epoch: 013 Batch: 00024/00094 | Loss: 429.5108 | CE: 0.1882 | KD: 1060.0557\n",
      "Train Epoch: 013 Batch: 00025/00094 | Loss: 429.4933 | CE: 0.1668 | KD: 1060.0653\n",
      "Train Epoch: 013 Batch: 00026/00094 | Loss: 429.4907 | CE: 0.1847 | KD: 1060.0148\n",
      "Train Epoch: 013 Batch: 00027/00094 | Loss: 429.4998 | CE: 0.1791 | KD: 1060.0510\n",
      "Train Epoch: 013 Batch: 00028/00094 | Loss: 429.4986 | CE: 0.2075 | KD: 1059.9779\n",
      "Train Epoch: 013 Batch: 00029/00094 | Loss: 429.5155 | CE: 0.2002 | KD: 1060.0378\n",
      "Train Epoch: 013 Batch: 00030/00094 | Loss: 429.5116 | CE: 0.2045 | KD: 1060.0176\n",
      "Train Epoch: 013 Batch: 00031/00094 | Loss: 429.4659 | CE: 0.1702 | KD: 1059.9894\n",
      "Train Epoch: 013 Batch: 00032/00094 | Loss: 429.5302 | CE: 0.2325 | KD: 1059.9941\n",
      "Train Epoch: 013 Batch: 00033/00094 | Loss: 429.5196 | CE: 0.1926 | KD: 1060.0665\n",
      "Train Epoch: 013 Batch: 00034/00094 | Loss: 429.4333 | CE: 0.1458 | KD: 1059.9690\n",
      "Train Epoch: 013 Batch: 00035/00094 | Loss: 429.5179 | CE: 0.2078 | KD: 1060.0250\n",
      "Train Epoch: 013 Batch: 00036/00094 | Loss: 429.5407 | CE: 0.2071 | KD: 1060.0830\n",
      "Train Epoch: 013 Batch: 00037/00094 | Loss: 429.4664 | CE: 0.1597 | KD: 1060.0165\n",
      "Train Epoch: 013 Batch: 00038/00094 | Loss: 429.4330 | CE: 0.1423 | KD: 1059.9771\n",
      "Train Epoch: 013 Batch: 00039/00094 | Loss: 429.4771 | CE: 0.1702 | KD: 1060.0170\n",
      "Train Epoch: 013 Batch: 00040/00094 | Loss: 429.4779 | CE: 0.1656 | KD: 1060.0303\n",
      "Train Epoch: 013 Batch: 00041/00094 | Loss: 429.5516 | CE: 0.2419 | KD: 1060.0240\n",
      "Train Epoch: 013 Batch: 00042/00094 | Loss: 429.5458 | CE: 0.2209 | KD: 1060.0613\n",
      "Train Epoch: 013 Batch: 00043/00094 | Loss: 429.5156 | CE: 0.2208 | KD: 1059.9873\n",
      "Train Epoch: 013 Batch: 00044/00094 | Loss: 429.4763 | CE: 0.1683 | KD: 1060.0195\n",
      "Train Epoch: 013 Batch: 00045/00094 | Loss: 429.4628 | CE: 0.1563 | KD: 1060.0161\n",
      "Train Epoch: 013 Batch: 00046/00094 | Loss: 429.4790 | CE: 0.1753 | KD: 1060.0092\n",
      "Train Epoch: 013 Batch: 00047/00094 | Loss: 429.4440 | CE: 0.1521 | KD: 1059.9801\n",
      "Train Epoch: 013 Batch: 00048/00094 | Loss: 429.4809 | CE: 0.1752 | KD: 1060.0139\n",
      "Train Epoch: 013 Batch: 00049/00094 | Loss: 429.4634 | CE: 0.1502 | KD: 1060.0326\n",
      "Train Epoch: 013 Batch: 00050/00094 | Loss: 429.5017 | CE: 0.1749 | KD: 1060.0662\n",
      "Train Epoch: 013 Batch: 00051/00094 | Loss: 429.5056 | CE: 0.1890 | KD: 1060.0410\n",
      "Train Epoch: 013 Batch: 00052/00094 | Loss: 429.4986 | CE: 0.2018 | KD: 1059.9921\n",
      "Train Epoch: 013 Batch: 00053/00094 | Loss: 429.4691 | CE: 0.1581 | KD: 1060.0271\n",
      "Train Epoch: 013 Batch: 00054/00094 | Loss: 429.5207 | CE: 0.2193 | KD: 1060.0035\n",
      "Train Epoch: 013 Batch: 00055/00094 | Loss: 429.5922 | CE: 0.2651 | KD: 1060.0669\n",
      "Train Epoch: 013 Batch: 00056/00094 | Loss: 429.5020 | CE: 0.2139 | KD: 1059.9703\n",
      "Train Epoch: 013 Batch: 00057/00094 | Loss: 429.4763 | CE: 0.1785 | KD: 1059.9945\n",
      "Train Epoch: 013 Batch: 00058/00094 | Loss: 429.4940 | CE: 0.1839 | KD: 1060.0251\n",
      "Train Epoch: 013 Batch: 00059/00094 | Loss: 429.5273 | CE: 0.2120 | KD: 1060.0378\n",
      "Train Epoch: 013 Batch: 00060/00094 | Loss: 429.5125 | CE: 0.1991 | KD: 1060.0331\n",
      "Train Epoch: 013 Batch: 00061/00094 | Loss: 429.5137 | CE: 0.2147 | KD: 1059.9974\n",
      "Train Epoch: 013 Batch: 00062/00094 | Loss: 429.4704 | CE: 0.1580 | KD: 1060.0306\n",
      "Train Epoch: 013 Batch: 00063/00094 | Loss: 429.4842 | CE: 0.1697 | KD: 1060.0358\n",
      "Train Epoch: 013 Batch: 00064/00094 | Loss: 429.5468 | CE: 0.2323 | KD: 1060.0358\n",
      "Train Epoch: 013 Batch: 00065/00094 | Loss: 429.4908 | CE: 0.1926 | KD: 1059.9955\n",
      "Train Epoch: 013 Batch: 00066/00094 | Loss: 429.5046 | CE: 0.1843 | KD: 1060.0502\n",
      "Train Epoch: 013 Batch: 00067/00094 | Loss: 429.4766 | CE: 0.1643 | KD: 1060.0304\n",
      "Train Epoch: 013 Batch: 00068/00094 | Loss: 429.4937 | CE: 0.1777 | KD: 1060.0396\n",
      "Train Epoch: 013 Batch: 00069/00094 | Loss: 429.4915 | CE: 0.1856 | KD: 1060.0146\n",
      "Train Epoch: 013 Batch: 00070/00094 | Loss: 429.4698 | CE: 0.1594 | KD: 1060.0258\n",
      "Train Epoch: 013 Batch: 00071/00094 | Loss: 429.5145 | CE: 0.1921 | KD: 1060.0553\n",
      "Train Epoch: 013 Batch: 00072/00094 | Loss: 429.5349 | CE: 0.2052 | KD: 1060.0734\n",
      "Train Epoch: 013 Batch: 00073/00094 | Loss: 429.4452 | CE: 0.1518 | KD: 1059.9835\n",
      "Train Epoch: 013 Batch: 00074/00094 | Loss: 429.5495 | CE: 0.2469 | KD: 1060.0063\n",
      "Train Epoch: 013 Batch: 00075/00094 | Loss: 429.4864 | CE: 0.1739 | KD: 1060.0308\n",
      "Train Epoch: 013 Batch: 00076/00094 | Loss: 429.5124 | CE: 0.1995 | KD: 1060.0317\n",
      "Train Epoch: 013 Batch: 00077/00094 | Loss: 429.5315 | CE: 0.2047 | KD: 1060.0660\n",
      "Train Epoch: 013 Batch: 00078/00094 | Loss: 429.5104 | CE: 0.2101 | KD: 1060.0006\n",
      "Train Epoch: 013 Batch: 00079/00094 | Loss: 429.5156 | CE: 0.1683 | KD: 1060.1168\n",
      "Train Epoch: 013 Batch: 00080/00094 | Loss: 429.4447 | CE: 0.1382 | KD: 1060.0161\n",
      "Train Epoch: 013 Batch: 00081/00094 | Loss: 429.4765 | CE: 0.1553 | KD: 1060.0522\n",
      "Train Epoch: 013 Batch: 00082/00094 | Loss: 429.4354 | CE: 0.1299 | KD: 1060.0137\n",
      "Train Epoch: 013 Batch: 00083/00094 | Loss: 429.4978 | CE: 0.1746 | KD: 1060.0573\n",
      "Train Epoch: 013 Batch: 00084/00094 | Loss: 429.5446 | CE: 0.2331 | KD: 1060.0286\n",
      "Train Epoch: 013 Batch: 00085/00094 | Loss: 429.4604 | CE: 0.1564 | KD: 1060.0100\n",
      "Train Epoch: 013 Batch: 00086/00094 | Loss: 429.5011 | CE: 0.1960 | KD: 1060.0126\n",
      "Train Epoch: 013 Batch: 00087/00094 | Loss: 429.5104 | CE: 0.2054 | KD: 1060.0125\n",
      "Train Epoch: 013 Batch: 00088/00094 | Loss: 429.5008 | CE: 0.1946 | KD: 1060.0153\n",
      "Train Epoch: 013 Batch: 00089/00094 | Loss: 429.4415 | CE: 0.1438 | KD: 1059.9943\n",
      "Train Epoch: 013 Batch: 00090/00094 | Loss: 429.4856 | CE: 0.1808 | KD: 1060.0118\n",
      "Train Epoch: 013 Batch: 00091/00094 | Loss: 429.4298 | CE: 0.1300 | KD: 1059.9994\n",
      "Train Epoch: 013 Batch: 00092/00094 | Loss: 429.5131 | CE: 0.2001 | KD: 1060.0321\n",
      "Train Epoch: 013 Batch: 00093/00094 | Loss: 429.5392 | CE: 0.2197 | KD: 1060.0482\n",
      "Train Epoch: 013 Batch: 00094/00094 | Loss: 429.6368 | CE: 0.3119 | KD: 1060.0616\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1813 | acc:96.4000\n",
      "[VAL Acc] Target: 96.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1288 | acc:51.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6381 | acc:66.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 66.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9032 | acc:47.9008\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7879 | acc:57.2884\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.29%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5415 | acc:72.8281\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.83%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6421 | acc:70.2978\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7130 | acc:60.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.56%\n",
      "[VAL Acc] Avg 65.50%\n",
      "Train Epoch: 014 Batch: 00001/00094 | Loss: 386.5820 | CE: 0.1989 | KD: 1060.0358\n",
      "Train Epoch: 014 Batch: 00002/00094 | Loss: 386.5816 | CE: 0.1914 | KD: 1060.0554\n",
      "Train Epoch: 014 Batch: 00003/00094 | Loss: 386.5580 | CE: 0.1735 | KD: 1060.0399\n",
      "Train Epoch: 014 Batch: 00004/00094 | Loss: 386.5850 | CE: 0.2094 | KD: 1060.0154\n",
      "Train Epoch: 014 Batch: 00005/00094 | Loss: 386.5553 | CE: 0.1759 | KD: 1060.0259\n",
      "Train Epoch: 014 Batch: 00006/00094 | Loss: 386.5737 | CE: 0.1894 | KD: 1060.0394\n",
      "Train Epoch: 014 Batch: 00007/00094 | Loss: 386.5661 | CE: 0.1868 | KD: 1060.0255\n",
      "Train Epoch: 014 Batch: 00008/00094 | Loss: 386.5690 | CE: 0.2008 | KD: 1059.9952\n",
      "Train Epoch: 014 Batch: 00009/00094 | Loss: 386.5658 | CE: 0.1780 | KD: 1060.0490\n",
      "Train Epoch: 014 Batch: 00010/00094 | Loss: 386.5124 | CE: 0.1491 | KD: 1059.9814\n",
      "Train Epoch: 014 Batch: 00011/00094 | Loss: 386.5541 | CE: 0.1692 | KD: 1060.0409\n",
      "Train Epoch: 014 Batch: 00012/00094 | Loss: 386.5877 | CE: 0.2164 | KD: 1060.0038\n",
      "Train Epoch: 014 Batch: 00013/00094 | Loss: 386.5097 | CE: 0.1301 | KD: 1060.0265\n",
      "Train Epoch: 014 Batch: 00014/00094 | Loss: 386.6345 | CE: 0.2333 | KD: 1060.0856\n",
      "Train Epoch: 014 Batch: 00015/00094 | Loss: 386.5198 | CE: 0.1588 | KD: 1059.9753\n",
      "Train Epoch: 014 Batch: 00016/00094 | Loss: 386.5826 | CE: 0.1920 | KD: 1060.0565\n",
      "Train Epoch: 014 Batch: 00017/00094 | Loss: 386.5495 | CE: 0.1690 | KD: 1060.0288\n",
      "Train Epoch: 014 Batch: 00018/00094 | Loss: 386.5419 | CE: 0.1582 | KD: 1060.0377\n",
      "Train Epoch: 014 Batch: 00019/00094 | Loss: 386.5602 | CE: 0.1723 | KD: 1060.0493\n",
      "Train Epoch: 014 Batch: 00020/00094 | Loss: 386.5270 | CE: 0.1448 | KD: 1060.0336\n",
      "Train Epoch: 014 Batch: 00021/00094 | Loss: 386.5513 | CE: 0.1659 | KD: 1060.0424\n",
      "Train Epoch: 014 Batch: 00022/00094 | Loss: 386.5460 | CE: 0.1716 | KD: 1060.0120\n",
      "Train Epoch: 014 Batch: 00023/00094 | Loss: 386.5515 | CE: 0.1568 | KD: 1060.0679\n",
      "Train Epoch: 014 Batch: 00024/00094 | Loss: 386.5463 | CE: 0.1736 | KD: 1060.0073\n",
      "Train Epoch: 014 Batch: 00025/00094 | Loss: 386.5538 | CE: 0.1735 | KD: 1060.0283\n",
      "Train Epoch: 014 Batch: 00026/00094 | Loss: 386.5533 | CE: 0.1843 | KD: 1059.9972\n",
      "Train Epoch: 014 Batch: 00027/00094 | Loss: 386.5966 | CE: 0.2225 | KD: 1060.0112\n",
      "Train Epoch: 014 Batch: 00028/00094 | Loss: 386.5744 | CE: 0.2082 | KD: 1059.9896\n",
      "Train Epoch: 014 Batch: 00029/00094 | Loss: 386.5782 | CE: 0.1987 | KD: 1060.0261\n",
      "Train Epoch: 014 Batch: 00030/00094 | Loss: 386.5791 | CE: 0.2043 | KD: 1060.0132\n",
      "Train Epoch: 014 Batch: 00031/00094 | Loss: 386.5212 | CE: 0.1546 | KD: 1059.9907\n",
      "Train Epoch: 014 Batch: 00032/00094 | Loss: 386.5371 | CE: 0.1585 | KD: 1060.0236\n",
      "Train Epoch: 014 Batch: 00033/00094 | Loss: 386.5752 | CE: 0.1891 | KD: 1060.0441\n",
      "Train Epoch: 014 Batch: 00034/00094 | Loss: 386.5650 | CE: 0.1893 | KD: 1060.0156\n",
      "Train Epoch: 014 Batch: 00035/00094 | Loss: 386.5958 | CE: 0.2061 | KD: 1060.0540\n",
      "Train Epoch: 014 Batch: 00036/00094 | Loss: 386.5330 | CE: 0.1505 | KD: 1060.0342\n",
      "Train Epoch: 014 Batch: 00037/00094 | Loss: 386.5454 | CE: 0.1700 | KD: 1060.0149\n",
      "Train Epoch: 014 Batch: 00038/00094 | Loss: 386.5596 | CE: 0.1725 | KD: 1060.0469\n",
      "Train Epoch: 014 Batch: 00039/00094 | Loss: 386.5581 | CE: 0.1803 | KD: 1060.0215\n",
      "Train Epoch: 014 Batch: 00040/00094 | Loss: 386.5461 | CE: 0.1627 | KD: 1060.0367\n",
      "Train Epoch: 014 Batch: 00041/00094 | Loss: 386.5828 | CE: 0.1996 | KD: 1060.0363\n",
      "Train Epoch: 014 Batch: 00042/00094 | Loss: 386.5562 | CE: 0.1581 | KD: 1060.0773\n",
      "Train Epoch: 014 Batch: 00043/00094 | Loss: 386.6049 | CE: 0.2219 | KD: 1060.0358\n",
      "Train Epoch: 014 Batch: 00044/00094 | Loss: 386.5262 | CE: 0.1582 | KD: 1059.9946\n",
      "Train Epoch: 014 Batch: 00045/00094 | Loss: 386.5240 | CE: 0.1473 | KD: 1060.0187\n",
      "Train Epoch: 014 Batch: 00046/00094 | Loss: 386.5190 | CE: 0.1523 | KD: 1059.9910\n",
      "Train Epoch: 014 Batch: 00047/00094 | Loss: 386.5493 | CE: 0.1739 | KD: 1060.0150\n",
      "Train Epoch: 014 Batch: 00048/00094 | Loss: 386.5690 | CE: 0.1913 | KD: 1060.0214\n",
      "Train Epoch: 014 Batch: 00049/00094 | Loss: 386.5498 | CE: 0.1620 | KD: 1060.0488\n",
      "Train Epoch: 014 Batch: 00050/00094 | Loss: 386.6128 | CE: 0.2218 | KD: 1060.0575\n",
      "Train Epoch: 014 Batch: 00051/00094 | Loss: 386.5458 | CE: 0.1720 | KD: 1060.0104\n",
      "Train Epoch: 014 Batch: 00052/00094 | Loss: 386.5775 | CE: 0.1985 | KD: 1060.0249\n",
      "Train Epoch: 014 Batch: 00053/00094 | Loss: 386.5503 | CE: 0.1557 | KD: 1060.0674\n",
      "Train Epoch: 014 Batch: 00054/00094 | Loss: 386.5506 | CE: 0.1745 | KD: 1060.0167\n",
      "Train Epoch: 014 Batch: 00055/00094 | Loss: 386.5490 | CE: 0.1847 | KD: 1059.9844\n",
      "Train Epoch: 014 Batch: 00056/00094 | Loss: 386.5846 | CE: 0.1823 | KD: 1060.0889\n",
      "Train Epoch: 014 Batch: 00057/00094 | Loss: 386.6430 | CE: 0.2653 | KD: 1060.0212\n",
      "Train Epoch: 014 Batch: 00058/00094 | Loss: 386.5997 | CE: 0.2109 | KD: 1060.0516\n",
      "Train Epoch: 014 Batch: 00059/00094 | Loss: 386.5673 | CE: 0.1661 | KD: 1060.0856\n",
      "Train Epoch: 014 Batch: 00060/00094 | Loss: 386.5739 | CE: 0.1956 | KD: 1060.0227\n",
      "Train Epoch: 014 Batch: 00061/00094 | Loss: 386.5635 | CE: 0.1855 | KD: 1060.0220\n",
      "Train Epoch: 014 Batch: 00062/00094 | Loss: 386.5924 | CE: 0.2147 | KD: 1060.0212\n",
      "Train Epoch: 014 Batch: 00063/00094 | Loss: 386.5325 | CE: 0.1667 | KD: 1059.9884\n",
      "Train Epoch: 014 Batch: 00064/00094 | Loss: 386.5437 | CE: 0.1621 | KD: 1060.0320\n",
      "Train Epoch: 014 Batch: 00065/00094 | Loss: 386.5351 | CE: 0.1575 | KD: 1060.0208\n",
      "Train Epoch: 014 Batch: 00066/00094 | Loss: 386.5602 | CE: 0.1748 | KD: 1060.0422\n",
      "Train Epoch: 014 Batch: 00067/00094 | Loss: 386.6416 | CE: 0.2422 | KD: 1060.0806\n",
      "Train Epoch: 014 Batch: 00068/00094 | Loss: 386.5509 | CE: 0.1836 | KD: 1059.9926\n",
      "Train Epoch: 014 Batch: 00069/00094 | Loss: 386.5731 | CE: 0.1723 | KD: 1060.0846\n",
      "Train Epoch: 014 Batch: 00070/00094 | Loss: 386.5333 | CE: 0.1594 | KD: 1060.0109\n",
      "Train Epoch: 014 Batch: 00071/00094 | Loss: 386.5690 | CE: 0.1926 | KD: 1060.0175\n",
      "Train Epoch: 014 Batch: 00072/00094 | Loss: 386.6281 | CE: 0.2233 | KD: 1060.0953\n",
      "Train Epoch: 014 Batch: 00073/00094 | Loss: 386.5661 | CE: 0.1817 | KD: 1060.0396\n",
      "Train Epoch: 014 Batch: 00074/00094 | Loss: 386.5992 | CE: 0.1982 | KD: 1060.0851\n",
      "Train Epoch: 014 Batch: 00075/00094 | Loss: 386.5334 | CE: 0.1674 | KD: 1059.9893\n",
      "Train Epoch: 014 Batch: 00076/00094 | Loss: 386.6028 | CE: 0.2241 | KD: 1060.0239\n",
      "Train Epoch: 014 Batch: 00077/00094 | Loss: 386.5032 | CE: 0.1290 | KD: 1060.0114\n",
      "Train Epoch: 014 Batch: 00078/00094 | Loss: 386.6113 | CE: 0.2161 | KD: 1060.0692\n",
      "Train Epoch: 014 Batch: 00079/00094 | Loss: 386.5805 | CE: 0.1849 | KD: 1060.0702\n",
      "Train Epoch: 014 Batch: 00080/00094 | Loss: 386.5461 | CE: 0.1638 | KD: 1060.0339\n",
      "Train Epoch: 014 Batch: 00081/00094 | Loss: 386.5838 | CE: 0.2105 | KD: 1060.0090\n",
      "Train Epoch: 014 Batch: 00082/00094 | Loss: 386.5411 | CE: 0.1663 | KD: 1060.0133\n",
      "Train Epoch: 014 Batch: 00083/00094 | Loss: 386.5695 | CE: 0.1794 | KD: 1060.0552\n",
      "Train Epoch: 014 Batch: 00084/00094 | Loss: 386.6512 | CE: 0.2448 | KD: 1060.0997\n",
      "Train Epoch: 014 Batch: 00085/00094 | Loss: 386.5084 | CE: 0.1361 | KD: 1060.0061\n",
      "Train Epoch: 014 Batch: 00086/00094 | Loss: 386.5525 | CE: 0.1745 | KD: 1060.0219\n",
      "Train Epoch: 014 Batch: 00087/00094 | Loss: 386.6003 | CE: 0.2132 | KD: 1060.0471\n",
      "Train Epoch: 014 Batch: 00088/00094 | Loss: 386.5448 | CE: 0.1767 | KD: 1059.9948\n",
      "Train Epoch: 014 Batch: 00089/00094 | Loss: 386.6414 | CE: 0.2516 | KD: 1060.0543\n",
      "Train Epoch: 014 Batch: 00090/00094 | Loss: 386.5868 | CE: 0.1976 | KD: 1060.0527\n",
      "Train Epoch: 014 Batch: 00091/00094 | Loss: 386.5915 | CE: 0.2097 | KD: 1060.0323\n",
      "Train Epoch: 014 Batch: 00092/00094 | Loss: 386.5432 | CE: 0.1669 | KD: 1060.0172\n",
      "Train Epoch: 014 Batch: 00093/00094 | Loss: 386.5717 | CE: 0.1807 | KD: 1060.0579\n",
      "Train Epoch: 014 Batch: 00094/00094 | Loss: 386.6131 | CE: 0.2113 | KD: 1060.0872\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1729 | acc:96.2500\n",
      "[VAL Acc] Target: 96.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1652 | acc:51.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6314 | acc:68.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9209 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8183 | acc:57.0925\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5243 | acc:74.9538\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6431 | acc:70.3370\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7144 | acc:59.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.69%\n",
      "[VAL Acc] Avg 65.72%\n",
      "Train Epoch: 015 Batch: 00001/00094 | Loss: 386.5767 | CE: 0.1933 | KD: 1060.0369\n",
      "Train Epoch: 015 Batch: 00002/00094 | Loss: 386.6229 | CE: 0.2415 | KD: 1060.0312\n",
      "Train Epoch: 015 Batch: 00003/00094 | Loss: 386.6479 | CE: 0.2380 | KD: 1060.1097\n",
      "Train Epoch: 015 Batch: 00004/00094 | Loss: 386.5458 | CE: 0.1712 | KD: 1060.0127\n",
      "Train Epoch: 015 Batch: 00005/00094 | Loss: 386.5384 | CE: 0.1349 | KD: 1060.0918\n",
      "Train Epoch: 015 Batch: 00006/00094 | Loss: 386.6265 | CE: 0.2246 | KD: 1060.0874\n",
      "Train Epoch: 015 Batch: 00007/00094 | Loss: 386.6149 | CE: 0.2056 | KD: 1060.1079\n",
      "Train Epoch: 015 Batch: 00008/00094 | Loss: 386.5605 | CE: 0.1654 | KD: 1060.0690\n",
      "Train Epoch: 015 Batch: 00009/00094 | Loss: 386.6422 | CE: 0.2576 | KD: 1060.0400\n",
      "Train Epoch: 015 Batch: 00010/00094 | Loss: 386.5033 | CE: 0.1352 | KD: 1059.9948\n",
      "Train Epoch: 015 Batch: 00011/00094 | Loss: 386.5836 | CE: 0.1986 | KD: 1060.0411\n",
      "Train Epoch: 015 Batch: 00012/00094 | Loss: 386.5566 | CE: 0.1916 | KD: 1059.9862\n",
      "Train Epoch: 015 Batch: 00013/00094 | Loss: 386.5499 | CE: 0.1692 | KD: 1060.0295\n",
      "Train Epoch: 015 Batch: 00014/00094 | Loss: 386.5921 | CE: 0.2077 | KD: 1060.0396\n",
      "Train Epoch: 015 Batch: 00015/00094 | Loss: 386.5854 | CE: 0.2027 | KD: 1060.0349\n",
      "Train Epoch: 015 Batch: 00016/00094 | Loss: 386.5337 | CE: 0.1467 | KD: 1060.0465\n",
      "Train Epoch: 015 Batch: 00017/00094 | Loss: 386.6289 | CE: 0.2044 | KD: 1060.1497\n",
      "Train Epoch: 015 Batch: 00018/00094 | Loss: 386.5774 | CE: 0.1956 | KD: 1060.0325\n",
      "Train Epoch: 015 Batch: 00019/00094 | Loss: 386.5600 | CE: 0.1634 | KD: 1060.0731\n",
      "Train Epoch: 015 Batch: 00020/00094 | Loss: 386.5770 | CE: 0.2107 | KD: 1059.9899\n",
      "Train Epoch: 015 Batch: 00021/00094 | Loss: 386.5416 | CE: 0.1724 | KD: 1059.9979\n",
      "Train Epoch: 015 Batch: 00022/00094 | Loss: 386.5629 | CE: 0.1785 | KD: 1060.0394\n",
      "Train Epoch: 015 Batch: 00023/00094 | Loss: 386.5419 | CE: 0.1886 | KD: 1059.9543\n",
      "Train Epoch: 015 Batch: 00024/00094 | Loss: 386.5723 | CE: 0.1776 | KD: 1060.0677\n",
      "Train Epoch: 015 Batch: 00025/00094 | Loss: 386.5407 | CE: 0.1392 | KD: 1060.0864\n",
      "Train Epoch: 015 Batch: 00026/00094 | Loss: 386.5436 | CE: 0.1689 | KD: 1060.0129\n",
      "Train Epoch: 015 Batch: 00027/00094 | Loss: 386.5236 | CE: 0.1412 | KD: 1060.0342\n",
      "Train Epoch: 015 Batch: 00028/00094 | Loss: 386.6374 | CE: 0.2483 | KD: 1060.0522\n",
      "Train Epoch: 015 Batch: 00029/00094 | Loss: 386.5275 | CE: 0.1378 | KD: 1060.0542\n",
      "Train Epoch: 015 Batch: 00030/00094 | Loss: 386.5781 | CE: 0.1917 | KD: 1060.0452\n",
      "Train Epoch: 015 Batch: 00031/00094 | Loss: 386.5807 | CE: 0.2003 | KD: 1060.0287\n",
      "Train Epoch: 015 Batch: 00032/00094 | Loss: 386.5747 | CE: 0.1873 | KD: 1060.0480\n",
      "Train Epoch: 015 Batch: 00033/00094 | Loss: 386.5299 | CE: 0.1529 | KD: 1060.0193\n",
      "Train Epoch: 015 Batch: 00034/00094 | Loss: 386.5648 | CE: 0.1895 | KD: 1060.0145\n",
      "Train Epoch: 015 Batch: 00035/00094 | Loss: 386.5663 | CE: 0.1996 | KD: 1059.9911\n",
      "Train Epoch: 015 Batch: 00036/00094 | Loss: 386.5624 | CE: 0.1837 | KD: 1060.0239\n",
      "Train Epoch: 015 Batch: 00037/00094 | Loss: 386.5849 | CE: 0.2053 | KD: 1060.0265\n",
      "Train Epoch: 015 Batch: 00038/00094 | Loss: 386.5482 | CE: 0.1779 | KD: 1060.0011\n",
      "Train Epoch: 015 Batch: 00039/00094 | Loss: 386.5934 | CE: 0.1852 | KD: 1060.1050\n",
      "Train Epoch: 015 Batch: 00040/00094 | Loss: 386.5667 | CE: 0.1731 | KD: 1060.0648\n",
      "Train Epoch: 015 Batch: 00041/00094 | Loss: 386.5772 | CE: 0.1986 | KD: 1060.0238\n",
      "Train Epoch: 015 Batch: 00042/00094 | Loss: 386.5648 | CE: 0.1890 | KD: 1060.0157\n",
      "Train Epoch: 015 Batch: 00043/00094 | Loss: 386.5331 | CE: 0.1408 | KD: 1060.0614\n",
      "Train Epoch: 015 Batch: 00044/00094 | Loss: 386.5482 | CE: 0.1693 | KD: 1060.0245\n",
      "Train Epoch: 015 Batch: 00045/00094 | Loss: 386.5179 | CE: 0.1537 | KD: 1059.9841\n",
      "Train Epoch: 015 Batch: 00046/00094 | Loss: 386.6183 | CE: 0.2315 | KD: 1060.0459\n",
      "Train Epoch: 015 Batch: 00047/00094 | Loss: 386.5398 | CE: 0.1579 | KD: 1060.0326\n",
      "Train Epoch: 015 Batch: 00048/00094 | Loss: 386.5502 | CE: 0.1794 | KD: 1060.0022\n",
      "Train Epoch: 015 Batch: 00049/00094 | Loss: 386.5616 | CE: 0.1819 | KD: 1060.0265\n",
      "Train Epoch: 015 Batch: 00050/00094 | Loss: 386.5935 | CE: 0.1826 | KD: 1060.1123\n",
      "Train Epoch: 015 Batch: 00051/00094 | Loss: 386.5634 | CE: 0.1717 | KD: 1060.0597\n",
      "Train Epoch: 015 Batch: 00052/00094 | Loss: 386.5640 | CE: 0.1733 | KD: 1060.0570\n",
      "Train Epoch: 015 Batch: 00053/00094 | Loss: 386.5423 | CE: 0.1569 | KD: 1060.0421\n",
      "Train Epoch: 015 Batch: 00054/00094 | Loss: 386.4977 | CE: 0.1366 | KD: 1059.9755\n",
      "Train Epoch: 015 Batch: 00055/00094 | Loss: 386.5785 | CE: 0.1893 | KD: 1060.0529\n",
      "Train Epoch: 015 Batch: 00056/00094 | Loss: 386.5645 | CE: 0.1814 | KD: 1060.0360\n",
      "Train Epoch: 015 Batch: 00057/00094 | Loss: 386.6022 | CE: 0.2190 | KD: 1060.0364\n",
      "Train Epoch: 015 Batch: 00058/00094 | Loss: 386.5386 | CE: 0.1671 | KD: 1060.0043\n",
      "Train Epoch: 015 Batch: 00059/00094 | Loss: 386.5478 | CE: 0.1811 | KD: 1059.9910\n",
      "Train Epoch: 015 Batch: 00060/00094 | Loss: 386.5366 | CE: 0.1560 | KD: 1060.0292\n",
      "Train Epoch: 015 Batch: 00061/00094 | Loss: 386.5464 | CE: 0.1468 | KD: 1060.0812\n",
      "Train Epoch: 015 Batch: 00062/00094 | Loss: 386.6012 | CE: 0.2147 | KD: 1060.0452\n",
      "Train Epoch: 015 Batch: 00063/00094 | Loss: 386.6063 | CE: 0.2111 | KD: 1060.0691\n",
      "Train Epoch: 015 Batch: 00064/00094 | Loss: 386.5428 | CE: 0.1643 | KD: 1060.0236\n",
      "Train Epoch: 015 Batch: 00065/00094 | Loss: 386.5521 | CE: 0.1808 | KD: 1060.0035\n",
      "Train Epoch: 015 Batch: 00066/00094 | Loss: 386.5254 | CE: 0.1424 | KD: 1060.0358\n",
      "Train Epoch: 015 Batch: 00067/00094 | Loss: 386.5179 | CE: 0.1466 | KD: 1060.0038\n",
      "Train Epoch: 015 Batch: 00068/00094 | Loss: 386.5528 | CE: 0.1682 | KD: 1060.0402\n",
      "Train Epoch: 015 Batch: 00069/00094 | Loss: 386.6479 | CE: 0.2520 | KD: 1060.0712\n",
      "Train Epoch: 015 Batch: 00070/00094 | Loss: 386.5676 | CE: 0.1765 | KD: 1060.0577\n",
      "Train Epoch: 015 Batch: 00071/00094 | Loss: 386.5540 | CE: 0.1806 | KD: 1060.0095\n",
      "Train Epoch: 015 Batch: 00072/00094 | Loss: 386.5890 | CE: 0.1884 | KD: 1060.0840\n",
      "Train Epoch: 015 Batch: 00073/00094 | Loss: 386.5355 | CE: 0.1604 | KD: 1060.0139\n",
      "Train Epoch: 015 Batch: 00074/00094 | Loss: 386.6322 | CE: 0.2283 | KD: 1060.0929\n",
      "Train Epoch: 015 Batch: 00075/00094 | Loss: 386.5375 | CE: 0.1449 | KD: 1060.0623\n",
      "Train Epoch: 015 Batch: 00076/00094 | Loss: 386.6020 | CE: 0.2009 | KD: 1060.0853\n",
      "Train Epoch: 015 Batch: 00077/00094 | Loss: 386.5694 | CE: 0.1922 | KD: 1060.0199\n",
      "Train Epoch: 015 Batch: 00078/00094 | Loss: 386.5475 | CE: 0.1700 | KD: 1060.0208\n",
      "Train Epoch: 015 Batch: 00079/00094 | Loss: 386.5448 | CE: 0.1527 | KD: 1060.0605\n",
      "Train Epoch: 015 Batch: 00080/00094 | Loss: 386.6442 | CE: 0.2469 | KD: 1060.0750\n",
      "Train Epoch: 015 Batch: 00081/00094 | Loss: 386.5211 | CE: 0.1396 | KD: 1060.0316\n",
      "Train Epoch: 015 Batch: 00082/00094 | Loss: 386.5831 | CE: 0.1747 | KD: 1060.1052\n",
      "Train Epoch: 015 Batch: 00083/00094 | Loss: 386.5685 | CE: 0.1925 | KD: 1060.0165\n",
      "Train Epoch: 015 Batch: 00084/00094 | Loss: 386.5624 | CE: 0.1578 | KD: 1060.0950\n",
      "Train Epoch: 015 Batch: 00085/00094 | Loss: 386.5642 | CE: 0.1855 | KD: 1060.0237\n",
      "Train Epoch: 015 Batch: 00086/00094 | Loss: 386.5450 | CE: 0.1621 | KD: 1060.0356\n",
      "Train Epoch: 015 Batch: 00087/00094 | Loss: 386.4997 | CE: 0.1362 | KD: 1059.9822\n",
      "Train Epoch: 015 Batch: 00088/00094 | Loss: 386.5651 | CE: 0.1715 | KD: 1060.0648\n",
      "Train Epoch: 015 Batch: 00089/00094 | Loss: 386.6111 | CE: 0.2135 | KD: 1060.0758\n",
      "Train Epoch: 015 Batch: 00090/00094 | Loss: 386.5753 | CE: 0.1803 | KD: 1060.0687\n",
      "Train Epoch: 015 Batch: 00091/00094 | Loss: 386.5587 | CE: 0.1734 | KD: 1060.0421\n",
      "Train Epoch: 015 Batch: 00092/00094 | Loss: 386.5384 | CE: 0.1470 | KD: 1060.0588\n",
      "Train Epoch: 015 Batch: 00093/00094 | Loss: 386.5645 | CE: 0.1871 | KD: 1060.0203\n",
      "Train Epoch: 015 Batch: 00094/00094 | Loss: 386.5568 | CE: 0.1790 | KD: 1060.0214\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1636 | acc:96.8000\n",
      "[VAL Acc] Target: 96.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1679 | acc:51.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6523 | acc:66.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 66.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8745 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8316 | acc:56.7006\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5363 | acc:72.6433\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6626 | acc:70.3762\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7158 | acc:60.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.50%\n",
      "[VAL Acc] Avg 65.55%\n",
      "Train Epoch: 016 Batch: 00001/00094 | Loss: 386.5653 | CE: 0.1640 | KD: 1060.0858\n",
      "Train Epoch: 016 Batch: 00002/00094 | Loss: 386.5954 | CE: 0.2153 | KD: 1060.0278\n",
      "Train Epoch: 016 Batch: 00003/00094 | Loss: 386.6319 | CE: 0.2342 | KD: 1060.0760\n",
      "Train Epoch: 016 Batch: 00004/00094 | Loss: 386.6128 | CE: 0.2009 | KD: 1060.1149\n",
      "Train Epoch: 016 Batch: 00005/00094 | Loss: 386.6008 | CE: 0.1994 | KD: 1060.0863\n",
      "Train Epoch: 016 Batch: 00006/00094 | Loss: 386.5446 | CE: 0.1566 | KD: 1060.0493\n",
      "Train Epoch: 016 Batch: 00007/00094 | Loss: 386.5319 | CE: 0.1372 | KD: 1060.0679\n",
      "Train Epoch: 016 Batch: 00008/00094 | Loss: 386.5276 | CE: 0.1495 | KD: 1060.0223\n",
      "Train Epoch: 016 Batch: 00009/00094 | Loss: 386.5809 | CE: 0.2099 | KD: 1060.0027\n",
      "Train Epoch: 016 Batch: 00010/00094 | Loss: 386.6197 | CE: 0.2385 | KD: 1060.0306\n",
      "Train Epoch: 016 Batch: 00011/00094 | Loss: 386.5482 | CE: 0.1662 | KD: 1060.0331\n",
      "Train Epoch: 016 Batch: 00012/00094 | Loss: 386.5652 | CE: 0.1717 | KD: 1060.0645\n",
      "Train Epoch: 016 Batch: 00013/00094 | Loss: 386.5191 | CE: 0.1536 | KD: 1059.9877\n",
      "Train Epoch: 016 Batch: 00014/00094 | Loss: 386.5246 | CE: 0.1469 | KD: 1060.0212\n",
      "Train Epoch: 016 Batch: 00015/00094 | Loss: 386.5114 | CE: 0.1487 | KD: 1059.9800\n",
      "Train Epoch: 016 Batch: 00016/00094 | Loss: 386.5776 | CE: 0.1757 | KD: 1060.0876\n",
      "Train Epoch: 016 Batch: 00017/00094 | Loss: 386.5458 | CE: 0.1650 | KD: 1060.0295\n",
      "Train Epoch: 016 Batch: 00018/00094 | Loss: 386.5278 | CE: 0.1391 | KD: 1060.0513\n",
      "Train Epoch: 016 Batch: 00019/00094 | Loss: 386.6833 | CE: 0.2722 | KD: 1060.1129\n",
      "Train Epoch: 016 Batch: 00020/00094 | Loss: 386.5435 | CE: 0.1526 | KD: 1060.0573\n",
      "Train Epoch: 016 Batch: 00021/00094 | Loss: 386.6001 | CE: 0.1851 | KD: 1060.1235\n",
      "Train Epoch: 016 Batch: 00022/00094 | Loss: 386.5956 | CE: 0.2130 | KD: 1060.0345\n",
      "Train Epoch: 016 Batch: 00023/00094 | Loss: 386.5394 | CE: 0.1632 | KD: 1060.0171\n",
      "Train Epoch: 016 Batch: 00024/00094 | Loss: 386.5366 | CE: 0.1476 | KD: 1060.0522\n",
      "Train Epoch: 016 Batch: 00025/00094 | Loss: 386.5838 | CE: 0.1756 | KD: 1060.1049\n",
      "Train Epoch: 016 Batch: 00026/00094 | Loss: 386.5182 | CE: 0.1234 | KD: 1060.0682\n",
      "Train Epoch: 016 Batch: 00027/00094 | Loss: 386.5380 | CE: 0.1504 | KD: 1060.0483\n",
      "Train Epoch: 016 Batch: 00028/00094 | Loss: 386.5924 | CE: 0.1897 | KD: 1060.0896\n",
      "Train Epoch: 016 Batch: 00029/00094 | Loss: 386.5639 | CE: 0.1761 | KD: 1060.0491\n",
      "Train Epoch: 016 Batch: 00030/00094 | Loss: 386.5500 | CE: 0.1626 | KD: 1060.0479\n",
      "Train Epoch: 016 Batch: 00031/00094 | Loss: 386.5796 | CE: 0.2001 | KD: 1060.0260\n",
      "Train Epoch: 016 Batch: 00032/00094 | Loss: 386.5614 | CE: 0.1807 | KD: 1060.0293\n",
      "Train Epoch: 016 Batch: 00033/00094 | Loss: 386.5278 | CE: 0.1640 | KD: 1059.9832\n",
      "Train Epoch: 016 Batch: 00034/00094 | Loss: 386.5454 | CE: 0.1533 | KD: 1060.0605\n",
      "Train Epoch: 016 Batch: 00035/00094 | Loss: 386.5925 | CE: 0.2040 | KD: 1060.0508\n",
      "Train Epoch: 016 Batch: 00036/00094 | Loss: 386.5836 | CE: 0.1965 | KD: 1060.0470\n",
      "Train Epoch: 016 Batch: 00037/00094 | Loss: 386.5260 | CE: 0.1577 | KD: 1059.9955\n",
      "Train Epoch: 016 Batch: 00038/00094 | Loss: 386.5391 | CE: 0.1540 | KD: 1060.0414\n",
      "Train Epoch: 016 Batch: 00039/00094 | Loss: 386.5367 | CE: 0.1575 | KD: 1060.0254\n",
      "Train Epoch: 016 Batch: 00040/00094 | Loss: 386.5393 | CE: 0.1603 | KD: 1060.0248\n",
      "Train Epoch: 016 Batch: 00041/00094 | Loss: 386.5354 | CE: 0.1579 | KD: 1060.0206\n",
      "Train Epoch: 016 Batch: 00042/00094 | Loss: 386.5730 | CE: 0.1759 | KD: 1060.0745\n",
      "Train Epoch: 016 Batch: 00043/00094 | Loss: 386.5630 | CE: 0.1649 | KD: 1060.0771\n",
      "Train Epoch: 016 Batch: 00044/00094 | Loss: 386.4901 | CE: 0.1296 | KD: 1059.9742\n",
      "Train Epoch: 016 Batch: 00045/00094 | Loss: 386.5719 | CE: 0.1766 | KD: 1060.0695\n",
      "Train Epoch: 016 Batch: 00046/00094 | Loss: 386.5585 | CE: 0.1692 | KD: 1060.0532\n",
      "Train Epoch: 016 Batch: 00047/00094 | Loss: 386.5717 | CE: 0.1743 | KD: 1060.0752\n",
      "Train Epoch: 016 Batch: 00048/00094 | Loss: 386.5657 | CE: 0.1884 | KD: 1060.0201\n",
      "Train Epoch: 016 Batch: 00049/00094 | Loss: 386.5154 | CE: 0.1485 | KD: 1059.9912\n",
      "Train Epoch: 016 Batch: 00050/00094 | Loss: 386.5357 | CE: 0.1485 | KD: 1060.0472\n",
      "Train Epoch: 016 Batch: 00051/00094 | Loss: 386.6333 | CE: 0.2256 | KD: 1060.1034\n",
      "Train Epoch: 016 Batch: 00052/00094 | Loss: 386.5865 | CE: 0.2007 | KD: 1060.0433\n",
      "Train Epoch: 016 Batch: 00053/00094 | Loss: 386.5403 | CE: 0.1607 | KD: 1060.0265\n",
      "Train Epoch: 016 Batch: 00054/00094 | Loss: 386.5914 | CE: 0.2124 | KD: 1060.0247\n",
      "Train Epoch: 016 Batch: 00055/00094 | Loss: 386.5781 | CE: 0.1970 | KD: 1060.0306\n",
      "Train Epoch: 016 Batch: 00056/00094 | Loss: 386.5925 | CE: 0.2070 | KD: 1060.0427\n",
      "Train Epoch: 016 Batch: 00057/00094 | Loss: 386.5321 | CE: 0.1444 | KD: 1060.0487\n",
      "Train Epoch: 016 Batch: 00058/00094 | Loss: 386.5267 | CE: 0.1568 | KD: 1059.9998\n",
      "Train Epoch: 016 Batch: 00059/00094 | Loss: 386.5646 | CE: 0.1768 | KD: 1060.0490\n",
      "Train Epoch: 016 Batch: 00060/00094 | Loss: 386.5558 | CE: 0.1600 | KD: 1060.0707\n",
      "Train Epoch: 016 Batch: 00061/00094 | Loss: 386.5777 | CE: 0.1881 | KD: 1060.0536\n",
      "Train Epoch: 016 Batch: 00062/00094 | Loss: 386.7025 | CE: 0.3016 | KD: 1060.0847\n",
      "Train Epoch: 016 Batch: 00063/00094 | Loss: 386.5732 | CE: 0.1877 | KD: 1060.0425\n",
      "Train Epoch: 016 Batch: 00064/00094 | Loss: 386.5308 | CE: 0.1480 | KD: 1060.0352\n",
      "Train Epoch: 016 Batch: 00065/00094 | Loss: 386.5648 | CE: 0.1584 | KD: 1060.0999\n",
      "Train Epoch: 016 Batch: 00066/00094 | Loss: 386.5582 | CE: 0.1555 | KD: 1060.0898\n",
      "Train Epoch: 016 Batch: 00067/00094 | Loss: 386.5754 | CE: 0.1703 | KD: 1060.0963\n",
      "Train Epoch: 016 Batch: 00068/00094 | Loss: 386.5877 | CE: 0.1959 | KD: 1060.0598\n",
      "Train Epoch: 016 Batch: 00069/00094 | Loss: 386.5131 | CE: 0.1428 | KD: 1060.0007\n",
      "Train Epoch: 016 Batch: 00070/00094 | Loss: 386.5322 | CE: 0.1541 | KD: 1060.0225\n",
      "Train Epoch: 016 Batch: 00071/00094 | Loss: 386.5621 | CE: 0.1885 | KD: 1060.0100\n",
      "Train Epoch: 016 Batch: 00072/00094 | Loss: 386.5625 | CE: 0.1863 | KD: 1060.0172\n",
      "Train Epoch: 016 Batch: 00073/00094 | Loss: 386.4925 | CE: 0.1058 | KD: 1060.0457\n",
      "Train Epoch: 016 Batch: 00074/00094 | Loss: 386.5227 | CE: 0.1634 | KD: 1059.9707\n",
      "Train Epoch: 016 Batch: 00075/00094 | Loss: 386.5573 | CE: 0.1685 | KD: 1060.0518\n",
      "Train Epoch: 016 Batch: 00076/00094 | Loss: 386.5954 | CE: 0.1736 | KD: 1060.1422\n",
      "Train Epoch: 016 Batch: 00077/00094 | Loss: 386.6192 | CE: 0.2342 | KD: 1060.0413\n",
      "Train Epoch: 016 Batch: 00078/00094 | Loss: 386.5415 | CE: 0.1353 | KD: 1060.0991\n",
      "Train Epoch: 016 Batch: 00079/00094 | Loss: 386.5518 | CE: 0.1568 | KD: 1060.0685\n",
      "Train Epoch: 016 Batch: 00080/00094 | Loss: 386.5324 | CE: 0.1660 | KD: 1059.9904\n",
      "Train Epoch: 016 Batch: 00081/00094 | Loss: 386.5529 | CE: 0.1631 | KD: 1060.0544\n",
      "Train Epoch: 016 Batch: 00082/00094 | Loss: 386.5904 | CE: 0.1878 | KD: 1060.0895\n",
      "Train Epoch: 016 Batch: 00083/00094 | Loss: 386.5709 | CE: 0.1784 | KD: 1060.0619\n",
      "Train Epoch: 016 Batch: 00084/00094 | Loss: 386.5695 | CE: 0.1688 | KD: 1060.0844\n",
      "Train Epoch: 016 Batch: 00085/00094 | Loss: 386.5877 | CE: 0.2080 | KD: 1060.0266\n",
      "Train Epoch: 016 Batch: 00086/00094 | Loss: 386.5649 | CE: 0.1801 | KD: 1060.0405\n",
      "Train Epoch: 016 Batch: 00087/00094 | Loss: 386.5788 | CE: 0.1916 | KD: 1060.0472\n",
      "Train Epoch: 016 Batch: 00088/00094 | Loss: 386.5510 | CE: 0.1650 | KD: 1060.0441\n",
      "Train Epoch: 016 Batch: 00089/00094 | Loss: 386.5359 | CE: 0.1677 | KD: 1059.9950\n",
      "Train Epoch: 016 Batch: 00090/00094 | Loss: 386.5883 | CE: 0.1985 | KD: 1060.0541\n",
      "Train Epoch: 016 Batch: 00091/00094 | Loss: 386.5493 | CE: 0.1633 | KD: 1060.0438\n",
      "Train Epoch: 016 Batch: 00092/00094 | Loss: 386.6134 | CE: 0.2044 | KD: 1060.1071\n",
      "Train Epoch: 016 Batch: 00093/00094 | Loss: 386.6506 | CE: 0.2470 | KD: 1060.0923\n",
      "Train Epoch: 016 Batch: 00094/00094 | Loss: 386.5888 | CE: 0.1804 | KD: 1060.1053\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1635 | acc:96.6500\n",
      "[VAL Acc] Target: 96.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1587 | acc:51.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6463 | acc:66.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 66.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8957 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8046 | acc:58.4248\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5550 | acc:73.7523\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6213 | acc:72.5313\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7228 | acc:60.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.50%\n",
      "[VAL Acc] Avg 66.10%\n",
      "Train Epoch: 017 Batch: 00001/00094 | Loss: 386.5563 | CE: 0.1664 | KD: 1060.0547\n",
      "Train Epoch: 017 Batch: 00002/00094 | Loss: 386.5850 | CE: 0.1898 | KD: 1060.0691\n",
      "Train Epoch: 017 Batch: 00003/00094 | Loss: 386.5654 | CE: 0.1987 | KD: 1059.9910\n",
      "Train Epoch: 017 Batch: 00004/00094 | Loss: 386.5838 | CE: 0.1862 | KD: 1060.0758\n",
      "Train Epoch: 017 Batch: 00005/00094 | Loss: 386.5674 | CE: 0.1784 | KD: 1060.0522\n",
      "Train Epoch: 017 Batch: 00006/00094 | Loss: 386.6216 | CE: 0.2161 | KD: 1060.0975\n",
      "Train Epoch: 017 Batch: 00007/00094 | Loss: 386.5764 | CE: 0.1750 | KD: 1060.0861\n",
      "Train Epoch: 017 Batch: 00008/00094 | Loss: 386.5946 | CE: 0.2054 | KD: 1060.0529\n",
      "Train Epoch: 017 Batch: 00009/00094 | Loss: 386.5566 | CE: 0.1543 | KD: 1060.0887\n",
      "Train Epoch: 017 Batch: 00010/00094 | Loss: 386.6063 | CE: 0.2141 | KD: 1060.0612\n",
      "Train Epoch: 017 Batch: 00011/00094 | Loss: 386.5833 | CE: 0.2071 | KD: 1060.0171\n",
      "Train Epoch: 017 Batch: 00012/00094 | Loss: 386.5159 | CE: 0.1252 | KD: 1060.0568\n",
      "Train Epoch: 017 Batch: 00013/00094 | Loss: 386.5540 | CE: 0.1657 | KD: 1060.0504\n",
      "Train Epoch: 017 Batch: 00014/00094 | Loss: 386.5531 | CE: 0.1626 | KD: 1060.0564\n",
      "Train Epoch: 017 Batch: 00015/00094 | Loss: 386.5401 | CE: 0.1642 | KD: 1060.0162\n",
      "Train Epoch: 017 Batch: 00016/00094 | Loss: 386.5116 | CE: 0.1341 | KD: 1060.0206\n",
      "Train Epoch: 017 Batch: 00017/00094 | Loss: 386.5159 | CE: 0.1342 | KD: 1060.0320\n",
      "Train Epoch: 017 Batch: 00018/00094 | Loss: 386.6017 | CE: 0.2069 | KD: 1060.0681\n",
      "Train Epoch: 017 Batch: 00019/00094 | Loss: 386.6157 | CE: 0.2325 | KD: 1060.0364\n",
      "Train Epoch: 017 Batch: 00020/00094 | Loss: 386.5565 | CE: 0.1850 | KD: 1060.0042\n",
      "Train Epoch: 017 Batch: 00021/00094 | Loss: 386.5370 | CE: 0.1554 | KD: 1060.0317\n",
      "Train Epoch: 017 Batch: 00022/00094 | Loss: 386.5447 | CE: 0.1657 | KD: 1060.0249\n",
      "Train Epoch: 017 Batch: 00023/00094 | Loss: 386.5207 | CE: 0.1547 | KD: 1059.9890\n",
      "Train Epoch: 017 Batch: 00024/00094 | Loss: 386.6269 | CE: 0.2374 | KD: 1060.0535\n",
      "Train Epoch: 017 Batch: 00025/00094 | Loss: 386.6470 | CE: 0.2347 | KD: 1060.1161\n",
      "Train Epoch: 017 Batch: 00026/00094 | Loss: 386.5218 | CE: 0.1522 | KD: 1059.9989\n",
      "Train Epoch: 017 Batch: 00027/00094 | Loss: 386.5850 | CE: 0.1917 | KD: 1060.0640\n",
      "Train Epoch: 017 Batch: 00028/00094 | Loss: 386.6142 | CE: 0.2037 | KD: 1060.1113\n",
      "Train Epoch: 017 Batch: 00029/00094 | Loss: 386.5493 | CE: 0.1602 | KD: 1060.0524\n",
      "Train Epoch: 017 Batch: 00030/00094 | Loss: 386.4948 | CE: 0.1240 | KD: 1060.0020\n",
      "Train Epoch: 017 Batch: 00031/00094 | Loss: 386.5843 | CE: 0.2070 | KD: 1060.0200\n",
      "Train Epoch: 017 Batch: 00032/00094 | Loss: 386.6147 | CE: 0.2359 | KD: 1060.0240\n",
      "Train Epoch: 017 Batch: 00033/00094 | Loss: 386.5535 | CE: 0.1869 | KD: 1059.9907\n",
      "Train Epoch: 017 Batch: 00034/00094 | Loss: 386.5292 | CE: 0.1234 | KD: 1060.0983\n",
      "Train Epoch: 017 Batch: 00035/00094 | Loss: 386.5371 | CE: 0.1510 | KD: 1060.0442\n",
      "Train Epoch: 017 Batch: 00036/00094 | Loss: 386.5471 | CE: 0.1644 | KD: 1060.0349\n",
      "Train Epoch: 017 Batch: 00037/00094 | Loss: 386.5014 | CE: 0.1258 | KD: 1060.0153\n",
      "Train Epoch: 017 Batch: 00038/00094 | Loss: 386.5671 | CE: 0.1887 | KD: 1060.0232\n",
      "Train Epoch: 017 Batch: 00039/00094 | Loss: 386.5280 | CE: 0.1411 | KD: 1060.0466\n",
      "Train Epoch: 017 Batch: 00040/00094 | Loss: 386.4992 | CE: 0.1135 | KD: 1060.0432\n",
      "Train Epoch: 017 Batch: 00041/00094 | Loss: 386.5497 | CE: 0.1648 | KD: 1060.0409\n",
      "Train Epoch: 017 Batch: 00042/00094 | Loss: 386.5475 | CE: 0.1589 | KD: 1060.0511\n",
      "Train Epoch: 017 Batch: 00043/00094 | Loss: 386.5570 | CE: 0.1629 | KD: 1060.0663\n",
      "Train Epoch: 017 Batch: 00044/00094 | Loss: 386.5335 | CE: 0.1437 | KD: 1060.0544\n",
      "Train Epoch: 017 Batch: 00045/00094 | Loss: 386.6279 | CE: 0.2294 | KD: 1060.0782\n",
      "Train Epoch: 017 Batch: 00046/00094 | Loss: 386.5578 | CE: 0.1644 | KD: 1060.0643\n",
      "Train Epoch: 017 Batch: 00047/00094 | Loss: 386.5737 | CE: 0.1965 | KD: 1060.0198\n",
      "Train Epoch: 017 Batch: 00048/00094 | Loss: 386.5670 | CE: 0.1820 | KD: 1060.0413\n",
      "Train Epoch: 017 Batch: 00049/00094 | Loss: 386.5456 | CE: 0.1654 | KD: 1060.0281\n",
      "Train Epoch: 017 Batch: 00050/00094 | Loss: 386.5442 | CE: 0.1542 | KD: 1060.0549\n",
      "Train Epoch: 017 Batch: 00051/00094 | Loss: 386.5203 | CE: 0.1382 | KD: 1060.0332\n",
      "Train Epoch: 017 Batch: 00052/00094 | Loss: 386.5689 | CE: 0.1739 | KD: 1060.0686\n",
      "Train Epoch: 017 Batch: 00053/00094 | Loss: 386.5023 | CE: 0.1326 | KD: 1059.9989\n",
      "Train Epoch: 017 Batch: 00054/00094 | Loss: 386.5202 | CE: 0.1335 | KD: 1060.0458\n",
      "Train Epoch: 017 Batch: 00055/00094 | Loss: 386.5680 | CE: 0.1666 | KD: 1060.0861\n",
      "Train Epoch: 017 Batch: 00056/00094 | Loss: 386.5792 | CE: 0.1893 | KD: 1060.0546\n",
      "Train Epoch: 017 Batch: 00057/00094 | Loss: 386.5334 | CE: 0.1458 | KD: 1060.0482\n",
      "Train Epoch: 017 Batch: 00058/00094 | Loss: 386.6635 | CE: 0.2521 | KD: 1060.1136\n",
      "Train Epoch: 017 Batch: 00059/00094 | Loss: 386.5653 | CE: 0.1843 | KD: 1060.0302\n",
      "Train Epoch: 017 Batch: 00060/00094 | Loss: 386.5590 | CE: 0.1743 | KD: 1060.0405\n",
      "Train Epoch: 017 Batch: 00061/00094 | Loss: 386.5573 | CE: 0.1735 | KD: 1060.0378\n",
      "Train Epoch: 017 Batch: 00062/00094 | Loss: 386.5410 | CE: 0.1279 | KD: 1060.1183\n",
      "Train Epoch: 017 Batch: 00063/00094 | Loss: 386.5726 | CE: 0.1931 | KD: 1060.0262\n",
      "Train Epoch: 017 Batch: 00064/00094 | Loss: 386.5099 | CE: 0.1180 | KD: 1060.0603\n",
      "Train Epoch: 017 Batch: 00065/00094 | Loss: 386.5644 | CE: 0.1774 | KD: 1060.0466\n",
      "Train Epoch: 017 Batch: 00066/00094 | Loss: 386.5977 | CE: 0.1897 | KD: 1060.1044\n",
      "Train Epoch: 017 Batch: 00067/00094 | Loss: 386.5549 | CE: 0.1708 | KD: 1060.0388\n",
      "Train Epoch: 017 Batch: 00068/00094 | Loss: 386.5622 | CE: 0.1512 | KD: 1060.1125\n",
      "Train Epoch: 017 Batch: 00069/00094 | Loss: 386.5601 | CE: 0.1542 | KD: 1060.0985\n",
      "Train Epoch: 017 Batch: 00070/00094 | Loss: 386.5226 | CE: 0.1418 | KD: 1060.0294\n",
      "Train Epoch: 017 Batch: 00071/00094 | Loss: 386.5533 | CE: 0.1586 | KD: 1060.0679\n",
      "Train Epoch: 017 Batch: 00072/00094 | Loss: 386.5441 | CE: 0.1421 | KD: 1060.0876\n",
      "Train Epoch: 017 Batch: 00073/00094 | Loss: 386.5449 | CE: 0.1786 | KD: 1059.9899\n",
      "Train Epoch: 017 Batch: 00074/00094 | Loss: 386.5427 | CE: 0.1601 | KD: 1060.0345\n",
      "Train Epoch: 017 Batch: 00075/00094 | Loss: 386.5464 | CE: 0.1589 | KD: 1060.0480\n",
      "Train Epoch: 017 Batch: 00076/00094 | Loss: 386.5088 | CE: 0.1236 | KD: 1060.0417\n",
      "Train Epoch: 017 Batch: 00077/00094 | Loss: 386.5422 | CE: 0.1686 | KD: 1060.0099\n",
      "Train Epoch: 017 Batch: 00078/00094 | Loss: 386.5370 | CE: 0.1443 | KD: 1060.0624\n",
      "Train Epoch: 017 Batch: 00079/00094 | Loss: 386.5840 | CE: 0.1722 | KD: 1060.1149\n",
      "Train Epoch: 017 Batch: 00080/00094 | Loss: 386.5825 | CE: 0.1748 | KD: 1060.1035\n",
      "Train Epoch: 017 Batch: 00081/00094 | Loss: 386.5479 | CE: 0.1676 | KD: 1060.0281\n",
      "Train Epoch: 017 Batch: 00082/00094 | Loss: 386.5223 | CE: 0.1400 | KD: 1060.0337\n",
      "Train Epoch: 017 Batch: 00083/00094 | Loss: 386.6054 | CE: 0.1902 | KD: 1060.1241\n",
      "Train Epoch: 017 Batch: 00084/00094 | Loss: 386.5432 | CE: 0.1648 | KD: 1060.0233\n",
      "Train Epoch: 017 Batch: 00085/00094 | Loss: 386.5325 | CE: 0.1342 | KD: 1060.0778\n",
      "Train Epoch: 017 Batch: 00086/00094 | Loss: 386.6165 | CE: 0.2055 | KD: 1060.1123\n",
      "Train Epoch: 017 Batch: 00087/00094 | Loss: 386.5901 | CE: 0.2029 | KD: 1060.0472\n",
      "Train Epoch: 017 Batch: 00088/00094 | Loss: 386.6273 | CE: 0.2380 | KD: 1060.0531\n",
      "Train Epoch: 017 Batch: 00089/00094 | Loss: 386.5368 | CE: 0.1510 | KD: 1060.0435\n",
      "Train Epoch: 017 Batch: 00090/00094 | Loss: 386.5434 | CE: 0.1409 | KD: 1060.0895\n",
      "Train Epoch: 017 Batch: 00091/00094 | Loss: 386.5374 | CE: 0.1612 | KD: 1060.0168\n",
      "Train Epoch: 017 Batch: 00092/00094 | Loss: 386.5216 | CE: 0.1313 | KD: 1060.0557\n",
      "Train Epoch: 017 Batch: 00093/00094 | Loss: 386.5552 | CE: 0.1663 | KD: 1060.0519\n",
      "Train Epoch: 017 Batch: 00094/00094 | Loss: 386.5331 | CE: 0.1467 | KD: 1060.0448\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1552 | acc:97.7000\n",
      "[VAL Acc] Target: 97.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1387 | acc:50.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5970 | acc:68.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8678 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7818 | acc:59.2868\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.29%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5630 | acc:72.7357\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6259 | acc:71.4734\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7243 | acc:61.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.56%\n",
      "[VAL Acc] Avg 66.46%\n",
      "Train Epoch: 018 Batch: 00001/00094 | Loss: 347.9090 | CE: 0.1475 | KD: 1060.0869\n",
      "Train Epoch: 018 Batch: 00002/00094 | Loss: 347.9556 | CE: 0.1810 | KD: 1060.1270\n",
      "Train Epoch: 018 Batch: 00003/00094 | Loss: 347.8708 | CE: 0.1180 | KD: 1060.0604\n",
      "Train Epoch: 018 Batch: 00004/00094 | Loss: 347.8772 | CE: 0.1378 | KD: 1060.0195\n",
      "Train Epoch: 018 Batch: 00005/00094 | Loss: 347.9011 | CE: 0.1433 | KD: 1060.0758\n",
      "Train Epoch: 018 Batch: 00006/00094 | Loss: 347.8893 | CE: 0.1467 | KD: 1060.0294\n",
      "Train Epoch: 018 Batch: 00007/00094 | Loss: 347.9165 | CE: 0.1709 | KD: 1060.0386\n",
      "Train Epoch: 018 Batch: 00008/00094 | Loss: 347.9060 | CE: 0.1633 | KD: 1060.0297\n",
      "Train Epoch: 018 Batch: 00009/00094 | Loss: 348.0344 | CE: 0.2654 | KD: 1060.1099\n",
      "Train Epoch: 018 Batch: 00010/00094 | Loss: 347.9608 | CE: 0.1944 | KD: 1060.1018\n",
      "Train Epoch: 018 Batch: 00011/00094 | Loss: 347.9186 | CE: 0.1567 | KD: 1060.0881\n",
      "Train Epoch: 018 Batch: 00012/00094 | Loss: 347.8901 | CE: 0.1615 | KD: 1059.9867\n",
      "Train Epoch: 018 Batch: 00013/00094 | Loss: 347.9373 | CE: 0.1834 | KD: 1060.0638\n",
      "Train Epoch: 018 Batch: 00014/00094 | Loss: 347.9229 | CE: 0.1841 | KD: 1060.0177\n",
      "Train Epoch: 018 Batch: 00015/00094 | Loss: 347.8677 | CE: 0.1199 | KD: 1060.0453\n",
      "Train Epoch: 018 Batch: 00016/00094 | Loss: 347.9484 | CE: 0.1825 | KD: 1060.1003\n",
      "Train Epoch: 018 Batch: 00017/00094 | Loss: 347.9289 | CE: 0.1768 | KD: 1060.0585\n",
      "Train Epoch: 018 Batch: 00018/00094 | Loss: 347.8735 | CE: 0.1408 | KD: 1059.9994\n",
      "Train Epoch: 018 Batch: 00019/00094 | Loss: 347.8918 | CE: 0.1251 | KD: 1060.1028\n",
      "Train Epoch: 018 Batch: 00020/00094 | Loss: 347.9043 | CE: 0.1651 | KD: 1060.0190\n",
      "Train Epoch: 018 Batch: 00021/00094 | Loss: 347.9256 | CE: 0.1571 | KD: 1060.1082\n",
      "Train Epoch: 018 Batch: 00022/00094 | Loss: 347.9323 | CE: 0.1709 | KD: 1060.0864\n",
      "Train Epoch: 018 Batch: 00023/00094 | Loss: 347.9119 | CE: 0.1768 | KD: 1060.0065\n",
      "Train Epoch: 018 Batch: 00024/00094 | Loss: 347.8890 | CE: 0.1474 | KD: 1060.0262\n",
      "Train Epoch: 018 Batch: 00025/00094 | Loss: 347.9012 | CE: 0.1462 | KD: 1060.0671\n",
      "Train Epoch: 018 Batch: 00026/00094 | Loss: 347.9015 | CE: 0.1388 | KD: 1060.0907\n",
      "Train Epoch: 018 Batch: 00027/00094 | Loss: 348.0155 | CE: 0.2541 | KD: 1060.0868\n",
      "Train Epoch: 018 Batch: 00028/00094 | Loss: 347.9143 | CE: 0.1618 | KD: 1060.0594\n",
      "Train Epoch: 018 Batch: 00029/00094 | Loss: 347.9055 | CE: 0.1506 | KD: 1060.0668\n",
      "Train Epoch: 018 Batch: 00030/00094 | Loss: 347.9094 | CE: 0.1428 | KD: 1060.1024\n",
      "Train Epoch: 018 Batch: 00031/00094 | Loss: 347.8865 | CE: 0.1434 | KD: 1060.0309\n",
      "Train Epoch: 018 Batch: 00032/00094 | Loss: 347.9371 | CE: 0.1703 | KD: 1060.1031\n",
      "Train Epoch: 018 Batch: 00033/00094 | Loss: 347.9248 | CE: 0.1720 | KD: 1060.0605\n",
      "Train Epoch: 018 Batch: 00034/00094 | Loss: 347.8966 | CE: 0.1641 | KD: 1059.9988\n",
      "Train Epoch: 018 Batch: 00035/00094 | Loss: 347.8577 | CE: 0.1138 | KD: 1060.0334\n",
      "Train Epoch: 018 Batch: 00036/00094 | Loss: 347.9080 | CE: 0.1536 | KD: 1060.0653\n",
      "Train Epoch: 018 Batch: 00037/00094 | Loss: 347.8790 | CE: 0.1185 | KD: 1060.0841\n",
      "Train Epoch: 018 Batch: 00038/00094 | Loss: 347.9057 | CE: 0.1418 | KD: 1060.0941\n",
      "Train Epoch: 018 Batch: 00039/00094 | Loss: 348.0421 | CE: 0.2570 | KD: 1060.1591\n",
      "Train Epoch: 018 Batch: 00040/00094 | Loss: 347.8890 | CE: 0.1340 | KD: 1060.0673\n",
      "Train Epoch: 018 Batch: 00041/00094 | Loss: 347.8934 | CE: 0.1391 | KD: 1060.0649\n",
      "Train Epoch: 018 Batch: 00042/00094 | Loss: 347.8659 | CE: 0.1177 | KD: 1060.0464\n",
      "Train Epoch: 018 Batch: 00043/00094 | Loss: 347.9399 | CE: 0.1679 | KD: 1060.1189\n",
      "Train Epoch: 018 Batch: 00044/00094 | Loss: 347.8855 | CE: 0.1367 | KD: 1060.0480\n",
      "Train Epoch: 018 Batch: 00045/00094 | Loss: 347.8763 | CE: 0.1330 | KD: 1060.0314\n",
      "Train Epoch: 018 Batch: 00046/00094 | Loss: 347.8900 | CE: 0.1568 | KD: 1060.0006\n",
      "Train Epoch: 018 Batch: 00047/00094 | Loss: 347.8841 | CE: 0.1352 | KD: 1060.0486\n",
      "Train Epoch: 018 Batch: 00048/00094 | Loss: 347.9009 | CE: 0.1606 | KD: 1060.0225\n",
      "Train Epoch: 018 Batch: 00049/00094 | Loss: 347.8937 | CE: 0.1528 | KD: 1060.0239\n",
      "Train Epoch: 018 Batch: 00050/00094 | Loss: 347.8889 | CE: 0.1514 | KD: 1060.0137\n",
      "Train Epoch: 018 Batch: 00051/00094 | Loss: 347.8977 | CE: 0.1500 | KD: 1060.0449\n",
      "Train Epoch: 018 Batch: 00052/00094 | Loss: 347.9268 | CE: 0.1684 | KD: 1060.0774\n",
      "Train Epoch: 018 Batch: 00053/00094 | Loss: 347.9102 | CE: 0.1539 | KD: 1060.0712\n",
      "Train Epoch: 018 Batch: 00054/00094 | Loss: 347.8814 | CE: 0.1403 | KD: 1060.0249\n",
      "Train Epoch: 018 Batch: 00055/00094 | Loss: 348.0167 | CE: 0.2309 | KD: 1060.1610\n",
      "Train Epoch: 018 Batch: 00056/00094 | Loss: 347.8832 | CE: 0.1353 | KD: 1060.0454\n",
      "Train Epoch: 018 Batch: 00057/00094 | Loss: 347.9754 | CE: 0.2089 | KD: 1060.1022\n",
      "Train Epoch: 018 Batch: 00058/00094 | Loss: 347.8862 | CE: 0.1444 | KD: 1060.0269\n",
      "Train Epoch: 018 Batch: 00059/00094 | Loss: 347.9227 | CE: 0.1749 | KD: 1060.0452\n",
      "Train Epoch: 018 Batch: 00060/00094 | Loss: 347.9259 | CE: 0.1634 | KD: 1060.0897\n",
      "Train Epoch: 018 Batch: 00061/00094 | Loss: 347.9046 | CE: 0.1343 | KD: 1060.1138\n",
      "Train Epoch: 018 Batch: 00062/00094 | Loss: 347.8871 | CE: 0.1282 | KD: 1060.0790\n",
      "Train Epoch: 018 Batch: 00063/00094 | Loss: 347.9073 | CE: 0.1522 | KD: 1060.0675\n",
      "Train Epoch: 018 Batch: 00064/00094 | Loss: 347.9534 | CE: 0.1973 | KD: 1060.0706\n",
      "Train Epoch: 018 Batch: 00065/00094 | Loss: 347.8981 | CE: 0.1418 | KD: 1060.0712\n",
      "Train Epoch: 018 Batch: 00066/00094 | Loss: 347.9138 | CE: 0.1596 | KD: 1060.0647\n",
      "Train Epoch: 018 Batch: 00067/00094 | Loss: 347.8886 | CE: 0.1469 | KD: 1060.0264\n",
      "Train Epoch: 018 Batch: 00068/00094 | Loss: 347.8985 | CE: 0.1464 | KD: 1060.0581\n",
      "Train Epoch: 018 Batch: 00069/00094 | Loss: 347.9940 | CE: 0.2256 | KD: 1060.1080\n",
      "Train Epoch: 018 Batch: 00070/00094 | Loss: 347.9341 | CE: 0.1912 | KD: 1060.0299\n",
      "Train Epoch: 018 Batch: 00071/00094 | Loss: 347.9005 | CE: 0.1304 | KD: 1060.1133\n",
      "Train Epoch: 018 Batch: 00072/00094 | Loss: 347.8936 | CE: 0.1411 | KD: 1060.0592\n",
      "Train Epoch: 018 Batch: 00073/00094 | Loss: 347.9249 | CE: 0.1760 | KD: 1060.0485\n",
      "Train Epoch: 018 Batch: 00074/00094 | Loss: 347.8898 | CE: 0.1479 | KD: 1060.0272\n",
      "Train Epoch: 018 Batch: 00075/00094 | Loss: 347.9318 | CE: 0.1769 | KD: 1060.0665\n",
      "Train Epoch: 018 Batch: 00076/00094 | Loss: 347.9381 | CE: 0.1775 | KD: 1060.0841\n",
      "Train Epoch: 018 Batch: 00077/00094 | Loss: 347.8882 | CE: 0.1414 | KD: 1060.0424\n",
      "Train Epoch: 018 Batch: 00078/00094 | Loss: 347.9215 | CE: 0.1735 | KD: 1060.0459\n",
      "Train Epoch: 018 Batch: 00079/00094 | Loss: 347.9600 | CE: 0.1934 | KD: 1060.1027\n",
      "Train Epoch: 018 Batch: 00080/00094 | Loss: 347.9766 | CE: 0.1904 | KD: 1060.1624\n",
      "Train Epoch: 018 Batch: 00081/00094 | Loss: 347.8773 | CE: 0.1224 | KD: 1060.0668\n",
      "Train Epoch: 018 Batch: 00082/00094 | Loss: 347.8910 | CE: 0.1474 | KD: 1060.0323\n",
      "Train Epoch: 018 Batch: 00083/00094 | Loss: 347.9118 | CE: 0.1784 | KD: 1060.0011\n",
      "Train Epoch: 018 Batch: 00084/00094 | Loss: 347.8932 | CE: 0.1492 | KD: 1060.0338\n",
      "Train Epoch: 018 Batch: 00085/00094 | Loss: 347.9324 | CE: 0.1662 | KD: 1060.1014\n",
      "Train Epoch: 018 Batch: 00086/00094 | Loss: 347.8367 | CE: 0.0925 | KD: 1060.0342\n",
      "Train Epoch: 018 Batch: 00087/00094 | Loss: 347.9183 | CE: 0.1618 | KD: 1060.0717\n",
      "Train Epoch: 018 Batch: 00088/00094 | Loss: 347.9300 | CE: 0.1771 | KD: 1060.0609\n",
      "Train Epoch: 018 Batch: 00089/00094 | Loss: 347.8983 | CE: 0.1340 | KD: 1060.0952\n",
      "Train Epoch: 018 Batch: 00090/00094 | Loss: 347.8784 | CE: 0.1214 | KD: 1060.0735\n",
      "Train Epoch: 018 Batch: 00091/00094 | Loss: 347.9304 | CE: 0.1677 | KD: 1060.0905\n",
      "Train Epoch: 018 Batch: 00092/00094 | Loss: 347.9115 | CE: 0.1620 | KD: 1060.0500\n",
      "Train Epoch: 018 Batch: 00093/00094 | Loss: 347.9088 | CE: 0.1692 | KD: 1060.0201\n",
      "Train Epoch: 018 Batch: 00094/00094 | Loss: 347.9141 | CE: 0.1679 | KD: 1060.0404\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1347 | acc:98.3000\n",
      "[VAL Acc] Target: 98.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2245 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6557 | acc:65.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 65.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9517 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8921 | acc:55.6818\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5270 | acc:75.6007\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6380 | acc:71.2382\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7388 | acc:59.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.50%\n",
      "[VAL Acc] Avg 65.57%\n",
      "Train Epoch: 019 Batch: 00001/00094 | Loss: 347.9380 | CE: 0.1948 | KD: 1060.0310\n",
      "Train Epoch: 019 Batch: 00002/00094 | Loss: 347.8642 | CE: 0.1215 | KD: 1060.0297\n",
      "Train Epoch: 019 Batch: 00003/00094 | Loss: 347.9124 | CE: 0.1579 | KD: 1060.0656\n",
      "Train Epoch: 019 Batch: 00004/00094 | Loss: 347.9102 | CE: 0.1416 | KD: 1060.1084\n",
      "Train Epoch: 019 Batch: 00005/00094 | Loss: 348.0118 | CE: 0.2496 | KD: 1060.0890\n",
      "Train Epoch: 019 Batch: 00006/00094 | Loss: 347.8722 | CE: 0.1303 | KD: 1060.0271\n",
      "Train Epoch: 019 Batch: 00007/00094 | Loss: 347.9449 | CE: 0.1809 | KD: 1060.0947\n",
      "Train Epoch: 019 Batch: 00008/00094 | Loss: 347.8542 | CE: 0.1065 | KD: 1060.0447\n",
      "Train Epoch: 019 Batch: 00009/00094 | Loss: 347.9207 | CE: 0.1776 | KD: 1060.0309\n",
      "Train Epoch: 019 Batch: 00010/00094 | Loss: 347.9433 | CE: 0.1815 | KD: 1060.0878\n",
      "Train Epoch: 019 Batch: 00011/00094 | Loss: 347.8923 | CE: 0.1351 | KD: 1060.0737\n",
      "Train Epoch: 019 Batch: 00012/00094 | Loss: 347.8677 | CE: 0.1141 | KD: 1060.0629\n",
      "Train Epoch: 019 Batch: 00013/00094 | Loss: 347.9362 | CE: 0.1856 | KD: 1060.0537\n",
      "Train Epoch: 019 Batch: 00014/00094 | Loss: 347.9756 | CE: 0.1935 | KD: 1060.1497\n",
      "Train Epoch: 019 Batch: 00015/00094 | Loss: 347.9196 | CE: 0.1601 | KD: 1060.0811\n",
      "Train Epoch: 019 Batch: 00016/00094 | Loss: 347.8723 | CE: 0.1262 | KD: 1060.0400\n",
      "Train Epoch: 019 Batch: 00017/00094 | Loss: 348.0274 | CE: 0.2679 | KD: 1060.0807\n",
      "Train Epoch: 019 Batch: 00018/00094 | Loss: 347.8846 | CE: 0.1281 | KD: 1060.0717\n",
      "Train Epoch: 019 Batch: 00019/00094 | Loss: 347.8873 | CE: 0.1296 | KD: 1060.0753\n",
      "Train Epoch: 019 Batch: 00020/00094 | Loss: 347.8582 | CE: 0.1059 | KD: 1060.0590\n",
      "Train Epoch: 019 Batch: 00021/00094 | Loss: 347.8654 | CE: 0.1226 | KD: 1060.0300\n",
      "Train Epoch: 019 Batch: 00022/00094 | Loss: 347.8650 | CE: 0.1213 | KD: 1060.0326\n",
      "Train Epoch: 019 Batch: 00023/00094 | Loss: 347.8609 | CE: 0.1193 | KD: 1060.0265\n",
      "Train Epoch: 019 Batch: 00024/00094 | Loss: 347.9478 | CE: 0.1658 | KD: 1060.1494\n",
      "Train Epoch: 019 Batch: 00025/00094 | Loss: 347.9034 | CE: 0.1399 | KD: 1060.0931\n",
      "Train Epoch: 019 Batch: 00026/00094 | Loss: 347.8751 | CE: 0.1245 | KD: 1060.0538\n",
      "Train Epoch: 019 Batch: 00027/00094 | Loss: 347.8880 | CE: 0.1287 | KD: 1060.0802\n",
      "Train Epoch: 019 Batch: 00028/00094 | Loss: 347.9131 | CE: 0.1538 | KD: 1060.0801\n",
      "Train Epoch: 019 Batch: 00029/00094 | Loss: 347.9020 | CE: 0.1420 | KD: 1060.0822\n",
      "Train Epoch: 019 Batch: 00030/00094 | Loss: 347.9256 | CE: 0.1805 | KD: 1060.0370\n",
      "Train Epoch: 019 Batch: 00031/00094 | Loss: 347.8927 | CE: 0.1395 | KD: 1060.0616\n",
      "Train Epoch: 019 Batch: 00032/00094 | Loss: 347.9413 | CE: 0.1912 | KD: 1060.0520\n",
      "Train Epoch: 019 Batch: 00033/00094 | Loss: 347.8757 | CE: 0.1294 | KD: 1060.0406\n",
      "Train Epoch: 019 Batch: 00034/00094 | Loss: 347.9530 | CE: 0.2062 | KD: 1060.0421\n",
      "Train Epoch: 019 Batch: 00035/00094 | Loss: 347.9035 | CE: 0.1629 | KD: 1060.0233\n",
      "Train Epoch: 019 Batch: 00036/00094 | Loss: 347.9183 | CE: 0.1618 | KD: 1060.0718\n",
      "Train Epoch: 019 Batch: 00037/00094 | Loss: 347.9460 | CE: 0.1903 | KD: 1060.0695\n",
      "Train Epoch: 019 Batch: 00038/00094 | Loss: 347.9284 | CE: 0.1704 | KD: 1060.0763\n",
      "Train Epoch: 019 Batch: 00039/00094 | Loss: 347.8807 | CE: 0.1431 | KD: 1060.0140\n",
      "Train Epoch: 019 Batch: 00040/00094 | Loss: 347.9549 | CE: 0.1949 | KD: 1060.0824\n",
      "Train Epoch: 019 Batch: 00041/00094 | Loss: 347.9527 | CE: 0.1812 | KD: 1060.1174\n",
      "Train Epoch: 019 Batch: 00042/00094 | Loss: 347.8686 | CE: 0.1203 | KD: 1060.0468\n",
      "Train Epoch: 019 Batch: 00043/00094 | Loss: 347.9166 | CE: 0.1646 | KD: 1060.0579\n",
      "Train Epoch: 019 Batch: 00044/00094 | Loss: 347.9138 | CE: 0.1685 | KD: 1060.0375\n",
      "Train Epoch: 019 Batch: 00045/00094 | Loss: 347.8697 | CE: 0.1380 | KD: 1059.9958\n",
      "Train Epoch: 019 Batch: 00046/00094 | Loss: 347.9566 | CE: 0.1680 | KD: 1060.1696\n",
      "Train Epoch: 019 Batch: 00047/00094 | Loss: 347.8883 | CE: 0.1295 | KD: 1060.0785\n",
      "Train Epoch: 019 Batch: 00048/00094 | Loss: 347.9639 | CE: 0.1729 | KD: 1060.1770\n",
      "Train Epoch: 019 Batch: 00049/00094 | Loss: 347.9055 | CE: 0.1506 | KD: 1060.0668\n",
      "Train Epoch: 019 Batch: 00050/00094 | Loss: 347.9279 | CE: 0.1781 | KD: 1060.0513\n",
      "Train Epoch: 019 Batch: 00051/00094 | Loss: 347.9591 | CE: 0.2011 | KD: 1060.0763\n",
      "Train Epoch: 019 Batch: 00052/00094 | Loss: 347.8997 | CE: 0.1555 | KD: 1060.0343\n",
      "Train Epoch: 019 Batch: 00053/00094 | Loss: 347.8799 | CE: 0.1193 | KD: 1060.0842\n",
      "Train Epoch: 019 Batch: 00054/00094 | Loss: 347.8974 | CE: 0.1264 | KD: 1060.1158\n",
      "Train Epoch: 019 Batch: 00055/00094 | Loss: 347.8619 | CE: 0.1130 | KD: 1060.0485\n",
      "Train Epoch: 019 Batch: 00056/00094 | Loss: 347.8971 | CE: 0.1438 | KD: 1060.0621\n",
      "Train Epoch: 019 Batch: 00057/00094 | Loss: 347.9413 | CE: 0.1874 | KD: 1060.0637\n",
      "Train Epoch: 019 Batch: 00058/00094 | Loss: 347.8688 | CE: 0.1296 | KD: 1060.0189\n",
      "Train Epoch: 019 Batch: 00059/00094 | Loss: 347.8943 | CE: 0.1282 | KD: 1060.1010\n",
      "Train Epoch: 019 Batch: 00060/00094 | Loss: 347.8720 | CE: 0.1042 | KD: 1060.1062\n",
      "Train Epoch: 019 Batch: 00061/00094 | Loss: 347.8571 | CE: 0.1228 | KD: 1060.0040\n",
      "Train Epoch: 019 Batch: 00062/00094 | Loss: 347.9586 | CE: 0.1999 | KD: 1060.0782\n",
      "Train Epoch: 019 Batch: 00063/00094 | Loss: 347.8614 | CE: 0.1228 | KD: 1060.0170\n",
      "Train Epoch: 019 Batch: 00064/00094 | Loss: 347.9140 | CE: 0.1573 | KD: 1060.0723\n",
      "Train Epoch: 019 Batch: 00065/00094 | Loss: 347.8722 | CE: 0.1327 | KD: 1060.0198\n",
      "Train Epoch: 019 Batch: 00066/00094 | Loss: 347.8803 | CE: 0.1219 | KD: 1060.0775\n",
      "Train Epoch: 019 Batch: 00067/00094 | Loss: 347.8947 | CE: 0.1534 | KD: 1060.0254\n",
      "Train Epoch: 019 Batch: 00068/00094 | Loss: 347.8932 | CE: 0.1450 | KD: 1060.0464\n",
      "Train Epoch: 019 Batch: 00069/00094 | Loss: 347.9443 | CE: 0.1757 | KD: 1060.1086\n",
      "Train Epoch: 019 Batch: 00070/00094 | Loss: 347.9642 | CE: 0.2059 | KD: 1060.0770\n",
      "Train Epoch: 019 Batch: 00071/00094 | Loss: 347.9205 | CE: 0.1616 | KD: 1060.0790\n",
      "Train Epoch: 019 Batch: 00072/00094 | Loss: 347.9319 | CE: 0.1710 | KD: 1060.0851\n",
      "Train Epoch: 019 Batch: 00073/00094 | Loss: 347.9403 | CE: 0.1882 | KD: 1060.0582\n",
      "Train Epoch: 019 Batch: 00074/00094 | Loss: 347.9458 | CE: 0.1760 | KD: 1060.1122\n",
      "Train Epoch: 019 Batch: 00075/00094 | Loss: 347.9108 | CE: 0.1421 | KD: 1060.1090\n",
      "Train Epoch: 019 Batch: 00076/00094 | Loss: 347.9198 | CE: 0.1441 | KD: 1060.1301\n",
      "Train Epoch: 019 Batch: 00077/00094 | Loss: 347.9400 | CE: 0.1613 | KD: 1060.1393\n",
      "Train Epoch: 019 Batch: 00078/00094 | Loss: 347.9451 | CE: 0.1831 | KD: 1060.0884\n",
      "Train Epoch: 019 Batch: 00079/00094 | Loss: 347.8834 | CE: 0.1160 | KD: 1060.1049\n",
      "Train Epoch: 019 Batch: 00080/00094 | Loss: 347.9451 | CE: 0.1589 | KD: 1060.1622\n",
      "Train Epoch: 019 Batch: 00081/00094 | Loss: 347.9091 | CE: 0.1691 | KD: 1060.0214\n",
      "Train Epoch: 019 Batch: 00082/00094 | Loss: 347.9031 | CE: 0.1582 | KD: 1060.0364\n",
      "Train Epoch: 019 Batch: 00083/00094 | Loss: 347.8969 | CE: 0.1418 | KD: 1060.0675\n",
      "Train Epoch: 019 Batch: 00084/00094 | Loss: 347.8949 | CE: 0.1394 | KD: 1060.0685\n",
      "Train Epoch: 019 Batch: 00085/00094 | Loss: 347.8733 | CE: 0.1275 | KD: 1060.0391\n",
      "Train Epoch: 019 Batch: 00086/00094 | Loss: 347.8783 | CE: 0.1285 | KD: 1060.0513\n",
      "Train Epoch: 019 Batch: 00087/00094 | Loss: 347.8521 | CE: 0.1031 | KD: 1060.0488\n",
      "Train Epoch: 019 Batch: 00088/00094 | Loss: 347.8909 | CE: 0.1351 | KD: 1060.0695\n",
      "Train Epoch: 019 Batch: 00089/00094 | Loss: 347.8967 | CE: 0.1470 | KD: 1060.0510\n",
      "Train Epoch: 019 Batch: 00090/00094 | Loss: 347.8769 | CE: 0.1290 | KD: 1060.0454\n",
      "Train Epoch: 019 Batch: 00091/00094 | Loss: 347.9142 | CE: 0.1619 | KD: 1060.0591\n",
      "Train Epoch: 019 Batch: 00092/00094 | Loss: 347.9562 | CE: 0.1813 | KD: 1060.1278\n",
      "Train Epoch: 019 Batch: 00093/00094 | Loss: 347.9742 | CE: 0.1994 | KD: 1060.1273\n",
      "Train Epoch: 019 Batch: 00094/00094 | Loss: 347.9267 | CE: 0.1547 | KD: 1060.1188\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1437 | acc:97.8500\n",
      "[VAL Acc] Target: 97.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1628 | acc:50.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6195 | acc:68.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9163 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8747 | acc:56.4263\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5297 | acc:75.5083\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6145 | acc:71.3950\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7858 | acc:58.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.81%\n",
      "[VAL Acc] Avg 65.90%\n",
      "Train Epoch: 020 Batch: 00001/00094 | Loss: 347.8843 | CE: 0.1404 | KD: 1060.0333\n",
      "Train Epoch: 020 Batch: 00002/00094 | Loss: 347.8834 | CE: 0.1246 | KD: 1060.0789\n",
      "Train Epoch: 020 Batch: 00003/00094 | Loss: 347.8740 | CE: 0.1166 | KD: 1060.0742\n",
      "Train Epoch: 020 Batch: 00004/00094 | Loss: 347.8843 | CE: 0.1240 | KD: 1060.0835\n",
      "Train Epoch: 020 Batch: 00005/00094 | Loss: 347.8647 | CE: 0.1327 | KD: 1059.9971\n",
      "Train Epoch: 020 Batch: 00006/00094 | Loss: 347.8942 | CE: 0.1392 | KD: 1060.0669\n",
      "Train Epoch: 020 Batch: 00007/00094 | Loss: 347.9314 | CE: 0.1657 | KD: 1060.0999\n",
      "Train Epoch: 020 Batch: 00008/00094 | Loss: 347.8942 | CE: 0.1209 | KD: 1060.1229\n",
      "Train Epoch: 020 Batch: 00009/00094 | Loss: 347.8893 | CE: 0.1483 | KD: 1060.0245\n",
      "Train Epoch: 020 Batch: 00010/00094 | Loss: 347.9124 | CE: 0.1650 | KD: 1060.0439\n",
      "Train Epoch: 020 Batch: 00011/00094 | Loss: 347.8949 | CE: 0.1469 | KD: 1060.0457\n",
      "Train Epoch: 020 Batch: 00012/00094 | Loss: 347.9059 | CE: 0.1533 | KD: 1060.0597\n",
      "Train Epoch: 020 Batch: 00013/00094 | Loss: 347.8849 | CE: 0.1317 | KD: 1060.0616\n",
      "Train Epoch: 020 Batch: 00014/00094 | Loss: 347.9692 | CE: 0.1903 | KD: 1060.1399\n",
      "Train Epoch: 020 Batch: 00015/00094 | Loss: 347.9513 | CE: 0.1787 | KD: 1060.1207\n",
      "Train Epoch: 020 Batch: 00016/00094 | Loss: 347.9374 | CE: 0.1839 | KD: 1060.0624\n",
      "Train Epoch: 020 Batch: 00017/00094 | Loss: 347.9391 | CE: 0.1559 | KD: 1060.1530\n",
      "Train Epoch: 020 Batch: 00018/00094 | Loss: 348.0095 | CE: 0.2345 | KD: 1060.1281\n",
      "Train Epoch: 020 Batch: 00019/00094 | Loss: 347.8914 | CE: 0.1244 | KD: 1060.1036\n",
      "Train Epoch: 020 Batch: 00020/00094 | Loss: 347.9331 | CE: 0.1686 | KD: 1060.0961\n",
      "Train Epoch: 020 Batch: 00021/00094 | Loss: 347.8866 | CE: 0.1336 | KD: 1060.0612\n",
      "Train Epoch: 020 Batch: 00022/00094 | Loss: 347.9048 | CE: 0.1552 | KD: 1060.0505\n",
      "Train Epoch: 020 Batch: 00023/00094 | Loss: 347.8987 | CE: 0.1506 | KD: 1060.0460\n",
      "Train Epoch: 020 Batch: 00024/00094 | Loss: 347.9566 | CE: 0.1804 | KD: 1060.1320\n",
      "Train Epoch: 020 Batch: 00025/00094 | Loss: 347.8782 | CE: 0.1298 | KD: 1060.0471\n",
      "Train Epoch: 020 Batch: 00026/00094 | Loss: 347.8979 | CE: 0.1504 | KD: 1060.0441\n",
      "Train Epoch: 020 Batch: 00027/00094 | Loss: 347.8841 | CE: 0.1283 | KD: 1060.0696\n",
      "Train Epoch: 020 Batch: 00028/00094 | Loss: 347.8872 | CE: 0.1427 | KD: 1060.0350\n",
      "Train Epoch: 020 Batch: 00029/00094 | Loss: 347.8781 | CE: 0.1416 | KD: 1060.0107\n",
      "Train Epoch: 020 Batch: 00030/00094 | Loss: 347.8994 | CE: 0.1549 | KD: 1060.0349\n",
      "Train Epoch: 020 Batch: 00031/00094 | Loss: 347.8829 | CE: 0.1253 | KD: 1060.0751\n",
      "Train Epoch: 020 Batch: 00032/00094 | Loss: 347.9012 | CE: 0.1461 | KD: 1060.0674\n",
      "Train Epoch: 020 Batch: 00033/00094 | Loss: 347.9075 | CE: 0.1541 | KD: 1060.0624\n",
      "Train Epoch: 020 Batch: 00034/00094 | Loss: 347.8690 | CE: 0.1126 | KD: 1060.0714\n",
      "Train Epoch: 020 Batch: 00035/00094 | Loss: 347.9013 | CE: 0.1476 | KD: 1060.0631\n",
      "Train Epoch: 020 Batch: 00036/00094 | Loss: 347.8957 | CE: 0.1326 | KD: 1060.0917\n",
      "Train Epoch: 020 Batch: 00037/00094 | Loss: 347.8623 | CE: 0.1127 | KD: 1060.0508\n",
      "Train Epoch: 020 Batch: 00038/00094 | Loss: 347.8830 | CE: 0.1133 | KD: 1060.1121\n",
      "Train Epoch: 020 Batch: 00039/00094 | Loss: 347.9240 | CE: 0.1541 | KD: 1060.1124\n",
      "Train Epoch: 020 Batch: 00040/00094 | Loss: 347.9039 | CE: 0.1446 | KD: 1060.0803\n",
      "Train Epoch: 020 Batch: 00041/00094 | Loss: 347.8902 | CE: 0.1415 | KD: 1060.0479\n",
      "Train Epoch: 020 Batch: 00042/00094 | Loss: 347.9000 | CE: 0.1587 | KD: 1060.0254\n",
      "Train Epoch: 020 Batch: 00043/00094 | Loss: 347.8909 | CE: 0.1357 | KD: 1060.0676\n",
      "Train Epoch: 020 Batch: 00044/00094 | Loss: 347.8764 | CE: 0.1203 | KD: 1060.0704\n",
      "Train Epoch: 020 Batch: 00045/00094 | Loss: 347.8826 | CE: 0.1433 | KD: 1060.0195\n",
      "Train Epoch: 020 Batch: 00046/00094 | Loss: 347.9107 | CE: 0.1378 | KD: 1060.1215\n",
      "Train Epoch: 020 Batch: 00047/00094 | Loss: 347.8757 | CE: 0.1171 | KD: 1060.0781\n",
      "Train Epoch: 020 Batch: 00048/00094 | Loss: 347.9660 | CE: 0.2078 | KD: 1060.0767\n",
      "Train Epoch: 020 Batch: 00049/00094 | Loss: 347.9234 | CE: 0.1589 | KD: 1060.0958\n",
      "Train Epoch: 020 Batch: 00050/00094 | Loss: 347.9446 | CE: 0.1980 | KD: 1060.0416\n",
      "Train Epoch: 020 Batch: 00051/00094 | Loss: 347.8815 | CE: 0.1157 | KD: 1060.0999\n",
      "Train Epoch: 020 Batch: 00052/00094 | Loss: 347.8421 | CE: 0.1025 | KD: 1060.0203\n",
      "Train Epoch: 020 Batch: 00053/00094 | Loss: 347.8989 | CE: 0.1350 | KD: 1060.0941\n",
      "Train Epoch: 020 Batch: 00054/00094 | Loss: 347.9395 | CE: 0.1646 | KD: 1060.1277\n",
      "Train Epoch: 020 Batch: 00055/00094 | Loss: 347.9420 | CE: 0.1849 | KD: 1060.0737\n",
      "Train Epoch: 020 Batch: 00056/00094 | Loss: 347.8873 | CE: 0.1454 | KD: 1060.0272\n",
      "Train Epoch: 020 Batch: 00057/00094 | Loss: 347.8920 | CE: 0.1308 | KD: 1060.0857\n",
      "Train Epoch: 020 Batch: 00058/00094 | Loss: 347.8826 | CE: 0.1314 | KD: 1060.0557\n",
      "Train Epoch: 020 Batch: 00059/00094 | Loss: 347.8947 | CE: 0.1342 | KD: 1060.0840\n",
      "Train Epoch: 020 Batch: 00060/00094 | Loss: 347.9108 | CE: 0.1518 | KD: 1060.0792\n",
      "Train Epoch: 020 Batch: 00061/00094 | Loss: 347.9037 | CE: 0.1598 | KD: 1060.0333\n",
      "Train Epoch: 020 Batch: 00062/00094 | Loss: 347.8806 | CE: 0.1341 | KD: 1060.0411\n",
      "Train Epoch: 020 Batch: 00063/00094 | Loss: 347.9269 | CE: 0.1790 | KD: 1060.0453\n",
      "Train Epoch: 020 Batch: 00064/00094 | Loss: 347.9604 | CE: 0.1909 | KD: 1060.1113\n",
      "Train Epoch: 020 Batch: 00065/00094 | Loss: 347.9493 | CE: 0.1780 | KD: 1060.1169\n",
      "Train Epoch: 020 Batch: 00066/00094 | Loss: 347.8987 | CE: 0.1410 | KD: 1060.0753\n",
      "Train Epoch: 020 Batch: 00067/00094 | Loss: 347.8702 | CE: 0.1113 | KD: 1060.0791\n",
      "Train Epoch: 020 Batch: 00068/00094 | Loss: 347.8564 | CE: 0.1134 | KD: 1060.0308\n",
      "Train Epoch: 020 Batch: 00069/00094 | Loss: 347.9165 | CE: 0.1591 | KD: 1060.0742\n",
      "Train Epoch: 020 Batch: 00070/00094 | Loss: 347.9396 | CE: 0.1624 | KD: 1060.1349\n",
      "Train Epoch: 020 Batch: 00071/00094 | Loss: 347.9066 | CE: 0.1659 | KD: 1060.0238\n",
      "Train Epoch: 020 Batch: 00072/00094 | Loss: 347.9031 | CE: 0.1491 | KD: 1060.0638\n",
      "Train Epoch: 020 Batch: 00073/00094 | Loss: 347.9024 | CE: 0.1382 | KD: 1060.0952\n",
      "Train Epoch: 020 Batch: 00074/00094 | Loss: 347.9063 | CE: 0.1616 | KD: 1060.0356\n",
      "Train Epoch: 020 Batch: 00075/00094 | Loss: 348.0028 | CE: 0.2434 | KD: 1060.0807\n",
      "Train Epoch: 020 Batch: 00076/00094 | Loss: 347.8694 | CE: 0.1223 | KD: 1060.0430\n",
      "Train Epoch: 020 Batch: 00077/00094 | Loss: 347.8979 | CE: 0.1343 | KD: 1060.0933\n",
      "Train Epoch: 020 Batch: 00078/00094 | Loss: 347.8855 | CE: 0.1395 | KD: 1060.0396\n",
      "Train Epoch: 020 Batch: 00079/00094 | Loss: 347.9123 | CE: 0.1516 | KD: 1060.0842\n",
      "Train Epoch: 020 Batch: 00080/00094 | Loss: 347.8637 | CE: 0.1157 | KD: 1060.0459\n",
      "Train Epoch: 020 Batch: 00081/00094 | Loss: 347.8738 | CE: 0.0930 | KD: 1060.1456\n",
      "Train Epoch: 020 Batch: 00082/00094 | Loss: 347.8928 | CE: 0.1518 | KD: 1060.0243\n",
      "Train Epoch: 020 Batch: 00083/00094 | Loss: 347.8794 | CE: 0.1192 | KD: 1060.0830\n",
      "Train Epoch: 020 Batch: 00084/00094 | Loss: 347.9037 | CE: 0.1425 | KD: 1060.0858\n",
      "Train Epoch: 020 Batch: 00085/00094 | Loss: 347.8907 | CE: 0.1338 | KD: 1060.0729\n",
      "Train Epoch: 020 Batch: 00086/00094 | Loss: 347.8577 | CE: 0.1136 | KD: 1060.0339\n",
      "Train Epoch: 020 Batch: 00087/00094 | Loss: 347.9015 | CE: 0.1206 | KD: 1060.1460\n",
      "Train Epoch: 020 Batch: 00088/00094 | Loss: 347.8645 | CE: 0.1008 | KD: 1060.0936\n",
      "Train Epoch: 020 Batch: 00089/00094 | Loss: 347.8676 | CE: 0.1196 | KD: 1060.0459\n",
      "Train Epoch: 020 Batch: 00090/00094 | Loss: 347.8864 | CE: 0.1579 | KD: 1059.9862\n",
      "Train Epoch: 020 Batch: 00091/00094 | Loss: 347.9893 | CE: 0.2191 | KD: 1060.1134\n",
      "Train Epoch: 020 Batch: 00092/00094 | Loss: 348.0077 | CE: 0.2348 | KD: 1060.1216\n",
      "Train Epoch: 020 Batch: 00093/00094 | Loss: 347.9199 | CE: 0.1614 | KD: 1060.0779\n",
      "Train Epoch: 020 Batch: 00094/00094 | Loss: 347.8626 | CE: 0.1082 | KD: 1060.0651\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1335 | acc:98.6500\n",
      "[VAL Acc] Target: 98.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2128 | acc:50.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6222 | acc:68.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8947 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8529 | acc:56.4263\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5420 | acc:73.9372\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6534 | acc:70.7680\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.77%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7574 | acc:58.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.31%\n",
      "[VAL Acc] Avg 65.75%\n",
      "Train Epoch: 021 Batch: 00001/00094 | Loss: 347.8564 | CE: 0.1169 | KD: 1060.0199\n",
      "Train Epoch: 021 Batch: 00002/00094 | Loss: 347.9273 | CE: 0.1744 | KD: 1060.0608\n",
      "Train Epoch: 021 Batch: 00003/00094 | Loss: 347.8716 | CE: 0.1148 | KD: 1060.0725\n",
      "Train Epoch: 021 Batch: 00004/00094 | Loss: 347.9197 | CE: 0.1537 | KD: 1060.1007\n",
      "Train Epoch: 021 Batch: 00005/00094 | Loss: 347.8672 | CE: 0.1309 | KD: 1060.0099\n",
      "Train Epoch: 021 Batch: 00006/00094 | Loss: 347.8832 | CE: 0.1356 | KD: 1060.0446\n",
      "Train Epoch: 021 Batch: 00007/00094 | Loss: 348.0070 | CE: 0.2111 | KD: 1060.1918\n",
      "Train Epoch: 021 Batch: 00008/00094 | Loss: 347.9129 | CE: 0.1536 | KD: 1060.0802\n",
      "Train Epoch: 021 Batch: 00009/00094 | Loss: 347.8748 | CE: 0.1238 | KD: 1060.0548\n",
      "Train Epoch: 021 Batch: 00010/00094 | Loss: 347.9026 | CE: 0.1467 | KD: 1060.0699\n",
      "Train Epoch: 021 Batch: 00011/00094 | Loss: 347.8900 | CE: 0.1405 | KD: 1060.0503\n",
      "Train Epoch: 021 Batch: 00012/00094 | Loss: 347.9515 | CE: 0.1664 | KD: 1060.1589\n",
      "Train Epoch: 021 Batch: 00013/00094 | Loss: 347.9416 | CE: 0.1612 | KD: 1060.1444\n",
      "Train Epoch: 021 Batch: 00014/00094 | Loss: 347.8759 | CE: 0.1364 | KD: 1060.0201\n",
      "Train Epoch: 021 Batch: 00015/00094 | Loss: 347.8979 | CE: 0.1509 | KD: 1060.0427\n",
      "Train Epoch: 021 Batch: 00016/00094 | Loss: 347.8530 | CE: 0.1099 | KD: 1060.0308\n",
      "Train Epoch: 021 Batch: 00017/00094 | Loss: 347.8765 | CE: 0.1267 | KD: 1060.0514\n",
      "Train Epoch: 021 Batch: 00018/00094 | Loss: 347.8624 | CE: 0.1279 | KD: 1060.0046\n",
      "Train Epoch: 021 Batch: 00019/00094 | Loss: 347.9349 | CE: 0.1547 | KD: 1060.1438\n",
      "Train Epoch: 021 Batch: 00020/00094 | Loss: 347.9268 | CE: 0.1608 | KD: 1060.1007\n",
      "Train Epoch: 021 Batch: 00021/00094 | Loss: 347.8966 | CE: 0.1278 | KD: 1060.1093\n",
      "Train Epoch: 021 Batch: 00022/00094 | Loss: 347.9015 | CE: 0.1361 | KD: 1060.0989\n",
      "Train Epoch: 021 Batch: 00023/00094 | Loss: 347.8736 | CE: 0.1239 | KD: 1060.0511\n",
      "Train Epoch: 021 Batch: 00024/00094 | Loss: 347.8656 | CE: 0.1173 | KD: 1060.0466\n",
      "Train Epoch: 021 Batch: 00025/00094 | Loss: 347.8699 | CE: 0.1136 | KD: 1060.0709\n",
      "Train Epoch: 021 Batch: 00026/00094 | Loss: 347.9095 | CE: 0.1521 | KD: 1060.0743\n",
      "Train Epoch: 021 Batch: 00027/00094 | Loss: 348.0227 | CE: 0.2540 | KD: 1060.1086\n",
      "Train Epoch: 021 Batch: 00028/00094 | Loss: 347.9047 | CE: 0.1435 | KD: 1060.0857\n",
      "Train Epoch: 021 Batch: 00029/00094 | Loss: 347.8904 | CE: 0.1328 | KD: 1060.0750\n",
      "Train Epoch: 021 Batch: 00030/00094 | Loss: 347.8749 | CE: 0.1241 | KD: 1060.0546\n",
      "Train Epoch: 021 Batch: 00031/00094 | Loss: 347.8958 | CE: 0.1258 | KD: 1060.1130\n",
      "Train Epoch: 021 Batch: 00032/00094 | Loss: 347.8820 | CE: 0.1261 | KD: 1060.0699\n",
      "Train Epoch: 021 Batch: 00033/00094 | Loss: 347.9357 | CE: 0.1659 | KD: 1060.1121\n",
      "Train Epoch: 021 Batch: 00034/00094 | Loss: 347.9454 | CE: 0.1814 | KD: 1060.0946\n",
      "Train Epoch: 021 Batch: 00035/00094 | Loss: 347.8868 | CE: 0.1221 | KD: 1060.0969\n",
      "Train Epoch: 021 Batch: 00036/00094 | Loss: 347.9181 | CE: 0.1639 | KD: 1060.0645\n",
      "Train Epoch: 021 Batch: 00037/00094 | Loss: 347.8858 | CE: 0.1345 | KD: 1060.0560\n",
      "Train Epoch: 021 Batch: 00038/00094 | Loss: 347.9044 | CE: 0.1413 | KD: 1060.0916\n",
      "Train Epoch: 021 Batch: 00039/00094 | Loss: 347.8519 | CE: 0.1009 | KD: 1060.0551\n",
      "Train Epoch: 021 Batch: 00040/00094 | Loss: 347.8948 | CE: 0.1487 | KD: 1060.0400\n",
      "Train Epoch: 021 Batch: 00041/00094 | Loss: 347.8619 | CE: 0.1059 | KD: 1060.0701\n",
      "Train Epoch: 021 Batch: 00042/00094 | Loss: 347.8571 | CE: 0.1029 | KD: 1060.0645\n",
      "Train Epoch: 021 Batch: 00043/00094 | Loss: 347.9116 | CE: 0.1487 | KD: 1060.0909\n",
      "Train Epoch: 021 Batch: 00044/00094 | Loss: 347.9047 | CE: 0.1466 | KD: 1060.0764\n",
      "Train Epoch: 021 Batch: 00045/00094 | Loss: 347.8560 | CE: 0.1083 | KD: 1060.0447\n",
      "Train Epoch: 021 Batch: 00046/00094 | Loss: 347.8900 | CE: 0.1187 | KD: 1060.1171\n",
      "Train Epoch: 021 Batch: 00047/00094 | Loss: 347.8617 | CE: 0.1086 | KD: 1060.0612\n",
      "Train Epoch: 021 Batch: 00048/00094 | Loss: 347.8806 | CE: 0.1183 | KD: 1060.0895\n",
      "Train Epoch: 021 Batch: 00049/00094 | Loss: 347.8750 | CE: 0.1253 | KD: 1060.0509\n",
      "Train Epoch: 021 Batch: 00050/00094 | Loss: 347.8572 | CE: 0.0997 | KD: 1060.0747\n",
      "Train Epoch: 021 Batch: 00051/00094 | Loss: 347.8412 | CE: 0.1142 | KD: 1059.9819\n",
      "Train Epoch: 021 Batch: 00052/00094 | Loss: 347.9427 | CE: 0.1830 | KD: 1060.0812\n",
      "Train Epoch: 021 Batch: 00053/00094 | Loss: 347.8912 | CE: 0.1251 | KD: 1060.1011\n",
      "Train Epoch: 021 Batch: 00054/00094 | Loss: 347.8822 | CE: 0.1277 | KD: 1060.0654\n",
      "Train Epoch: 021 Batch: 00055/00094 | Loss: 347.8686 | CE: 0.1224 | KD: 1060.0403\n",
      "Train Epoch: 021 Batch: 00056/00094 | Loss: 347.8813 | CE: 0.1222 | KD: 1060.0797\n",
      "Train Epoch: 021 Batch: 00057/00094 | Loss: 347.8948 | CE: 0.1259 | KD: 1060.1096\n",
      "Train Epoch: 021 Batch: 00058/00094 | Loss: 347.9156 | CE: 0.1559 | KD: 1060.0815\n",
      "Train Epoch: 021 Batch: 00059/00094 | Loss: 347.8940 | CE: 0.1344 | KD: 1060.0809\n",
      "Train Epoch: 021 Batch: 00060/00094 | Loss: 347.8513 | CE: 0.0959 | KD: 1060.0682\n",
      "Train Epoch: 021 Batch: 00061/00094 | Loss: 347.9216 | CE: 0.1345 | KD: 1060.1648\n",
      "Train Epoch: 021 Batch: 00062/00094 | Loss: 347.9210 | CE: 0.1569 | KD: 1060.0948\n",
      "Train Epoch: 021 Batch: 00063/00094 | Loss: 347.9019 | CE: 0.1317 | KD: 1060.1135\n",
      "Train Epoch: 021 Batch: 00064/00094 | Loss: 347.9004 | CE: 0.1389 | KD: 1060.0872\n",
      "Train Epoch: 021 Batch: 00065/00094 | Loss: 347.9452 | CE: 0.1841 | KD: 1060.0857\n",
      "Train Epoch: 021 Batch: 00066/00094 | Loss: 347.9299 | CE: 0.1508 | KD: 1060.1405\n",
      "Train Epoch: 021 Batch: 00067/00094 | Loss: 347.9330 | CE: 0.1664 | KD: 1060.1023\n",
      "Train Epoch: 021 Batch: 00068/00094 | Loss: 347.9480 | CE: 0.1825 | KD: 1060.0991\n",
      "Train Epoch: 021 Batch: 00069/00094 | Loss: 347.9088 | CE: 0.1501 | KD: 1060.0785\n",
      "Train Epoch: 021 Batch: 00070/00094 | Loss: 347.8958 | CE: 0.1370 | KD: 1060.0787\n",
      "Train Epoch: 021 Batch: 00071/00094 | Loss: 347.8757 | CE: 0.1224 | KD: 1060.0620\n",
      "Train Epoch: 021 Batch: 00072/00094 | Loss: 347.8990 | CE: 0.1293 | KD: 1060.1119\n",
      "Train Epoch: 021 Batch: 00073/00094 | Loss: 347.8794 | CE: 0.1281 | KD: 1060.0559\n",
      "Train Epoch: 021 Batch: 00074/00094 | Loss: 347.8944 | CE: 0.1505 | KD: 1060.0334\n",
      "Train Epoch: 021 Batch: 00075/00094 | Loss: 347.9050 | CE: 0.1293 | KD: 1060.1300\n",
      "Train Epoch: 021 Batch: 00076/00094 | Loss: 347.8578 | CE: 0.1184 | KD: 1060.0195\n",
      "Train Epoch: 021 Batch: 00077/00094 | Loss: 347.9663 | CE: 0.2085 | KD: 1060.0758\n",
      "Train Epoch: 021 Batch: 00078/00094 | Loss: 347.9154 | CE: 0.1486 | KD: 1060.1031\n",
      "Train Epoch: 021 Batch: 00079/00094 | Loss: 347.8873 | CE: 0.1196 | KD: 1060.1060\n",
      "Train Epoch: 021 Batch: 00080/00094 | Loss: 347.8726 | CE: 0.1395 | KD: 1060.0004\n",
      "Train Epoch: 021 Batch: 00081/00094 | Loss: 347.8655 | CE: 0.1153 | KD: 1060.0525\n",
      "Train Epoch: 021 Batch: 00082/00094 | Loss: 347.8899 | CE: 0.1195 | KD: 1060.1141\n",
      "Train Epoch: 021 Batch: 00083/00094 | Loss: 347.8956 | CE: 0.1324 | KD: 1060.0922\n",
      "Train Epoch: 021 Batch: 00084/00094 | Loss: 347.9254 | CE: 0.1632 | KD: 1060.0890\n",
      "Train Epoch: 021 Batch: 00085/00094 | Loss: 347.8960 | CE: 0.1351 | KD: 1060.0851\n",
      "Train Epoch: 021 Batch: 00086/00094 | Loss: 347.8798 | CE: 0.1294 | KD: 1060.0530\n",
      "Train Epoch: 021 Batch: 00087/00094 | Loss: 347.8621 | CE: 0.1232 | KD: 1060.0181\n",
      "Train Epoch: 021 Batch: 00088/00094 | Loss: 347.8982 | CE: 0.1574 | KD: 1060.0238\n",
      "Train Epoch: 021 Batch: 00089/00094 | Loss: 347.9549 | CE: 0.1938 | KD: 1060.0857\n",
      "Train Epoch: 021 Batch: 00090/00094 | Loss: 347.8835 | CE: 0.1348 | KD: 1060.0477\n",
      "Train Epoch: 021 Batch: 00091/00094 | Loss: 347.8556 | CE: 0.1102 | KD: 1060.0380\n",
      "Train Epoch: 021 Batch: 00092/00094 | Loss: 347.9591 | CE: 0.1997 | KD: 1060.0803\n",
      "Train Epoch: 021 Batch: 00093/00094 | Loss: 347.8898 | CE: 0.1444 | KD: 1060.0377\n",
      "Train Epoch: 021 Batch: 00094/00094 | Loss: 347.9366 | CE: 0.1550 | KD: 1060.1482\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1290 | acc:98.3500\n",
      "[VAL Acc] Target: 98.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1320 | acc:51.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5674 | acc:72.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 72.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9397 | acc:46.7557\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7700 | acc:60.1097\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6373 | acc:69.1312\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5684 | acc:74.2555\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.26%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7422 | acc:61.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.75%\n",
      "[VAL Acc] Avg 66.74%\n",
      "Train Epoch: 022 Batch: 00001/00094 | Loss: 313.1041 | CE: 0.1210 | KD: 1060.0793\n",
      "Train Epoch: 022 Batch: 00002/00094 | Loss: 313.0556 | CE: 0.1012 | KD: 1059.9823\n",
      "Train Epoch: 022 Batch: 00003/00094 | Loss: 313.1306 | CE: 0.1512 | KD: 1060.0668\n",
      "Train Epoch: 022 Batch: 00004/00094 | Loss: 313.1060 | CE: 0.1309 | KD: 1060.0520\n",
      "Train Epoch: 022 Batch: 00005/00094 | Loss: 313.1635 | CE: 0.1891 | KD: 1060.0499\n",
      "Train Epoch: 022 Batch: 00006/00094 | Loss: 313.1205 | CE: 0.1252 | KD: 1060.1205\n",
      "Train Epoch: 022 Batch: 00007/00094 | Loss: 313.0926 | CE: 0.1157 | KD: 1060.0583\n",
      "Train Epoch: 022 Batch: 00008/00094 | Loss: 313.1735 | CE: 0.1823 | KD: 1060.1067\n",
      "Train Epoch: 022 Batch: 00009/00094 | Loss: 313.0882 | CE: 0.1118 | KD: 1060.0564\n",
      "Train Epoch: 022 Batch: 00010/00094 | Loss: 313.1386 | CE: 0.1429 | KD: 1060.1219\n",
      "Train Epoch: 022 Batch: 00011/00094 | Loss: 313.1075 | CE: 0.1149 | KD: 1060.1115\n",
      "Train Epoch: 022 Batch: 00012/00094 | Loss: 313.1235 | CE: 0.1236 | KD: 1060.1361\n",
      "Train Epoch: 022 Batch: 00013/00094 | Loss: 313.1175 | CE: 0.1478 | KD: 1060.0341\n",
      "Train Epoch: 022 Batch: 00014/00094 | Loss: 313.1275 | CE: 0.1538 | KD: 1060.0475\n",
      "Train Epoch: 022 Batch: 00015/00094 | Loss: 313.1830 | CE: 0.1824 | KD: 1060.1385\n",
      "Train Epoch: 022 Batch: 00016/00094 | Loss: 313.1302 | CE: 0.1389 | KD: 1060.1072\n",
      "Train Epoch: 022 Batch: 00017/00094 | Loss: 313.1137 | CE: 0.1357 | KD: 1060.0620\n",
      "Train Epoch: 022 Batch: 00018/00094 | Loss: 313.1198 | CE: 0.1439 | KD: 1060.0548\n",
      "Train Epoch: 022 Batch: 00019/00094 | Loss: 313.1224 | CE: 0.1420 | KD: 1060.0702\n",
      "Train Epoch: 022 Batch: 00020/00094 | Loss: 313.1722 | CE: 0.1760 | KD: 1060.1237\n",
      "Train Epoch: 022 Batch: 00021/00094 | Loss: 313.0849 | CE: 0.1225 | KD: 1060.0093\n",
      "Train Epoch: 022 Batch: 00022/00094 | Loss: 313.1134 | CE: 0.1238 | KD: 1060.1013\n",
      "Train Epoch: 022 Batch: 00023/00094 | Loss: 313.2330 | CE: 0.2545 | KD: 1060.0636\n",
      "Train Epoch: 022 Batch: 00024/00094 | Loss: 313.1011 | CE: 0.1182 | KD: 1060.0786\n",
      "Train Epoch: 022 Batch: 00025/00094 | Loss: 313.1204 | CE: 0.1503 | KD: 1060.0352\n",
      "Train Epoch: 022 Batch: 00026/00094 | Loss: 313.0899 | CE: 0.1176 | KD: 1060.0427\n",
      "Train Epoch: 022 Batch: 00027/00094 | Loss: 313.1258 | CE: 0.1448 | KD: 1060.0724\n",
      "Train Epoch: 022 Batch: 00028/00094 | Loss: 313.1225 | CE: 0.1387 | KD: 1060.0818\n",
      "Train Epoch: 022 Batch: 00029/00094 | Loss: 313.1412 | CE: 0.1527 | KD: 1060.0978\n",
      "Train Epoch: 022 Batch: 00030/00094 | Loss: 313.1874 | CE: 0.1987 | KD: 1060.0984\n",
      "Train Epoch: 022 Batch: 00031/00094 | Loss: 313.0968 | CE: 0.1146 | KD: 1060.0764\n",
      "Train Epoch: 022 Batch: 00032/00094 | Loss: 313.1468 | CE: 0.1645 | KD: 1060.0764\n",
      "Train Epoch: 022 Batch: 00033/00094 | Loss: 313.0945 | CE: 0.1143 | KD: 1060.0693\n",
      "Train Epoch: 022 Batch: 00034/00094 | Loss: 313.1396 | CE: 0.1618 | KD: 1060.0612\n",
      "Train Epoch: 022 Batch: 00035/00094 | Loss: 313.1089 | CE: 0.1307 | KD: 1060.0629\n",
      "Train Epoch: 022 Batch: 00036/00094 | Loss: 313.1021 | CE: 0.1231 | KD: 1060.0651\n",
      "Train Epoch: 022 Batch: 00037/00094 | Loss: 313.2230 | CE: 0.2332 | KD: 1060.1019\n",
      "Train Epoch: 022 Batch: 00038/00094 | Loss: 313.1088 | CE: 0.1188 | KD: 1060.1027\n",
      "Train Epoch: 022 Batch: 00039/00094 | Loss: 313.0891 | CE: 0.1203 | KD: 1060.0309\n",
      "Train Epoch: 022 Batch: 00040/00094 | Loss: 313.1167 | CE: 0.1364 | KD: 1060.0697\n",
      "Train Epoch: 022 Batch: 00041/00094 | Loss: 313.1044 | CE: 0.1085 | KD: 1060.1229\n",
      "Train Epoch: 022 Batch: 00042/00094 | Loss: 313.0682 | CE: 0.0843 | KD: 1060.0819\n",
      "Train Epoch: 022 Batch: 00043/00094 | Loss: 313.1041 | CE: 0.1349 | KD: 1060.0322\n",
      "Train Epoch: 022 Batch: 00044/00094 | Loss: 313.1281 | CE: 0.1397 | KD: 1060.0974\n",
      "Train Epoch: 022 Batch: 00045/00094 | Loss: 313.1276 | CE: 0.1372 | KD: 1060.1038\n",
      "Train Epoch: 022 Batch: 00046/00094 | Loss: 313.1254 | CE: 0.1486 | KD: 1060.0577\n",
      "Train Epoch: 022 Batch: 00047/00094 | Loss: 313.1181 | CE: 0.1506 | KD: 1060.0265\n",
      "Train Epoch: 022 Batch: 00048/00094 | Loss: 313.1212 | CE: 0.1195 | KD: 1060.1426\n",
      "Train Epoch: 022 Batch: 00049/00094 | Loss: 313.1647 | CE: 0.1698 | KD: 1060.1193\n",
      "Train Epoch: 022 Batch: 00050/00094 | Loss: 313.1320 | CE: 0.1281 | KD: 1060.1498\n",
      "Train Epoch: 022 Batch: 00051/00094 | Loss: 313.1288 | CE: 0.1162 | KD: 1060.1792\n",
      "Train Epoch: 022 Batch: 00052/00094 | Loss: 313.1363 | CE: 0.1400 | KD: 1060.1239\n",
      "Train Epoch: 022 Batch: 00053/00094 | Loss: 313.1298 | CE: 0.1555 | KD: 1060.0496\n",
      "Train Epoch: 022 Batch: 00054/00094 | Loss: 313.0983 | CE: 0.1267 | KD: 1060.0402\n",
      "Train Epoch: 022 Batch: 00055/00094 | Loss: 313.1721 | CE: 0.1718 | KD: 1060.1376\n",
      "Train Epoch: 022 Batch: 00056/00094 | Loss: 313.0923 | CE: 0.1083 | KD: 1060.0820\n",
      "Train Epoch: 022 Batch: 00057/00094 | Loss: 313.1514 | CE: 0.1705 | KD: 1060.0719\n",
      "Train Epoch: 022 Batch: 00058/00094 | Loss: 313.1349 | CE: 0.1352 | KD: 1060.1357\n",
      "Train Epoch: 022 Batch: 00059/00094 | Loss: 313.1313 | CE: 0.1424 | KD: 1060.0990\n",
      "Train Epoch: 022 Batch: 00060/00094 | Loss: 313.0793 | CE: 0.1090 | KD: 1060.0361\n",
      "Train Epoch: 022 Batch: 00061/00094 | Loss: 313.0805 | CE: 0.1027 | KD: 1060.0615\n",
      "Train Epoch: 022 Batch: 00062/00094 | Loss: 313.1159 | CE: 0.1299 | KD: 1060.0891\n",
      "Train Epoch: 022 Batch: 00063/00094 | Loss: 313.2790 | CE: 0.2775 | KD: 1060.1417\n",
      "Train Epoch: 022 Batch: 00064/00094 | Loss: 313.0907 | CE: 0.1103 | KD: 1060.0703\n",
      "Train Epoch: 022 Batch: 00065/00094 | Loss: 313.0923 | CE: 0.1212 | KD: 1060.0386\n",
      "Train Epoch: 022 Batch: 00066/00094 | Loss: 313.1144 | CE: 0.1253 | KD: 1060.0995\n",
      "Train Epoch: 022 Batch: 00067/00094 | Loss: 313.1010 | CE: 0.0998 | KD: 1060.1407\n",
      "Train Epoch: 022 Batch: 00068/00094 | Loss: 313.0862 | CE: 0.1008 | KD: 1060.0869\n",
      "Train Epoch: 022 Batch: 00069/00094 | Loss: 313.1433 | CE: 0.1445 | KD: 1060.1323\n",
      "Train Epoch: 022 Batch: 00070/00094 | Loss: 313.1055 | CE: 0.1211 | KD: 1060.0835\n",
      "Train Epoch: 022 Batch: 00071/00094 | Loss: 313.1039 | CE: 0.1234 | KD: 1060.0707\n",
      "Train Epoch: 022 Batch: 00072/00094 | Loss: 313.1477 | CE: 0.1537 | KD: 1060.1161\n",
      "Train Epoch: 022 Batch: 00073/00094 | Loss: 313.1662 | CE: 0.1807 | KD: 1060.0874\n",
      "Train Epoch: 022 Batch: 00074/00094 | Loss: 313.1125 | CE: 0.1274 | KD: 1060.0859\n",
      "Train Epoch: 022 Batch: 00075/00094 | Loss: 313.1619 | CE: 0.1745 | KD: 1060.0940\n",
      "Train Epoch: 022 Batch: 00076/00094 | Loss: 313.1439 | CE: 0.1634 | KD: 1060.0702\n",
      "Train Epoch: 022 Batch: 00077/00094 | Loss: 313.0816 | CE: 0.1053 | KD: 1060.0563\n",
      "Train Epoch: 022 Batch: 00078/00094 | Loss: 313.0817 | CE: 0.0939 | KD: 1060.0952\n",
      "Train Epoch: 022 Batch: 00079/00094 | Loss: 313.0792 | CE: 0.1069 | KD: 1060.0427\n",
      "Train Epoch: 022 Batch: 00080/00094 | Loss: 313.1592 | CE: 0.1751 | KD: 1060.0826\n",
      "Train Epoch: 022 Batch: 00081/00094 | Loss: 313.0872 | CE: 0.0996 | KD: 1060.0947\n",
      "Train Epoch: 022 Batch: 00082/00094 | Loss: 313.1210 | CE: 0.1388 | KD: 1060.0760\n",
      "Train Epoch: 022 Batch: 00083/00094 | Loss: 313.1114 | CE: 0.1130 | KD: 1060.1310\n",
      "Train Epoch: 022 Batch: 00084/00094 | Loss: 313.0862 | CE: 0.0987 | KD: 1060.0945\n",
      "Train Epoch: 022 Batch: 00085/00094 | Loss: 313.1867 | CE: 0.1948 | KD: 1060.1089\n",
      "Train Epoch: 022 Batch: 00086/00094 | Loss: 313.1142 | CE: 0.1308 | KD: 1060.0804\n",
      "Train Epoch: 022 Batch: 00087/00094 | Loss: 313.1322 | CE: 0.1531 | KD: 1060.0658\n",
      "Train Epoch: 022 Batch: 00088/00094 | Loss: 313.0664 | CE: 0.0861 | KD: 1060.0697\n",
      "Train Epoch: 022 Batch: 00089/00094 | Loss: 313.1148 | CE: 0.1334 | KD: 1060.0734\n",
      "Train Epoch: 022 Batch: 00090/00094 | Loss: 313.1738 | CE: 0.1832 | KD: 1060.1046\n",
      "Train Epoch: 022 Batch: 00091/00094 | Loss: 313.0940 | CE: 0.1122 | KD: 1060.0748\n",
      "Train Epoch: 022 Batch: 00092/00094 | Loss: 313.1256 | CE: 0.1581 | KD: 1060.0264\n",
      "Train Epoch: 022 Batch: 00093/00094 | Loss: 313.1155 | CE: 0.1273 | KD: 1060.0963\n",
      "Train Epoch: 022 Batch: 00094/00094 | Loss: 313.0805 | CE: 0.1138 | KD: 1060.0237\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1278 | acc:98.2000\n",
      "[VAL Acc] Target: 98.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1749 | acc:51.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5912 | acc:70.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 70.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9414 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8274 | acc:57.4451\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5082 | acc:77.2643\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 77.26%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5896 | acc:73.3150\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.32%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8487 | acc:54.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.87%\n",
      "[VAL Acc] Avg 66.42%\n",
      "Train Epoch: 023 Batch: 00001/00094 | Loss: 313.1121 | CE: 0.1258 | KD: 1060.0900\n",
      "Train Epoch: 023 Batch: 00002/00094 | Loss: 313.1762 | CE: 0.1818 | KD: 1060.1178\n",
      "Train Epoch: 023 Batch: 00003/00094 | Loss: 313.1305 | CE: 0.1277 | KD: 1060.1459\n",
      "Train Epoch: 023 Batch: 00004/00094 | Loss: 313.0892 | CE: 0.1061 | KD: 1060.0795\n",
      "Train Epoch: 023 Batch: 00005/00094 | Loss: 313.1351 | CE: 0.1449 | KD: 1060.1033\n",
      "Train Epoch: 023 Batch: 00006/00094 | Loss: 313.1053 | CE: 0.1254 | KD: 1060.0684\n",
      "Train Epoch: 023 Batch: 00007/00094 | Loss: 313.1024 | CE: 0.1244 | KD: 1060.0620\n",
      "Train Epoch: 023 Batch: 00008/00094 | Loss: 313.1359 | CE: 0.1319 | KD: 1060.1499\n",
      "Train Epoch: 023 Batch: 00009/00094 | Loss: 313.0936 | CE: 0.1273 | KD: 1060.0223\n",
      "Train Epoch: 023 Batch: 00010/00094 | Loss: 313.1321 | CE: 0.1568 | KD: 1060.0530\n",
      "Train Epoch: 023 Batch: 00011/00094 | Loss: 313.1518 | CE: 0.1621 | KD: 1060.1018\n",
      "Train Epoch: 023 Batch: 00012/00094 | Loss: 313.1004 | CE: 0.1213 | KD: 1060.0659\n",
      "Train Epoch: 023 Batch: 00013/00094 | Loss: 313.1653 | CE: 0.1710 | KD: 1060.1169\n",
      "Train Epoch: 023 Batch: 00014/00094 | Loss: 313.1081 | CE: 0.1334 | KD: 1060.0510\n",
      "Train Epoch: 023 Batch: 00015/00094 | Loss: 313.0801 | CE: 0.1108 | KD: 1060.0327\n",
      "Train Epoch: 023 Batch: 00016/00094 | Loss: 313.0811 | CE: 0.1035 | KD: 1060.0608\n",
      "Train Epoch: 023 Batch: 00017/00094 | Loss: 313.0903 | CE: 0.0921 | KD: 1060.1302\n",
      "Train Epoch: 023 Batch: 00018/00094 | Loss: 313.1117 | CE: 0.1259 | KD: 1060.0884\n",
      "Train Epoch: 023 Batch: 00019/00094 | Loss: 313.0959 | CE: 0.1157 | KD: 1060.0696\n",
      "Train Epoch: 023 Batch: 00020/00094 | Loss: 313.2304 | CE: 0.2136 | KD: 1060.1936\n",
      "Train Epoch: 023 Batch: 00021/00094 | Loss: 313.1370 | CE: 0.1399 | KD: 1060.1265\n",
      "Train Epoch: 023 Batch: 00022/00094 | Loss: 313.1244 | CE: 0.1336 | KD: 1060.1052\n",
      "Train Epoch: 023 Batch: 00023/00094 | Loss: 313.1279 | CE: 0.1496 | KD: 1060.0630\n",
      "Train Epoch: 023 Batch: 00024/00094 | Loss: 313.1408 | CE: 0.1550 | KD: 1060.0885\n",
      "Train Epoch: 023 Batch: 00025/00094 | Loss: 313.1106 | CE: 0.1347 | KD: 1060.0548\n",
      "Train Epoch: 023 Batch: 00026/00094 | Loss: 313.1387 | CE: 0.1570 | KD: 1060.0746\n",
      "Train Epoch: 023 Batch: 00027/00094 | Loss: 313.1431 | CE: 0.1584 | KD: 1060.0850\n",
      "Train Epoch: 023 Batch: 00028/00094 | Loss: 313.1611 | CE: 0.1742 | KD: 1060.0920\n",
      "Train Epoch: 023 Batch: 00029/00094 | Loss: 313.1620 | CE: 0.1737 | KD: 1060.0970\n",
      "Train Epoch: 023 Batch: 00030/00094 | Loss: 313.1392 | CE: 0.1545 | KD: 1060.0846\n",
      "Train Epoch: 023 Batch: 00031/00094 | Loss: 313.1512 | CE: 0.1445 | KD: 1060.1593\n",
      "Train Epoch: 023 Batch: 00032/00094 | Loss: 313.0925 | CE: 0.1324 | KD: 1060.0013\n",
      "Train Epoch: 023 Batch: 00033/00094 | Loss: 313.0846 | CE: 0.1070 | KD: 1060.0605\n",
      "Train Epoch: 023 Batch: 00034/00094 | Loss: 313.1176 | CE: 0.1247 | KD: 1060.1124\n",
      "Train Epoch: 023 Batch: 00035/00094 | Loss: 313.1257 | CE: 0.1437 | KD: 1060.0757\n",
      "Train Epoch: 023 Batch: 00036/00094 | Loss: 313.1071 | CE: 0.1187 | KD: 1060.0972\n",
      "Train Epoch: 023 Batch: 00037/00094 | Loss: 313.1301 | CE: 0.1366 | KD: 1060.1144\n",
      "Train Epoch: 023 Batch: 00038/00094 | Loss: 313.1441 | CE: 0.1680 | KD: 1060.0555\n",
      "Train Epoch: 023 Batch: 00039/00094 | Loss: 313.1700 | CE: 0.1589 | KD: 1060.1741\n",
      "Train Epoch: 023 Batch: 00040/00094 | Loss: 313.1685 | CE: 0.1463 | KD: 1060.2118\n",
      "Train Epoch: 023 Batch: 00041/00094 | Loss: 313.1274 | CE: 0.1357 | KD: 1060.1086\n",
      "Train Epoch: 023 Batch: 00042/00094 | Loss: 313.0994 | CE: 0.1125 | KD: 1060.0924\n",
      "Train Epoch: 023 Batch: 00043/00094 | Loss: 313.1760 | CE: 0.1940 | KD: 1060.0756\n",
      "Train Epoch: 023 Batch: 00044/00094 | Loss: 313.1053 | CE: 0.1267 | KD: 1060.0641\n",
      "Train Epoch: 023 Batch: 00045/00094 | Loss: 313.1422 | CE: 0.1464 | KD: 1060.1222\n",
      "Train Epoch: 023 Batch: 00046/00094 | Loss: 313.1114 | CE: 0.1338 | KD: 1060.0609\n",
      "Train Epoch: 023 Batch: 00047/00094 | Loss: 313.1809 | CE: 0.1936 | KD: 1060.0936\n",
      "Train Epoch: 023 Batch: 00048/00094 | Loss: 313.0960 | CE: 0.1145 | KD: 1060.0739\n",
      "Train Epoch: 023 Batch: 00049/00094 | Loss: 313.1145 | CE: 0.1368 | KD: 1060.0610\n",
      "Train Epoch: 023 Batch: 00050/00094 | Loss: 313.1687 | CE: 0.1711 | KD: 1060.1287\n",
      "Train Epoch: 023 Batch: 00051/00094 | Loss: 313.1305 | CE: 0.1395 | KD: 1060.1058\n",
      "Train Epoch: 023 Batch: 00052/00094 | Loss: 313.1350 | CE: 0.1378 | KD: 1060.1272\n",
      "Train Epoch: 023 Batch: 00053/00094 | Loss: 313.1160 | CE: 0.1326 | KD: 1060.0802\n",
      "Train Epoch: 023 Batch: 00054/00094 | Loss: 313.0992 | CE: 0.1237 | KD: 1060.0535\n",
      "Train Epoch: 023 Batch: 00055/00094 | Loss: 313.0918 | CE: 0.1075 | KD: 1060.0833\n",
      "Train Epoch: 023 Batch: 00056/00094 | Loss: 313.0956 | CE: 0.1181 | KD: 1060.0602\n",
      "Train Epoch: 023 Batch: 00057/00094 | Loss: 313.0804 | CE: 0.1016 | KD: 1060.0647\n",
      "Train Epoch: 023 Batch: 00058/00094 | Loss: 313.1083 | CE: 0.1200 | KD: 1060.0969\n",
      "Train Epoch: 023 Batch: 00059/00094 | Loss: 313.1079 | CE: 0.1393 | KD: 1060.0303\n",
      "Train Epoch: 023 Batch: 00060/00094 | Loss: 313.0974 | CE: 0.1184 | KD: 1060.0653\n",
      "Train Epoch: 023 Batch: 00061/00094 | Loss: 313.1010 | CE: 0.1099 | KD: 1060.1062\n",
      "Train Epoch: 023 Batch: 00062/00094 | Loss: 313.1037 | CE: 0.1213 | KD: 1060.0768\n",
      "Train Epoch: 023 Batch: 00063/00094 | Loss: 313.1069 | CE: 0.1217 | KD: 1060.0867\n",
      "Train Epoch: 023 Batch: 00064/00094 | Loss: 313.1314 | CE: 0.1511 | KD: 1060.0697\n",
      "Train Epoch: 023 Batch: 00065/00094 | Loss: 313.0858 | CE: 0.0957 | KD: 1060.1030\n",
      "Train Epoch: 023 Batch: 00066/00094 | Loss: 313.0893 | CE: 0.0889 | KD: 1060.1378\n",
      "Train Epoch: 023 Batch: 00067/00094 | Loss: 313.0842 | CE: 0.0946 | KD: 1060.1013\n",
      "Train Epoch: 023 Batch: 00068/00094 | Loss: 313.0792 | CE: 0.0960 | KD: 1060.0795\n",
      "Train Epoch: 023 Batch: 00069/00094 | Loss: 313.1613 | CE: 0.1569 | KD: 1060.1515\n",
      "Train Epoch: 023 Batch: 00070/00094 | Loss: 313.1651 | CE: 0.1692 | KD: 1060.1227\n",
      "Train Epoch: 023 Batch: 00071/00094 | Loss: 313.1450 | CE: 0.1722 | KD: 1060.0443\n",
      "Train Epoch: 023 Batch: 00072/00094 | Loss: 313.1095 | CE: 0.1236 | KD: 1060.0886\n",
      "Train Epoch: 023 Batch: 00073/00094 | Loss: 313.1046 | CE: 0.1292 | KD: 1060.0531\n",
      "Train Epoch: 023 Batch: 00074/00094 | Loss: 313.0988 | CE: 0.1226 | KD: 1060.0559\n",
      "Train Epoch: 023 Batch: 00075/00094 | Loss: 313.1395 | CE: 0.1599 | KD: 1060.0671\n",
      "Train Epoch: 023 Batch: 00076/00094 | Loss: 313.1575 | CE: 0.1695 | KD: 1060.0959\n",
      "Train Epoch: 023 Batch: 00077/00094 | Loss: 313.1256 | CE: 0.1312 | KD: 1060.1176\n",
      "Train Epoch: 023 Batch: 00078/00094 | Loss: 313.1128 | CE: 0.1472 | KD: 1060.0200\n",
      "Train Epoch: 023 Batch: 00079/00094 | Loss: 313.1010 | CE: 0.1179 | KD: 1060.0795\n",
      "Train Epoch: 023 Batch: 00080/00094 | Loss: 313.1579 | CE: 0.1539 | KD: 1060.1500\n",
      "Train Epoch: 023 Batch: 00081/00094 | Loss: 313.1385 | CE: 0.1711 | KD: 1060.0260\n",
      "Train Epoch: 023 Batch: 00082/00094 | Loss: 313.1298 | CE: 0.1292 | KD: 1060.1383\n",
      "Train Epoch: 023 Batch: 00083/00094 | Loss: 313.0809 | CE: 0.1135 | KD: 1060.0261\n",
      "Train Epoch: 023 Batch: 00084/00094 | Loss: 313.0786 | CE: 0.0931 | KD: 1060.0873\n",
      "Train Epoch: 023 Batch: 00085/00094 | Loss: 313.1039 | CE: 0.1255 | KD: 1060.0635\n",
      "Train Epoch: 023 Batch: 00086/00094 | Loss: 313.1340 | CE: 0.1277 | KD: 1060.1577\n",
      "Train Epoch: 023 Batch: 00087/00094 | Loss: 313.0839 | CE: 0.0974 | KD: 1060.0909\n",
      "Train Epoch: 023 Batch: 00088/00094 | Loss: 313.1787 | CE: 0.1752 | KD: 1060.1484\n",
      "Train Epoch: 023 Batch: 00089/00094 | Loss: 313.1010 | CE: 0.1150 | KD: 1060.0890\n",
      "Train Epoch: 023 Batch: 00090/00094 | Loss: 313.1150 | CE: 0.1229 | KD: 1060.1100\n",
      "Train Epoch: 023 Batch: 00091/00094 | Loss: 313.0700 | CE: 0.1057 | KD: 1060.0154\n",
      "Train Epoch: 023 Batch: 00092/00094 | Loss: 313.0992 | CE: 0.1237 | KD: 1060.0532\n",
      "Train Epoch: 023 Batch: 00093/00094 | Loss: 313.1298 | CE: 0.1361 | KD: 1060.1152\n",
      "Train Epoch: 023 Batch: 00094/00094 | Loss: 313.1087 | CE: 0.1381 | KD: 1060.0369\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1200 | acc:98.4500\n",
      "[VAL Acc] Target: 98.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1674 | acc:50.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5966 | acc:70.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 70.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9727 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8234 | acc:57.8370\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5196 | acc:76.6174\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5700 | acc:74.1379\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8302 | acc:56.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.19%\n",
      "[VAL Acc] Avg 66.63%\n",
      "Train Epoch: 024 Batch: 00001/00094 | Loss: 313.0744 | CE: 0.1052 | KD: 1060.0321\n",
      "Train Epoch: 024 Batch: 00002/00094 | Loss: 313.0961 | CE: 0.1133 | KD: 1060.0782\n",
      "Train Epoch: 024 Batch: 00003/00094 | Loss: 313.1415 | CE: 0.1557 | KD: 1060.0884\n",
      "Train Epoch: 024 Batch: 00004/00094 | Loss: 313.1050 | CE: 0.1184 | KD: 1060.0912\n",
      "Train Epoch: 024 Batch: 00005/00094 | Loss: 313.1084 | CE: 0.1125 | KD: 1060.1226\n",
      "Train Epoch: 024 Batch: 00006/00094 | Loss: 313.1507 | CE: 0.1615 | KD: 1060.1000\n",
      "Train Epoch: 024 Batch: 00007/00094 | Loss: 313.0946 | CE: 0.1205 | KD: 1060.0488\n",
      "Train Epoch: 024 Batch: 00008/00094 | Loss: 313.1790 | CE: 0.1821 | KD: 1060.1260\n",
      "Train Epoch: 024 Batch: 00009/00094 | Loss: 313.1745 | CE: 0.1926 | KD: 1060.0753\n",
      "Train Epoch: 024 Batch: 00010/00094 | Loss: 313.0847 | CE: 0.1005 | KD: 1060.0828\n",
      "Train Epoch: 024 Batch: 00011/00094 | Loss: 313.1530 | CE: 0.1544 | KD: 1060.1315\n",
      "Train Epoch: 024 Batch: 00012/00094 | Loss: 313.1288 | CE: 0.1357 | KD: 1060.1134\n",
      "Train Epoch: 024 Batch: 00013/00094 | Loss: 313.0683 | CE: 0.0889 | KD: 1060.0667\n",
      "Train Epoch: 024 Batch: 00014/00094 | Loss: 313.1129 | CE: 0.1238 | KD: 1060.0996\n",
      "Train Epoch: 024 Batch: 00015/00094 | Loss: 313.1706 | CE: 0.1772 | KD: 1060.1143\n",
      "Train Epoch: 024 Batch: 00016/00094 | Loss: 313.0768 | CE: 0.1177 | KD: 1059.9978\n",
      "Train Epoch: 024 Batch: 00017/00094 | Loss: 313.1017 | CE: 0.1233 | KD: 1060.0631\n",
      "Train Epoch: 024 Batch: 00018/00094 | Loss: 313.1199 | CE: 0.1527 | KD: 1060.0253\n",
      "Train Epoch: 024 Batch: 00019/00094 | Loss: 313.1058 | CE: 0.1099 | KD: 1060.1226\n",
      "Train Epoch: 024 Batch: 00020/00094 | Loss: 313.1107 | CE: 0.1173 | KD: 1060.1140\n",
      "Train Epoch: 024 Batch: 00021/00094 | Loss: 313.0832 | CE: 0.0983 | KD: 1060.0853\n",
      "Train Epoch: 024 Batch: 00022/00094 | Loss: 313.0884 | CE: 0.1062 | KD: 1060.0760\n",
      "Train Epoch: 024 Batch: 00023/00094 | Loss: 313.1442 | CE: 0.1726 | KD: 1060.0404\n",
      "Train Epoch: 024 Batch: 00024/00094 | Loss: 313.0683 | CE: 0.0979 | KD: 1060.0363\n",
      "Train Epoch: 024 Batch: 00025/00094 | Loss: 313.1268 | CE: 0.1303 | KD: 1060.1250\n",
      "Train Epoch: 024 Batch: 00026/00094 | Loss: 313.1141 | CE: 0.1217 | KD: 1060.1108\n",
      "Train Epoch: 024 Batch: 00027/00094 | Loss: 313.1308 | CE: 0.1503 | KD: 1060.0704\n",
      "Train Epoch: 024 Batch: 00028/00094 | Loss: 313.1039 | CE: 0.1335 | KD: 1060.0360\n",
      "Train Epoch: 024 Batch: 00029/00094 | Loss: 313.0796 | CE: 0.1124 | KD: 1060.0253\n",
      "Train Epoch: 024 Batch: 00030/00094 | Loss: 313.1190 | CE: 0.1352 | KD: 1060.0814\n",
      "Train Epoch: 024 Batch: 00031/00094 | Loss: 313.1356 | CE: 0.1543 | KD: 1060.0732\n",
      "Train Epoch: 024 Batch: 00032/00094 | Loss: 313.1455 | CE: 0.1389 | KD: 1060.1591\n",
      "Train Epoch: 024 Batch: 00033/00094 | Loss: 313.1327 | CE: 0.1519 | KD: 1060.0715\n",
      "Train Epoch: 024 Batch: 00034/00094 | Loss: 313.1084 | CE: 0.1326 | KD: 1060.0543\n",
      "Train Epoch: 024 Batch: 00035/00094 | Loss: 313.1530 | CE: 0.1672 | KD: 1060.0885\n",
      "Train Epoch: 024 Batch: 00036/00094 | Loss: 313.1107 | CE: 0.1243 | KD: 1060.0905\n",
      "Train Epoch: 024 Batch: 00037/00094 | Loss: 313.1396 | CE: 0.1356 | KD: 1060.1501\n",
      "Train Epoch: 024 Batch: 00038/00094 | Loss: 313.0829 | CE: 0.1082 | KD: 1060.0509\n",
      "Train Epoch: 024 Batch: 00039/00094 | Loss: 313.1068 | CE: 0.1297 | KD: 1060.0588\n",
      "Train Epoch: 024 Batch: 00040/00094 | Loss: 313.0761 | CE: 0.0985 | KD: 1060.0604\n",
      "Train Epoch: 024 Batch: 00041/00094 | Loss: 313.1001 | CE: 0.1077 | KD: 1060.1110\n",
      "Train Epoch: 024 Batch: 00042/00094 | Loss: 313.0929 | CE: 0.1253 | KD: 1060.0270\n",
      "Train Epoch: 024 Batch: 00043/00094 | Loss: 313.0966 | CE: 0.1169 | KD: 1060.0679\n",
      "Train Epoch: 024 Batch: 00044/00094 | Loss: 313.1202 | CE: 0.1420 | KD: 1060.0627\n",
      "Train Epoch: 024 Batch: 00045/00094 | Loss: 313.1471 | CE: 0.1675 | KD: 1060.0671\n",
      "Train Epoch: 024 Batch: 00046/00094 | Loss: 313.1113 | CE: 0.1321 | KD: 1060.0660\n",
      "Train Epoch: 024 Batch: 00047/00094 | Loss: 313.1177 | CE: 0.1380 | KD: 1060.0677\n",
      "Train Epoch: 024 Batch: 00048/00094 | Loss: 313.1130 | CE: 0.1428 | KD: 1060.0355\n",
      "Train Epoch: 024 Batch: 00049/00094 | Loss: 313.0934 | CE: 0.1133 | KD: 1060.0691\n",
      "Train Epoch: 024 Batch: 00050/00094 | Loss: 313.0978 | CE: 0.1032 | KD: 1060.1184\n",
      "Train Epoch: 024 Batch: 00051/00094 | Loss: 313.1181 | CE: 0.1191 | KD: 1060.1333\n",
      "Train Epoch: 024 Batch: 00052/00094 | Loss: 313.1025 | CE: 0.1243 | KD: 1060.0629\n",
      "Train Epoch: 024 Batch: 00053/00094 | Loss: 313.1185 | CE: 0.1369 | KD: 1060.0743\n",
      "Train Epoch: 024 Batch: 00054/00094 | Loss: 313.1291 | CE: 0.1561 | KD: 1060.0449\n",
      "Train Epoch: 024 Batch: 00055/00094 | Loss: 313.1620 | CE: 0.1830 | KD: 1060.0652\n",
      "Train Epoch: 024 Batch: 00056/00094 | Loss: 313.0876 | CE: 0.1076 | KD: 1060.0688\n",
      "Train Epoch: 024 Batch: 00057/00094 | Loss: 313.1346 | CE: 0.1573 | KD: 1060.0596\n",
      "Train Epoch: 024 Batch: 00058/00094 | Loss: 313.1018 | CE: 0.1209 | KD: 1060.0719\n",
      "Train Epoch: 024 Batch: 00059/00094 | Loss: 313.1180 | CE: 0.1305 | KD: 1060.0944\n",
      "Train Epoch: 024 Batch: 00060/00094 | Loss: 313.1298 | CE: 0.1397 | KD: 1060.1029\n",
      "Train Epoch: 024 Batch: 00061/00094 | Loss: 313.1007 | CE: 0.1143 | KD: 1060.0903\n",
      "Train Epoch: 024 Batch: 00062/00094 | Loss: 313.1447 | CE: 0.1388 | KD: 1060.1564\n",
      "Train Epoch: 024 Batch: 00063/00094 | Loss: 313.1055 | CE: 0.1199 | KD: 1060.0879\n",
      "Train Epoch: 024 Batch: 00064/00094 | Loss: 313.1072 | CE: 0.1277 | KD: 1060.0673\n",
      "Train Epoch: 024 Batch: 00065/00094 | Loss: 313.1152 | CE: 0.1261 | KD: 1060.0995\n",
      "Train Epoch: 024 Batch: 00066/00094 | Loss: 313.1375 | CE: 0.1595 | KD: 1060.0619\n",
      "Train Epoch: 024 Batch: 00067/00094 | Loss: 313.0946 | CE: 0.1183 | KD: 1060.0563\n",
      "Train Epoch: 024 Batch: 00068/00094 | Loss: 313.1069 | CE: 0.1313 | KD: 1060.0537\n",
      "Train Epoch: 024 Batch: 00069/00094 | Loss: 313.1369 | CE: 0.1461 | KD: 1060.1051\n",
      "Train Epoch: 024 Batch: 00070/00094 | Loss: 313.1430 | CE: 0.1429 | KD: 1060.1368\n",
      "Train Epoch: 024 Batch: 00071/00094 | Loss: 313.1640 | CE: 0.1737 | KD: 1060.1036\n",
      "Train Epoch: 024 Batch: 00072/00094 | Loss: 313.1122 | CE: 0.1163 | KD: 1060.1228\n",
      "Train Epoch: 024 Batch: 00073/00094 | Loss: 313.1381 | CE: 0.1434 | KD: 1060.1188\n",
      "Train Epoch: 024 Batch: 00074/00094 | Loss: 313.1819 | CE: 0.2024 | KD: 1060.0673\n",
      "Train Epoch: 024 Batch: 00075/00094 | Loss: 313.0747 | CE: 0.1079 | KD: 1060.0240\n",
      "Train Epoch: 024 Batch: 00076/00094 | Loss: 313.1966 | CE: 0.2122 | KD: 1060.0835\n",
      "Train Epoch: 024 Batch: 00077/00094 | Loss: 313.1034 | CE: 0.1303 | KD: 1060.0455\n",
      "Train Epoch: 024 Batch: 00078/00094 | Loss: 313.1091 | CE: 0.0993 | KD: 1060.1696\n",
      "Train Epoch: 024 Batch: 00079/00094 | Loss: 313.1234 | CE: 0.1370 | KD: 1060.0905\n",
      "Train Epoch: 024 Batch: 00080/00094 | Loss: 313.1043 | CE: 0.1160 | KD: 1060.0968\n",
      "Train Epoch: 024 Batch: 00081/00094 | Loss: 313.1368 | CE: 0.1568 | KD: 1060.0690\n",
      "Train Epoch: 024 Batch: 00082/00094 | Loss: 313.1189 | CE: 0.1309 | KD: 1060.0957\n",
      "Train Epoch: 024 Batch: 00083/00094 | Loss: 313.0762 | CE: 0.0860 | KD: 1060.1035\n",
      "Train Epoch: 024 Batch: 00084/00094 | Loss: 313.1379 | CE: 0.1290 | KD: 1060.1666\n",
      "Train Epoch: 024 Batch: 00085/00094 | Loss: 313.0811 | CE: 0.1040 | KD: 1060.0588\n",
      "Train Epoch: 024 Batch: 00086/00094 | Loss: 313.1457 | CE: 0.1632 | KD: 1060.0773\n",
      "Train Epoch: 024 Batch: 00087/00094 | Loss: 313.1155 | CE: 0.1304 | KD: 1060.0862\n",
      "Train Epoch: 024 Batch: 00088/00094 | Loss: 313.0848 | CE: 0.0857 | KD: 1060.1334\n",
      "Train Epoch: 024 Batch: 00089/00094 | Loss: 313.0961 | CE: 0.0972 | KD: 1060.1328\n",
      "Train Epoch: 024 Batch: 00090/00094 | Loss: 313.0840 | CE: 0.1106 | KD: 1060.0468\n",
      "Train Epoch: 024 Batch: 00091/00094 | Loss: 313.1268 | CE: 0.1308 | KD: 1060.1230\n",
      "Train Epoch: 024 Batch: 00092/00094 | Loss: 313.0981 | CE: 0.1106 | KD: 1060.0940\n",
      "Train Epoch: 024 Batch: 00093/00094 | Loss: 313.1130 | CE: 0.1435 | KD: 1060.0332\n",
      "Train Epoch: 024 Batch: 00094/00094 | Loss: 313.1136 | CE: 0.1311 | KD: 1060.0770\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1182 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1925 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5622 | acc:72.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 72.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9919 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8374 | acc:57.4843\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.48%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5433 | acc:74.3068\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5928 | acc:73.0799\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8104 | acc:58.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.06%\n",
      "[VAL Acc] Avg 66.33%\n",
      "Train Epoch: 025 Batch: 00001/00094 | Loss: 313.0913 | CE: 0.1224 | KD: 1060.0311\n",
      "Train Epoch: 025 Batch: 00002/00094 | Loss: 313.1011 | CE: 0.1222 | KD: 1060.0652\n",
      "Train Epoch: 025 Batch: 00003/00094 | Loss: 313.0795 | CE: 0.1097 | KD: 1060.0341\n",
      "Train Epoch: 025 Batch: 00004/00094 | Loss: 313.0909 | CE: 0.1271 | KD: 1060.0138\n",
      "Train Epoch: 025 Batch: 00005/00094 | Loss: 313.0979 | CE: 0.1095 | KD: 1060.0973\n",
      "Train Epoch: 025 Batch: 00006/00094 | Loss: 313.1080 | CE: 0.1147 | KD: 1060.1139\n",
      "Train Epoch: 025 Batch: 00007/00094 | Loss: 313.1294 | CE: 0.1544 | KD: 1060.0520\n",
      "Train Epoch: 025 Batch: 00008/00094 | Loss: 313.1123 | CE: 0.1278 | KD: 1060.0844\n",
      "Train Epoch: 025 Batch: 00009/00094 | Loss: 313.1005 | CE: 0.1215 | KD: 1060.0654\n",
      "Train Epoch: 025 Batch: 00010/00094 | Loss: 313.1760 | CE: 0.1656 | KD: 1060.1718\n",
      "Train Epoch: 025 Batch: 00011/00094 | Loss: 313.1698 | CE: 0.1867 | KD: 1060.0792\n",
      "Train Epoch: 025 Batch: 00012/00094 | Loss: 313.1057 | CE: 0.1314 | KD: 1060.0496\n",
      "Train Epoch: 025 Batch: 00013/00094 | Loss: 313.1327 | CE: 0.1476 | KD: 1060.0859\n",
      "Train Epoch: 025 Batch: 00014/00094 | Loss: 313.1222 | CE: 0.1486 | KD: 1060.0472\n",
      "Train Epoch: 025 Batch: 00015/00094 | Loss: 313.1916 | CE: 0.1907 | KD: 1060.1396\n",
      "Train Epoch: 025 Batch: 00016/00094 | Loss: 313.1290 | CE: 0.1506 | KD: 1060.0635\n",
      "Train Epoch: 025 Batch: 00017/00094 | Loss: 313.1816 | CE: 0.1847 | KD: 1060.1262\n",
      "Train Epoch: 025 Batch: 00018/00094 | Loss: 313.1100 | CE: 0.1219 | KD: 1060.0963\n",
      "Train Epoch: 025 Batch: 00019/00094 | Loss: 313.1014 | CE: 0.1091 | KD: 1060.1105\n",
      "Train Epoch: 025 Batch: 00020/00094 | Loss: 313.1133 | CE: 0.1321 | KD: 1060.0729\n",
      "Train Epoch: 025 Batch: 00021/00094 | Loss: 313.1120 | CE: 0.1166 | KD: 1060.1211\n",
      "Train Epoch: 025 Batch: 00022/00094 | Loss: 313.0775 | CE: 0.1025 | KD: 1060.0518\n",
      "Train Epoch: 025 Batch: 00023/00094 | Loss: 313.1422 | CE: 0.1493 | KD: 1060.1127\n",
      "Train Epoch: 025 Batch: 00024/00094 | Loss: 313.1401 | CE: 0.1293 | KD: 1060.1732\n",
      "Train Epoch: 025 Batch: 00025/00094 | Loss: 313.1524 | CE: 0.1680 | KD: 1060.0836\n",
      "Train Epoch: 025 Batch: 00026/00094 | Loss: 313.0849 | CE: 0.1008 | KD: 1060.0824\n",
      "Train Epoch: 025 Batch: 00027/00094 | Loss: 313.1490 | CE: 0.1723 | KD: 1060.0575\n",
      "Train Epoch: 025 Batch: 00028/00094 | Loss: 313.1063 | CE: 0.1163 | KD: 1060.1027\n",
      "Train Epoch: 025 Batch: 00029/00094 | Loss: 313.1761 | CE: 0.1920 | KD: 1060.0828\n",
      "Train Epoch: 025 Batch: 00030/00094 | Loss: 313.1667 | CE: 0.1558 | KD: 1060.1733\n",
      "Train Epoch: 025 Batch: 00031/00094 | Loss: 313.1240 | CE: 0.1229 | KD: 1060.1401\n",
      "Train Epoch: 025 Batch: 00032/00094 | Loss: 313.1129 | CE: 0.1292 | KD: 1060.0815\n",
      "Train Epoch: 025 Batch: 00033/00094 | Loss: 313.1084 | CE: 0.1115 | KD: 1060.1261\n",
      "Train Epoch: 025 Batch: 00034/00094 | Loss: 313.1127 | CE: 0.1045 | KD: 1060.1644\n",
      "Train Epoch: 025 Batch: 00035/00094 | Loss: 313.1169 | CE: 0.1373 | KD: 1060.0674\n",
      "Train Epoch: 025 Batch: 00036/00094 | Loss: 313.0915 | CE: 0.1171 | KD: 1060.0499\n",
      "Train Epoch: 025 Batch: 00037/00094 | Loss: 313.0800 | CE: 0.1080 | KD: 1060.0420\n",
      "Train Epoch: 025 Batch: 00038/00094 | Loss: 313.0652 | CE: 0.0934 | KD: 1060.0413\n",
      "Train Epoch: 025 Batch: 00039/00094 | Loss: 313.0798 | CE: 0.0886 | KD: 1060.1067\n",
      "Train Epoch: 025 Batch: 00040/00094 | Loss: 313.1428 | CE: 0.1574 | KD: 1060.0870\n",
      "Train Epoch: 025 Batch: 00041/00094 | Loss: 313.1155 | CE: 0.1130 | KD: 1060.1448\n",
      "Train Epoch: 025 Batch: 00042/00094 | Loss: 313.1642 | CE: 0.1780 | KD: 1060.0898\n",
      "Train Epoch: 025 Batch: 00043/00094 | Loss: 313.1080 | CE: 0.1274 | KD: 1060.0709\n",
      "Train Epoch: 025 Batch: 00044/00094 | Loss: 313.1042 | CE: 0.1185 | KD: 1060.0881\n",
      "Train Epoch: 025 Batch: 00045/00094 | Loss: 313.1187 | CE: 0.1181 | KD: 1060.1384\n",
      "Train Epoch: 025 Batch: 00046/00094 | Loss: 313.1424 | CE: 0.1698 | KD: 1060.0438\n",
      "Train Epoch: 025 Batch: 00047/00094 | Loss: 313.1250 | CE: 0.1283 | KD: 1060.1254\n",
      "Train Epoch: 025 Batch: 00048/00094 | Loss: 313.1676 | CE: 0.1829 | KD: 1060.0846\n",
      "Train Epoch: 025 Batch: 00049/00094 | Loss: 313.0934 | CE: 0.1168 | KD: 1060.0574\n",
      "Train Epoch: 025 Batch: 00050/00094 | Loss: 313.1404 | CE: 0.1660 | KD: 1060.0496\n",
      "Train Epoch: 025 Batch: 00051/00094 | Loss: 313.1472 | CE: 0.1557 | KD: 1060.1078\n",
      "Train Epoch: 025 Batch: 00052/00094 | Loss: 313.1249 | CE: 0.1262 | KD: 1060.1320\n",
      "Train Epoch: 025 Batch: 00053/00094 | Loss: 313.1245 | CE: 0.1384 | KD: 1060.0894\n",
      "Train Epoch: 025 Batch: 00054/00094 | Loss: 313.0911 | CE: 0.0955 | KD: 1060.1217\n",
      "Train Epoch: 025 Batch: 00055/00094 | Loss: 313.1352 | CE: 0.1323 | KD: 1060.1464\n",
      "Train Epoch: 025 Batch: 00056/00094 | Loss: 313.0974 | CE: 0.1144 | KD: 1060.0791\n",
      "Train Epoch: 025 Batch: 00057/00094 | Loss: 313.2125 | CE: 0.2328 | KD: 1060.0676\n",
      "Train Epoch: 025 Batch: 00058/00094 | Loss: 313.1423 | CE: 0.1546 | KD: 1060.0948\n",
      "Train Epoch: 025 Batch: 00059/00094 | Loss: 313.1427 | CE: 0.1537 | KD: 1060.0992\n",
      "Train Epoch: 025 Batch: 00060/00094 | Loss: 313.1229 | CE: 0.1297 | KD: 1060.1135\n",
      "Train Epoch: 025 Batch: 00061/00094 | Loss: 313.1219 | CE: 0.1222 | KD: 1060.1356\n",
      "Train Epoch: 025 Batch: 00062/00094 | Loss: 313.0709 | CE: 0.0894 | KD: 1060.0740\n",
      "Train Epoch: 025 Batch: 00063/00094 | Loss: 313.1063 | CE: 0.1055 | KD: 1060.1392\n",
      "Train Epoch: 025 Batch: 00064/00094 | Loss: 313.1132 | CE: 0.1264 | KD: 1060.0917\n",
      "Train Epoch: 025 Batch: 00065/00094 | Loss: 313.1191 | CE: 0.1357 | KD: 1060.0801\n",
      "Train Epoch: 025 Batch: 00066/00094 | Loss: 313.1395 | CE: 0.1484 | KD: 1060.1062\n",
      "Train Epoch: 025 Batch: 00067/00094 | Loss: 313.1088 | CE: 0.1046 | KD: 1060.1505\n",
      "Train Epoch: 025 Batch: 00068/00094 | Loss: 313.1243 | CE: 0.1432 | KD: 1060.0726\n",
      "Train Epoch: 025 Batch: 00069/00094 | Loss: 313.1383 | CE: 0.1497 | KD: 1060.0981\n",
      "Train Epoch: 025 Batch: 00070/00094 | Loss: 313.1252 | CE: 0.1231 | KD: 1060.1438\n",
      "Train Epoch: 025 Batch: 00071/00094 | Loss: 313.1430 | CE: 0.1632 | KD: 1060.0682\n",
      "Train Epoch: 025 Batch: 00072/00094 | Loss: 313.1108 | CE: 0.1298 | KD: 1060.0720\n",
      "Train Epoch: 025 Batch: 00073/00094 | Loss: 313.0802 | CE: 0.0979 | KD: 1060.0764\n",
      "Train Epoch: 025 Batch: 00074/00094 | Loss: 313.1178 | CE: 0.1369 | KD: 1060.0717\n",
      "Train Epoch: 025 Batch: 00075/00094 | Loss: 313.1079 | CE: 0.1000 | KD: 1060.1633\n",
      "Train Epoch: 025 Batch: 00076/00094 | Loss: 313.1189 | CE: 0.1269 | KD: 1060.1091\n",
      "Train Epoch: 025 Batch: 00077/00094 | Loss: 313.0987 | CE: 0.1008 | KD: 1060.1294\n",
      "Train Epoch: 025 Batch: 00078/00094 | Loss: 313.0959 | CE: 0.1075 | KD: 1060.0970\n",
      "Train Epoch: 025 Batch: 00079/00094 | Loss: 313.1053 | CE: 0.0969 | KD: 1060.1650\n",
      "Train Epoch: 025 Batch: 00080/00094 | Loss: 313.1297 | CE: 0.1364 | KD: 1060.1141\n",
      "Train Epoch: 025 Batch: 00081/00094 | Loss: 313.1147 | CE: 0.1084 | KD: 1060.1578\n",
      "Train Epoch: 025 Batch: 00082/00094 | Loss: 313.0832 | CE: 0.1128 | KD: 1060.0361\n",
      "Train Epoch: 025 Batch: 00083/00094 | Loss: 313.0873 | CE: 0.1040 | KD: 1060.0798\n",
      "Train Epoch: 025 Batch: 00084/00094 | Loss: 313.1201 | CE: 0.1288 | KD: 1060.1072\n",
      "Train Epoch: 025 Batch: 00085/00094 | Loss: 313.1318 | CE: 0.1489 | KD: 1060.0786\n",
      "Train Epoch: 025 Batch: 00086/00094 | Loss: 313.1096 | CE: 0.1340 | KD: 1060.0538\n",
      "Train Epoch: 025 Batch: 00087/00094 | Loss: 313.0719 | CE: 0.0949 | KD: 1060.0586\n",
      "Train Epoch: 025 Batch: 00088/00094 | Loss: 313.1186 | CE: 0.1324 | KD: 1060.0897\n",
      "Train Epoch: 025 Batch: 00089/00094 | Loss: 313.1218 | CE: 0.1484 | KD: 1060.0464\n",
      "Train Epoch: 025 Batch: 00090/00094 | Loss: 313.1029 | CE: 0.1075 | KD: 1060.1207\n",
      "Train Epoch: 025 Batch: 00091/00094 | Loss: 313.0801 | CE: 0.0960 | KD: 1060.0826\n",
      "Train Epoch: 025 Batch: 00092/00094 | Loss: 313.1189 | CE: 0.1263 | KD: 1060.1116\n",
      "Train Epoch: 025 Batch: 00093/00094 | Loss: 313.1205 | CE: 0.1274 | KD: 1060.1133\n",
      "Train Epoch: 025 Batch: 00094/00094 | Loss: 313.1099 | CE: 0.1320 | KD: 1060.0616\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1092 | acc:99.0000\n",
      "[VAL Acc] Target: 99.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2222 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5816 | acc:69.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 69.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0089 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8026 | acc:58.9342\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.93%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6027 | acc:71.3494\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5653 | acc:74.9216\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8034 | acc:57.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.06%\n",
      "[VAL Acc] Avg 66.04%\n",
      "Train Epoch: 026 Batch: 00001/00094 | Loss: 281.7924 | CE: 0.1084 | KD: 1060.0767\n",
      "Train Epoch: 026 Batch: 00002/00094 | Loss: 281.7734 | CE: 0.0940 | KD: 1060.0590\n",
      "Train Epoch: 026 Batch: 00003/00094 | Loss: 281.8068 | CE: 0.1223 | KD: 1060.0782\n",
      "Train Epoch: 026 Batch: 00004/00094 | Loss: 281.7967 | CE: 0.1154 | KD: 1060.0658\n",
      "Train Epoch: 026 Batch: 00005/00094 | Loss: 281.7865 | CE: 0.1049 | KD: 1060.0674\n",
      "Train Epoch: 026 Batch: 00006/00094 | Loss: 281.8061 | CE: 0.1345 | KD: 1060.0297\n",
      "Train Epoch: 026 Batch: 00007/00094 | Loss: 281.8729 | CE: 0.1962 | KD: 1060.0490\n",
      "Train Epoch: 026 Batch: 00008/00094 | Loss: 281.8163 | CE: 0.0983 | KD: 1060.2041\n",
      "Train Epoch: 026 Batch: 00009/00094 | Loss: 281.7946 | CE: 0.1202 | KD: 1060.0403\n",
      "Train Epoch: 026 Batch: 00010/00094 | Loss: 281.7949 | CE: 0.1096 | KD: 1060.0813\n",
      "Train Epoch: 026 Batch: 00011/00094 | Loss: 281.7864 | CE: 0.1059 | KD: 1060.0634\n",
      "Train Epoch: 026 Batch: 00012/00094 | Loss: 281.8713 | CE: 0.1813 | KD: 1060.0988\n",
      "Train Epoch: 026 Batch: 00013/00094 | Loss: 281.8258 | CE: 0.1446 | KD: 1060.0659\n",
      "Train Epoch: 026 Batch: 00014/00094 | Loss: 281.7834 | CE: 0.1123 | KD: 1060.0278\n",
      "Train Epoch: 026 Batch: 00015/00094 | Loss: 281.8222 | CE: 0.1391 | KD: 1060.0728\n",
      "Train Epoch: 026 Batch: 00016/00094 | Loss: 281.7822 | CE: 0.1068 | KD: 1060.0442\n",
      "Train Epoch: 026 Batch: 00017/00094 | Loss: 281.8435 | CE: 0.1437 | KD: 1060.1357\n",
      "Train Epoch: 026 Batch: 00018/00094 | Loss: 281.8176 | CE: 0.1185 | KD: 1060.1331\n",
      "Train Epoch: 026 Batch: 00019/00094 | Loss: 281.8211 | CE: 0.1266 | KD: 1060.1157\n",
      "Train Epoch: 026 Batch: 00020/00094 | Loss: 281.7874 | CE: 0.1063 | KD: 1060.0654\n",
      "Train Epoch: 026 Batch: 00021/00094 | Loss: 281.7930 | CE: 0.1037 | KD: 1060.0961\n",
      "Train Epoch: 026 Batch: 00022/00094 | Loss: 281.7792 | CE: 0.1133 | KD: 1060.0083\n",
      "Train Epoch: 026 Batch: 00023/00094 | Loss: 281.8258 | CE: 0.1251 | KD: 1060.1390\n",
      "Train Epoch: 026 Batch: 00024/00094 | Loss: 281.7857 | CE: 0.1052 | KD: 1060.0629\n",
      "Train Epoch: 026 Batch: 00025/00094 | Loss: 281.7870 | CE: 0.1050 | KD: 1060.0690\n",
      "Train Epoch: 026 Batch: 00026/00094 | Loss: 281.7944 | CE: 0.1178 | KD: 1060.0482\n",
      "Train Epoch: 026 Batch: 00027/00094 | Loss: 281.7797 | CE: 0.0950 | KD: 1060.0787\n",
      "Train Epoch: 026 Batch: 00028/00094 | Loss: 281.7811 | CE: 0.0989 | KD: 1060.0695\n",
      "Train Epoch: 026 Batch: 00029/00094 | Loss: 281.7784 | CE: 0.0925 | KD: 1060.0837\n",
      "Train Epoch: 026 Batch: 00030/00094 | Loss: 281.8120 | CE: 0.1211 | KD: 1060.1023\n",
      "Train Epoch: 026 Batch: 00031/00094 | Loss: 281.8090 | CE: 0.1235 | KD: 1060.0818\n",
      "Train Epoch: 026 Batch: 00032/00094 | Loss: 281.8088 | CE: 0.1304 | KD: 1060.0552\n",
      "Train Epoch: 026 Batch: 00033/00094 | Loss: 281.7919 | CE: 0.1005 | KD: 1060.1042\n",
      "Train Epoch: 026 Batch: 00034/00094 | Loss: 281.7766 | CE: 0.0899 | KD: 1060.0862\n",
      "Train Epoch: 026 Batch: 00035/00094 | Loss: 281.8277 | CE: 0.1333 | KD: 1060.1154\n",
      "Train Epoch: 026 Batch: 00036/00094 | Loss: 281.8245 | CE: 0.1252 | KD: 1060.1339\n",
      "Train Epoch: 026 Batch: 00037/00094 | Loss: 281.8492 | CE: 0.1456 | KD: 1060.1498\n",
      "Train Epoch: 026 Batch: 00038/00094 | Loss: 281.8166 | CE: 0.1284 | KD: 1060.0920\n",
      "Train Epoch: 026 Batch: 00039/00094 | Loss: 281.8261 | CE: 0.1296 | KD: 1060.1234\n",
      "Train Epoch: 026 Batch: 00040/00094 | Loss: 281.8142 | CE: 0.1208 | KD: 1060.1116\n",
      "Train Epoch: 026 Batch: 00041/00094 | Loss: 281.8062 | CE: 0.1082 | KD: 1060.1289\n",
      "Train Epoch: 026 Batch: 00042/00094 | Loss: 281.8137 | CE: 0.1226 | KD: 1060.1030\n",
      "Train Epoch: 026 Batch: 00043/00094 | Loss: 281.8189 | CE: 0.1204 | KD: 1060.1309\n",
      "Train Epoch: 026 Batch: 00044/00094 | Loss: 281.8146 | CE: 0.1117 | KD: 1060.1475\n",
      "Train Epoch: 026 Batch: 00045/00094 | Loss: 281.7953 | CE: 0.1051 | KD: 1060.1000\n",
      "Train Epoch: 026 Batch: 00046/00094 | Loss: 281.8249 | CE: 0.1331 | KD: 1060.1055\n",
      "Train Epoch: 026 Batch: 00047/00094 | Loss: 281.8187 | CE: 0.1186 | KD: 1060.1372\n",
      "Train Epoch: 026 Batch: 00048/00094 | Loss: 281.8015 | CE: 0.1243 | KD: 1060.0507\n",
      "Train Epoch: 026 Batch: 00049/00094 | Loss: 281.8522 | CE: 0.1554 | KD: 1060.1245\n",
      "Train Epoch: 026 Batch: 00050/00094 | Loss: 281.8362 | CE: 0.1447 | KD: 1060.1047\n",
      "Train Epoch: 026 Batch: 00051/00094 | Loss: 281.7771 | CE: 0.0928 | KD: 1060.0774\n",
      "Train Epoch: 026 Batch: 00052/00094 | Loss: 281.7960 | CE: 0.1030 | KD: 1060.1102\n",
      "Train Epoch: 026 Batch: 00053/00094 | Loss: 281.8283 | CE: 0.1268 | KD: 1060.1422\n",
      "Train Epoch: 026 Batch: 00054/00094 | Loss: 281.8514 | CE: 0.1526 | KD: 1060.1320\n",
      "Train Epoch: 026 Batch: 00055/00094 | Loss: 281.8187 | CE: 0.1240 | KD: 1060.1168\n",
      "Train Epoch: 026 Batch: 00056/00094 | Loss: 281.8320 | CE: 0.1386 | KD: 1060.1117\n",
      "Train Epoch: 026 Batch: 00057/00094 | Loss: 281.7994 | CE: 0.1018 | KD: 1060.1276\n",
      "Train Epoch: 026 Batch: 00058/00094 | Loss: 281.8076 | CE: 0.1214 | KD: 1060.0846\n",
      "Train Epoch: 026 Batch: 00059/00094 | Loss: 281.8638 | CE: 0.1701 | KD: 1060.1130\n",
      "Train Epoch: 026 Batch: 00060/00094 | Loss: 281.7997 | CE: 0.0978 | KD: 1060.1434\n",
      "Train Epoch: 026 Batch: 00061/00094 | Loss: 281.8134 | CE: 0.1237 | KD: 1060.0979\n",
      "Train Epoch: 026 Batch: 00062/00094 | Loss: 281.8351 | CE: 0.1510 | KD: 1060.0771\n",
      "Train Epoch: 026 Batch: 00063/00094 | Loss: 281.8286 | CE: 0.1458 | KD: 1060.0718\n",
      "Train Epoch: 026 Batch: 00064/00094 | Loss: 281.8047 | CE: 0.1006 | KD: 1060.1516\n",
      "Train Epoch: 026 Batch: 00065/00094 | Loss: 281.7867 | CE: 0.1127 | KD: 1060.0386\n",
      "Train Epoch: 026 Batch: 00066/00094 | Loss: 281.7728 | CE: 0.0925 | KD: 1060.0624\n",
      "Train Epoch: 026 Batch: 00067/00094 | Loss: 281.8224 | CE: 0.1342 | KD: 1060.0920\n",
      "Train Epoch: 026 Batch: 00068/00094 | Loss: 281.7897 | CE: 0.1152 | KD: 1060.0405\n",
      "Train Epoch: 026 Batch: 00069/00094 | Loss: 281.9306 | CE: 0.2291 | KD: 1060.1421\n",
      "Train Epoch: 026 Batch: 00070/00094 | Loss: 281.7742 | CE: 0.0889 | KD: 1060.0812\n",
      "Train Epoch: 026 Batch: 00071/00094 | Loss: 281.7958 | CE: 0.1012 | KD: 1060.1161\n",
      "Train Epoch: 026 Batch: 00072/00094 | Loss: 281.7814 | CE: 0.0921 | KD: 1060.0964\n",
      "Train Epoch: 026 Batch: 00073/00094 | Loss: 281.8056 | CE: 0.1142 | KD: 1060.1042\n",
      "Train Epoch: 026 Batch: 00074/00094 | Loss: 281.8715 | CE: 0.1684 | KD: 1060.1479\n",
      "Train Epoch: 026 Batch: 00075/00094 | Loss: 281.8135 | CE: 0.1210 | KD: 1060.1084\n",
      "Train Epoch: 026 Batch: 00076/00094 | Loss: 281.8128 | CE: 0.1440 | KD: 1060.0190\n",
      "Train Epoch: 026 Batch: 00077/00094 | Loss: 281.8489 | CE: 0.1397 | KD: 1060.1711\n",
      "Train Epoch: 026 Batch: 00078/00094 | Loss: 281.8323 | CE: 0.1443 | KD: 1060.0914\n",
      "Train Epoch: 026 Batch: 00079/00094 | Loss: 281.9850 | CE: 0.2853 | KD: 1060.1355\n",
      "Train Epoch: 026 Batch: 00080/00094 | Loss: 281.8089 | CE: 0.1156 | KD: 1060.1113\n",
      "Train Epoch: 026 Batch: 00081/00094 | Loss: 281.8277 | CE: 0.1303 | KD: 1060.1270\n",
      "Train Epoch: 026 Batch: 00082/00094 | Loss: 281.8471 | CE: 0.1439 | KD: 1060.1486\n",
      "Train Epoch: 026 Batch: 00083/00094 | Loss: 281.8475 | CE: 0.1491 | KD: 1060.1305\n",
      "Train Epoch: 026 Batch: 00084/00094 | Loss: 281.8307 | CE: 0.1413 | KD: 1060.0966\n",
      "Train Epoch: 026 Batch: 00085/00094 | Loss: 281.8847 | CE: 0.1955 | KD: 1060.0959\n",
      "Train Epoch: 026 Batch: 00086/00094 | Loss: 281.7967 | CE: 0.1147 | KD: 1060.0688\n",
      "Train Epoch: 026 Batch: 00087/00094 | Loss: 281.8201 | CE: 0.1209 | KD: 1060.1334\n",
      "Train Epoch: 026 Batch: 00088/00094 | Loss: 281.8382 | CE: 0.1340 | KD: 1060.1522\n",
      "Train Epoch: 026 Batch: 00089/00094 | Loss: 281.8016 | CE: 0.1220 | KD: 1060.0598\n",
      "Train Epoch: 026 Batch: 00090/00094 | Loss: 281.7904 | CE: 0.0997 | KD: 1060.1016\n",
      "Train Epoch: 026 Batch: 00091/00094 | Loss: 281.8026 | CE: 0.1014 | KD: 1060.1412\n",
      "Train Epoch: 026 Batch: 00092/00094 | Loss: 281.7866 | CE: 0.0897 | KD: 1060.1249\n",
      "Train Epoch: 026 Batch: 00093/00094 | Loss: 281.8743 | CE: 0.1657 | KD: 1060.1688\n",
      "Train Epoch: 026 Batch: 00094/00094 | Loss: 281.7991 | CE: 0.1075 | KD: 1060.1047\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1141 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1998 | acc:49.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6014 | acc:71.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 71.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0344 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8928 | acc:56.8182\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5014 | acc:76.8022\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5944 | acc:73.7461\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8860 | acc:55.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.12%\n",
      "[VAL Acc] Avg 65.80%\n",
      "Train Epoch: 027 Batch: 00001/00094 | Loss: 281.8149 | CE: 0.1198 | KD: 1060.1182\n",
      "Train Epoch: 027 Batch: 00002/00094 | Loss: 281.8260 | CE: 0.1313 | KD: 1060.1165\n",
      "Train Epoch: 027 Batch: 00003/00094 | Loss: 281.7795 | CE: 0.1002 | KD: 1060.0586\n",
      "Train Epoch: 027 Batch: 00004/00094 | Loss: 281.8067 | CE: 0.1040 | KD: 1060.1467\n",
      "Train Epoch: 027 Batch: 00005/00094 | Loss: 281.8216 | CE: 0.1438 | KD: 1060.0532\n",
      "Train Epoch: 027 Batch: 00006/00094 | Loss: 281.8202 | CE: 0.1405 | KD: 1060.0602\n",
      "Train Epoch: 027 Batch: 00007/00094 | Loss: 281.8256 | CE: 0.1393 | KD: 1060.0848\n",
      "Train Epoch: 027 Batch: 00008/00094 | Loss: 281.8122 | CE: 0.1153 | KD: 1060.1246\n",
      "Train Epoch: 027 Batch: 00009/00094 | Loss: 281.8845 | CE: 0.1955 | KD: 1060.0953\n",
      "Train Epoch: 027 Batch: 00010/00094 | Loss: 281.7930 | CE: 0.1086 | KD: 1060.0780\n",
      "Train Epoch: 027 Batch: 00011/00094 | Loss: 281.8095 | CE: 0.1043 | KD: 1060.1564\n",
      "Train Epoch: 027 Batch: 00012/00094 | Loss: 281.7755 | CE: 0.0937 | KD: 1060.0681\n",
      "Train Epoch: 027 Batch: 00013/00094 | Loss: 281.8438 | CE: 0.1399 | KD: 1060.1512\n",
      "Train Epoch: 027 Batch: 00014/00094 | Loss: 281.8049 | CE: 0.1069 | KD: 1060.1292\n",
      "Train Epoch: 027 Batch: 00015/00094 | Loss: 281.8237 | CE: 0.1222 | KD: 1060.1420\n",
      "Train Epoch: 027 Batch: 00016/00094 | Loss: 281.7760 | CE: 0.0857 | KD: 1060.1001\n",
      "Train Epoch: 027 Batch: 00017/00094 | Loss: 281.8016 | CE: 0.1207 | KD: 1060.0646\n",
      "Train Epoch: 027 Batch: 00018/00094 | Loss: 281.7875 | CE: 0.0968 | KD: 1060.1012\n",
      "Train Epoch: 027 Batch: 00019/00094 | Loss: 281.8372 | CE: 0.1376 | KD: 1060.1353\n",
      "Train Epoch: 027 Batch: 00020/00094 | Loss: 281.7825 | CE: 0.0897 | KD: 1060.1097\n",
      "Train Epoch: 027 Batch: 00021/00094 | Loss: 281.7861 | CE: 0.1137 | KD: 1060.0325\n",
      "Train Epoch: 027 Batch: 00022/00094 | Loss: 281.7790 | CE: 0.0943 | KD: 1060.0789\n",
      "Train Epoch: 027 Batch: 00023/00094 | Loss: 281.7993 | CE: 0.1063 | KD: 1060.1102\n",
      "Train Epoch: 027 Batch: 00024/00094 | Loss: 281.8250 | CE: 0.1146 | KD: 1060.1753\n",
      "Train Epoch: 027 Batch: 00025/00094 | Loss: 281.7964 | CE: 0.1180 | KD: 1060.0553\n",
      "Train Epoch: 027 Batch: 00026/00094 | Loss: 281.8053 | CE: 0.1141 | KD: 1060.1036\n",
      "Train Epoch: 027 Batch: 00027/00094 | Loss: 281.8102 | CE: 0.1346 | KD: 1060.0450\n",
      "Train Epoch: 027 Batch: 00028/00094 | Loss: 281.8211 | CE: 0.1221 | KD: 1060.1328\n",
      "Train Epoch: 027 Batch: 00029/00094 | Loss: 281.7947 | CE: 0.1075 | KD: 1060.0881\n",
      "Train Epoch: 027 Batch: 00030/00094 | Loss: 281.7912 | CE: 0.1061 | KD: 1060.0807\n",
      "Train Epoch: 027 Batch: 00031/00094 | Loss: 281.7907 | CE: 0.1108 | KD: 1060.0608\n",
      "Train Epoch: 027 Batch: 00032/00094 | Loss: 281.9073 | CE: 0.2062 | KD: 1060.1405\n",
      "Train Epoch: 027 Batch: 00033/00094 | Loss: 281.8234 | CE: 0.1358 | KD: 1060.0896\n",
      "Train Epoch: 027 Batch: 00034/00094 | Loss: 281.8404 | CE: 0.1517 | KD: 1060.0944\n",
      "Train Epoch: 027 Batch: 00035/00094 | Loss: 281.8133 | CE: 0.1292 | KD: 1060.0768\n",
      "Train Epoch: 027 Batch: 00036/00094 | Loss: 281.8541 | CE: 0.1669 | KD: 1060.0884\n",
      "Train Epoch: 027 Batch: 00037/00094 | Loss: 281.8152 | CE: 0.1250 | KD: 1060.0999\n",
      "Train Epoch: 027 Batch: 00038/00094 | Loss: 281.8453 | CE: 0.1446 | KD: 1060.1394\n",
      "Train Epoch: 027 Batch: 00039/00094 | Loss: 281.8489 | CE: 0.1556 | KD: 1060.1115\n",
      "Train Epoch: 027 Batch: 00040/00094 | Loss: 281.7979 | CE: 0.1251 | KD: 1060.0342\n",
      "Train Epoch: 027 Batch: 00041/00094 | Loss: 281.8910 | CE: 0.1830 | KD: 1060.1665\n",
      "Train Epoch: 027 Batch: 00042/00094 | Loss: 281.8065 | CE: 0.1137 | KD: 1060.1094\n",
      "Train Epoch: 027 Batch: 00043/00094 | Loss: 281.7832 | CE: 0.1069 | KD: 1060.0472\n",
      "Train Epoch: 027 Batch: 00044/00094 | Loss: 281.8070 | CE: 0.1314 | KD: 1060.0447\n",
      "Train Epoch: 027 Batch: 00045/00094 | Loss: 281.8019 | CE: 0.1158 | KD: 1060.0842\n",
      "Train Epoch: 027 Batch: 00046/00094 | Loss: 281.8443 | CE: 0.1393 | KD: 1060.1554\n",
      "Train Epoch: 027 Batch: 00047/00094 | Loss: 281.7961 | CE: 0.1003 | KD: 1060.1206\n",
      "Train Epoch: 027 Batch: 00048/00094 | Loss: 281.8000 | CE: 0.1092 | KD: 1060.1019\n",
      "Train Epoch: 027 Batch: 00049/00094 | Loss: 281.8149 | CE: 0.1129 | KD: 1060.1443\n",
      "Train Epoch: 027 Batch: 00050/00094 | Loss: 281.7746 | CE: 0.0860 | KD: 1060.0935\n",
      "Train Epoch: 027 Batch: 00051/00094 | Loss: 281.7844 | CE: 0.1005 | KD: 1060.0758\n",
      "Train Epoch: 027 Batch: 00052/00094 | Loss: 281.7905 | CE: 0.0891 | KD: 1060.1420\n",
      "Train Epoch: 027 Batch: 00053/00094 | Loss: 281.8042 | CE: 0.1165 | KD: 1060.0902\n",
      "Train Epoch: 027 Batch: 00054/00094 | Loss: 281.7689 | CE: 0.0872 | KD: 1060.0677\n",
      "Train Epoch: 027 Batch: 00055/00094 | Loss: 281.7630 | CE: 0.0912 | KD: 1060.0303\n",
      "Train Epoch: 027 Batch: 00056/00094 | Loss: 281.7978 | CE: 0.1184 | KD: 1060.0590\n",
      "Train Epoch: 027 Batch: 00057/00094 | Loss: 281.8221 | CE: 0.1290 | KD: 1060.1104\n",
      "Train Epoch: 027 Batch: 00058/00094 | Loss: 281.8014 | CE: 0.1154 | KD: 1060.0836\n",
      "Train Epoch: 027 Batch: 00059/00094 | Loss: 281.7935 | CE: 0.1135 | KD: 1060.0614\n",
      "Train Epoch: 027 Batch: 00060/00094 | Loss: 281.8012 | CE: 0.1169 | KD: 1060.0775\n",
      "Train Epoch: 027 Batch: 00061/00094 | Loss: 281.7702 | CE: 0.0863 | KD: 1060.0757\n",
      "Train Epoch: 027 Batch: 00062/00094 | Loss: 281.8081 | CE: 0.1171 | KD: 1060.1029\n",
      "Train Epoch: 027 Batch: 00063/00094 | Loss: 281.8174 | CE: 0.1250 | KD: 1060.1082\n",
      "Train Epoch: 027 Batch: 00064/00094 | Loss: 281.8982 | CE: 0.2061 | KD: 1060.1069\n",
      "Train Epoch: 027 Batch: 00065/00094 | Loss: 281.8382 | CE: 0.1396 | KD: 1060.1311\n",
      "Train Epoch: 027 Batch: 00066/00094 | Loss: 281.8069 | CE: 0.1125 | KD: 1060.1154\n",
      "Train Epoch: 027 Batch: 00067/00094 | Loss: 281.7953 | CE: 0.1140 | KD: 1060.0663\n",
      "Train Epoch: 027 Batch: 00068/00094 | Loss: 281.7796 | CE: 0.0914 | KD: 1060.0922\n",
      "Train Epoch: 027 Batch: 00069/00094 | Loss: 281.8208 | CE: 0.1246 | KD: 1060.1223\n",
      "Train Epoch: 027 Batch: 00070/00094 | Loss: 281.8125 | CE: 0.1193 | KD: 1060.1110\n",
      "Train Epoch: 027 Batch: 00071/00094 | Loss: 281.7907 | CE: 0.1066 | KD: 1060.0768\n",
      "Train Epoch: 027 Batch: 00072/00094 | Loss: 281.7973 | CE: 0.1024 | KD: 1060.1172\n",
      "Train Epoch: 027 Batch: 00073/00094 | Loss: 281.8210 | CE: 0.1330 | KD: 1060.0914\n",
      "Train Epoch: 027 Batch: 00074/00094 | Loss: 281.7738 | CE: 0.0886 | KD: 1060.0808\n",
      "Train Epoch: 027 Batch: 00075/00094 | Loss: 281.8554 | CE: 0.1654 | KD: 1060.0988\n",
      "Train Epoch: 027 Batch: 00076/00094 | Loss: 281.8092 | CE: 0.1144 | KD: 1060.1168\n",
      "Train Epoch: 027 Batch: 00077/00094 | Loss: 281.7927 | CE: 0.1180 | KD: 1060.0411\n",
      "Train Epoch: 027 Batch: 00078/00094 | Loss: 281.8304 | CE: 0.1393 | KD: 1060.1030\n",
      "Train Epoch: 027 Batch: 00079/00094 | Loss: 281.7763 | CE: 0.1048 | KD: 1060.0294\n",
      "Train Epoch: 027 Batch: 00080/00094 | Loss: 281.8136 | CE: 0.1252 | KD: 1060.0929\n",
      "Train Epoch: 027 Batch: 00081/00094 | Loss: 281.7993 | CE: 0.1115 | KD: 1060.0906\n",
      "Train Epoch: 027 Batch: 00082/00094 | Loss: 281.7918 | CE: 0.1003 | KD: 1060.1047\n",
      "Train Epoch: 027 Batch: 00083/00094 | Loss: 281.7858 | CE: 0.1051 | KD: 1060.0638\n",
      "Train Epoch: 027 Batch: 00084/00094 | Loss: 281.8830 | CE: 0.1913 | KD: 1060.1052\n",
      "Train Epoch: 027 Batch: 00085/00094 | Loss: 281.8448 | CE: 0.1609 | KD: 1060.0758\n",
      "Train Epoch: 027 Batch: 00086/00094 | Loss: 281.8267 | CE: 0.1549 | KD: 1060.0305\n",
      "Train Epoch: 027 Batch: 00087/00094 | Loss: 281.7889 | CE: 0.0953 | KD: 1060.1125\n",
      "Train Epoch: 027 Batch: 00088/00094 | Loss: 281.8460 | CE: 0.1394 | KD: 1060.1614\n",
      "Train Epoch: 027 Batch: 00089/00094 | Loss: 281.8160 | CE: 0.1281 | KD: 1060.0909\n",
      "Train Epoch: 027 Batch: 00090/00094 | Loss: 281.8125 | CE: 0.1187 | KD: 1060.1132\n",
      "Train Epoch: 027 Batch: 00091/00094 | Loss: 281.8874 | CE: 0.1684 | KD: 1060.2083\n",
      "Train Epoch: 027 Batch: 00092/00094 | Loss: 281.8216 | CE: 0.1231 | KD: 1060.1309\n",
      "Train Epoch: 027 Batch: 00093/00094 | Loss: 282.0036 | CE: 0.3047 | KD: 1060.1321\n",
      "Train Epoch: 027 Batch: 00094/00094 | Loss: 281.8100 | CE: 0.0965 | KD: 1060.1874\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1087 | acc:99.0500\n",
      "[VAL Acc] Target: 99.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2868 | acc:49.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6433 | acc:67.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 67.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0455 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9294 | acc:55.7210\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4545 | acc:81.3309\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 81.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6032 | acc:73.5893\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.59%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9416 | acc:54.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.31%\n",
      "[VAL Acc] Avg 66.12%\n",
      "Train Epoch: 028 Batch: 00001/00094 | Loss: 281.8156 | CE: 0.1269 | KD: 1060.0942\n",
      "Train Epoch: 028 Batch: 00002/00094 | Loss: 281.8088 | CE: 0.0979 | KD: 1060.1776\n",
      "Train Epoch: 028 Batch: 00003/00094 | Loss: 281.8073 | CE: 0.1416 | KD: 1060.0078\n",
      "Train Epoch: 028 Batch: 00004/00094 | Loss: 281.8470 | CE: 0.1382 | KD: 1060.1694\n",
      "Train Epoch: 028 Batch: 00005/00094 | Loss: 281.8419 | CE: 0.1618 | KD: 1060.0615\n",
      "Train Epoch: 028 Batch: 00006/00094 | Loss: 281.8026 | CE: 0.1212 | KD: 1060.0667\n",
      "Train Epoch: 028 Batch: 00007/00094 | Loss: 281.8004 | CE: 0.1178 | KD: 1060.0712\n",
      "Train Epoch: 028 Batch: 00008/00094 | Loss: 281.8103 | CE: 0.1175 | KD: 1060.1097\n",
      "Train Epoch: 028 Batch: 00009/00094 | Loss: 281.7976 | CE: 0.1048 | KD: 1060.1096\n",
      "Train Epoch: 028 Batch: 00010/00094 | Loss: 281.7983 | CE: 0.1042 | KD: 1060.1144\n",
      "Train Epoch: 028 Batch: 00011/00094 | Loss: 281.8101 | CE: 0.1434 | KD: 1060.0111\n",
      "Train Epoch: 028 Batch: 00012/00094 | Loss: 281.8034 | CE: 0.1193 | KD: 1060.0765\n",
      "Train Epoch: 028 Batch: 00013/00094 | Loss: 281.7853 | CE: 0.1074 | KD: 1060.0535\n",
      "Train Epoch: 028 Batch: 00014/00094 | Loss: 281.7842 | CE: 0.1174 | KD: 1060.0118\n",
      "Train Epoch: 028 Batch: 00015/00094 | Loss: 281.8016 | CE: 0.0986 | KD: 1060.1477\n",
      "Train Epoch: 028 Batch: 00016/00094 | Loss: 281.8327 | CE: 0.1440 | KD: 1060.0940\n",
      "Train Epoch: 028 Batch: 00017/00094 | Loss: 281.8094 | CE: 0.1212 | KD: 1060.0918\n",
      "Train Epoch: 028 Batch: 00018/00094 | Loss: 281.8301 | CE: 0.1389 | KD: 1060.1033\n",
      "Train Epoch: 028 Batch: 00019/00094 | Loss: 281.7841 | CE: 0.0958 | KD: 1060.0924\n",
      "Train Epoch: 028 Batch: 00020/00094 | Loss: 281.8600 | CE: 0.1416 | KD: 1060.2058\n",
      "Train Epoch: 028 Batch: 00021/00094 | Loss: 281.7974 | CE: 0.1086 | KD: 1060.0946\n",
      "Train Epoch: 028 Batch: 00022/00094 | Loss: 281.8213 | CE: 0.1201 | KD: 1060.1412\n",
      "Train Epoch: 028 Batch: 00023/00094 | Loss: 281.8087 | CE: 0.0897 | KD: 1060.2079\n",
      "Train Epoch: 028 Batch: 00024/00094 | Loss: 281.8009 | CE: 0.1097 | KD: 1060.1034\n",
      "Train Epoch: 028 Batch: 00025/00094 | Loss: 281.8121 | CE: 0.1115 | KD: 1060.1388\n",
      "Train Epoch: 028 Batch: 00026/00094 | Loss: 281.8033 | CE: 0.1120 | KD: 1060.1036\n",
      "Train Epoch: 028 Batch: 00027/00094 | Loss: 281.7937 | CE: 0.1068 | KD: 1060.0873\n",
      "Train Epoch: 028 Batch: 00028/00094 | Loss: 281.8164 | CE: 0.1223 | KD: 1060.1144\n",
      "Train Epoch: 028 Batch: 00029/00094 | Loss: 281.8011 | CE: 0.1033 | KD: 1060.1283\n",
      "Train Epoch: 028 Batch: 00030/00094 | Loss: 281.8248 | CE: 0.1304 | KD: 1060.1156\n",
      "Train Epoch: 028 Batch: 00031/00094 | Loss: 281.7901 | CE: 0.0896 | KD: 1060.1382\n",
      "Train Epoch: 028 Batch: 00032/00094 | Loss: 281.8393 | CE: 0.1499 | KD: 1060.0964\n",
      "Train Epoch: 028 Batch: 00033/00094 | Loss: 281.8111 | CE: 0.1240 | KD: 1060.0881\n",
      "Train Epoch: 028 Batch: 00034/00094 | Loss: 281.8130 | CE: 0.1170 | KD: 1060.1217\n",
      "Train Epoch: 028 Batch: 00035/00094 | Loss: 281.8195 | CE: 0.1272 | KD: 1060.1073\n",
      "Train Epoch: 028 Batch: 00036/00094 | Loss: 281.8103 | CE: 0.1262 | KD: 1060.0768\n",
      "Train Epoch: 028 Batch: 00037/00094 | Loss: 281.8091 | CE: 0.1254 | KD: 1060.0754\n",
      "Train Epoch: 028 Batch: 00038/00094 | Loss: 281.7976 | CE: 0.1093 | KD: 1060.0927\n",
      "Train Epoch: 028 Batch: 00039/00094 | Loss: 281.8097 | CE: 0.1230 | KD: 1060.0867\n",
      "Train Epoch: 028 Batch: 00040/00094 | Loss: 281.7994 | CE: 0.1094 | KD: 1060.0991\n",
      "Train Epoch: 028 Batch: 00041/00094 | Loss: 281.7899 | CE: 0.0908 | KD: 1060.1329\n",
      "Train Epoch: 028 Batch: 00042/00094 | Loss: 281.8120 | CE: 0.1079 | KD: 1060.1521\n",
      "Train Epoch: 028 Batch: 00043/00094 | Loss: 281.8322 | CE: 0.1525 | KD: 1060.0603\n",
      "Train Epoch: 028 Batch: 00044/00094 | Loss: 281.7677 | CE: 0.1032 | KD: 1060.0031\n",
      "Train Epoch: 028 Batch: 00045/00094 | Loss: 281.8440 | CE: 0.1575 | KD: 1060.0856\n",
      "Train Epoch: 028 Batch: 00046/00094 | Loss: 281.7984 | CE: 0.1094 | KD: 1060.0951\n",
      "Train Epoch: 028 Batch: 00047/00094 | Loss: 281.7746 | CE: 0.0942 | KD: 1060.0630\n",
      "Train Epoch: 028 Batch: 00048/00094 | Loss: 281.8398 | CE: 0.1458 | KD: 1060.1140\n",
      "Train Epoch: 028 Batch: 00049/00094 | Loss: 281.7942 | CE: 0.0989 | KD: 1060.1188\n",
      "Train Epoch: 028 Batch: 00050/00094 | Loss: 281.7941 | CE: 0.1047 | KD: 1060.0967\n",
      "Train Epoch: 028 Batch: 00051/00094 | Loss: 281.7948 | CE: 0.1061 | KD: 1060.0944\n",
      "Train Epoch: 028 Batch: 00052/00094 | Loss: 281.8272 | CE: 0.1514 | KD: 1060.0453\n",
      "Train Epoch: 028 Batch: 00053/00094 | Loss: 281.8153 | CE: 0.1260 | KD: 1060.0962\n",
      "Train Epoch: 028 Batch: 00054/00094 | Loss: 281.8549 | CE: 0.1467 | KD: 1060.1676\n",
      "Train Epoch: 028 Batch: 00055/00094 | Loss: 281.8528 | CE: 0.1497 | KD: 1060.1479\n",
      "Train Epoch: 028 Batch: 00056/00094 | Loss: 281.7740 | CE: 0.0931 | KD: 1060.0646\n",
      "Train Epoch: 028 Batch: 00057/00094 | Loss: 281.7997 | CE: 0.1143 | KD: 1060.0815\n",
      "Train Epoch: 028 Batch: 00058/00094 | Loss: 281.8073 | CE: 0.1164 | KD: 1060.1022\n",
      "Train Epoch: 028 Batch: 00059/00094 | Loss: 281.7833 | CE: 0.1130 | KD: 1060.0245\n",
      "Train Epoch: 028 Batch: 00060/00094 | Loss: 281.8160 | CE: 0.1272 | KD: 1060.0946\n",
      "Train Epoch: 028 Batch: 00061/00094 | Loss: 281.8047 | CE: 0.1198 | KD: 1060.0800\n",
      "Train Epoch: 028 Batch: 00062/00094 | Loss: 281.7962 | CE: 0.1154 | KD: 1060.0643\n",
      "Train Epoch: 028 Batch: 00063/00094 | Loss: 281.7764 | CE: 0.1134 | KD: 1059.9973\n",
      "Train Epoch: 028 Batch: 00064/00094 | Loss: 281.8063 | CE: 0.1020 | KD: 1060.1527\n",
      "Train Epoch: 028 Batch: 00065/00094 | Loss: 281.8148 | CE: 0.1158 | KD: 1060.1326\n",
      "Train Epoch: 028 Batch: 00066/00094 | Loss: 281.8003 | CE: 0.1134 | KD: 1060.0872\n",
      "Train Epoch: 028 Batch: 00067/00094 | Loss: 281.7827 | CE: 0.0927 | KD: 1060.0991\n",
      "Train Epoch: 028 Batch: 00068/00094 | Loss: 281.7771 | CE: 0.0783 | KD: 1060.1320\n",
      "Train Epoch: 028 Batch: 00069/00094 | Loss: 281.8350 | CE: 0.1366 | KD: 1060.1305\n",
      "Train Epoch: 028 Batch: 00070/00094 | Loss: 281.7818 | CE: 0.0988 | KD: 1060.0726\n",
      "Train Epoch: 028 Batch: 00071/00094 | Loss: 281.7890 | CE: 0.0983 | KD: 1060.1017\n",
      "Train Epoch: 028 Batch: 00072/00094 | Loss: 281.8065 | CE: 0.1025 | KD: 1060.1512\n",
      "Train Epoch: 028 Batch: 00073/00094 | Loss: 281.8086 | CE: 0.1102 | KD: 1060.1307\n",
      "Train Epoch: 028 Batch: 00074/00094 | Loss: 281.8560 | CE: 0.1596 | KD: 1060.1233\n",
      "Train Epoch: 028 Batch: 00075/00094 | Loss: 281.7980 | CE: 0.1148 | KD: 1060.0735\n",
      "Train Epoch: 028 Batch: 00076/00094 | Loss: 281.8048 | CE: 0.1157 | KD: 1060.0957\n",
      "Train Epoch: 028 Batch: 00077/00094 | Loss: 281.7983 | CE: 0.1148 | KD: 1060.0747\n",
      "Train Epoch: 028 Batch: 00078/00094 | Loss: 281.8084 | CE: 0.1028 | KD: 1060.1576\n",
      "Train Epoch: 028 Batch: 00079/00094 | Loss: 281.8431 | CE: 0.1280 | KD: 1060.1931\n",
      "Train Epoch: 028 Batch: 00080/00094 | Loss: 281.8414 | CE: 0.1552 | KD: 1060.0845\n",
      "Train Epoch: 028 Batch: 00081/00094 | Loss: 281.7997 | CE: 0.1168 | KD: 1060.0723\n",
      "Train Epoch: 028 Batch: 00082/00094 | Loss: 281.8350 | CE: 0.1355 | KD: 1060.1344\n",
      "Train Epoch: 028 Batch: 00083/00094 | Loss: 281.7630 | CE: 0.0922 | KD: 1060.0269\n",
      "Train Epoch: 028 Batch: 00084/00094 | Loss: 281.8133 | CE: 0.1304 | KD: 1060.0721\n",
      "Train Epoch: 028 Batch: 00085/00094 | Loss: 281.8250 | CE: 0.1224 | KD: 1060.1466\n",
      "Train Epoch: 028 Batch: 00086/00094 | Loss: 281.7917 | CE: 0.1073 | KD: 1060.0778\n",
      "Train Epoch: 028 Batch: 00087/00094 | Loss: 281.8221 | CE: 0.1405 | KD: 1060.0676\n",
      "Train Epoch: 028 Batch: 00088/00094 | Loss: 281.7987 | CE: 0.1028 | KD: 1060.1211\n",
      "Train Epoch: 028 Batch: 00089/00094 | Loss: 281.7843 | CE: 0.1061 | KD: 1060.0546\n",
      "Train Epoch: 028 Batch: 00090/00094 | Loss: 281.8609 | CE: 0.1796 | KD: 1060.0662\n",
      "Train Epoch: 028 Batch: 00091/00094 | Loss: 281.8208 | CE: 0.1464 | KD: 1060.0400\n",
      "Train Epoch: 028 Batch: 00092/00094 | Loss: 281.8718 | CE: 0.1648 | KD: 1060.1628\n",
      "Train Epoch: 028 Batch: 00093/00094 | Loss: 281.8324 | CE: 0.1433 | KD: 1060.0952\n",
      "Train Epoch: 028 Batch: 00094/00094 | Loss: 281.8226 | CE: 0.1226 | KD: 1060.1367\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1072 | acc:99.2000\n",
      "[VAL Acc] Target: 99.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2178 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5601 | acc:72.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 72.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0204 | acc:46.7557\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7733 | acc:60.7759\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6139 | acc:71.4418\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5237 | acc:76.6850\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7993 | acc:58.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.25%\n",
      "[VAL Acc] Avg 66.87%\n",
      "Train Epoch: 029 Batch: 00001/00094 | Loss: 281.8202 | CE: 0.1361 | KD: 1060.0767\n",
      "Train Epoch: 029 Batch: 00002/00094 | Loss: 281.8078 | CE: 0.1207 | KD: 1060.0879\n",
      "Train Epoch: 029 Batch: 00003/00094 | Loss: 281.8373 | CE: 0.1410 | KD: 1060.1224\n",
      "Train Epoch: 029 Batch: 00004/00094 | Loss: 281.8206 | CE: 0.1317 | KD: 1060.0946\n",
      "Train Epoch: 029 Batch: 00005/00094 | Loss: 281.7761 | CE: 0.0927 | KD: 1060.0742\n",
      "Train Epoch: 029 Batch: 00006/00094 | Loss: 281.8349 | CE: 0.1431 | KD: 1060.1055\n",
      "Train Epoch: 029 Batch: 00007/00094 | Loss: 281.8507 | CE: 0.1430 | KD: 1060.1656\n",
      "Train Epoch: 029 Batch: 00008/00094 | Loss: 281.7578 | CE: 0.0772 | KD: 1060.0635\n",
      "Train Epoch: 029 Batch: 00009/00094 | Loss: 281.7787 | CE: 0.1028 | KD: 1060.0460\n",
      "Train Epoch: 029 Batch: 00010/00094 | Loss: 281.7985 | CE: 0.0944 | KD: 1060.1519\n",
      "Train Epoch: 029 Batch: 00011/00094 | Loss: 281.7968 | CE: 0.1004 | KD: 1060.1228\n",
      "Train Epoch: 029 Batch: 00012/00094 | Loss: 281.8165 | CE: 0.1355 | KD: 1060.0653\n",
      "Train Epoch: 029 Batch: 00013/00094 | Loss: 281.7910 | CE: 0.1057 | KD: 1060.0812\n",
      "Train Epoch: 029 Batch: 00014/00094 | Loss: 281.8114 | CE: 0.1129 | KD: 1060.1310\n",
      "Train Epoch: 029 Batch: 00015/00094 | Loss: 281.8466 | CE: 0.1467 | KD: 1060.1361\n",
      "Train Epoch: 029 Batch: 00016/00094 | Loss: 281.7932 | CE: 0.1065 | KD: 1060.0867\n",
      "Train Epoch: 029 Batch: 00017/00094 | Loss: 281.8663 | CE: 0.1601 | KD: 1060.1598\n",
      "Train Epoch: 029 Batch: 00018/00094 | Loss: 281.8802 | CE: 0.1682 | KD: 1060.1815\n",
      "Train Epoch: 029 Batch: 00019/00094 | Loss: 281.7861 | CE: 0.1029 | KD: 1060.0731\n",
      "Train Epoch: 029 Batch: 00020/00094 | Loss: 281.8033 | CE: 0.0998 | KD: 1060.1495\n",
      "Train Epoch: 029 Batch: 00021/00094 | Loss: 281.8017 | CE: 0.1096 | KD: 1060.1066\n",
      "Train Epoch: 029 Batch: 00022/00094 | Loss: 281.8300 | CE: 0.1370 | KD: 1060.1105\n",
      "Train Epoch: 029 Batch: 00023/00094 | Loss: 281.8170 | CE: 0.1191 | KD: 1060.1288\n",
      "Train Epoch: 029 Batch: 00024/00094 | Loss: 281.8036 | CE: 0.1160 | KD: 1060.0900\n",
      "Train Epoch: 029 Batch: 00025/00094 | Loss: 281.7950 | CE: 0.1077 | KD: 1060.0887\n",
      "Train Epoch: 029 Batch: 00026/00094 | Loss: 281.7950 | CE: 0.1072 | KD: 1060.0907\n",
      "Train Epoch: 029 Batch: 00027/00094 | Loss: 281.7876 | CE: 0.1122 | KD: 1060.0439\n",
      "Train Epoch: 029 Batch: 00028/00094 | Loss: 281.8094 | CE: 0.1090 | KD: 1060.1382\n",
      "Train Epoch: 029 Batch: 00029/00094 | Loss: 281.8197 | CE: 0.1296 | KD: 1060.0990\n",
      "Train Epoch: 029 Batch: 00030/00094 | Loss: 281.8198 | CE: 0.1192 | KD: 1060.1387\n",
      "Train Epoch: 029 Batch: 00031/00094 | Loss: 281.8310 | CE: 0.1375 | KD: 1060.1119\n",
      "Train Epoch: 029 Batch: 00032/00094 | Loss: 281.7756 | CE: 0.1006 | KD: 1060.0426\n",
      "Train Epoch: 029 Batch: 00033/00094 | Loss: 281.7840 | CE: 0.1024 | KD: 1060.0673\n",
      "Train Epoch: 029 Batch: 00034/00094 | Loss: 281.7933 | CE: 0.1111 | KD: 1060.0697\n",
      "Train Epoch: 029 Batch: 00035/00094 | Loss: 281.8055 | CE: 0.1122 | KD: 1060.1115\n",
      "Train Epoch: 029 Batch: 00036/00094 | Loss: 281.8330 | CE: 0.1412 | KD: 1060.1060\n",
      "Train Epoch: 029 Batch: 00037/00094 | Loss: 281.8199 | CE: 0.1127 | KD: 1060.1639\n",
      "Train Epoch: 029 Batch: 00038/00094 | Loss: 281.8282 | CE: 0.1456 | KD: 1060.0709\n",
      "Train Epoch: 029 Batch: 00039/00094 | Loss: 281.7898 | CE: 0.0977 | KD: 1060.1068\n",
      "Train Epoch: 029 Batch: 00040/00094 | Loss: 281.8396 | CE: 0.1585 | KD: 1060.0654\n",
      "Train Epoch: 029 Batch: 00041/00094 | Loss: 281.8423 | CE: 0.1346 | KD: 1060.1653\n",
      "Train Epoch: 029 Batch: 00042/00094 | Loss: 281.8052 | CE: 0.1138 | KD: 1060.1041\n",
      "Train Epoch: 029 Batch: 00043/00094 | Loss: 281.7722 | CE: 0.0921 | KD: 1060.0619\n",
      "Train Epoch: 029 Batch: 00044/00094 | Loss: 281.8077 | CE: 0.1279 | KD: 1060.0609\n",
      "Train Epoch: 029 Batch: 00045/00094 | Loss: 281.7672 | CE: 0.0939 | KD: 1060.0361\n",
      "Train Epoch: 029 Batch: 00046/00094 | Loss: 281.8123 | CE: 0.1099 | KD: 1060.1455\n",
      "Train Epoch: 029 Batch: 00047/00094 | Loss: 281.8493 | CE: 0.1559 | KD: 1060.1118\n",
      "Train Epoch: 029 Batch: 00048/00094 | Loss: 281.7726 | CE: 0.0931 | KD: 1060.0591\n",
      "Train Epoch: 029 Batch: 00049/00094 | Loss: 281.7975 | CE: 0.1028 | KD: 1060.1166\n",
      "Train Epoch: 029 Batch: 00050/00094 | Loss: 281.8230 | CE: 0.1178 | KD: 1060.1562\n",
      "Train Epoch: 029 Batch: 00051/00094 | Loss: 281.8161 | CE: 0.1161 | KD: 1060.1365\n",
      "Train Epoch: 029 Batch: 00052/00094 | Loss: 281.8043 | CE: 0.1252 | KD: 1060.0580\n",
      "Train Epoch: 029 Batch: 00053/00094 | Loss: 281.7986 | CE: 0.1086 | KD: 1060.0988\n",
      "Train Epoch: 029 Batch: 00054/00094 | Loss: 281.8273 | CE: 0.1343 | KD: 1060.1104\n",
      "Train Epoch: 029 Batch: 00055/00094 | Loss: 281.7843 | CE: 0.0927 | KD: 1060.1047\n",
      "Train Epoch: 029 Batch: 00056/00094 | Loss: 281.8054 | CE: 0.1103 | KD: 1060.1180\n",
      "Train Epoch: 029 Batch: 00057/00094 | Loss: 281.8271 | CE: 0.1369 | KD: 1060.0997\n",
      "Train Epoch: 029 Batch: 00058/00094 | Loss: 281.8367 | CE: 0.1362 | KD: 1060.1387\n",
      "Train Epoch: 029 Batch: 00059/00094 | Loss: 281.8348 | CE: 0.1370 | KD: 1060.1285\n",
      "Train Epoch: 029 Batch: 00060/00094 | Loss: 281.8156 | CE: 0.1326 | KD: 1060.0724\n",
      "Train Epoch: 029 Batch: 00061/00094 | Loss: 281.8626 | CE: 0.1690 | KD: 1060.1124\n",
      "Train Epoch: 029 Batch: 00062/00094 | Loss: 281.8029 | CE: 0.1146 | KD: 1060.0927\n",
      "Train Epoch: 029 Batch: 00063/00094 | Loss: 281.8160 | CE: 0.1256 | KD: 1060.1003\n",
      "Train Epoch: 029 Batch: 00064/00094 | Loss: 281.7951 | CE: 0.1001 | KD: 1060.1177\n",
      "Train Epoch: 029 Batch: 00065/00094 | Loss: 281.8310 | CE: 0.1326 | KD: 1060.1302\n",
      "Train Epoch: 029 Batch: 00066/00094 | Loss: 281.7922 | CE: 0.1004 | KD: 1060.1056\n",
      "Train Epoch: 029 Batch: 00067/00094 | Loss: 281.8022 | CE: 0.1061 | KD: 1060.1218\n",
      "Train Epoch: 029 Batch: 00068/00094 | Loss: 281.8260 | CE: 0.1458 | KD: 1060.0619\n",
      "Train Epoch: 029 Batch: 00069/00094 | Loss: 281.8003 | CE: 0.1138 | KD: 1060.0857\n",
      "Train Epoch: 029 Batch: 00070/00094 | Loss: 281.7823 | CE: 0.0966 | KD: 1060.0830\n",
      "Train Epoch: 029 Batch: 00071/00094 | Loss: 281.7722 | CE: 0.0825 | KD: 1060.0979\n",
      "Train Epoch: 029 Batch: 00072/00094 | Loss: 281.7814 | CE: 0.0907 | KD: 1060.1014\n",
      "Train Epoch: 029 Batch: 00073/00094 | Loss: 281.8047 | CE: 0.1143 | KD: 1060.1003\n",
      "Train Epoch: 029 Batch: 00074/00094 | Loss: 281.7812 | CE: 0.0937 | KD: 1060.0896\n",
      "Train Epoch: 029 Batch: 00075/00094 | Loss: 281.8388 | CE: 0.1442 | KD: 1060.1161\n",
      "Train Epoch: 029 Batch: 00076/00094 | Loss: 281.9038 | CE: 0.1882 | KD: 1060.1953\n",
      "Train Epoch: 029 Batch: 00077/00094 | Loss: 281.8112 | CE: 0.1181 | KD: 1060.1104\n",
      "Train Epoch: 029 Batch: 00078/00094 | Loss: 281.7709 | CE: 0.1068 | KD: 1060.0013\n",
      "Train Epoch: 029 Batch: 00079/00094 | Loss: 281.8200 | CE: 0.1279 | KD: 1060.1069\n",
      "Train Epoch: 029 Batch: 00080/00094 | Loss: 281.7738 | CE: 0.0924 | KD: 1060.0667\n",
      "Train Epoch: 029 Batch: 00081/00094 | Loss: 281.8041 | CE: 0.1286 | KD: 1060.0447\n",
      "Train Epoch: 029 Batch: 00082/00094 | Loss: 281.7879 | CE: 0.0945 | KD: 1060.1116\n",
      "Train Epoch: 029 Batch: 00083/00094 | Loss: 281.8272 | CE: 0.1291 | KD: 1060.1296\n",
      "Train Epoch: 029 Batch: 00084/00094 | Loss: 281.8690 | CE: 0.1769 | KD: 1060.1068\n",
      "Train Epoch: 029 Batch: 00085/00094 | Loss: 281.8249 | CE: 0.1115 | KD: 1060.1871\n",
      "Train Epoch: 029 Batch: 00086/00094 | Loss: 281.8362 | CE: 0.1618 | KD: 1060.0402\n",
      "Train Epoch: 029 Batch: 00087/00094 | Loss: 281.8633 | CE: 0.1737 | KD: 1060.0972\n",
      "Train Epoch: 029 Batch: 00088/00094 | Loss: 281.7784 | CE: 0.0964 | KD: 1060.0687\n",
      "Train Epoch: 029 Batch: 00089/00094 | Loss: 281.8304 | CE: 0.1280 | KD: 1060.1455\n",
      "Train Epoch: 029 Batch: 00090/00094 | Loss: 281.8067 | CE: 0.1043 | KD: 1060.1456\n",
      "Train Epoch: 029 Batch: 00091/00094 | Loss: 281.7908 | CE: 0.1019 | KD: 1060.0946\n",
      "Train Epoch: 029 Batch: 00092/00094 | Loss: 281.8996 | CE: 0.2083 | KD: 1060.1039\n",
      "Train Epoch: 029 Batch: 00093/00094 | Loss: 281.7795 | CE: 0.0866 | KD: 1060.1101\n",
      "Train Epoch: 029 Batch: 00094/00094 | Loss: 281.8462 | CE: 0.1572 | KD: 1060.0950\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1176 | acc:98.3500\n",
      "[VAL Acc] Target: 98.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1601 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5494 | acc:72.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 72.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9896 | acc:44.2748\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7505 | acc:61.7947\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6375 | acc:68.4843\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.48%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5217 | acc:76.6066\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7770 | acc:59.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.38%\n",
      "[VAL Acc] Avg 66.37%\n",
      "Train Epoch: 030 Batch: 00001/00094 | Loss: 253.5988 | CE: 0.0963 | KD: 1060.0212\n",
      "Train Epoch: 030 Batch: 00002/00094 | Loss: 253.6100 | CE: 0.0955 | KD: 1060.0719\n",
      "Train Epoch: 030 Batch: 00003/00094 | Loss: 253.6779 | CE: 0.1538 | KD: 1060.1117\n",
      "Train Epoch: 030 Batch: 00004/00094 | Loss: 253.6185 | CE: 0.1086 | KD: 1060.0522\n",
      "Train Epoch: 030 Batch: 00005/00094 | Loss: 253.6379 | CE: 0.1080 | KD: 1060.1362\n",
      "Train Epoch: 030 Batch: 00006/00094 | Loss: 253.6197 | CE: 0.0936 | KD: 1060.1202\n",
      "Train Epoch: 030 Batch: 00007/00094 | Loss: 253.6155 | CE: 0.0990 | KD: 1060.0800\n",
      "Train Epoch: 030 Batch: 00008/00094 | Loss: 253.6521 | CE: 0.1237 | KD: 1060.1295\n",
      "Train Epoch: 030 Batch: 00009/00094 | Loss: 253.6105 | CE: 0.0950 | KD: 1060.0756\n",
      "Train Epoch: 030 Batch: 00010/00094 | Loss: 253.5980 | CE: 0.0760 | KD: 1060.1030\n",
      "Train Epoch: 030 Batch: 00011/00094 | Loss: 253.6681 | CE: 0.1468 | KD: 1060.1000\n",
      "Train Epoch: 030 Batch: 00012/00094 | Loss: 253.6703 | CE: 0.1351 | KD: 1060.1580\n",
      "Train Epoch: 030 Batch: 00013/00094 | Loss: 253.6199 | CE: 0.0906 | KD: 1060.1337\n",
      "Train Epoch: 030 Batch: 00014/00094 | Loss: 253.6519 | CE: 0.1304 | KD: 1060.1011\n",
      "Train Epoch: 030 Batch: 00015/00094 | Loss: 253.6284 | CE: 0.1148 | KD: 1060.0679\n",
      "Train Epoch: 030 Batch: 00016/00094 | Loss: 253.6045 | CE: 0.0853 | KD: 1060.0913\n",
      "Train Epoch: 030 Batch: 00017/00094 | Loss: 253.6651 | CE: 0.1381 | KD: 1060.1238\n",
      "Train Epoch: 030 Batch: 00018/00094 | Loss: 253.6456 | CE: 0.1071 | KD: 1060.1722\n",
      "Train Epoch: 030 Batch: 00019/00094 | Loss: 253.6594 | CE: 0.1302 | KD: 1060.1332\n",
      "Train Epoch: 030 Batch: 00020/00094 | Loss: 253.6315 | CE: 0.1122 | KD: 1060.0919\n",
      "Train Epoch: 030 Batch: 00021/00094 | Loss: 253.6575 | CE: 0.1286 | KD: 1060.1318\n",
      "Train Epoch: 030 Batch: 00022/00094 | Loss: 253.6246 | CE: 0.1213 | KD: 1060.0250\n",
      "Train Epoch: 030 Batch: 00023/00094 | Loss: 253.6515 | CE: 0.1218 | KD: 1060.1351\n",
      "Train Epoch: 030 Batch: 00024/00094 | Loss: 253.6194 | CE: 0.0957 | KD: 1060.1105\n",
      "Train Epoch: 030 Batch: 00025/00094 | Loss: 253.6616 | CE: 0.1305 | KD: 1060.1409\n",
      "Train Epoch: 030 Batch: 00026/00094 | Loss: 253.6172 | CE: 0.0925 | KD: 1060.1144\n",
      "Train Epoch: 030 Batch: 00027/00094 | Loss: 253.6235 | CE: 0.1053 | KD: 1060.0870\n",
      "Train Epoch: 030 Batch: 00028/00094 | Loss: 253.6452 | CE: 0.1203 | KD: 1060.1151\n",
      "Train Epoch: 030 Batch: 00029/00094 | Loss: 253.6304 | CE: 0.1003 | KD: 1060.1371\n",
      "Train Epoch: 030 Batch: 00030/00094 | Loss: 253.6744 | CE: 0.1583 | KD: 1060.0784\n",
      "Train Epoch: 030 Batch: 00031/00094 | Loss: 253.6361 | CE: 0.1114 | KD: 1060.1145\n",
      "Train Epoch: 030 Batch: 00032/00094 | Loss: 253.6327 | CE: 0.0936 | KD: 1060.1744\n",
      "Train Epoch: 030 Batch: 00033/00094 | Loss: 253.6610 | CE: 0.1365 | KD: 1060.1138\n",
      "Train Epoch: 030 Batch: 00034/00094 | Loss: 253.6833 | CE: 0.1580 | KD: 1060.1171\n",
      "Train Epoch: 030 Batch: 00035/00094 | Loss: 253.6082 | CE: 0.0880 | KD: 1060.0957\n",
      "Train Epoch: 030 Batch: 00036/00094 | Loss: 253.6150 | CE: 0.0921 | KD: 1060.1067\n",
      "Train Epoch: 030 Batch: 00037/00094 | Loss: 253.6293 | CE: 0.1062 | KD: 1060.1074\n",
      "Train Epoch: 030 Batch: 00038/00094 | Loss: 253.6111 | CE: 0.0937 | KD: 1060.0837\n",
      "Train Epoch: 030 Batch: 00039/00094 | Loss: 253.5969 | CE: 0.0879 | KD: 1060.0488\n",
      "Train Epoch: 030 Batch: 00040/00094 | Loss: 253.6396 | CE: 0.1257 | KD: 1060.0692\n",
      "Train Epoch: 030 Batch: 00041/00094 | Loss: 253.6284 | CE: 0.1132 | KD: 1060.0747\n",
      "Train Epoch: 030 Batch: 00042/00094 | Loss: 253.7247 | CE: 0.2056 | KD: 1060.0911\n",
      "Train Epoch: 030 Batch: 00043/00094 | Loss: 253.6620 | CE: 0.1361 | KD: 1060.1193\n",
      "Train Epoch: 030 Batch: 00044/00094 | Loss: 253.6430 | CE: 0.1269 | KD: 1060.0784\n",
      "Train Epoch: 030 Batch: 00045/00094 | Loss: 253.6692 | CE: 0.1373 | KD: 1060.1444\n",
      "Train Epoch: 030 Batch: 00046/00094 | Loss: 253.6542 | CE: 0.1346 | KD: 1060.0929\n",
      "Train Epoch: 030 Batch: 00047/00094 | Loss: 253.6568 | CE: 0.1221 | KD: 1060.1560\n",
      "Train Epoch: 030 Batch: 00048/00094 | Loss: 253.6456 | CE: 0.1212 | KD: 1060.1133\n",
      "Train Epoch: 030 Batch: 00049/00094 | Loss: 253.6405 | CE: 0.1344 | KD: 1060.0363\n",
      "Train Epoch: 030 Batch: 00050/00094 | Loss: 253.6395 | CE: 0.1184 | KD: 1060.0991\n",
      "Train Epoch: 030 Batch: 00051/00094 | Loss: 253.6435 | CE: 0.1211 | KD: 1060.1047\n",
      "Train Epoch: 030 Batch: 00052/00094 | Loss: 253.6404 | CE: 0.1127 | KD: 1060.1270\n",
      "Train Epoch: 030 Batch: 00053/00094 | Loss: 253.6487 | CE: 0.1199 | KD: 1060.1317\n",
      "Train Epoch: 030 Batch: 00054/00094 | Loss: 253.6471 | CE: 0.1300 | KD: 1060.0824\n",
      "Train Epoch: 030 Batch: 00055/00094 | Loss: 253.6665 | CE: 0.1330 | KD: 1060.1509\n",
      "Train Epoch: 030 Batch: 00056/00094 | Loss: 253.6644 | CE: 0.1466 | KD: 1060.0853\n",
      "Train Epoch: 030 Batch: 00057/00094 | Loss: 253.6479 | CE: 0.1254 | KD: 1060.1052\n",
      "Train Epoch: 030 Batch: 00058/00094 | Loss: 253.6124 | CE: 0.0961 | KD: 1060.0791\n",
      "Train Epoch: 030 Batch: 00059/00094 | Loss: 253.6833 | CE: 0.1574 | KD: 1060.1190\n",
      "Train Epoch: 030 Batch: 00060/00094 | Loss: 253.6638 | CE: 0.1250 | KD: 1060.1733\n",
      "Train Epoch: 030 Batch: 00061/00094 | Loss: 253.6564 | CE: 0.1301 | KD: 1060.1211\n",
      "Train Epoch: 030 Batch: 00062/00094 | Loss: 253.6547 | CE: 0.1177 | KD: 1060.1658\n",
      "Train Epoch: 030 Batch: 00063/00094 | Loss: 253.6154 | CE: 0.0931 | KD: 1060.1045\n",
      "Train Epoch: 030 Batch: 00064/00094 | Loss: 253.6479 | CE: 0.1327 | KD: 1060.0746\n",
      "Train Epoch: 030 Batch: 00065/00094 | Loss: 253.6248 | CE: 0.1137 | KD: 1060.0574\n",
      "Train Epoch: 030 Batch: 00066/00094 | Loss: 253.6249 | CE: 0.0988 | KD: 1060.1201\n",
      "Train Epoch: 030 Batch: 00067/00094 | Loss: 253.6441 | CE: 0.1141 | KD: 1060.1365\n",
      "Train Epoch: 030 Batch: 00068/00094 | Loss: 253.5954 | CE: 0.0839 | KD: 1060.0588\n",
      "Train Epoch: 030 Batch: 00069/00094 | Loss: 253.6186 | CE: 0.1015 | KD: 1060.0826\n",
      "Train Epoch: 030 Batch: 00070/00094 | Loss: 253.6034 | CE: 0.0956 | KD: 1060.0438\n",
      "Train Epoch: 030 Batch: 00071/00094 | Loss: 253.6146 | CE: 0.0990 | KD: 1060.0765\n",
      "Train Epoch: 030 Batch: 00072/00094 | Loss: 253.6225 | CE: 0.1132 | KD: 1060.0498\n",
      "Train Epoch: 030 Batch: 00073/00094 | Loss: 253.6891 | CE: 0.1748 | KD: 1060.0710\n",
      "Train Epoch: 030 Batch: 00074/00094 | Loss: 253.6658 | CE: 0.1380 | KD: 1060.1272\n",
      "Train Epoch: 030 Batch: 00075/00094 | Loss: 253.6628 | CE: 0.1379 | KD: 1060.1154\n",
      "Train Epoch: 030 Batch: 00076/00094 | Loss: 253.6245 | CE: 0.1088 | KD: 1060.0764\n",
      "Train Epoch: 030 Batch: 00077/00094 | Loss: 253.6424 | CE: 0.1194 | KD: 1060.1073\n",
      "Train Epoch: 030 Batch: 00078/00094 | Loss: 253.6444 | CE: 0.1322 | KD: 1060.0618\n",
      "Train Epoch: 030 Batch: 00079/00094 | Loss: 253.6150 | CE: 0.0963 | KD: 1060.0894\n",
      "Train Epoch: 030 Batch: 00080/00094 | Loss: 253.6236 | CE: 0.0913 | KD: 1060.1459\n",
      "Train Epoch: 030 Batch: 00081/00094 | Loss: 253.6380 | CE: 0.0980 | KD: 1060.1783\n",
      "Train Epoch: 030 Batch: 00082/00094 | Loss: 253.6282 | CE: 0.1124 | KD: 1060.0771\n",
      "Train Epoch: 030 Batch: 00083/00094 | Loss: 253.6188 | CE: 0.1002 | KD: 1060.0889\n",
      "Train Epoch: 030 Batch: 00084/00094 | Loss: 253.6300 | CE: 0.1064 | KD: 1060.1097\n",
      "Train Epoch: 030 Batch: 00085/00094 | Loss: 253.6333 | CE: 0.1135 | KD: 1060.0940\n",
      "Train Epoch: 030 Batch: 00086/00094 | Loss: 253.6195 | CE: 0.0980 | KD: 1060.1010\n",
      "Train Epoch: 030 Batch: 00087/00094 | Loss: 253.6198 | CE: 0.0990 | KD: 1060.0980\n",
      "Train Epoch: 030 Batch: 00088/00094 | Loss: 253.6541 | CE: 0.1300 | KD: 1060.1119\n",
      "Train Epoch: 030 Batch: 00089/00094 | Loss: 253.6212 | CE: 0.0891 | KD: 1060.1451\n",
      "Train Epoch: 030 Batch: 00090/00094 | Loss: 253.6614 | CE: 0.1392 | KD: 1060.1040\n",
      "Train Epoch: 030 Batch: 00091/00094 | Loss: 253.6243 | CE: 0.1002 | KD: 1060.1117\n",
      "Train Epoch: 030 Batch: 00092/00094 | Loss: 253.7014 | CE: 0.1884 | KD: 1060.0653\n",
      "Train Epoch: 030 Batch: 00093/00094 | Loss: 253.6898 | CE: 0.1730 | KD: 1060.0814\n",
      "Train Epoch: 030 Batch: 00094/00094 | Loss: 253.6212 | CE: 0.1187 | KD: 1060.0211\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1129 | acc:98.6500\n",
      "[VAL Acc] Target: 98.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2144 | acc:49.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5600 | acc:72.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 72.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0260 | acc:45.8015\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7977 | acc:60.1881\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5849 | acc:72.7357\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5680 | acc:74.4122\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7957 | acc:57.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.63%\n",
      "[VAL Acc] Avg 66.45%\n",
      "Train Epoch: 031 Batch: 00001/00094 | Loss: 253.6655 | CE: 0.1328 | KD: 1060.1476\n",
      "Train Epoch: 031 Batch: 00002/00094 | Loss: 253.6877 | CE: 0.1559 | KD: 1060.1439\n",
      "Train Epoch: 031 Batch: 00003/00094 | Loss: 253.6477 | CE: 0.1315 | KD: 1060.0789\n",
      "Train Epoch: 031 Batch: 00004/00094 | Loss: 253.6310 | CE: 0.1228 | KD: 1060.0450\n",
      "Train Epoch: 031 Batch: 00005/00094 | Loss: 253.6300 | CE: 0.1137 | KD: 1060.0791\n",
      "Train Epoch: 031 Batch: 00006/00094 | Loss: 253.6757 | CE: 0.1423 | KD: 1060.1508\n",
      "Train Epoch: 031 Batch: 00007/00094 | Loss: 253.6436 | CE: 0.1142 | KD: 1060.1338\n",
      "Train Epoch: 031 Batch: 00008/00094 | Loss: 253.6295 | CE: 0.1038 | KD: 1060.1183\n",
      "Train Epoch: 031 Batch: 00009/00094 | Loss: 253.6281 | CE: 0.1114 | KD: 1060.0809\n",
      "Train Epoch: 031 Batch: 00010/00094 | Loss: 253.7062 | CE: 0.1650 | KD: 1060.1837\n",
      "Train Epoch: 031 Batch: 00011/00094 | Loss: 253.6241 | CE: 0.0996 | KD: 1060.1133\n",
      "Train Epoch: 031 Batch: 00012/00094 | Loss: 253.6311 | CE: 0.1060 | KD: 1060.1160\n",
      "Train Epoch: 031 Batch: 00013/00094 | Loss: 253.7154 | CE: 0.1923 | KD: 1060.1077\n",
      "Train Epoch: 031 Batch: 00014/00094 | Loss: 253.6372 | CE: 0.1117 | KD: 1060.1177\n",
      "Train Epoch: 031 Batch: 00015/00094 | Loss: 253.6414 | CE: 0.1262 | KD: 1060.0743\n",
      "Train Epoch: 031 Batch: 00016/00094 | Loss: 253.6180 | CE: 0.0894 | KD: 1060.1304\n",
      "Train Epoch: 031 Batch: 00017/00094 | Loss: 253.6111 | CE: 0.0928 | KD: 1060.0876\n",
      "Train Epoch: 031 Batch: 00018/00094 | Loss: 253.6477 | CE: 0.1260 | KD: 1060.1018\n",
      "Train Epoch: 031 Batch: 00019/00094 | Loss: 253.6126 | CE: 0.1020 | KD: 1060.0553\n",
      "Train Epoch: 031 Batch: 00020/00094 | Loss: 253.6317 | CE: 0.1079 | KD: 1060.1104\n",
      "Train Epoch: 031 Batch: 00021/00094 | Loss: 253.6694 | CE: 0.1472 | KD: 1060.1039\n",
      "Train Epoch: 031 Batch: 00022/00094 | Loss: 253.6514 | CE: 0.1254 | KD: 1060.1199\n",
      "Train Epoch: 031 Batch: 00023/00094 | Loss: 253.6497 | CE: 0.1231 | KD: 1060.1221\n",
      "Train Epoch: 031 Batch: 00024/00094 | Loss: 253.6313 | CE: 0.1106 | KD: 1060.0973\n",
      "Train Epoch: 031 Batch: 00025/00094 | Loss: 253.6463 | CE: 0.1273 | KD: 1060.0907\n",
      "Train Epoch: 031 Batch: 00026/00094 | Loss: 253.6433 | CE: 0.1182 | KD: 1060.1158\n",
      "Train Epoch: 031 Batch: 00027/00094 | Loss: 253.6689 | CE: 0.1510 | KD: 1060.0861\n",
      "Train Epoch: 031 Batch: 00028/00094 | Loss: 253.6188 | CE: 0.1126 | KD: 1060.0367\n",
      "Train Epoch: 031 Batch: 00029/00094 | Loss: 253.6734 | CE: 0.1438 | KD: 1060.1345\n",
      "Train Epoch: 031 Batch: 00030/00094 | Loss: 253.6489 | CE: 0.1233 | KD: 1060.1180\n",
      "Train Epoch: 031 Batch: 00031/00094 | Loss: 253.6442 | CE: 0.1215 | KD: 1060.1062\n",
      "Train Epoch: 031 Batch: 00032/00094 | Loss: 253.6436 | CE: 0.1104 | KD: 1060.1500\n",
      "Train Epoch: 031 Batch: 00033/00094 | Loss: 253.6267 | CE: 0.0982 | KD: 1060.1300\n",
      "Train Epoch: 031 Batch: 00034/00094 | Loss: 253.6237 | CE: 0.0990 | KD: 1060.1144\n",
      "Train Epoch: 031 Batch: 00035/00094 | Loss: 253.6268 | CE: 0.1134 | KD: 1060.0673\n",
      "Train Epoch: 031 Batch: 00036/00094 | Loss: 253.6738 | CE: 0.1535 | KD: 1060.0961\n",
      "Train Epoch: 031 Batch: 00037/00094 | Loss: 253.6395 | CE: 0.1195 | KD: 1060.0947\n",
      "Train Epoch: 031 Batch: 00038/00094 | Loss: 253.6162 | CE: 0.0982 | KD: 1060.0864\n",
      "Train Epoch: 031 Batch: 00039/00094 | Loss: 253.6350 | CE: 0.1144 | KD: 1060.0970\n",
      "Train Epoch: 031 Batch: 00040/00094 | Loss: 253.6053 | CE: 0.0872 | KD: 1060.0865\n",
      "Train Epoch: 031 Batch: 00041/00094 | Loss: 253.6186 | CE: 0.0995 | KD: 1060.0907\n",
      "Train Epoch: 031 Batch: 00042/00094 | Loss: 253.6661 | CE: 0.1497 | KD: 1060.0795\n",
      "Train Epoch: 031 Batch: 00043/00094 | Loss: 253.6135 | CE: 0.0941 | KD: 1060.0918\n",
      "Train Epoch: 031 Batch: 00044/00094 | Loss: 253.6185 | CE: 0.0968 | KD: 1060.1019\n",
      "Train Epoch: 031 Batch: 00045/00094 | Loss: 253.6048 | CE: 0.1053 | KD: 1060.0090\n",
      "Train Epoch: 031 Batch: 00046/00094 | Loss: 253.6298 | CE: 0.1111 | KD: 1060.0892\n",
      "Train Epoch: 031 Batch: 00047/00094 | Loss: 253.6277 | CE: 0.1006 | KD: 1060.1243\n",
      "Train Epoch: 031 Batch: 00048/00094 | Loss: 253.6417 | CE: 0.1232 | KD: 1060.0884\n",
      "Train Epoch: 031 Batch: 00049/00094 | Loss: 253.6348 | CE: 0.1116 | KD: 1060.1083\n",
      "Train Epoch: 031 Batch: 00050/00094 | Loss: 253.6514 | CE: 0.1364 | KD: 1060.0739\n",
      "Train Epoch: 031 Batch: 00051/00094 | Loss: 253.6399 | CE: 0.1249 | KD: 1060.0737\n",
      "Train Epoch: 031 Batch: 00052/00094 | Loss: 253.6535 | CE: 0.1372 | KD: 1060.0791\n",
      "Train Epoch: 031 Batch: 00053/00094 | Loss: 253.6302 | CE: 0.1110 | KD: 1060.0912\n",
      "Train Epoch: 031 Batch: 00054/00094 | Loss: 253.6179 | CE: 0.1006 | KD: 1060.0830\n",
      "Train Epoch: 031 Batch: 00055/00094 | Loss: 253.6358 | CE: 0.1240 | KD: 1060.0604\n",
      "Train Epoch: 031 Batch: 00056/00094 | Loss: 253.6218 | CE: 0.0929 | KD: 1060.1321\n",
      "Train Epoch: 031 Batch: 00057/00094 | Loss: 253.6478 | CE: 0.1200 | KD: 1060.1272\n",
      "Train Epoch: 031 Batch: 00058/00094 | Loss: 253.6237 | CE: 0.0988 | KD: 1060.1154\n",
      "Train Epoch: 031 Batch: 00059/00094 | Loss: 253.6345 | CE: 0.1150 | KD: 1060.0928\n",
      "Train Epoch: 031 Batch: 00060/00094 | Loss: 253.6135 | CE: 0.0874 | KD: 1060.1201\n",
      "Train Epoch: 031 Batch: 00061/00094 | Loss: 253.6776 | CE: 0.1598 | KD: 1060.0853\n",
      "Train Epoch: 031 Batch: 00062/00094 | Loss: 253.6260 | CE: 0.1032 | KD: 1060.1066\n",
      "Train Epoch: 031 Batch: 00063/00094 | Loss: 253.6562 | CE: 0.1251 | KD: 1060.1411\n",
      "Train Epoch: 031 Batch: 00064/00094 | Loss: 253.7389 | CE: 0.2222 | KD: 1060.0809\n",
      "Train Epoch: 031 Batch: 00065/00094 | Loss: 253.6804 | CE: 0.1477 | KD: 1060.1477\n",
      "Train Epoch: 031 Batch: 00066/00094 | Loss: 253.6342 | CE: 0.1041 | KD: 1060.1370\n",
      "Train Epoch: 031 Batch: 00067/00094 | Loss: 253.6306 | CE: 0.1105 | KD: 1060.0951\n",
      "Train Epoch: 031 Batch: 00068/00094 | Loss: 253.6339 | CE: 0.1163 | KD: 1060.0844\n",
      "Train Epoch: 031 Batch: 00069/00094 | Loss: 253.6194 | CE: 0.0896 | KD: 1060.1357\n",
      "Train Epoch: 031 Batch: 00070/00094 | Loss: 253.6651 | CE: 0.1293 | KD: 1060.1610\n",
      "Train Epoch: 031 Batch: 00071/00094 | Loss: 253.6117 | CE: 0.1047 | KD: 1060.0403\n",
      "Train Epoch: 031 Batch: 00072/00094 | Loss: 253.6271 | CE: 0.1103 | KD: 1060.0814\n",
      "Train Epoch: 031 Batch: 00073/00094 | Loss: 253.6436 | CE: 0.1336 | KD: 1060.0531\n",
      "Train Epoch: 031 Batch: 00074/00094 | Loss: 253.6741 | CE: 0.1550 | KD: 1060.0909\n",
      "Train Epoch: 031 Batch: 00075/00094 | Loss: 253.6118 | CE: 0.0870 | KD: 1060.1145\n",
      "Train Epoch: 031 Batch: 00076/00094 | Loss: 253.6388 | CE: 0.1278 | KD: 1060.0573\n",
      "Train Epoch: 031 Batch: 00077/00094 | Loss: 253.6582 | CE: 0.1315 | KD: 1060.1227\n",
      "Train Epoch: 031 Batch: 00078/00094 | Loss: 253.6208 | CE: 0.0975 | KD: 1060.1088\n",
      "Train Epoch: 031 Batch: 00079/00094 | Loss: 253.6337 | CE: 0.1055 | KD: 1060.1287\n",
      "Train Epoch: 031 Batch: 00080/00094 | Loss: 253.6699 | CE: 0.1392 | KD: 1060.1395\n",
      "Train Epoch: 031 Batch: 00081/00094 | Loss: 253.6248 | CE: 0.1011 | KD: 1060.1101\n",
      "Train Epoch: 031 Batch: 00082/00094 | Loss: 253.6961 | CE: 0.1599 | KD: 1060.1625\n",
      "Train Epoch: 031 Batch: 00083/00094 | Loss: 253.7714 | CE: 0.2521 | KD: 1060.0918\n",
      "Train Epoch: 031 Batch: 00084/00094 | Loss: 253.6363 | CE: 0.1113 | KD: 1060.1157\n",
      "Train Epoch: 031 Batch: 00085/00094 | Loss: 253.6153 | CE: 0.0919 | KD: 1060.1090\n",
      "Train Epoch: 031 Batch: 00086/00094 | Loss: 253.6455 | CE: 0.1148 | KD: 1060.1394\n",
      "Train Epoch: 031 Batch: 00087/00094 | Loss: 253.6260 | CE: 0.1110 | KD: 1060.0737\n",
      "Train Epoch: 031 Batch: 00088/00094 | Loss: 253.6312 | CE: 0.1051 | KD: 1060.1204\n",
      "Train Epoch: 031 Batch: 00089/00094 | Loss: 253.6189 | CE: 0.1026 | KD: 1060.0792\n",
      "Train Epoch: 031 Batch: 00090/00094 | Loss: 253.6131 | CE: 0.1000 | KD: 1060.0658\n",
      "Train Epoch: 031 Batch: 00091/00094 | Loss: 253.6673 | CE: 0.1411 | KD: 1060.1205\n",
      "Train Epoch: 031 Batch: 00092/00094 | Loss: 253.6608 | CE: 0.1286 | KD: 1060.1455\n",
      "Train Epoch: 031 Batch: 00093/00094 | Loss: 253.6884 | CE: 0.1472 | KD: 1060.1831\n",
      "Train Epoch: 031 Batch: 00094/00094 | Loss: 253.6003 | CE: 0.0931 | KD: 1060.0410\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1167 | acc:98.4000\n",
      "[VAL Acc] Target: 98.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2196 | acc:50.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5756 | acc:70.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 70.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9953 | acc:47.1374\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8476 | acc:58.2680\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5392 | acc:75.5083\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5592 | acc:75.4702\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8338 | acc:56.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.94%\n",
      "[VAL Acc] Avg 66.58%\n",
      "Train Epoch: 032 Batch: 00001/00094 | Loss: 253.6245 | CE: 0.0967 | KD: 1060.1272\n",
      "Train Epoch: 032 Batch: 00002/00094 | Loss: 253.6312 | CE: 0.1055 | KD: 1060.1184\n",
      "Train Epoch: 032 Batch: 00003/00094 | Loss: 253.6500 | CE: 0.1271 | KD: 1060.1066\n",
      "Train Epoch: 032 Batch: 00004/00094 | Loss: 253.6379 | CE: 0.1050 | KD: 1060.1489\n",
      "Train Epoch: 032 Batch: 00005/00094 | Loss: 253.6208 | CE: 0.0896 | KD: 1060.1418\n",
      "Train Epoch: 032 Batch: 00006/00094 | Loss: 253.6248 | CE: 0.1155 | KD: 1060.0497\n",
      "Train Epoch: 032 Batch: 00007/00094 | Loss: 253.6484 | CE: 0.1176 | KD: 1060.1398\n",
      "Train Epoch: 032 Batch: 00008/00094 | Loss: 253.6523 | CE: 0.1368 | KD: 1060.0757\n",
      "Train Epoch: 032 Batch: 00009/00094 | Loss: 253.6056 | CE: 0.0961 | KD: 1060.0509\n",
      "Train Epoch: 032 Batch: 00010/00094 | Loss: 253.6333 | CE: 0.1063 | KD: 1060.1237\n",
      "Train Epoch: 032 Batch: 00011/00094 | Loss: 253.6648 | CE: 0.1281 | KD: 1060.1643\n",
      "Train Epoch: 032 Batch: 00012/00094 | Loss: 253.6644 | CE: 0.1479 | KD: 1060.0800\n",
      "Train Epoch: 032 Batch: 00013/00094 | Loss: 253.6437 | CE: 0.1211 | KD: 1060.1053\n",
      "Train Epoch: 032 Batch: 00014/00094 | Loss: 253.6175 | CE: 0.0957 | KD: 1060.1021\n",
      "Train Epoch: 032 Batch: 00015/00094 | Loss: 253.6447 | CE: 0.1303 | KD: 1060.0715\n",
      "Train Epoch: 032 Batch: 00016/00094 | Loss: 253.6797 | CE: 0.1469 | KD: 1060.1484\n",
      "Train Epoch: 032 Batch: 00017/00094 | Loss: 253.6673 | CE: 0.1408 | KD: 1060.1219\n",
      "Train Epoch: 032 Batch: 00018/00094 | Loss: 253.6191 | CE: 0.0958 | KD: 1060.1084\n",
      "Train Epoch: 032 Batch: 00019/00094 | Loss: 253.6200 | CE: 0.1049 | KD: 1060.0742\n",
      "Train Epoch: 032 Batch: 00020/00094 | Loss: 253.6321 | CE: 0.0951 | KD: 1060.1656\n",
      "Train Epoch: 032 Batch: 00021/00094 | Loss: 253.6320 | CE: 0.1211 | KD: 1060.0569\n",
      "Train Epoch: 032 Batch: 00022/00094 | Loss: 253.6453 | CE: 0.1346 | KD: 1060.0554\n",
      "Train Epoch: 032 Batch: 00023/00094 | Loss: 253.6891 | CE: 0.1605 | KD: 1060.1307\n",
      "Train Epoch: 032 Batch: 00024/00094 | Loss: 253.6915 | CE: 0.1625 | KD: 1060.1323\n",
      "Train Epoch: 032 Batch: 00025/00094 | Loss: 253.6277 | CE: 0.0890 | KD: 1060.1730\n",
      "Train Epoch: 032 Batch: 00026/00094 | Loss: 253.6320 | CE: 0.1077 | KD: 1060.1127\n",
      "Train Epoch: 032 Batch: 00027/00094 | Loss: 253.6061 | CE: 0.0911 | KD: 1060.0741\n",
      "Train Epoch: 032 Batch: 00028/00094 | Loss: 253.6300 | CE: 0.1177 | KD: 1060.0621\n",
      "Train Epoch: 032 Batch: 00029/00094 | Loss: 253.6618 | CE: 0.1409 | KD: 1060.0988\n",
      "Train Epoch: 032 Batch: 00030/00094 | Loss: 253.6314 | CE: 0.1006 | KD: 1060.1401\n",
      "Train Epoch: 032 Batch: 00031/00094 | Loss: 253.6553 | CE: 0.1275 | KD: 1060.1274\n",
      "Train Epoch: 032 Batch: 00032/00094 | Loss: 253.6020 | CE: 0.0923 | KD: 1060.0515\n",
      "Train Epoch: 032 Batch: 00033/00094 | Loss: 253.6626 | CE: 0.1490 | KD: 1060.0681\n",
      "Train Epoch: 032 Batch: 00034/00094 | Loss: 253.6159 | CE: 0.0968 | KD: 1060.0909\n",
      "Train Epoch: 032 Batch: 00035/00094 | Loss: 253.6708 | CE: 0.1554 | KD: 1060.0756\n",
      "Train Epoch: 032 Batch: 00036/00094 | Loss: 253.6119 | CE: 0.0944 | KD: 1060.0842\n",
      "Train Epoch: 032 Batch: 00037/00094 | Loss: 253.6413 | CE: 0.1296 | KD: 1060.0597\n",
      "Train Epoch: 032 Batch: 00038/00094 | Loss: 253.6558 | CE: 0.1336 | KD: 1060.1041\n",
      "Train Epoch: 032 Batch: 00039/00094 | Loss: 253.6958 | CE: 0.1791 | KD: 1060.0808\n",
      "Train Epoch: 032 Batch: 00040/00094 | Loss: 253.6699 | CE: 0.1284 | KD: 1060.1846\n",
      "Train Epoch: 032 Batch: 00041/00094 | Loss: 253.6373 | CE: 0.1182 | KD: 1060.0908\n",
      "Train Epoch: 032 Batch: 00042/00094 | Loss: 253.6248 | CE: 0.1019 | KD: 1060.1067\n",
      "Train Epoch: 032 Batch: 00043/00094 | Loss: 253.6557 | CE: 0.1150 | KD: 1060.1814\n",
      "Train Epoch: 032 Batch: 00044/00094 | Loss: 253.6043 | CE: 0.0876 | KD: 1060.0809\n",
      "Train Epoch: 032 Batch: 00045/00094 | Loss: 253.6483 | CE: 0.1284 | KD: 1060.0945\n",
      "Train Epoch: 032 Batch: 00046/00094 | Loss: 253.6448 | CE: 0.1153 | KD: 1060.1345\n",
      "Train Epoch: 032 Batch: 00047/00094 | Loss: 253.6563 | CE: 0.1196 | KD: 1060.1644\n",
      "Train Epoch: 032 Batch: 00048/00094 | Loss: 253.6412 | CE: 0.1197 | KD: 1060.1007\n",
      "Train Epoch: 032 Batch: 00049/00094 | Loss: 253.6615 | CE: 0.1421 | KD: 1060.0920\n",
      "Train Epoch: 032 Batch: 00050/00094 | Loss: 253.6159 | CE: 0.0927 | KD: 1060.1078\n",
      "Train Epoch: 032 Batch: 00051/00094 | Loss: 253.6256 | CE: 0.1110 | KD: 1060.0721\n",
      "Train Epoch: 032 Batch: 00052/00094 | Loss: 253.6320 | CE: 0.1114 | KD: 1060.0974\n",
      "Train Epoch: 032 Batch: 00053/00094 | Loss: 253.6140 | CE: 0.0934 | KD: 1060.0972\n",
      "Train Epoch: 032 Batch: 00054/00094 | Loss: 253.6529 | CE: 0.1236 | KD: 1060.1333\n",
      "Train Epoch: 032 Batch: 00055/00094 | Loss: 253.6109 | CE: 0.0837 | KD: 1060.1248\n",
      "Train Epoch: 032 Batch: 00056/00094 | Loss: 253.6388 | CE: 0.1242 | KD: 1060.0719\n",
      "Train Epoch: 032 Batch: 00057/00094 | Loss: 253.6509 | CE: 0.1393 | KD: 1060.0593\n",
      "Train Epoch: 032 Batch: 00058/00094 | Loss: 253.6644 | CE: 0.1313 | KD: 1060.1497\n",
      "Train Epoch: 032 Batch: 00059/00094 | Loss: 253.6697 | CE: 0.1259 | KD: 1060.1943\n",
      "Train Epoch: 032 Batch: 00060/00094 | Loss: 253.6444 | CE: 0.1268 | KD: 1060.0846\n",
      "Train Epoch: 032 Batch: 00061/00094 | Loss: 253.6021 | CE: 0.0958 | KD: 1060.0372\n",
      "Train Epoch: 032 Batch: 00062/00094 | Loss: 253.6310 | CE: 0.1117 | KD: 1060.0919\n",
      "Train Epoch: 032 Batch: 00063/00094 | Loss: 253.6188 | CE: 0.0839 | KD: 1060.1572\n",
      "Train Epoch: 032 Batch: 00064/00094 | Loss: 253.6889 | CE: 0.1880 | KD: 1060.0151\n",
      "Train Epoch: 032 Batch: 00065/00094 | Loss: 253.6231 | CE: 0.1148 | KD: 1060.0459\n",
      "Train Epoch: 032 Batch: 00066/00094 | Loss: 253.7570 | CE: 0.2272 | KD: 1060.1354\n",
      "Train Epoch: 032 Batch: 00067/00094 | Loss: 253.6265 | CE: 0.0984 | KD: 1060.1287\n",
      "Train Epoch: 032 Batch: 00068/00094 | Loss: 253.6261 | CE: 0.1143 | KD: 1060.0603\n",
      "Train Epoch: 032 Batch: 00069/00094 | Loss: 253.6311 | CE: 0.1108 | KD: 1060.0958\n",
      "Train Epoch: 032 Batch: 00070/00094 | Loss: 253.6429 | CE: 0.1234 | KD: 1060.0928\n",
      "Train Epoch: 032 Batch: 00071/00094 | Loss: 253.6243 | CE: 0.1048 | KD: 1060.0927\n",
      "Train Epoch: 032 Batch: 00072/00094 | Loss: 253.6456 | CE: 0.1078 | KD: 1060.1691\n",
      "Train Epoch: 032 Batch: 00073/00094 | Loss: 253.6283 | CE: 0.1102 | KD: 1060.0868\n",
      "Train Epoch: 032 Batch: 00074/00094 | Loss: 253.6065 | CE: 0.0920 | KD: 1060.0719\n",
      "Train Epoch: 032 Batch: 00075/00094 | Loss: 253.6328 | CE: 0.1114 | KD: 1060.1006\n",
      "Train Epoch: 032 Batch: 00076/00094 | Loss: 253.6248 | CE: 0.1045 | KD: 1060.0957\n",
      "Train Epoch: 032 Batch: 00077/00094 | Loss: 253.6978 | CE: 0.1748 | KD: 1060.1074\n",
      "Train Epoch: 032 Batch: 00078/00094 | Loss: 253.6443 | CE: 0.1113 | KD: 1060.1490\n",
      "Train Epoch: 032 Batch: 00079/00094 | Loss: 253.6383 | CE: 0.1188 | KD: 1060.0925\n",
      "Train Epoch: 032 Batch: 00080/00094 | Loss: 253.6054 | CE: 0.1061 | KD: 1060.0082\n",
      "Train Epoch: 032 Batch: 00081/00094 | Loss: 253.6262 | CE: 0.1083 | KD: 1060.0859\n",
      "Train Epoch: 032 Batch: 00082/00094 | Loss: 253.6381 | CE: 0.1193 | KD: 1060.0897\n",
      "Train Epoch: 032 Batch: 00083/00094 | Loss: 253.6479 | CE: 0.1235 | KD: 1060.1130\n",
      "Train Epoch: 032 Batch: 00084/00094 | Loss: 253.6216 | CE: 0.0930 | KD: 1060.1305\n",
      "Train Epoch: 032 Batch: 00085/00094 | Loss: 253.6678 | CE: 0.1382 | KD: 1060.1348\n",
      "Train Epoch: 032 Batch: 00086/00094 | Loss: 253.6137 | CE: 0.1016 | KD: 1060.0615\n",
      "Train Epoch: 032 Batch: 00087/00094 | Loss: 253.6455 | CE: 0.1210 | KD: 1060.1135\n",
      "Train Epoch: 032 Batch: 00088/00094 | Loss: 253.6406 | CE: 0.1142 | KD: 1060.1215\n",
      "Train Epoch: 032 Batch: 00089/00094 | Loss: 253.6367 | CE: 0.1175 | KD: 1060.0912\n",
      "Train Epoch: 032 Batch: 00090/00094 | Loss: 253.6271 | CE: 0.1045 | KD: 1060.1055\n",
      "Train Epoch: 032 Batch: 00091/00094 | Loss: 253.7634 | CE: 0.2391 | KD: 1060.1124\n",
      "Train Epoch: 032 Batch: 00092/00094 | Loss: 253.6577 | CE: 0.1461 | KD: 1060.0598\n",
      "Train Epoch: 032 Batch: 00093/00094 | Loss: 253.5943 | CE: 0.0909 | KD: 1060.0250\n",
      "Train Epoch: 032 Batch: 00094/00094 | Loss: 253.6846 | CE: 0.1598 | KD: 1060.1146\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1071 | acc:99.1500\n",
      "[VAL Acc] Target: 99.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2194 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5905 | acc:71.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 71.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0417 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7892 | acc:60.5016\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6303 | acc:69.5933\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.59%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5452 | acc:75.6270\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7936 | acc:59.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.19%\n",
      "[VAL Acc] Avg 66.56%\n",
      "Train Epoch: 033 Batch: 00001/00094 | Loss: 253.6173 | CE: 0.0979 | KD: 1060.0922\n",
      "Train Epoch: 033 Batch: 00002/00094 | Loss: 253.6463 | CE: 0.1062 | KD: 1060.1788\n",
      "Train Epoch: 033 Batch: 00003/00094 | Loss: 253.6401 | CE: 0.1106 | KD: 1060.1348\n",
      "Train Epoch: 033 Batch: 00004/00094 | Loss: 253.6266 | CE: 0.1026 | KD: 1060.1115\n",
      "Train Epoch: 033 Batch: 00005/00094 | Loss: 253.6418 | CE: 0.1070 | KD: 1060.1562\n",
      "Train Epoch: 033 Batch: 00006/00094 | Loss: 253.6141 | CE: 0.0937 | KD: 1060.0964\n",
      "Train Epoch: 033 Batch: 00007/00094 | Loss: 253.6382 | CE: 0.1142 | KD: 1060.1113\n",
      "Train Epoch: 033 Batch: 00008/00094 | Loss: 253.6069 | CE: 0.0845 | KD: 1060.1045\n",
      "Train Epoch: 033 Batch: 00009/00094 | Loss: 253.6387 | CE: 0.1216 | KD: 1060.0826\n",
      "Train Epoch: 033 Batch: 00010/00094 | Loss: 253.6343 | CE: 0.1085 | KD: 1060.1190\n",
      "Train Epoch: 033 Batch: 00011/00094 | Loss: 253.6174 | CE: 0.0995 | KD: 1060.0858\n",
      "Train Epoch: 033 Batch: 00012/00094 | Loss: 253.6261 | CE: 0.1085 | KD: 1060.0846\n",
      "Train Epoch: 033 Batch: 00013/00094 | Loss: 253.6380 | CE: 0.1051 | KD: 1060.1484\n",
      "Train Epoch: 033 Batch: 00014/00094 | Loss: 253.6131 | CE: 0.0963 | KD: 1060.0813\n",
      "Train Epoch: 033 Batch: 00015/00094 | Loss: 253.6052 | CE: 0.0936 | KD: 1060.0597\n",
      "Train Epoch: 033 Batch: 00016/00094 | Loss: 253.6219 | CE: 0.1051 | KD: 1060.0814\n",
      "Train Epoch: 033 Batch: 00017/00094 | Loss: 253.6778 | CE: 0.1489 | KD: 1060.1317\n",
      "Train Epoch: 033 Batch: 00018/00094 | Loss: 253.7061 | CE: 0.1805 | KD: 1060.1179\n",
      "Train Epoch: 033 Batch: 00019/00094 | Loss: 253.6176 | CE: 0.0955 | KD: 1060.1036\n",
      "Train Epoch: 033 Batch: 00020/00094 | Loss: 253.6953 | CE: 0.1614 | KD: 1060.1530\n",
      "Train Epoch: 033 Batch: 00021/00094 | Loss: 253.6247 | CE: 0.1043 | KD: 1060.0964\n",
      "Train Epoch: 033 Batch: 00022/00094 | Loss: 253.7593 | CE: 0.2314 | KD: 1060.1277\n",
      "Train Epoch: 033 Batch: 00023/00094 | Loss: 253.6547 | CE: 0.1298 | KD: 1060.1152\n",
      "Train Epoch: 033 Batch: 00024/00094 | Loss: 253.6128 | CE: 0.1001 | KD: 1060.0642\n",
      "Train Epoch: 033 Batch: 00025/00094 | Loss: 253.6537 | CE: 0.1205 | KD: 1060.1501\n",
      "Train Epoch: 033 Batch: 00026/00094 | Loss: 253.6029 | CE: 0.0906 | KD: 1060.0624\n",
      "Train Epoch: 033 Batch: 00027/00094 | Loss: 253.6548 | CE: 0.1282 | KD: 1060.1224\n",
      "Train Epoch: 033 Batch: 00028/00094 | Loss: 253.6411 | CE: 0.1229 | KD: 1060.0873\n",
      "Train Epoch: 033 Batch: 00029/00094 | Loss: 253.6214 | CE: 0.0924 | KD: 1060.1321\n",
      "Train Epoch: 033 Batch: 00030/00094 | Loss: 253.6362 | CE: 0.1277 | KD: 1060.0463\n",
      "Train Epoch: 033 Batch: 00031/00094 | Loss: 253.6776 | CE: 0.1576 | KD: 1060.0946\n",
      "Train Epoch: 033 Batch: 00032/00094 | Loss: 253.7260 | CE: 0.1833 | KD: 1060.1898\n",
      "Train Epoch: 033 Batch: 00033/00094 | Loss: 253.6339 | CE: 0.0984 | KD: 1060.1595\n",
      "Train Epoch: 033 Batch: 00034/00094 | Loss: 253.6303 | CE: 0.1183 | KD: 1060.0610\n",
      "Train Epoch: 033 Batch: 00035/00094 | Loss: 253.5904 | CE: 0.0801 | KD: 1060.0540\n",
      "Train Epoch: 033 Batch: 00036/00094 | Loss: 253.7219 | CE: 0.1989 | KD: 1060.1069\n",
      "Train Epoch: 033 Batch: 00037/00094 | Loss: 253.6437 | CE: 0.1166 | KD: 1060.1244\n",
      "Train Epoch: 033 Batch: 00038/00094 | Loss: 253.6334 | CE: 0.1117 | KD: 1060.1017\n",
      "Train Epoch: 033 Batch: 00039/00094 | Loss: 253.6289 | CE: 0.1094 | KD: 1060.0925\n",
      "Train Epoch: 033 Batch: 00040/00094 | Loss: 253.6035 | CE: 0.0920 | KD: 1060.0590\n",
      "Train Epoch: 033 Batch: 00041/00094 | Loss: 253.6332 | CE: 0.1117 | KD: 1060.1010\n",
      "Train Epoch: 033 Batch: 00042/00094 | Loss: 253.6045 | CE: 0.0803 | KD: 1060.1122\n",
      "Train Epoch: 033 Batch: 00043/00094 | Loss: 253.6079 | CE: 0.1005 | KD: 1060.0420\n",
      "Train Epoch: 033 Batch: 00044/00094 | Loss: 253.6238 | CE: 0.1107 | KD: 1060.0657\n",
      "Train Epoch: 033 Batch: 00045/00094 | Loss: 253.6514 | CE: 0.1260 | KD: 1060.1174\n",
      "Train Epoch: 033 Batch: 00046/00094 | Loss: 253.6175 | CE: 0.1013 | KD: 1060.0785\n",
      "Train Epoch: 033 Batch: 00047/00094 | Loss: 253.7270 | CE: 0.1893 | KD: 1060.1686\n",
      "Train Epoch: 033 Batch: 00048/00094 | Loss: 253.6426 | CE: 0.1066 | KD: 1060.1616\n",
      "Train Epoch: 033 Batch: 00049/00094 | Loss: 253.6485 | CE: 0.1351 | KD: 1060.0669\n",
      "Train Epoch: 033 Batch: 00050/00094 | Loss: 253.6728 | CE: 0.1507 | KD: 1060.1034\n",
      "Train Epoch: 033 Batch: 00051/00094 | Loss: 253.6369 | CE: 0.1103 | KD: 1060.1223\n",
      "Train Epoch: 033 Batch: 00052/00094 | Loss: 253.5926 | CE: 0.0884 | KD: 1060.0289\n",
      "Train Epoch: 033 Batch: 00053/00094 | Loss: 253.6590 | CE: 0.1270 | KD: 1060.1447\n",
      "Train Epoch: 033 Batch: 00054/00094 | Loss: 253.6503 | CE: 0.1453 | KD: 1060.0320\n",
      "Train Epoch: 033 Batch: 00055/00094 | Loss: 253.6311 | CE: 0.1099 | KD: 1060.0996\n",
      "Train Epoch: 033 Batch: 00056/00094 | Loss: 253.6177 | CE: 0.1055 | KD: 1060.0618\n",
      "Train Epoch: 033 Batch: 00057/00094 | Loss: 253.6191 | CE: 0.1050 | KD: 1060.0701\n",
      "Train Epoch: 033 Batch: 00058/00094 | Loss: 253.6215 | CE: 0.0959 | KD: 1060.1179\n",
      "Train Epoch: 033 Batch: 00059/00094 | Loss: 253.6363 | CE: 0.1129 | KD: 1060.1085\n",
      "Train Epoch: 033 Batch: 00060/00094 | Loss: 253.6478 | CE: 0.1372 | KD: 1060.0552\n",
      "Train Epoch: 033 Batch: 00061/00094 | Loss: 253.6314 | CE: 0.1115 | KD: 1060.0944\n",
      "Train Epoch: 033 Batch: 00062/00094 | Loss: 253.6569 | CE: 0.1184 | KD: 1060.1720\n",
      "Train Epoch: 033 Batch: 00063/00094 | Loss: 253.6292 | CE: 0.1034 | KD: 1060.1189\n",
      "Train Epoch: 033 Batch: 00064/00094 | Loss: 253.7121 | CE: 0.1836 | KD: 1060.1304\n",
      "Train Epoch: 033 Batch: 00065/00094 | Loss: 253.6390 | CE: 0.1102 | KD: 1060.1315\n",
      "Train Epoch: 033 Batch: 00066/00094 | Loss: 253.6299 | CE: 0.1128 | KD: 1060.0826\n",
      "Train Epoch: 033 Batch: 00067/00094 | Loss: 253.6644 | CE: 0.1333 | KD: 1060.1410\n",
      "Train Epoch: 033 Batch: 00068/00094 | Loss: 253.6316 | CE: 0.0923 | KD: 1060.1753\n",
      "Train Epoch: 033 Batch: 00069/00094 | Loss: 253.6568 | CE: 0.1227 | KD: 1060.1537\n",
      "Train Epoch: 033 Batch: 00070/00094 | Loss: 253.6160 | CE: 0.0934 | KD: 1060.1056\n",
      "Train Epoch: 033 Batch: 00071/00094 | Loss: 253.6543 | CE: 0.1371 | KD: 1060.0828\n",
      "Train Epoch: 033 Batch: 00072/00094 | Loss: 253.6304 | CE: 0.1158 | KD: 1060.0723\n",
      "Train Epoch: 033 Batch: 00073/00094 | Loss: 253.6201 | CE: 0.1027 | KD: 1060.0841\n",
      "Train Epoch: 033 Batch: 00074/00094 | Loss: 253.6328 | CE: 0.1100 | KD: 1060.1062\n",
      "Train Epoch: 033 Batch: 00075/00094 | Loss: 253.6696 | CE: 0.1522 | KD: 1060.0836\n",
      "Train Epoch: 033 Batch: 00076/00094 | Loss: 253.6314 | CE: 0.1210 | KD: 1060.0548\n",
      "Train Epoch: 033 Batch: 00077/00094 | Loss: 253.6303 | CE: 0.1066 | KD: 1060.1101\n",
      "Train Epoch: 033 Batch: 00078/00094 | Loss: 253.6105 | CE: 0.0934 | KD: 1060.0824\n",
      "Train Epoch: 033 Batch: 00079/00094 | Loss: 253.6307 | CE: 0.1070 | KD: 1060.1101\n",
      "Train Epoch: 033 Batch: 00080/00094 | Loss: 253.6723 | CE: 0.1455 | KD: 1060.1232\n",
      "Train Epoch: 033 Batch: 00081/00094 | Loss: 253.6222 | CE: 0.1184 | KD: 1060.0269\n",
      "Train Epoch: 033 Batch: 00082/00094 | Loss: 253.7178 | CE: 0.1961 | KD: 1060.1022\n",
      "Train Epoch: 033 Batch: 00083/00094 | Loss: 253.6131 | CE: 0.0917 | KD: 1060.1006\n",
      "Train Epoch: 033 Batch: 00084/00094 | Loss: 253.6678 | CE: 0.1388 | KD: 1060.1324\n",
      "Train Epoch: 033 Batch: 00085/00094 | Loss: 253.6312 | CE: 0.1032 | KD: 1060.1283\n",
      "Train Epoch: 033 Batch: 00086/00094 | Loss: 253.6285 | CE: 0.1007 | KD: 1060.1270\n",
      "Train Epoch: 033 Batch: 00087/00094 | Loss: 253.6102 | CE: 0.0959 | KD: 1060.0706\n",
      "Train Epoch: 033 Batch: 00088/00094 | Loss: 253.6216 | CE: 0.0939 | KD: 1060.1266\n",
      "Train Epoch: 033 Batch: 00089/00094 | Loss: 253.6246 | CE: 0.1065 | KD: 1060.0869\n",
      "Train Epoch: 033 Batch: 00090/00094 | Loss: 253.6413 | CE: 0.1233 | KD: 1060.0863\n",
      "Train Epoch: 033 Batch: 00091/00094 | Loss: 253.6533 | CE: 0.1211 | KD: 1060.1458\n",
      "Train Epoch: 033 Batch: 00092/00094 | Loss: 253.6478 | CE: 0.1171 | KD: 1060.1394\n",
      "Train Epoch: 033 Batch: 00093/00094 | Loss: 253.6182 | CE: 0.0940 | KD: 1060.1125\n",
      "Train Epoch: 033 Batch: 00094/00094 | Loss: 253.6180 | CE: 0.1060 | KD: 1060.0615\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1108 | acc:98.7500\n",
      "[VAL Acc] Target: 98.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2142 | acc:49.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5765 | acc:70.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 70.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0394 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7683 | acc:60.6975\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5941 | acc:71.9039\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5277 | acc:76.4890\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8030 | acc:58.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.50%\n",
      "[VAL Acc] Avg 66.55%\n",
      "Train Epoch: 034 Batch: 00001/00094 | Loss: 228.3028 | CE: 0.1268 | KD: 1060.1316\n",
      "Train Epoch: 034 Batch: 00002/00094 | Loss: 228.2589 | CE: 0.0997 | KD: 1060.0537\n",
      "Train Epoch: 034 Batch: 00003/00094 | Loss: 228.2708 | CE: 0.0992 | KD: 1060.1110\n",
      "Train Epoch: 034 Batch: 00004/00094 | Loss: 228.2726 | CE: 0.1028 | KD: 1060.1029\n",
      "Train Epoch: 034 Batch: 00005/00094 | Loss: 228.2767 | CE: 0.1063 | KD: 1060.1058\n",
      "Train Epoch: 034 Batch: 00006/00094 | Loss: 228.2851 | CE: 0.1155 | KD: 1060.1022\n",
      "Train Epoch: 034 Batch: 00007/00094 | Loss: 228.3272 | CE: 0.1523 | KD: 1060.1268\n",
      "Train Epoch: 034 Batch: 00008/00094 | Loss: 228.2568 | CE: 0.0892 | KD: 1060.0929\n",
      "Train Epoch: 034 Batch: 00009/00094 | Loss: 228.3006 | CE: 0.1257 | KD: 1060.1266\n",
      "Train Epoch: 034 Batch: 00010/00094 | Loss: 228.2659 | CE: 0.0986 | KD: 1060.0914\n",
      "Train Epoch: 034 Batch: 00011/00094 | Loss: 228.2928 | CE: 0.1188 | KD: 1060.1226\n",
      "Train Epoch: 034 Batch: 00012/00094 | Loss: 228.2689 | CE: 0.1113 | KD: 1060.0464\n",
      "Train Epoch: 034 Batch: 00013/00094 | Loss: 228.2958 | CE: 0.1260 | KD: 1060.1030\n",
      "Train Epoch: 034 Batch: 00014/00094 | Loss: 228.3145 | CE: 0.1402 | KD: 1060.1235\n",
      "Train Epoch: 034 Batch: 00015/00094 | Loss: 228.2903 | CE: 0.1081 | KD: 1060.1608\n",
      "Train Epoch: 034 Batch: 00016/00094 | Loss: 228.2696 | CE: 0.0970 | KD: 1060.1157\n",
      "Train Epoch: 034 Batch: 00017/00094 | Loss: 228.2689 | CE: 0.0983 | KD: 1060.1069\n",
      "Train Epoch: 034 Batch: 00018/00094 | Loss: 228.3000 | CE: 0.1329 | KD: 1060.0906\n",
      "Train Epoch: 034 Batch: 00019/00094 | Loss: 228.2883 | CE: 0.1102 | KD: 1060.1416\n",
      "Train Epoch: 034 Batch: 00020/00094 | Loss: 228.2870 | CE: 0.1245 | KD: 1060.0690\n",
      "Train Epoch: 034 Batch: 00021/00094 | Loss: 228.2698 | CE: 0.0970 | KD: 1060.1168\n",
      "Train Epoch: 034 Batch: 00022/00094 | Loss: 228.2501 | CE: 0.0980 | KD: 1060.0206\n",
      "Train Epoch: 034 Batch: 00023/00094 | Loss: 228.3432 | CE: 0.1786 | KD: 1060.0787\n",
      "Train Epoch: 034 Batch: 00024/00094 | Loss: 228.2698 | CE: 0.0957 | KD: 1060.1230\n",
      "Train Epoch: 034 Batch: 00025/00094 | Loss: 228.2600 | CE: 0.1008 | KD: 1060.0536\n",
      "Train Epoch: 034 Batch: 00026/00094 | Loss: 228.2621 | CE: 0.0896 | KD: 1060.1155\n",
      "Train Epoch: 034 Batch: 00027/00094 | Loss: 228.3210 | CE: 0.1442 | KD: 1060.1356\n",
      "Train Epoch: 034 Batch: 00028/00094 | Loss: 228.2545 | CE: 0.0936 | KD: 1060.0616\n",
      "Train Epoch: 034 Batch: 00029/00094 | Loss: 228.3180 | CE: 0.1389 | KD: 1060.1464\n",
      "Train Epoch: 034 Batch: 00030/00094 | Loss: 228.3080 | CE: 0.1283 | KD: 1060.1490\n",
      "Train Epoch: 034 Batch: 00031/00094 | Loss: 228.2601 | CE: 0.0915 | KD: 1060.0975\n",
      "Train Epoch: 034 Batch: 00032/00094 | Loss: 228.2495 | CE: 0.0904 | KD: 1060.0532\n",
      "Train Epoch: 034 Batch: 00033/00094 | Loss: 228.2985 | CE: 0.1194 | KD: 1060.1461\n",
      "Train Epoch: 034 Batch: 00034/00094 | Loss: 228.3053 | CE: 0.1373 | KD: 1060.0948\n",
      "Train Epoch: 034 Batch: 00035/00094 | Loss: 228.2713 | CE: 0.1071 | KD: 1060.0770\n",
      "Train Epoch: 034 Batch: 00036/00094 | Loss: 228.2662 | CE: 0.1023 | KD: 1060.0757\n",
      "Train Epoch: 034 Batch: 00037/00094 | Loss: 228.2718 | CE: 0.0977 | KD: 1060.1228\n",
      "Train Epoch: 034 Batch: 00038/00094 | Loss: 228.2904 | CE: 0.1245 | KD: 1060.0846\n",
      "Train Epoch: 034 Batch: 00039/00094 | Loss: 228.2656 | CE: 0.0965 | KD: 1060.0999\n",
      "Train Epoch: 034 Batch: 00040/00094 | Loss: 228.3156 | CE: 0.1401 | KD: 1060.1294\n",
      "Train Epoch: 034 Batch: 00041/00094 | Loss: 228.2620 | CE: 0.0970 | KD: 1060.0807\n",
      "Train Epoch: 034 Batch: 00042/00094 | Loss: 228.2679 | CE: 0.0937 | KD: 1060.1232\n",
      "Train Epoch: 034 Batch: 00043/00094 | Loss: 228.2681 | CE: 0.0876 | KD: 1060.1523\n",
      "Train Epoch: 034 Batch: 00044/00094 | Loss: 228.2849 | CE: 0.1212 | KD: 1060.0747\n",
      "Train Epoch: 034 Batch: 00045/00094 | Loss: 228.2642 | CE: 0.1012 | KD: 1060.0712\n",
      "Train Epoch: 034 Batch: 00046/00094 | Loss: 228.2780 | CE: 0.1208 | KD: 1060.0446\n",
      "Train Epoch: 034 Batch: 00047/00094 | Loss: 228.2934 | CE: 0.1193 | KD: 1060.1229\n",
      "Train Epoch: 034 Batch: 00048/00094 | Loss: 228.2849 | CE: 0.1200 | KD: 1060.0802\n",
      "Train Epoch: 034 Batch: 00049/00094 | Loss: 228.3491 | CE: 0.1687 | KD: 1060.1525\n",
      "Train Epoch: 034 Batch: 00050/00094 | Loss: 228.2737 | CE: 0.1106 | KD: 1060.0715\n",
      "Train Epoch: 034 Batch: 00051/00094 | Loss: 228.2739 | CE: 0.1112 | KD: 1060.0701\n",
      "Train Epoch: 034 Batch: 00052/00094 | Loss: 228.2937 | CE: 0.1237 | KD: 1060.1039\n",
      "Train Epoch: 034 Batch: 00053/00094 | Loss: 228.2987 | CE: 0.1270 | KD: 1060.1117\n",
      "Train Epoch: 034 Batch: 00054/00094 | Loss: 228.2742 | CE: 0.1015 | KD: 1060.1165\n",
      "Train Epoch: 034 Batch: 00055/00094 | Loss: 228.2790 | CE: 0.1120 | KD: 1060.0900\n",
      "Train Epoch: 034 Batch: 00056/00094 | Loss: 228.2634 | CE: 0.0999 | KD: 1060.0736\n",
      "Train Epoch: 034 Batch: 00057/00094 | Loss: 228.3030 | CE: 0.1210 | KD: 1060.1599\n",
      "Train Epoch: 034 Batch: 00058/00094 | Loss: 228.2735 | CE: 0.0943 | KD: 1060.1467\n",
      "Train Epoch: 034 Batch: 00059/00094 | Loss: 228.2586 | CE: 0.0961 | KD: 1060.0692\n",
      "Train Epoch: 034 Batch: 00060/00094 | Loss: 228.2851 | CE: 0.1030 | KD: 1060.1602\n",
      "Train Epoch: 034 Batch: 00061/00094 | Loss: 228.2716 | CE: 0.0996 | KD: 1060.1130\n",
      "Train Epoch: 034 Batch: 00062/00094 | Loss: 228.2494 | CE: 0.0778 | KD: 1060.1117\n",
      "Train Epoch: 034 Batch: 00063/00094 | Loss: 228.2728 | CE: 0.1000 | KD: 1060.1169\n",
      "Train Epoch: 034 Batch: 00064/00094 | Loss: 228.2671 | CE: 0.0987 | KD: 1060.0963\n",
      "Train Epoch: 034 Batch: 00065/00094 | Loss: 228.3066 | CE: 0.1374 | KD: 1060.1001\n",
      "Train Epoch: 034 Batch: 00066/00094 | Loss: 228.2850 | CE: 0.1124 | KD: 1060.1160\n",
      "Train Epoch: 034 Batch: 00067/00094 | Loss: 228.3104 | CE: 0.1308 | KD: 1060.1487\n",
      "Train Epoch: 034 Batch: 00068/00094 | Loss: 228.3030 | CE: 0.1159 | KD: 1060.1835\n",
      "Train Epoch: 034 Batch: 00069/00094 | Loss: 228.3293 | CE: 0.1505 | KD: 1060.1450\n",
      "Train Epoch: 034 Batch: 00070/00094 | Loss: 228.2967 | CE: 0.1172 | KD: 1060.1479\n",
      "Train Epoch: 034 Batch: 00071/00094 | Loss: 228.3211 | CE: 0.1470 | KD: 1060.1230\n",
      "Train Epoch: 034 Batch: 00072/00094 | Loss: 228.2701 | CE: 0.1189 | KD: 1060.0166\n",
      "Train Epoch: 034 Batch: 00073/00094 | Loss: 228.2689 | CE: 0.0842 | KD: 1060.1722\n",
      "Train Epoch: 034 Batch: 00074/00094 | Loss: 228.2893 | CE: 0.1130 | KD: 1060.1334\n",
      "Train Epoch: 034 Batch: 00075/00094 | Loss: 228.2494 | CE: 0.0836 | KD: 1060.0845\n",
      "Train Epoch: 034 Batch: 00076/00094 | Loss: 228.3312 | CE: 0.1662 | KD: 1060.0807\n",
      "Train Epoch: 034 Batch: 00077/00094 | Loss: 228.2783 | CE: 0.1130 | KD: 1060.0824\n",
      "Train Epoch: 034 Batch: 00078/00094 | Loss: 228.2614 | CE: 0.0793 | KD: 1060.1598\n",
      "Train Epoch: 034 Batch: 00079/00094 | Loss: 228.2544 | CE: 0.1007 | KD: 1060.0283\n",
      "Train Epoch: 034 Batch: 00080/00094 | Loss: 228.3114 | CE: 0.1321 | KD: 1060.1473\n",
      "Train Epoch: 034 Batch: 00081/00094 | Loss: 228.3135 | CE: 0.1378 | KD: 1060.1304\n",
      "Train Epoch: 034 Batch: 00082/00094 | Loss: 228.2719 | CE: 0.0946 | KD: 1060.1375\n",
      "Train Epoch: 034 Batch: 00083/00094 | Loss: 228.2585 | CE: 0.1023 | KD: 1060.0398\n",
      "Train Epoch: 034 Batch: 00084/00094 | Loss: 228.2735 | CE: 0.0951 | KD: 1060.1429\n",
      "Train Epoch: 034 Batch: 00085/00094 | Loss: 228.2643 | CE: 0.0788 | KD: 1060.1760\n",
      "Train Epoch: 034 Batch: 00086/00094 | Loss: 228.2734 | CE: 0.1005 | KD: 1060.1171\n",
      "Train Epoch: 034 Batch: 00087/00094 | Loss: 228.2920 | CE: 0.1161 | KD: 1060.1315\n",
      "Train Epoch: 034 Batch: 00088/00094 | Loss: 228.3048 | CE: 0.1468 | KD: 1060.0486\n",
      "Train Epoch: 034 Batch: 00089/00094 | Loss: 228.2736 | CE: 0.0990 | KD: 1060.1254\n",
      "Train Epoch: 034 Batch: 00090/00094 | Loss: 228.3141 | CE: 0.1535 | KD: 1060.0603\n",
      "Train Epoch: 034 Batch: 00091/00094 | Loss: 228.2569 | CE: 0.0982 | KD: 1060.0513\n",
      "Train Epoch: 034 Batch: 00092/00094 | Loss: 228.3172 | CE: 0.1416 | KD: 1060.1296\n",
      "Train Epoch: 034 Batch: 00093/00094 | Loss: 228.2562 | CE: 0.0967 | KD: 1060.0552\n",
      "Train Epoch: 034 Batch: 00094/00094 | Loss: 228.2643 | CE: 0.0970 | KD: 1060.0912\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1074 | acc:98.5000\n",
      "[VAL Acc] Target: 98.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2239 | acc:50.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5478 | acc:73.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 73.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0435 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8484 | acc:58.3072\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5350 | acc:75.8780\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5739 | acc:74.4906\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8699 | acc:56.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.00%\n",
      "[VAL Acc] Avg 66.45%\n",
      "Train Epoch: 035 Batch: 00001/00094 | Loss: 228.2574 | CE: 0.0761 | KD: 1060.1561\n",
      "Train Epoch: 035 Batch: 00002/00094 | Loss: 228.2676 | CE: 0.0894 | KD: 1060.1421\n",
      "Train Epoch: 035 Batch: 00003/00094 | Loss: 228.2738 | CE: 0.0997 | KD: 1060.1229\n",
      "Train Epoch: 035 Batch: 00004/00094 | Loss: 228.2994 | CE: 0.1187 | KD: 1060.1533\n",
      "Train Epoch: 035 Batch: 00005/00094 | Loss: 228.2796 | CE: 0.1144 | KD: 1060.0819\n",
      "Train Epoch: 035 Batch: 00006/00094 | Loss: 228.3077 | CE: 0.1273 | KD: 1060.1525\n",
      "Train Epoch: 035 Batch: 00007/00094 | Loss: 228.2700 | CE: 0.0973 | KD: 1060.1166\n",
      "Train Epoch: 035 Batch: 00008/00094 | Loss: 228.2516 | CE: 0.0891 | KD: 1060.0691\n",
      "Train Epoch: 035 Batch: 00009/00094 | Loss: 228.2837 | CE: 0.1305 | KD: 1060.0258\n",
      "Train Epoch: 035 Batch: 00010/00094 | Loss: 228.2685 | CE: 0.1024 | KD: 1060.0858\n",
      "Train Epoch: 035 Batch: 00011/00094 | Loss: 228.2753 | CE: 0.1031 | KD: 1060.1141\n",
      "Train Epoch: 035 Batch: 00012/00094 | Loss: 228.2543 | CE: 0.0772 | KD: 1060.1366\n",
      "Train Epoch: 035 Batch: 00013/00094 | Loss: 228.2918 | CE: 0.1170 | KD: 1060.1261\n",
      "Train Epoch: 035 Batch: 00014/00094 | Loss: 228.2702 | CE: 0.0950 | KD: 1060.1285\n",
      "Train Epoch: 035 Batch: 00015/00094 | Loss: 228.2640 | CE: 0.0839 | KD: 1060.1508\n",
      "Train Epoch: 035 Batch: 00016/00094 | Loss: 228.2634 | CE: 0.0883 | KD: 1060.1274\n",
      "Train Epoch: 035 Batch: 00017/00094 | Loss: 228.2665 | CE: 0.0906 | KD: 1060.1312\n",
      "Train Epoch: 035 Batch: 00018/00094 | Loss: 228.2605 | CE: 0.1036 | KD: 1060.0432\n",
      "Train Epoch: 035 Batch: 00019/00094 | Loss: 228.3046 | CE: 0.1313 | KD: 1060.1191\n",
      "Train Epoch: 035 Batch: 00020/00094 | Loss: 228.2660 | CE: 0.0929 | KD: 1060.1187\n",
      "Train Epoch: 035 Batch: 00021/00094 | Loss: 228.2644 | CE: 0.0881 | KD: 1060.1332\n",
      "Train Epoch: 035 Batch: 00022/00094 | Loss: 228.3016 | CE: 0.1213 | KD: 1060.1517\n",
      "Train Epoch: 035 Batch: 00023/00094 | Loss: 228.2387 | CE: 0.0668 | KD: 1060.1132\n",
      "Train Epoch: 035 Batch: 00024/00094 | Loss: 228.3021 | CE: 0.1151 | KD: 1060.1829\n",
      "Train Epoch: 035 Batch: 00025/00094 | Loss: 228.3622 | CE: 0.1860 | KD: 1060.1327\n",
      "Train Epoch: 035 Batch: 00026/00094 | Loss: 228.2799 | CE: 0.1066 | KD: 1060.1193\n",
      "Train Epoch: 035 Batch: 00027/00094 | Loss: 228.2718 | CE: 0.1003 | KD: 1060.1110\n",
      "Train Epoch: 035 Batch: 00028/00094 | Loss: 228.2667 | CE: 0.0939 | KD: 1060.1169\n",
      "Train Epoch: 035 Batch: 00029/00094 | Loss: 228.2972 | CE: 0.1229 | KD: 1060.1241\n",
      "Train Epoch: 035 Batch: 00030/00094 | Loss: 228.2872 | CE: 0.1074 | KD: 1060.1494\n",
      "Train Epoch: 035 Batch: 00031/00094 | Loss: 228.2772 | CE: 0.1016 | KD: 1060.1296\n",
      "Train Epoch: 035 Batch: 00032/00094 | Loss: 228.2671 | CE: 0.0835 | KD: 1060.1667\n",
      "Train Epoch: 035 Batch: 00033/00094 | Loss: 228.3528 | CE: 0.1766 | KD: 1060.1324\n",
      "Train Epoch: 035 Batch: 00034/00094 | Loss: 228.2512 | CE: 0.0860 | KD: 1060.0815\n",
      "Train Epoch: 035 Batch: 00035/00094 | Loss: 228.2738 | CE: 0.1151 | KD: 1060.0513\n",
      "Train Epoch: 035 Batch: 00036/00094 | Loss: 228.3208 | CE: 0.1338 | KD: 1060.1833\n",
      "Train Epoch: 035 Batch: 00037/00094 | Loss: 228.2898 | CE: 0.1091 | KD: 1060.1538\n",
      "Train Epoch: 035 Batch: 00038/00094 | Loss: 228.3019 | CE: 0.1301 | KD: 1060.1124\n",
      "Train Epoch: 035 Batch: 00039/00094 | Loss: 228.2816 | CE: 0.1074 | KD: 1060.1232\n",
      "Train Epoch: 035 Batch: 00040/00094 | Loss: 228.3622 | CE: 0.1789 | KD: 1060.1656\n",
      "Train Epoch: 035 Batch: 00041/00094 | Loss: 228.3019 | CE: 0.1321 | KD: 1060.1029\n",
      "Train Epoch: 035 Batch: 00042/00094 | Loss: 228.2573 | CE: 0.0971 | KD: 1060.0581\n",
      "Train Epoch: 035 Batch: 00043/00094 | Loss: 228.2883 | CE: 0.1189 | KD: 1060.1014\n",
      "Train Epoch: 035 Batch: 00044/00094 | Loss: 228.2596 | CE: 0.0926 | KD: 1060.0900\n",
      "Train Epoch: 035 Batch: 00045/00094 | Loss: 228.2507 | CE: 0.0962 | KD: 1060.0316\n",
      "Train Epoch: 035 Batch: 00046/00094 | Loss: 228.2704 | CE: 0.1027 | KD: 1060.0936\n",
      "Train Epoch: 035 Batch: 00047/00094 | Loss: 228.2919 | CE: 0.1074 | KD: 1060.1713\n",
      "Train Epoch: 035 Batch: 00048/00094 | Loss: 228.2495 | CE: 0.0804 | KD: 1060.0997\n",
      "Train Epoch: 035 Batch: 00049/00094 | Loss: 228.2793 | CE: 0.1007 | KD: 1060.1437\n",
      "Train Epoch: 035 Batch: 00050/00094 | Loss: 228.2830 | CE: 0.1057 | KD: 1060.1381\n",
      "Train Epoch: 035 Batch: 00051/00094 | Loss: 228.2780 | CE: 0.1021 | KD: 1060.1316\n",
      "Train Epoch: 035 Batch: 00052/00094 | Loss: 228.2830 | CE: 0.1202 | KD: 1060.0704\n",
      "Train Epoch: 035 Batch: 00053/00094 | Loss: 228.3859 | CE: 0.2155 | KD: 1060.1058\n",
      "Train Epoch: 035 Batch: 00054/00094 | Loss: 228.2831 | CE: 0.1023 | KD: 1060.1537\n",
      "Train Epoch: 035 Batch: 00055/00094 | Loss: 228.2598 | CE: 0.0843 | KD: 1060.1293\n",
      "Train Epoch: 035 Batch: 00056/00094 | Loss: 228.3329 | CE: 0.1577 | KD: 1060.1284\n",
      "Train Epoch: 035 Batch: 00057/00094 | Loss: 228.2615 | CE: 0.0866 | KD: 1060.1263\n",
      "Train Epoch: 035 Batch: 00058/00094 | Loss: 228.2790 | CE: 0.1048 | KD: 1060.1233\n",
      "Train Epoch: 035 Batch: 00059/00094 | Loss: 228.3131 | CE: 0.1341 | KD: 1060.1460\n",
      "Train Epoch: 035 Batch: 00060/00094 | Loss: 228.2767 | CE: 0.1051 | KD: 1060.1115\n",
      "Train Epoch: 035 Batch: 00061/00094 | Loss: 228.3442 | CE: 0.1687 | KD: 1060.1295\n",
      "Train Epoch: 035 Batch: 00062/00094 | Loss: 228.2539 | CE: 0.0849 | KD: 1060.0991\n",
      "Train Epoch: 035 Batch: 00063/00094 | Loss: 228.3278 | CE: 0.1472 | KD: 1060.1528\n",
      "Train Epoch: 035 Batch: 00064/00094 | Loss: 228.2579 | CE: 0.0848 | KD: 1060.1182\n",
      "Train Epoch: 035 Batch: 00065/00094 | Loss: 228.2641 | CE: 0.1026 | KD: 1060.0645\n",
      "Train Epoch: 035 Batch: 00066/00094 | Loss: 228.3438 | CE: 0.1687 | KD: 1060.1278\n",
      "Train Epoch: 035 Batch: 00067/00094 | Loss: 228.2535 | CE: 0.0789 | KD: 1060.1255\n",
      "Train Epoch: 035 Batch: 00068/00094 | Loss: 228.3151 | CE: 0.1392 | KD: 1060.1313\n",
      "Train Epoch: 035 Batch: 00069/00094 | Loss: 228.3180 | CE: 0.1506 | KD: 1060.0919\n",
      "Train Epoch: 035 Batch: 00070/00094 | Loss: 228.2899 | CE: 0.1100 | KD: 1060.1495\n",
      "Train Epoch: 035 Batch: 00071/00094 | Loss: 228.2481 | CE: 0.0763 | KD: 1060.1124\n",
      "Train Epoch: 035 Batch: 00072/00094 | Loss: 228.3304 | CE: 0.1519 | KD: 1060.1436\n",
      "Train Epoch: 035 Batch: 00073/00094 | Loss: 228.2914 | CE: 0.1022 | KD: 1060.1930\n",
      "Train Epoch: 035 Batch: 00074/00094 | Loss: 228.3242 | CE: 0.1516 | KD: 1060.1158\n",
      "Train Epoch: 035 Batch: 00075/00094 | Loss: 228.2832 | CE: 0.1008 | KD: 1060.1616\n",
      "Train Epoch: 035 Batch: 00076/00094 | Loss: 228.2703 | CE: 0.0963 | KD: 1060.1227\n",
      "Train Epoch: 035 Batch: 00077/00094 | Loss: 228.2831 | CE: 0.1134 | KD: 1060.1024\n",
      "Train Epoch: 035 Batch: 00078/00094 | Loss: 228.2904 | CE: 0.1169 | KD: 1060.1202\n",
      "Train Epoch: 035 Batch: 00079/00094 | Loss: 228.2644 | CE: 0.0893 | KD: 1060.1278\n",
      "Train Epoch: 035 Batch: 00080/00094 | Loss: 228.2775 | CE: 0.0984 | KD: 1060.1461\n",
      "Train Epoch: 035 Batch: 00081/00094 | Loss: 228.3411 | CE: 0.1624 | KD: 1060.1445\n",
      "Train Epoch: 035 Batch: 00082/00094 | Loss: 228.2850 | CE: 0.1119 | KD: 1060.1187\n",
      "Train Epoch: 035 Batch: 00083/00094 | Loss: 228.3191 | CE: 0.1524 | KD: 1060.0885\n",
      "Train Epoch: 035 Batch: 00084/00094 | Loss: 228.2695 | CE: 0.0885 | KD: 1060.1552\n",
      "Train Epoch: 035 Batch: 00085/00094 | Loss: 228.3124 | CE: 0.1393 | KD: 1060.1187\n",
      "Train Epoch: 035 Batch: 00086/00094 | Loss: 228.2860 | CE: 0.1147 | KD: 1060.1099\n",
      "Train Epoch: 035 Batch: 00087/00094 | Loss: 228.2647 | CE: 0.1016 | KD: 1060.0720\n",
      "Train Epoch: 035 Batch: 00088/00094 | Loss: 228.2738 | CE: 0.0915 | KD: 1060.1610\n",
      "Train Epoch: 035 Batch: 00089/00094 | Loss: 228.2803 | CE: 0.1139 | KD: 1060.0873\n",
      "Train Epoch: 035 Batch: 00090/00094 | Loss: 228.2685 | CE: 0.0995 | KD: 1060.0992\n",
      "Train Epoch: 035 Batch: 00091/00094 | Loss: 228.2885 | CE: 0.1049 | KD: 1060.1671\n",
      "Train Epoch: 035 Batch: 00092/00094 | Loss: 228.2579 | CE: 0.0910 | KD: 1060.0892\n",
      "Train Epoch: 035 Batch: 00093/00094 | Loss: 228.3366 | CE: 0.1670 | KD: 1060.1022\n",
      "Train Epoch: 035 Batch: 00094/00094 | Loss: 228.2824 | CE: 0.1073 | KD: 1060.1277\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1011 | acc:98.9000\n",
      "[VAL Acc] Target: 98.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2260 | acc:49.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.5488 | acc:72.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 72.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0473 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8250 | acc:60.1489\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5588 | acc:73.1978\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5513 | acc:76.0188\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8235 | acc:58.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.31%\n",
      "[VAL Acc] Avg 66.85%\n",
      "Train Epoch: 036 Batch: 00001/00094 | Loss: 228.2749 | CE: 0.1190 | KD: 1060.0386\n",
      "Train Epoch: 036 Batch: 00002/00094 | Loss: 228.2669 | CE: 0.0884 | KD: 1060.1438\n",
      "Train Epoch: 036 Batch: 00003/00094 | Loss: 228.2981 | CE: 0.1213 | KD: 1060.1356\n",
      "Train Epoch: 036 Batch: 00004/00094 | Loss: 228.3619 | CE: 0.1885 | KD: 1060.1198\n",
      "Train Epoch: 036 Batch: 00005/00094 | Loss: 228.2813 | CE: 0.1145 | KD: 1060.0887\n",
      "Train Epoch: 036 Batch: 00006/00094 | Loss: 228.2484 | CE: 0.0789 | KD: 1060.1019\n",
      "Train Epoch: 036 Batch: 00007/00094 | Loss: 228.3053 | CE: 0.1303 | KD: 1060.1271\n",
      "Train Epoch: 036 Batch: 00008/00094 | Loss: 228.2689 | CE: 0.0975 | KD: 1060.1107\n",
      "Train Epoch: 036 Batch: 00009/00094 | Loss: 228.2753 | CE: 0.1057 | KD: 1060.1021\n",
      "Train Epoch: 036 Batch: 00010/00094 | Loss: 228.3117 | CE: 0.1362 | KD: 1060.1294\n",
      "Train Epoch: 036 Batch: 00011/00094 | Loss: 228.2789 | CE: 0.1036 | KD: 1060.1284\n",
      "Train Epoch: 036 Batch: 00012/00094 | Loss: 228.2510 | CE: 0.0816 | KD: 1060.1011\n",
      "Train Epoch: 036 Batch: 00013/00094 | Loss: 228.3411 | CE: 0.1689 | KD: 1060.1144\n",
      "Train Epoch: 036 Batch: 00014/00094 | Loss: 228.2809 | CE: 0.1005 | KD: 1060.1526\n",
      "Train Epoch: 036 Batch: 00015/00094 | Loss: 228.2909 | CE: 0.1191 | KD: 1060.1122\n",
      "Train Epoch: 036 Batch: 00016/00094 | Loss: 228.2792 | CE: 0.1122 | KD: 1060.0900\n",
      "Train Epoch: 036 Batch: 00017/00094 | Loss: 228.3283 | CE: 0.1292 | KD: 1060.2390\n",
      "Train Epoch: 036 Batch: 00018/00094 | Loss: 228.2543 | CE: 0.0887 | KD: 1060.0833\n",
      "Train Epoch: 036 Batch: 00019/00094 | Loss: 228.3240 | CE: 0.1477 | KD: 1060.1334\n",
      "Train Epoch: 036 Batch: 00020/00094 | Loss: 228.2842 | CE: 0.1091 | KD: 1060.1281\n",
      "Train Epoch: 036 Batch: 00021/00094 | Loss: 228.2789 | CE: 0.0989 | KD: 1060.1504\n",
      "Train Epoch: 036 Batch: 00022/00094 | Loss: 228.2930 | CE: 0.1156 | KD: 1060.1384\n",
      "Train Epoch: 036 Batch: 00023/00094 | Loss: 228.2769 | CE: 0.1092 | KD: 1060.0931\n",
      "Train Epoch: 036 Batch: 00024/00094 | Loss: 228.2996 | CE: 0.1124 | KD: 1060.1838\n",
      "Train Epoch: 036 Batch: 00025/00094 | Loss: 228.3217 | CE: 0.1449 | KD: 1060.1359\n",
      "Train Epoch: 036 Batch: 00026/00094 | Loss: 228.3002 | CE: 0.1187 | KD: 1060.1571\n",
      "Train Epoch: 036 Batch: 00027/00094 | Loss: 228.2983 | CE: 0.1183 | KD: 1060.1505\n",
      "Train Epoch: 036 Batch: 00028/00094 | Loss: 228.2960 | CE: 0.1240 | KD: 1060.1133\n",
      "Train Epoch: 036 Batch: 00029/00094 | Loss: 228.2893 | CE: 0.1187 | KD: 1060.1067\n",
      "Train Epoch: 036 Batch: 00030/00094 | Loss: 228.2675 | CE: 0.1004 | KD: 1060.0906\n",
      "Train Epoch: 036 Batch: 00031/00094 | Loss: 228.3010 | CE: 0.1147 | KD: 1060.1799\n",
      "Train Epoch: 036 Batch: 00032/00094 | Loss: 228.2743 | CE: 0.1005 | KD: 1060.1213\n",
      "Train Epoch: 036 Batch: 00033/00094 | Loss: 228.2804 | CE: 0.1078 | KD: 1060.1160\n",
      "Train Epoch: 036 Batch: 00034/00094 | Loss: 228.3086 | CE: 0.1417 | KD: 1060.0896\n",
      "Train Epoch: 036 Batch: 00035/00094 | Loss: 228.2734 | CE: 0.1050 | KD: 1060.0968\n",
      "Train Epoch: 036 Batch: 00036/00094 | Loss: 228.2872 | CE: 0.1140 | KD: 1060.1185\n",
      "Train Epoch: 036 Batch: 00037/00094 | Loss: 228.3402 | CE: 0.1683 | KD: 1060.1127\n",
      "Train Epoch: 036 Batch: 00038/00094 | Loss: 228.3187 | CE: 0.1320 | KD: 1060.1815\n",
      "Train Epoch: 036 Batch: 00039/00094 | Loss: 228.2596 | CE: 0.0766 | KD: 1060.1643\n",
      "Train Epoch: 036 Batch: 00040/00094 | Loss: 228.2892 | CE: 0.1206 | KD: 1060.0972\n",
      "Train Epoch: 036 Batch: 00041/00094 | Loss: 228.3049 | CE: 0.1311 | KD: 1060.1212\n",
      "Train Epoch: 036 Batch: 00042/00094 | Loss: 228.2843 | CE: 0.1175 | KD: 1060.0891\n",
      "Train Epoch: 036 Batch: 00043/00094 | Loss: 228.2944 | CE: 0.1129 | KD: 1060.1577\n",
      "Train Epoch: 036 Batch: 00044/00094 | Loss: 228.2878 | CE: 0.0992 | KD: 1060.1902\n",
      "Train Epoch: 036 Batch: 00045/00094 | Loss: 228.3548 | CE: 0.1751 | KD: 1060.1490\n",
      "Train Epoch: 036 Batch: 00046/00094 | Loss: 228.3150 | CE: 0.1383 | KD: 1060.1351\n",
      "Train Epoch: 036 Batch: 00047/00094 | Loss: 228.2555 | CE: 0.0803 | KD: 1060.1283\n",
      "Train Epoch: 036 Batch: 00048/00094 | Loss: 228.2902 | CE: 0.1029 | KD: 1060.1841\n",
      "Train Epoch: 036 Batch: 00049/00094 | Loss: 228.2838 | CE: 0.1021 | KD: 1060.1584\n",
      "Train Epoch: 036 Batch: 00050/00094 | Loss: 228.3111 | CE: 0.1356 | KD: 1060.1295\n",
      "Train Epoch: 036 Batch: 00051/00094 | Loss: 228.2678 | CE: 0.0936 | KD: 1060.1232\n",
      "Train Epoch: 036 Batch: 00052/00094 | Loss: 228.2619 | CE: 0.0891 | KD: 1060.1172\n",
      "Train Epoch: 036 Batch: 00053/00094 | Loss: 228.2841 | CE: 0.0998 | KD: 1060.1708\n",
      "Train Epoch: 036 Batch: 00054/00094 | Loss: 228.2720 | CE: 0.0935 | KD: 1060.1432\n",
      "Train Epoch: 036 Batch: 00055/00094 | Loss: 228.2670 | CE: 0.0969 | KD: 1060.1041\n",
      "Train Epoch: 036 Batch: 00056/00094 | Loss: 228.2540 | CE: 0.0826 | KD: 1060.1108\n",
      "Train Epoch: 036 Batch: 00057/00094 | Loss: 228.3313 | CE: 0.1479 | KD: 1060.1663\n",
      "Train Epoch: 036 Batch: 00058/00094 | Loss: 228.3069 | CE: 0.1257 | KD: 1060.1558\n",
      "Train Epoch: 036 Batch: 00059/00094 | Loss: 228.2763 | CE: 0.1102 | KD: 1060.0858\n",
      "Train Epoch: 036 Batch: 00060/00094 | Loss: 228.3895 | CE: 0.2106 | KD: 1060.1453\n",
      "Train Epoch: 036 Batch: 00061/00094 | Loss: 228.2860 | CE: 0.1142 | KD: 1060.1124\n",
      "Train Epoch: 036 Batch: 00062/00094 | Loss: 228.2729 | CE: 0.0923 | KD: 1060.1531\n",
      "Train Epoch: 036 Batch: 00063/00094 | Loss: 228.2987 | CE: 0.1273 | KD: 1060.1104\n",
      "Train Epoch: 036 Batch: 00064/00094 | Loss: 228.2517 | CE: 0.0791 | KD: 1060.1165\n",
      "Train Epoch: 036 Batch: 00065/00094 | Loss: 228.3365 | CE: 0.1487 | KD: 1060.1864\n",
      "Train Epoch: 036 Batch: 00066/00094 | Loss: 228.2554 | CE: 0.0884 | KD: 1060.0901\n",
      "Train Epoch: 036 Batch: 00067/00094 | Loss: 228.2661 | CE: 0.0857 | KD: 1060.1522\n",
      "Train Epoch: 036 Batch: 00068/00094 | Loss: 228.2717 | CE: 0.1036 | KD: 1060.0955\n",
      "Train Epoch: 036 Batch: 00069/00094 | Loss: 228.3017 | CE: 0.1340 | KD: 1060.0930\n",
      "Train Epoch: 036 Batch: 00070/00094 | Loss: 228.2767 | CE: 0.1056 | KD: 1060.1088\n",
      "Train Epoch: 036 Batch: 00071/00094 | Loss: 228.2486 | CE: 0.0755 | KD: 1060.1188\n",
      "Train Epoch: 036 Batch: 00072/00094 | Loss: 228.2580 | CE: 0.0775 | KD: 1060.1527\n",
      "Train Epoch: 036 Batch: 00073/00094 | Loss: 228.2770 | CE: 0.0957 | KD: 1060.1565\n",
      "Train Epoch: 036 Batch: 00074/00094 | Loss: 228.2621 | CE: 0.0866 | KD: 1060.1294\n",
      "Train Epoch: 036 Batch: 00075/00094 | Loss: 228.2749 | CE: 0.1020 | KD: 1060.1176\n",
      "Train Epoch: 036 Batch: 00076/00094 | Loss: 228.2595 | CE: 0.0866 | KD: 1060.1174\n",
      "Train Epoch: 036 Batch: 00077/00094 | Loss: 228.2752 | CE: 0.0916 | KD: 1060.1671\n",
      "Train Epoch: 036 Batch: 00078/00094 | Loss: 228.3762 | CE: 0.1923 | KD: 1060.1688\n",
      "Train Epoch: 036 Batch: 00079/00094 | Loss: 228.3077 | CE: 0.1298 | KD: 1060.1404\n",
      "Train Epoch: 036 Batch: 00080/00094 | Loss: 228.2530 | CE: 0.0832 | KD: 1060.1031\n",
      "Train Epoch: 036 Batch: 00081/00094 | Loss: 228.3469 | CE: 0.1714 | KD: 1060.1296\n",
      "Train Epoch: 036 Batch: 00082/00094 | Loss: 228.2990 | CE: 0.1098 | KD: 1060.1932\n",
      "Train Epoch: 036 Batch: 00083/00094 | Loss: 228.2741 | CE: 0.0903 | KD: 1060.1680\n",
      "Train Epoch: 036 Batch: 00084/00094 | Loss: 228.2491 | CE: 0.0874 | KD: 1060.0652\n",
      "Train Epoch: 036 Batch: 00085/00094 | Loss: 228.2735 | CE: 0.1087 | KD: 1060.0801\n",
      "Train Epoch: 036 Batch: 00086/00094 | Loss: 228.3388 | CE: 0.1696 | KD: 1060.1002\n",
      "Train Epoch: 036 Batch: 00087/00094 | Loss: 228.2731 | CE: 0.1027 | KD: 1060.1061\n",
      "Train Epoch: 036 Batch: 00088/00094 | Loss: 228.3316 | CE: 0.1539 | KD: 1060.1395\n",
      "Train Epoch: 036 Batch: 00089/00094 | Loss: 228.3499 | CE: 0.1746 | KD: 1060.1285\n",
      "Train Epoch: 036 Batch: 00090/00094 | Loss: 228.3354 | CE: 0.1639 | KD: 1060.1110\n",
      "Train Epoch: 036 Batch: 00091/00094 | Loss: 228.2837 | CE: 0.1198 | KD: 1060.0753\n",
      "Train Epoch: 036 Batch: 00092/00094 | Loss: 228.2876 | CE: 0.1198 | KD: 1060.0935\n",
      "Train Epoch: 036 Batch: 00093/00094 | Loss: 228.2879 | CE: 0.0912 | KD: 1060.2277\n",
      "Train Epoch: 036 Batch: 00094/00094 | Loss: 228.3319 | CE: 0.1583 | KD: 1060.1208\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1006 | acc:99.0000\n",
      "[VAL Acc] Target: 99.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3016 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.6199 | acc:68.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0522 | acc:47.1374\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8678 | acc:58.5423\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5682 | acc:74.3993\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5864 | acc:73.3542\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8568 | acc:56.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.56%\n",
      "[VAL Acc] Avg 65.93%\n",
      "Early stopping ...\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : tmn_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/c667a4373aa74fbab3625be034854214\u001b[0m\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/biggan_val_acc [36]        : (88.0, 99.2)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/crn_val_acc [36]           : (88.0, 99.2)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/cyclegan_val_acc [36]      : (88.0, 99.2)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/faceforensics_val_acc [36] : (88.0, 99.2)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/gaugan_val_acc [36]        : (88.0, 99.2)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/imle_val_acc [36]          : (88.0, 99.2)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/wild_val_acc [36]          : (88.0, 99.2)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/target_val_acc [36]                                             : (88.0, 99.2)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/val_acc [36]                                                    : (65.49719579670372, 70.00741407103513)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [866]                                                          : (228.2493896484375, 530.68896484375)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss [3384]                                                  : (228.23873901367188, 531.0128173828125)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_kd [3384]                                               : (1059.931884765625, 1060.2506103515625)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_main [3384]                                             : (0.06675731390714645, 0.887511670589447)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start_acc                                                           : 59.95\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : tmn_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size           : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config_file          : model_config.conf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decay_factor         : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stop           : 35\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs               : 250\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flip                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kd_alpha             : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr                   : 0.005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_schedule          : cosine\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_gpu              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     output_model_version : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resolution           : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (8.36 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1 (7.34 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\", \"biggan\", \"cyclegan\", \"imle\", \"faceforensics\", \"crn\", \"wild\"] \n",
    "TARGET_DATASET = \"diffusionshort\"\n",
    "NETWORK = \"MobileNet2\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/KD_mn/cddb_easy\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/KD_mn/diff\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "COMET_NAME = \"tmn_diff\"\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "if NETWORK == \"ViT\": input_model = f\"{CHECKPOINT_DIR_TARGET}/{source_string}/model_best_accuracy.pth\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='model_config.conf', network='Xception', input_model='../KD-AIGC-Detection/checkpoints/KD_Xception/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild', output_dir='../KD-AIGC-Detection/checkpoints/KD_Xception/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild', target_dataset='../KD-AIGC-Detection/datasets/custom/diffusionshort', use_comet=True, comet_name='txc_diff', num_gpu='0', output_model_version='3', epochs='250', early_stop='35', batch_size='64', resolution='128', lr_schedule='cosine', lr='0.005', kd_alpha='0.5', decay_factor='0.9', flip='False')\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/84111ee143e9439d9c715571fa1a4c72\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/imle\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/crn\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/wild\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/train\n",
      "Dataset available in train_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan / ../KD-AIGC-Detection/datasets/custom/biggan / ../KD-AIGC-Detection/datasets/custom/cyclegan / ../KD-AIGC-Detection/datasets/custom/imle / ../KD-AIGC-Detection/datasets/custom/faceforensics / ../KD-AIGC-Detection/datasets/custom/crn / ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fra/MLOpsDeepfakeDetection/./src/train.py\", line 206, in <module>\n",
      "    train(args)\n",
      "  File \"/home/fra/MLOpsDeepfakeDetection/./src/train.py\", line 67, in train\n",
      "    teacher_model, student_model = load_models(args.input_model, args.network, num_gpu = args.num_gpu)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fra/MLOpsDeepfakeDetection/src/utils/model_loader.py\", line 20, in load_models\n",
      "    assert weight == \"\" or os.path.isfile(weight), f\"Pretrained weights {weight} not found\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: Pretrained weights ../KD-AIGC-Detection/checkpoints/KD_Xception/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild not found\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : txc_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/84111ee143e9439d9c715571fa1a4c72\u001b[0m\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : txc_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size           : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config_file          : model_config.conf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decay_factor         : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stop           : 35\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs               : 250\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flip                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kd_alpha             : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr                   : 0.005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_schedule          : cosine\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_gpu              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     output_model_version : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resolution           : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (8.36 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1 (7.34 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\", \"biggan\", \"cyclegan\", \"imle\", \"faceforensics\", \"crn\", \"wild\"] \n",
    "TARGET_DATASET = \"diffusionshort\"\n",
    "NETWORK = \"Xception\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/KD_Xception/cddb_easy\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/KD_Xception/diff\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "COMET_NAME = \"txc_diff\"\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "if NETWORK == \"ViT\": input_model = f\"{CHECKPOINT_DIR_TARGET}/{source_string}/model_best_accuracy.pth\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
