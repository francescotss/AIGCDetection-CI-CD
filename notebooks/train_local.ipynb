{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='model_config.conf', network='ViT', input_model='../KD-AIGC-Detection/checkpoints/KD_ViT/cddb_easy/gaugan', output_dir='../KD-AIGC-Detection/checkpoints/KD_ViT/cddb_easy/gaugan_biggan', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan', target_dataset='../KD-AIGC-Detection/datasets/custom/biggan', use_comet=False, comet_name='test', num_gpu='0', output_model_version='3', epochs='1', early_stop='35', batch_size='64', resolution='224', lr_schedule='cosine', lr='0.005', kd_alpha='1', decay_factor='0.9', flip='False')\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/biggan/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/biggan/train\n",
      "Dataset available in train_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Apply Cosine learning rate schedule\n",
      "/home/fra/miniconda3/envs/paper/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:1.4106 | acc:68.7500\n",
      "Start Target Validation ACC: 68.75%\n",
      "Start training in 1 epochs\n",
      "Train Epoch: 001 Batch: 00001/00038 | Loss: 1062.1940 | CE: 1.6869 | KD: 1060.5071\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\"] \n",
    "TARGET_DATASET = \"biggan\"\n",
    "NETWORK = \"ViT\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/KD_ViT/cddb_easy\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/KD_ViT/cddb_easy\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "#COMET_NAME = \"tvit_gau_big\"\n",
    "COMET_NAME=\"test\"\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='model_config.conf', network='ResNet18', input_model='../KD-AIGC-Detection/checkpoints/KD_r18/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort', output_dir='../KD-AIGC-Detection/checkpoints/KD_r18/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort_elsa', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild,../KD-AIGC-Detection/datasets/custom/diffusionshort', target_dataset='../KD-AIGC-Detection/datasets/custom/elsa', use_comet=True, comet_name='t18_diff_elsa', num_gpu='0', output_model_version='3', epochs='250', early_stop='35', batch_size='64', resolution='128', lr_schedule='cosine', lr='0.005', kd_alpha='1', decay_factor='0.9', flip='False')\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/6942265c854546f181de957f3116e751\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/imle\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/crn\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/wild\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild,../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/elsa/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/elsa/train\n",
      "Dataset available in train_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan / ../KD-AIGC-Detection/datasets/custom/biggan / ../KD-AIGC-Detection/datasets/custom/cyclegan / ../KD-AIGC-Detection/datasets/custom/imle / ../KD-AIGC-Detection/datasets/custom/faceforensics / ../KD-AIGC-Detection/datasets/custom/crn / ../KD-AIGC-Detection/datasets/custom/wild / ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Loading ResNet18 from ../KD-AIGC-Detection/checkpoints/KD_r18/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort/model_best_accuracy.pth\n",
      "Loaded\n",
      "Apply Cosine learning rate schedule\n",
      "/home/fra/miniconda3/envs/paper/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.8145 | acc:61.0000\n",
      "Start training in 250 epochs\n",
      "Train Epoch: 001 Batch: 00001/00094 | Loss: 1062.0045 | CE: 1.2185 | KD: 1060.7860\n",
      "Train Epoch: 001 Batch: 00002/00094 | Loss: 1061.1517 | CE: 0.9001 | KD: 1060.2517\n",
      "Train Epoch: 001 Batch: 00003/00094 | Loss: 1061.0170 | CE: 0.8168 | KD: 1060.2002\n",
      "Train Epoch: 001 Batch: 00004/00094 | Loss: 1060.7849 | CE: 0.6612 | KD: 1060.1238\n",
      "Train Epoch: 001 Batch: 00005/00094 | Loss: 1060.5835 | CE: 0.4852 | KD: 1060.0983\n",
      "Train Epoch: 001 Batch: 00006/00094 | Loss: 1060.7715 | CE: 0.6523 | KD: 1060.1191\n",
      "Train Epoch: 001 Batch: 00007/00094 | Loss: 1060.6128 | CE: 0.5135 | KD: 1060.0992\n",
      "Train Epoch: 001 Batch: 00008/00094 | Loss: 1060.5848 | CE: 0.5091 | KD: 1060.0758\n",
      "Train Epoch: 001 Batch: 00009/00094 | Loss: 1060.9076 | CE: 0.7127 | KD: 1060.1949\n",
      "Train Epoch: 001 Batch: 00010/00094 | Loss: 1060.7590 | CE: 0.6145 | KD: 1060.1445\n",
      "Train Epoch: 001 Batch: 00011/00094 | Loss: 1060.6370 | CE: 0.5042 | KD: 1060.1327\n",
      "Train Epoch: 001 Batch: 00012/00094 | Loss: 1060.5325 | CE: 0.4487 | KD: 1060.0837\n",
      "Train Epoch: 001 Batch: 00013/00094 | Loss: 1060.7997 | CE: 0.5834 | KD: 1060.2163\n",
      "Train Epoch: 001 Batch: 00014/00094 | Loss: 1060.9960 | CE: 0.7468 | KD: 1060.2491\n",
      "Train Epoch: 001 Batch: 00015/00094 | Loss: 1060.6910 | CE: 0.5524 | KD: 1060.1387\n",
      "Train Epoch: 001 Batch: 00016/00094 | Loss: 1060.6256 | CE: 0.5435 | KD: 1060.0820\n",
      "Train Epoch: 001 Batch: 00017/00094 | Loss: 1060.4098 | CE: 0.4045 | KD: 1060.0052\n",
      "Train Epoch: 001 Batch: 00018/00094 | Loss: 1060.6732 | CE: 0.5736 | KD: 1060.0996\n",
      "Train Epoch: 001 Batch: 00019/00094 | Loss: 1060.4298 | CE: 0.4268 | KD: 1060.0029\n",
      "Train Epoch: 001 Batch: 00020/00094 | Loss: 1060.4684 | CE: 0.4340 | KD: 1060.0344\n",
      "Train Epoch: 001 Batch: 00021/00094 | Loss: 1060.5718 | CE: 0.4718 | KD: 1060.1000\n",
      "Train Epoch: 001 Batch: 00022/00094 | Loss: 1060.5566 | CE: 0.5070 | KD: 1060.0496\n",
      "Train Epoch: 001 Batch: 00023/00094 | Loss: 1060.3364 | CE: 0.3742 | KD: 1059.9623\n",
      "Train Epoch: 001 Batch: 00024/00094 | Loss: 1060.5491 | CE: 0.4835 | KD: 1060.0656\n",
      "Train Epoch: 001 Batch: 00025/00094 | Loss: 1060.5194 | CE: 0.4210 | KD: 1060.0984\n",
      "Train Epoch: 001 Batch: 00026/00094 | Loss: 1060.6257 | CE: 0.5146 | KD: 1060.1111\n",
      "Train Epoch: 001 Batch: 00027/00094 | Loss: 1060.6562 | CE: 0.5275 | KD: 1060.1288\n",
      "Train Epoch: 001 Batch: 00028/00094 | Loss: 1060.5176 | CE: 0.4563 | KD: 1060.0613\n",
      "Train Epoch: 001 Batch: 00029/00094 | Loss: 1060.5227 | CE: 0.4764 | KD: 1060.0464\n",
      "Train Epoch: 001 Batch: 00030/00094 | Loss: 1060.6658 | CE: 0.5591 | KD: 1060.1067\n",
      "Train Epoch: 001 Batch: 00031/00094 | Loss: 1060.5092 | CE: 0.4858 | KD: 1060.0234\n",
      "Train Epoch: 001 Batch: 00032/00094 | Loss: 1060.5144 | CE: 0.4373 | KD: 1060.0770\n",
      "Train Epoch: 001 Batch: 00033/00094 | Loss: 1060.4968 | CE: 0.4207 | KD: 1060.0760\n",
      "Train Epoch: 001 Batch: 00034/00094 | Loss: 1060.6140 | CE: 0.5308 | KD: 1060.0833\n",
      "Train Epoch: 001 Batch: 00035/00094 | Loss: 1060.3110 | CE: 0.3177 | KD: 1059.9933\n",
      "Train Epoch: 001 Batch: 00036/00094 | Loss: 1060.6715 | CE: 0.5092 | KD: 1060.1624\n",
      "Train Epoch: 001 Batch: 00037/00094 | Loss: 1060.4667 | CE: 0.4285 | KD: 1060.0382\n",
      "Train Epoch: 001 Batch: 00038/00094 | Loss: 1060.4181 | CE: 0.4193 | KD: 1059.9988\n",
      "Train Epoch: 001 Batch: 00039/00094 | Loss: 1060.3983 | CE: 0.3902 | KD: 1060.0082\n",
      "Train Epoch: 001 Batch: 00040/00094 | Loss: 1060.7395 | CE: 0.5750 | KD: 1060.1644\n",
      "Train Epoch: 001 Batch: 00041/00094 | Loss: 1060.5778 | CE: 0.4879 | KD: 1060.0898\n",
      "Train Epoch: 001 Batch: 00042/00094 | Loss: 1060.4171 | CE: 0.3967 | KD: 1060.0204\n",
      "Train Epoch: 001 Batch: 00043/00094 | Loss: 1060.3889 | CE: 0.3459 | KD: 1060.0430\n",
      "Train Epoch: 001 Batch: 00044/00094 | Loss: 1060.5266 | CE: 0.4332 | KD: 1060.0934\n",
      "Train Epoch: 001 Batch: 00045/00094 | Loss: 1060.6294 | CE: 0.5306 | KD: 1060.0988\n",
      "Train Epoch: 001 Batch: 00046/00094 | Loss: 1060.7197 | CE: 0.5460 | KD: 1060.1737\n",
      "Train Epoch: 001 Batch: 00047/00094 | Loss: 1060.3954 | CE: 0.3881 | KD: 1060.0073\n",
      "Train Epoch: 001 Batch: 00048/00094 | Loss: 1060.5616 | CE: 0.4692 | KD: 1060.0924\n",
      "Train Epoch: 001 Batch: 00049/00094 | Loss: 1060.5165 | CE: 0.4683 | KD: 1060.0481\n",
      "Train Epoch: 001 Batch: 00050/00094 | Loss: 1060.4832 | CE: 0.4444 | KD: 1060.0388\n",
      "Train Epoch: 001 Batch: 00051/00094 | Loss: 1060.5060 | CE: 0.4856 | KD: 1060.0204\n",
      "Train Epoch: 001 Batch: 00052/00094 | Loss: 1060.5382 | CE: 0.4348 | KD: 1060.1034\n",
      "Train Epoch: 001 Batch: 00053/00094 | Loss: 1060.4282 | CE: 0.4018 | KD: 1060.0264\n",
      "Train Epoch: 001 Batch: 00054/00094 | Loss: 1060.3677 | CE: 0.3609 | KD: 1060.0068\n",
      "Train Epoch: 001 Batch: 00055/00094 | Loss: 1060.3636 | CE: 0.3631 | KD: 1060.0005\n",
      "Train Epoch: 001 Batch: 00056/00094 | Loss: 1060.5314 | CE: 0.4376 | KD: 1060.0938\n",
      "Train Epoch: 001 Batch: 00057/00094 | Loss: 1060.4875 | CE: 0.3818 | KD: 1060.1058\n",
      "Train Epoch: 001 Batch: 00058/00094 | Loss: 1060.4604 | CE: 0.4179 | KD: 1060.0426\n",
      "Train Epoch: 001 Batch: 00059/00094 | Loss: 1060.7208 | CE: 0.5548 | KD: 1060.1660\n",
      "Train Epoch: 001 Batch: 00060/00094 | Loss: 1060.4283 | CE: 0.3725 | KD: 1060.0558\n",
      "Train Epoch: 001 Batch: 00061/00094 | Loss: 1060.5186 | CE: 0.4694 | KD: 1060.0492\n",
      "Train Epoch: 001 Batch: 00062/00094 | Loss: 1060.6362 | CE: 0.5555 | KD: 1060.0808\n",
      "Train Epoch: 001 Batch: 00063/00094 | Loss: 1060.3553 | CE: 0.3616 | KD: 1059.9938\n",
      "Train Epoch: 001 Batch: 00064/00094 | Loss: 1060.4513 | CE: 0.4041 | KD: 1060.0472\n",
      "Train Epoch: 001 Batch: 00065/00094 | Loss: 1060.4548 | CE: 0.4208 | KD: 1060.0341\n",
      "Train Epoch: 001 Batch: 00066/00094 | Loss: 1060.6401 | CE: 0.5345 | KD: 1060.1057\n",
      "Train Epoch: 001 Batch: 00067/00094 | Loss: 1060.3190 | CE: 0.3528 | KD: 1059.9662\n",
      "Train Epoch: 001 Batch: 00068/00094 | Loss: 1060.6932 | CE: 0.5392 | KD: 1060.1541\n",
      "Train Epoch: 001 Batch: 00069/00094 | Loss: 1060.4558 | CE: 0.4104 | KD: 1060.0454\n",
      "Train Epoch: 001 Batch: 00070/00094 | Loss: 1060.3654 | CE: 0.3816 | KD: 1059.9838\n",
      "Train Epoch: 001 Batch: 00071/00094 | Loss: 1060.6218 | CE: 0.5046 | KD: 1060.1172\n",
      "Train Epoch: 001 Batch: 00072/00094 | Loss: 1060.3722 | CE: 0.3915 | KD: 1059.9807\n",
      "Train Epoch: 001 Batch: 00073/00094 | Loss: 1060.4438 | CE: 0.3629 | KD: 1060.0809\n",
      "Train Epoch: 001 Batch: 00074/00094 | Loss: 1060.3486 | CE: 0.3706 | KD: 1059.9780\n",
      "Train Epoch: 001 Batch: 00075/00094 | Loss: 1060.5621 | CE: 0.4706 | KD: 1060.0916\n",
      "Train Epoch: 001 Batch: 00076/00094 | Loss: 1060.6145 | CE: 0.5206 | KD: 1060.0939\n",
      "Train Epoch: 001 Batch: 00077/00094 | Loss: 1060.2972 | CE: 0.3264 | KD: 1059.9708\n",
      "Train Epoch: 001 Batch: 00078/00094 | Loss: 1060.4342 | CE: 0.3600 | KD: 1060.0742\n",
      "Train Epoch: 001 Batch: 00079/00094 | Loss: 1060.4431 | CE: 0.3871 | KD: 1060.0560\n",
      "Train Epoch: 001 Batch: 00080/00094 | Loss: 1060.5002 | CE: 0.4516 | KD: 1060.0486\n",
      "Train Epoch: 001 Batch: 00081/00094 | Loss: 1060.5411 | CE: 0.4984 | KD: 1060.0427\n",
      "Train Epoch: 001 Batch: 00082/00094 | Loss: 1060.6804 | CE: 0.5868 | KD: 1060.0936\n",
      "Train Epoch: 001 Batch: 00083/00094 | Loss: 1060.4324 | CE: 0.4150 | KD: 1060.0173\n",
      "Train Epoch: 001 Batch: 00084/00094 | Loss: 1060.5194 | CE: 0.4864 | KD: 1060.0330\n",
      "Train Epoch: 001 Batch: 00085/00094 | Loss: 1060.6366 | CE: 0.5236 | KD: 1060.1130\n",
      "Train Epoch: 001 Batch: 00086/00094 | Loss: 1060.6189 | CE: 0.4375 | KD: 1060.1814\n",
      "Train Epoch: 001 Batch: 00087/00094 | Loss: 1060.3517 | CE: 0.3484 | KD: 1060.0033\n",
      "Train Epoch: 001 Batch: 00088/00094 | Loss: 1060.3705 | CE: 0.3698 | KD: 1060.0007\n",
      "Train Epoch: 001 Batch: 00089/00094 | Loss: 1060.4604 | CE: 0.4237 | KD: 1060.0367\n",
      "Train Epoch: 001 Batch: 00090/00094 | Loss: 1060.4822 | CE: 0.4351 | KD: 1060.0471\n",
      "Train Epoch: 001 Batch: 00091/00094 | Loss: 1060.3650 | CE: 0.3912 | KD: 1059.9738\n",
      "Train Epoch: 001 Batch: 00092/00094 | Loss: 1060.5706 | CE: 0.4466 | KD: 1060.1240\n",
      "Train Epoch: 001 Batch: 00093/00094 | Loss: 1060.3635 | CE: 0.3732 | KD: 1059.9904\n",
      "Train Epoch: 001 Batch: 00094/00094 | Loss: 1060.4125 | CE: 0.3217 | KD: 1060.0908\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3896 | acc:82.8500\n",
      "[VAL Acc] Target: 82.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2180 | acc:50.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9691 | acc:53.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0569 | acc:47.1374\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2013 | acc:50.4702\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8558 | acc:53.9741\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.97%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8592 | acc:56.1129\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 56.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8967 | acc:55.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4644 | acc:78.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 78.40%\n",
      "[VAL Acc] Avg 58.60%\n",
      "VAL Acc improve from 0.00% to 58.60%\n",
      "Save best model\n",
      "Train Epoch: 002 Batch: 00001/00094 | Loss: 1060.7603 | CE: 0.5941 | KD: 1060.1661\n",
      "Train Epoch: 002 Batch: 00002/00094 | Loss: 1060.4451 | CE: 0.3768 | KD: 1060.0682\n",
      "Train Epoch: 002 Batch: 00003/00094 | Loss: 1060.4081 | CE: 0.3714 | KD: 1060.0367\n",
      "Train Epoch: 002 Batch: 00004/00094 | Loss: 1060.3917 | CE: 0.3974 | KD: 1059.9944\n",
      "Train Epoch: 002 Batch: 00005/00094 | Loss: 1060.3666 | CE: 0.3299 | KD: 1060.0366\n",
      "Train Epoch: 002 Batch: 00006/00094 | Loss: 1060.3761 | CE: 0.3453 | KD: 1060.0308\n",
      "Train Epoch: 002 Batch: 00007/00094 | Loss: 1060.4656 | CE: 0.4259 | KD: 1060.0397\n",
      "Train Epoch: 002 Batch: 00008/00094 | Loss: 1060.5886 | CE: 0.4518 | KD: 1060.1368\n",
      "Train Epoch: 002 Batch: 00009/00094 | Loss: 1060.4812 | CE: 0.4286 | KD: 1060.0526\n",
      "Train Epoch: 002 Batch: 00010/00094 | Loss: 1060.6311 | CE: 0.5029 | KD: 1060.1282\n",
      "Train Epoch: 002 Batch: 00011/00094 | Loss: 1060.6166 | CE: 0.5307 | KD: 1060.0858\n",
      "Train Epoch: 002 Batch: 00012/00094 | Loss: 1060.4260 | CE: 0.3952 | KD: 1060.0308\n",
      "Train Epoch: 002 Batch: 00013/00094 | Loss: 1060.5879 | CE: 0.4204 | KD: 1060.1675\n",
      "Train Epoch: 002 Batch: 00014/00094 | Loss: 1060.4583 | CE: 0.4089 | KD: 1060.0493\n",
      "Train Epoch: 002 Batch: 00015/00094 | Loss: 1060.4612 | CE: 0.4272 | KD: 1060.0341\n",
      "Train Epoch: 002 Batch: 00016/00094 | Loss: 1060.3529 | CE: 0.3583 | KD: 1059.9946\n",
      "Train Epoch: 002 Batch: 00017/00094 | Loss: 1060.4277 | CE: 0.3967 | KD: 1060.0311\n",
      "Train Epoch: 002 Batch: 00018/00094 | Loss: 1060.4225 | CE: 0.3867 | KD: 1060.0358\n",
      "Train Epoch: 002 Batch: 00019/00094 | Loss: 1060.4703 | CE: 0.4313 | KD: 1060.0391\n",
      "Train Epoch: 002 Batch: 00020/00094 | Loss: 1060.2886 | CE: 0.2874 | KD: 1060.0011\n",
      "Train Epoch: 002 Batch: 00021/00094 | Loss: 1060.4525 | CE: 0.4280 | KD: 1060.0245\n",
      "Train Epoch: 002 Batch: 00022/00094 | Loss: 1060.6033 | CE: 0.4610 | KD: 1060.1423\n",
      "Train Epoch: 002 Batch: 00023/00094 | Loss: 1060.3148 | CE: 0.3166 | KD: 1059.9983\n",
      "Train Epoch: 002 Batch: 00024/00094 | Loss: 1060.3037 | CE: 0.3169 | KD: 1059.9868\n",
      "Train Epoch: 002 Batch: 00025/00094 | Loss: 1060.3635 | CE: 0.3180 | KD: 1060.0455\n",
      "Train Epoch: 002 Batch: 00026/00094 | Loss: 1060.4984 | CE: 0.3933 | KD: 1060.1051\n",
      "Train Epoch: 002 Batch: 00027/00094 | Loss: 1060.4230 | CE: 0.4116 | KD: 1060.0114\n",
      "Train Epoch: 002 Batch: 00028/00094 | Loss: 1060.4344 | CE: 0.4274 | KD: 1060.0071\n",
      "Train Epoch: 002 Batch: 00029/00094 | Loss: 1060.2667 | CE: 0.2849 | KD: 1059.9818\n",
      "Train Epoch: 002 Batch: 00030/00094 | Loss: 1060.3534 | CE: 0.3439 | KD: 1060.0094\n",
      "Train Epoch: 002 Batch: 00031/00094 | Loss: 1060.3853 | CE: 0.3758 | KD: 1060.0094\n",
      "Train Epoch: 002 Batch: 00032/00094 | Loss: 1060.4659 | CE: 0.4215 | KD: 1060.0444\n",
      "Train Epoch: 002 Batch: 00033/00094 | Loss: 1060.4388 | CE: 0.3486 | KD: 1060.0902\n",
      "Train Epoch: 002 Batch: 00034/00094 | Loss: 1060.4723 | CE: 0.4152 | KD: 1060.0571\n",
      "Train Epoch: 002 Batch: 00035/00094 | Loss: 1060.2866 | CE: 0.3330 | KD: 1059.9536\n",
      "Train Epoch: 002 Batch: 00036/00094 | Loss: 1060.3977 | CE: 0.3991 | KD: 1059.9987\n",
      "Train Epoch: 002 Batch: 00037/00094 | Loss: 1060.6381 | CE: 0.4846 | KD: 1060.1534\n",
      "Train Epoch: 002 Batch: 00038/00094 | Loss: 1060.4330 | CE: 0.3875 | KD: 1060.0455\n",
      "Train Epoch: 002 Batch: 00039/00094 | Loss: 1060.4033 | CE: 0.3696 | KD: 1060.0337\n",
      "Train Epoch: 002 Batch: 00040/00094 | Loss: 1060.4146 | CE: 0.3926 | KD: 1060.0220\n",
      "Train Epoch: 002 Batch: 00041/00094 | Loss: 1060.2716 | CE: 0.3011 | KD: 1059.9706\n",
      "Train Epoch: 002 Batch: 00042/00094 | Loss: 1060.3955 | CE: 0.3838 | KD: 1060.0117\n",
      "Train Epoch: 002 Batch: 00043/00094 | Loss: 1060.4551 | CE: 0.3789 | KD: 1060.0762\n",
      "Train Epoch: 002 Batch: 00044/00094 | Loss: 1060.4642 | CE: 0.3754 | KD: 1060.0889\n",
      "Train Epoch: 002 Batch: 00045/00094 | Loss: 1060.4976 | CE: 0.4502 | KD: 1060.0474\n",
      "Train Epoch: 002 Batch: 00046/00094 | Loss: 1060.3862 | CE: 0.3268 | KD: 1060.0594\n",
      "Train Epoch: 002 Batch: 00047/00094 | Loss: 1060.4868 | CE: 0.4178 | KD: 1060.0691\n",
      "Train Epoch: 002 Batch: 00048/00094 | Loss: 1060.5640 | CE: 0.4569 | KD: 1060.1071\n",
      "Train Epoch: 002 Batch: 00049/00094 | Loss: 1060.4890 | CE: 0.4330 | KD: 1060.0560\n",
      "Train Epoch: 002 Batch: 00050/00094 | Loss: 1060.4762 | CE: 0.4363 | KD: 1060.0399\n",
      "Train Epoch: 002 Batch: 00051/00094 | Loss: 1060.2819 | CE: 0.2932 | KD: 1059.9886\n",
      "Train Epoch: 002 Batch: 00052/00094 | Loss: 1060.4491 | CE: 0.4046 | KD: 1060.0446\n",
      "Train Epoch: 002 Batch: 00053/00094 | Loss: 1060.2522 | CE: 0.2689 | KD: 1059.9833\n",
      "Train Epoch: 002 Batch: 00054/00094 | Loss: 1060.6481 | CE: 0.4714 | KD: 1060.1766\n",
      "Train Epoch: 002 Batch: 00055/00094 | Loss: 1060.5956 | CE: 0.4744 | KD: 1060.1212\n",
      "Train Epoch: 002 Batch: 00056/00094 | Loss: 1060.3540 | CE: 0.3102 | KD: 1060.0438\n",
      "Train Epoch: 002 Batch: 00057/00094 | Loss: 1060.3735 | CE: 0.3418 | KD: 1060.0317\n",
      "Train Epoch: 002 Batch: 00058/00094 | Loss: 1060.4659 | CE: 0.3572 | KD: 1060.1088\n",
      "Train Epoch: 002 Batch: 00059/00094 | Loss: 1060.4318 | CE: 0.3816 | KD: 1060.0502\n",
      "Train Epoch: 002 Batch: 00060/00094 | Loss: 1060.4889 | CE: 0.4153 | KD: 1060.0736\n",
      "Train Epoch: 002 Batch: 00061/00094 | Loss: 1060.4667 | CE: 0.4187 | KD: 1060.0480\n",
      "Train Epoch: 002 Batch: 00062/00094 | Loss: 1060.4622 | CE: 0.3973 | KD: 1060.0648\n",
      "Train Epoch: 002 Batch: 00063/00094 | Loss: 1060.3409 | CE: 0.3311 | KD: 1060.0098\n",
      "Train Epoch: 002 Batch: 00064/00094 | Loss: 1060.3751 | CE: 0.3414 | KD: 1060.0337\n",
      "Train Epoch: 002 Batch: 00065/00094 | Loss: 1060.6263 | CE: 0.5038 | KD: 1060.1226\n",
      "Train Epoch: 002 Batch: 00066/00094 | Loss: 1060.3904 | CE: 0.3830 | KD: 1060.0074\n",
      "Train Epoch: 002 Batch: 00067/00094 | Loss: 1060.5021 | CE: 0.4501 | KD: 1060.0520\n",
      "Train Epoch: 002 Batch: 00068/00094 | Loss: 1060.3569 | CE: 0.3817 | KD: 1059.9752\n",
      "Train Epoch: 002 Batch: 00069/00094 | Loss: 1060.4166 | CE: 0.3720 | KD: 1060.0447\n",
      "Train Epoch: 002 Batch: 00070/00094 | Loss: 1060.3204 | CE: 0.3149 | KD: 1060.0056\n",
      "Train Epoch: 002 Batch: 00071/00094 | Loss: 1060.3597 | CE: 0.3463 | KD: 1060.0134\n",
      "Train Epoch: 002 Batch: 00072/00094 | Loss: 1060.3752 | CE: 0.3673 | KD: 1060.0079\n",
      "Train Epoch: 002 Batch: 00073/00094 | Loss: 1060.4943 | CE: 0.4310 | KD: 1060.0632\n",
      "Train Epoch: 002 Batch: 00074/00094 | Loss: 1060.3784 | CE: 0.3747 | KD: 1060.0038\n",
      "Train Epoch: 002 Batch: 00075/00094 | Loss: 1060.4980 | CE: 0.3902 | KD: 1060.1079\n",
      "Train Epoch: 002 Batch: 00076/00094 | Loss: 1060.3059 | CE: 0.3109 | KD: 1059.9950\n",
      "Train Epoch: 002 Batch: 00077/00094 | Loss: 1060.4318 | CE: 0.3478 | KD: 1060.0840\n",
      "Train Epoch: 002 Batch: 00078/00094 | Loss: 1060.8154 | CE: 0.6566 | KD: 1060.1588\n",
      "Train Epoch: 002 Batch: 00079/00094 | Loss: 1060.3450 | CE: 0.3216 | KD: 1060.0233\n",
      "Train Epoch: 002 Batch: 00080/00094 | Loss: 1060.5453 | CE: 0.4286 | KD: 1060.1167\n",
      "Train Epoch: 002 Batch: 00081/00094 | Loss: 1060.4149 | CE: 0.4066 | KD: 1060.0083\n",
      "Train Epoch: 002 Batch: 00082/00094 | Loss: 1060.4319 | CE: 0.4115 | KD: 1060.0204\n",
      "Train Epoch: 002 Batch: 00083/00094 | Loss: 1060.6047 | CE: 0.4865 | KD: 1060.1183\n",
      "Train Epoch: 002 Batch: 00084/00094 | Loss: 1060.4038 | CE: 0.3795 | KD: 1060.0243\n",
      "Train Epoch: 002 Batch: 00085/00094 | Loss: 1060.3661 | CE: 0.3077 | KD: 1060.0583\n",
      "Train Epoch: 002 Batch: 00086/00094 | Loss: 1060.3521 | CE: 0.3426 | KD: 1060.0095\n",
      "Train Epoch: 002 Batch: 00087/00094 | Loss: 1060.3619 | CE: 0.3323 | KD: 1060.0297\n",
      "Train Epoch: 002 Batch: 00088/00094 | Loss: 1060.3781 | CE: 0.3500 | KD: 1060.0280\n",
      "Train Epoch: 002 Batch: 00089/00094 | Loss: 1060.4249 | CE: 0.3678 | KD: 1060.0571\n",
      "Train Epoch: 002 Batch: 00090/00094 | Loss: 1060.3645 | CE: 0.3704 | KD: 1059.9940\n",
      "Train Epoch: 002 Batch: 00091/00094 | Loss: 1060.3911 | CE: 0.3314 | KD: 1060.0597\n",
      "Train Epoch: 002 Batch: 00092/00094 | Loss: 1060.3690 | CE: 0.3728 | KD: 1059.9962\n",
      "Train Epoch: 002 Batch: 00093/00094 | Loss: 1060.3533 | CE: 0.3504 | KD: 1060.0028\n",
      "Train Epoch: 002 Batch: 00094/00094 | Loss: 1060.5559 | CE: 0.4524 | KD: 1060.1035\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3844 | acc:84.7000\n",
      "[VAL Acc] Target: 84.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:0.9455 | acc:50.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8432 | acc:49.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8673 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8217 | acc:53.5658\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9781 | acc:52.7726\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.77%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6256 | acc:64.9687\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.97%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0612 | acc:52.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 52.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4362 | acc:79.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 79.75%\n",
      "[VAL Acc] Avg 59.81%\n",
      "VAL Acc improve from 58.60% to 59.81%\n",
      "Save best model\n",
      "Train Epoch: 003 Batch: 00001/00094 | Loss: 1060.3900 | CE: 0.3646 | KD: 1060.0255\n",
      "Train Epoch: 003 Batch: 00002/00094 | Loss: 1060.2732 | CE: 0.2944 | KD: 1059.9788\n",
      "Train Epoch: 003 Batch: 00003/00094 | Loss: 1060.4020 | CE: 0.3748 | KD: 1060.0271\n",
      "Train Epoch: 003 Batch: 00004/00094 | Loss: 1060.6182 | CE: 0.4365 | KD: 1060.1816\n",
      "Train Epoch: 003 Batch: 00005/00094 | Loss: 1060.4066 | CE: 0.3124 | KD: 1060.0942\n",
      "Train Epoch: 003 Batch: 00006/00094 | Loss: 1060.4359 | CE: 0.3921 | KD: 1060.0438\n",
      "Train Epoch: 003 Batch: 00007/00094 | Loss: 1060.2012 | CE: 0.3104 | KD: 1059.8907\n",
      "Train Epoch: 003 Batch: 00008/00094 | Loss: 1060.3657 | CE: 0.3629 | KD: 1060.0028\n",
      "Train Epoch: 003 Batch: 00009/00094 | Loss: 1060.4686 | CE: 0.4192 | KD: 1060.0494\n",
      "Train Epoch: 003 Batch: 00010/00094 | Loss: 1060.4153 | CE: 0.3810 | KD: 1060.0343\n",
      "Train Epoch: 003 Batch: 00011/00094 | Loss: 1060.3926 | CE: 0.3742 | KD: 1060.0184\n",
      "Train Epoch: 003 Batch: 00012/00094 | Loss: 1060.2687 | CE: 0.2948 | KD: 1059.9739\n",
      "Train Epoch: 003 Batch: 00013/00094 | Loss: 1060.4961 | CE: 0.4139 | KD: 1060.0822\n",
      "Train Epoch: 003 Batch: 00014/00094 | Loss: 1060.3668 | CE: 0.3394 | KD: 1060.0273\n",
      "Train Epoch: 003 Batch: 00015/00094 | Loss: 1060.3646 | CE: 0.3799 | KD: 1059.9847\n",
      "Train Epoch: 003 Batch: 00016/00094 | Loss: 1060.3263 | CE: 0.3395 | KD: 1059.9868\n",
      "Train Epoch: 003 Batch: 00017/00094 | Loss: 1060.2568 | CE: 0.3208 | KD: 1059.9360\n",
      "Train Epoch: 003 Batch: 00018/00094 | Loss: 1060.4009 | CE: 0.3875 | KD: 1060.0133\n",
      "Train Epoch: 003 Batch: 00019/00094 | Loss: 1060.3020 | CE: 0.2880 | KD: 1060.0139\n",
      "Train Epoch: 003 Batch: 00020/00094 | Loss: 1060.2512 | CE: 0.2832 | KD: 1059.9680\n",
      "Train Epoch: 003 Batch: 00021/00094 | Loss: 1060.4397 | CE: 0.3784 | KD: 1060.0613\n",
      "Train Epoch: 003 Batch: 00022/00094 | Loss: 1060.2860 | CE: 0.2973 | KD: 1059.9888\n",
      "Train Epoch: 003 Batch: 00023/00094 | Loss: 1060.3345 | CE: 0.3530 | KD: 1059.9814\n",
      "Train Epoch: 003 Batch: 00024/00094 | Loss: 1060.3158 | CE: 0.2875 | KD: 1060.0283\n",
      "Train Epoch: 003 Batch: 00025/00094 | Loss: 1060.4301 | CE: 0.3749 | KD: 1060.0552\n",
      "Train Epoch: 003 Batch: 00026/00094 | Loss: 1060.4232 | CE: 0.4169 | KD: 1060.0063\n",
      "Train Epoch: 003 Batch: 00027/00094 | Loss: 1060.4323 | CE: 0.4104 | KD: 1060.0219\n",
      "Train Epoch: 003 Batch: 00028/00094 | Loss: 1060.4336 | CE: 0.4110 | KD: 1060.0226\n",
      "Train Epoch: 003 Batch: 00029/00094 | Loss: 1060.4093 | CE: 0.3901 | KD: 1060.0193\n",
      "Train Epoch: 003 Batch: 00030/00094 | Loss: 1060.4541 | CE: 0.3772 | KD: 1060.0769\n",
      "Train Epoch: 003 Batch: 00031/00094 | Loss: 1060.3259 | CE: 0.3026 | KD: 1060.0233\n",
      "Train Epoch: 003 Batch: 00032/00094 | Loss: 1060.3419 | CE: 0.3010 | KD: 1060.0409\n",
      "Train Epoch: 003 Batch: 00033/00094 | Loss: 1060.2046 | CE: 0.2507 | KD: 1059.9539\n",
      "Train Epoch: 003 Batch: 00034/00094 | Loss: 1060.3774 | CE: 0.3261 | KD: 1060.0514\n",
      "Train Epoch: 003 Batch: 00035/00094 | Loss: 1060.3938 | CE: 0.4171 | KD: 1059.9767\n",
      "Train Epoch: 003 Batch: 00036/00094 | Loss: 1060.5907 | CE: 0.4362 | KD: 1060.1544\n",
      "Train Epoch: 003 Batch: 00037/00094 | Loss: 1060.3157 | CE: 0.3317 | KD: 1059.9840\n",
      "Train Epoch: 003 Batch: 00038/00094 | Loss: 1060.3438 | CE: 0.3179 | KD: 1060.0259\n",
      "Train Epoch: 003 Batch: 00039/00094 | Loss: 1060.3667 | CE: 0.3811 | KD: 1059.9856\n",
      "Train Epoch: 003 Batch: 00040/00094 | Loss: 1060.3977 | CE: 0.3294 | KD: 1060.0682\n",
      "Train Epoch: 003 Batch: 00041/00094 | Loss: 1060.4828 | CE: 0.4123 | KD: 1060.0704\n",
      "Train Epoch: 003 Batch: 00042/00094 | Loss: 1060.5897 | CE: 0.4917 | KD: 1060.0980\n",
      "Train Epoch: 003 Batch: 00043/00094 | Loss: 1060.5414 | CE: 0.4215 | KD: 1060.1199\n",
      "Train Epoch: 003 Batch: 00044/00094 | Loss: 1060.4240 | CE: 0.3368 | KD: 1060.0872\n",
      "Train Epoch: 003 Batch: 00045/00094 | Loss: 1060.4659 | CE: 0.3571 | KD: 1060.1089\n",
      "Train Epoch: 003 Batch: 00046/00094 | Loss: 1060.4155 | CE: 0.3671 | KD: 1060.0485\n",
      "Train Epoch: 003 Batch: 00047/00094 | Loss: 1060.4448 | CE: 0.3841 | KD: 1060.0608\n",
      "Train Epoch: 003 Batch: 00048/00094 | Loss: 1060.4105 | CE: 0.3775 | KD: 1060.0330\n",
      "Train Epoch: 003 Batch: 00049/00094 | Loss: 1060.3916 | CE: 0.3050 | KD: 1060.0867\n",
      "Train Epoch: 003 Batch: 00050/00094 | Loss: 1060.4099 | CE: 0.3987 | KD: 1060.0112\n",
      "Train Epoch: 003 Batch: 00051/00094 | Loss: 1060.4702 | CE: 0.4130 | KD: 1060.0571\n",
      "Train Epoch: 003 Batch: 00052/00094 | Loss: 1060.4088 | CE: 0.3710 | KD: 1060.0377\n",
      "Train Epoch: 003 Batch: 00053/00094 | Loss: 1060.4519 | CE: 0.3867 | KD: 1060.0652\n",
      "Train Epoch: 003 Batch: 00054/00094 | Loss: 1060.2712 | CE: 0.3053 | KD: 1059.9659\n",
      "Train Epoch: 003 Batch: 00055/00094 | Loss: 1060.4633 | CE: 0.3645 | KD: 1060.0988\n",
      "Train Epoch: 003 Batch: 00056/00094 | Loss: 1060.4388 | CE: 0.3344 | KD: 1060.1045\n",
      "Train Epoch: 003 Batch: 00057/00094 | Loss: 1060.4623 | CE: 0.3768 | KD: 1060.0856\n",
      "Train Epoch: 003 Batch: 00058/00094 | Loss: 1060.4379 | CE: 0.4109 | KD: 1060.0270\n",
      "Train Epoch: 003 Batch: 00059/00094 | Loss: 1060.3682 | CE: 0.3487 | KD: 1060.0195\n",
      "Train Epoch: 003 Batch: 00060/00094 | Loss: 1060.4731 | CE: 0.3698 | KD: 1060.1034\n",
      "Train Epoch: 003 Batch: 00061/00094 | Loss: 1060.3407 | CE: 0.3021 | KD: 1060.0386\n",
      "Train Epoch: 003 Batch: 00062/00094 | Loss: 1060.3102 | CE: 0.3392 | KD: 1059.9709\n",
      "Train Epoch: 003 Batch: 00063/00094 | Loss: 1060.4432 | CE: 0.3890 | KD: 1060.0542\n",
      "Train Epoch: 003 Batch: 00064/00094 | Loss: 1060.3882 | CE: 0.2988 | KD: 1060.0894\n",
      "Train Epoch: 003 Batch: 00065/00094 | Loss: 1060.3861 | CE: 0.3157 | KD: 1060.0704\n",
      "Train Epoch: 003 Batch: 00066/00094 | Loss: 1060.4720 | CE: 0.3759 | KD: 1060.0962\n",
      "Train Epoch: 003 Batch: 00067/00094 | Loss: 1060.4729 | CE: 0.3993 | KD: 1060.0736\n",
      "Train Epoch: 003 Batch: 00068/00094 | Loss: 1060.2537 | CE: 0.2856 | KD: 1059.9680\n",
      "Train Epoch: 003 Batch: 00069/00094 | Loss: 1060.4508 | CE: 0.3631 | KD: 1060.0878\n",
      "Train Epoch: 003 Batch: 00070/00094 | Loss: 1060.4880 | CE: 0.4265 | KD: 1060.0615\n",
      "Train Epoch: 003 Batch: 00071/00094 | Loss: 1060.3431 | CE: 0.3420 | KD: 1060.0011\n",
      "Train Epoch: 003 Batch: 00072/00094 | Loss: 1060.3525 | CE: 0.3923 | KD: 1059.9602\n",
      "Train Epoch: 003 Batch: 00073/00094 | Loss: 1060.1813 | CE: 0.2671 | KD: 1059.9142\n",
      "Train Epoch: 003 Batch: 00074/00094 | Loss: 1060.4408 | CE: 0.4240 | KD: 1060.0168\n",
      "Train Epoch: 003 Batch: 00075/00094 | Loss: 1060.2535 | CE: 0.2276 | KD: 1060.0260\n",
      "Train Epoch: 003 Batch: 00076/00094 | Loss: 1060.5072 | CE: 0.3969 | KD: 1060.1104\n",
      "Train Epoch: 003 Batch: 00077/00094 | Loss: 1060.3699 | CE: 0.3356 | KD: 1060.0343\n",
      "Train Epoch: 003 Batch: 00078/00094 | Loss: 1060.4657 | CE: 0.3985 | KD: 1060.0671\n",
      "Train Epoch: 003 Batch: 00079/00094 | Loss: 1060.5217 | CE: 0.4723 | KD: 1060.0494\n",
      "Train Epoch: 003 Batch: 00080/00094 | Loss: 1060.3754 | CE: 0.3560 | KD: 1060.0194\n",
      "Train Epoch: 003 Batch: 00081/00094 | Loss: 1060.4767 | CE: 0.4060 | KD: 1060.0707\n",
      "Train Epoch: 003 Batch: 00082/00094 | Loss: 1060.3623 | CE: 0.3601 | KD: 1060.0022\n",
      "Train Epoch: 003 Batch: 00083/00094 | Loss: 1060.2871 | CE: 0.3133 | KD: 1059.9738\n",
      "Train Epoch: 003 Batch: 00084/00094 | Loss: 1060.3975 | CE: 0.3285 | KD: 1060.0690\n",
      "Train Epoch: 003 Batch: 00085/00094 | Loss: 1060.4120 | CE: 0.3240 | KD: 1060.0880\n",
      "Train Epoch: 003 Batch: 00086/00094 | Loss: 1060.2445 | CE: 0.2695 | KD: 1059.9750\n",
      "Train Epoch: 003 Batch: 00087/00094 | Loss: 1060.3795 | CE: 0.3714 | KD: 1060.0081\n",
      "Train Epoch: 003 Batch: 00088/00094 | Loss: 1060.5115 | CE: 0.4240 | KD: 1060.0874\n",
      "Train Epoch: 003 Batch: 00089/00094 | Loss: 1060.6652 | CE: 0.4783 | KD: 1060.1869\n",
      "Train Epoch: 003 Batch: 00090/00094 | Loss: 1060.4375 | CE: 0.3814 | KD: 1060.0562\n",
      "Train Epoch: 003 Batch: 00091/00094 | Loss: 1060.2542 | CE: 0.3099 | KD: 1059.9443\n",
      "Train Epoch: 003 Batch: 00092/00094 | Loss: 1060.4541 | CE: 0.3251 | KD: 1060.1290\n",
      "Train Epoch: 003 Batch: 00093/00094 | Loss: 1060.5305 | CE: 0.4360 | KD: 1060.0946\n",
      "Train Epoch: 003 Batch: 00094/00094 | Loss: 1060.3350 | CE: 0.3399 | KD: 1059.9950\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3994 | acc:83.8000\n",
      "[VAL Acc] Target: 83.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:0.8941 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.7987 | acc:53.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8083 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7902 | acc:50.4310\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8056 | acc:55.8226\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6259 | acc:63.6364\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8669 | acc:56.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4417 | acc:79.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 79.70%\n",
      "[VAL Acc] Avg 60.09%\n",
      "VAL Acc improve from 59.81% to 60.09%\n",
      "Save best model\n",
      "Train Epoch: 004 Batch: 00001/00094 | Loss: 1060.3710 | CE: 0.3712 | KD: 1059.9998\n",
      "Train Epoch: 004 Batch: 00002/00094 | Loss: 1060.3160 | CE: 0.2939 | KD: 1060.0221\n",
      "Train Epoch: 004 Batch: 00003/00094 | Loss: 1060.4865 | CE: 0.4549 | KD: 1060.0316\n",
      "Train Epoch: 004 Batch: 00004/00094 | Loss: 1060.2665 | CE: 0.3238 | KD: 1059.9426\n",
      "Train Epoch: 004 Batch: 00005/00094 | Loss: 1060.7139 | CE: 0.5413 | KD: 1060.1725\n",
      "Train Epoch: 004 Batch: 00006/00094 | Loss: 1060.5046 | CE: 0.4002 | KD: 1060.1044\n",
      "Train Epoch: 004 Batch: 00007/00094 | Loss: 1060.3809 | CE: 0.3119 | KD: 1060.0690\n",
      "Train Epoch: 004 Batch: 00008/00094 | Loss: 1060.4253 | CE: 0.3370 | KD: 1060.0884\n",
      "Train Epoch: 004 Batch: 00009/00094 | Loss: 1060.4330 | CE: 0.3862 | KD: 1060.0468\n",
      "Train Epoch: 004 Batch: 00010/00094 | Loss: 1060.3784 | CE: 0.3881 | KD: 1059.9902\n",
      "Train Epoch: 004 Batch: 00011/00094 | Loss: 1060.4296 | CE: 0.4050 | KD: 1060.0245\n",
      "Train Epoch: 004 Batch: 00012/00094 | Loss: 1060.3158 | CE: 0.3334 | KD: 1059.9824\n",
      "Train Epoch: 004 Batch: 00013/00094 | Loss: 1060.3038 | CE: 0.3514 | KD: 1059.9524\n",
      "Train Epoch: 004 Batch: 00014/00094 | Loss: 1060.3618 | CE: 0.3623 | KD: 1059.9995\n",
      "Train Epoch: 004 Batch: 00015/00094 | Loss: 1060.2961 | CE: 0.3116 | KD: 1059.9845\n",
      "Train Epoch: 004 Batch: 00016/00094 | Loss: 1060.3541 | CE: 0.2619 | KD: 1060.0923\n",
      "Train Epoch: 004 Batch: 00017/00094 | Loss: 1060.3868 | CE: 0.3358 | KD: 1060.0510\n",
      "Train Epoch: 004 Batch: 00018/00094 | Loss: 1060.4447 | CE: 0.3567 | KD: 1060.0880\n",
      "Train Epoch: 004 Batch: 00019/00094 | Loss: 1060.5251 | CE: 0.4061 | KD: 1060.1190\n",
      "Train Epoch: 004 Batch: 00020/00094 | Loss: 1060.3680 | CE: 0.3422 | KD: 1060.0259\n",
      "Train Epoch: 004 Batch: 00021/00094 | Loss: 1060.5243 | CE: 0.4134 | KD: 1060.1110\n",
      "Train Epoch: 004 Batch: 00022/00094 | Loss: 1060.2888 | CE: 0.3040 | KD: 1059.9849\n",
      "Train Epoch: 004 Batch: 00023/00094 | Loss: 1060.4392 | CE: 0.3559 | KD: 1060.0833\n",
      "Train Epoch: 004 Batch: 00024/00094 | Loss: 1060.3815 | CE: 0.3499 | KD: 1060.0315\n",
      "Train Epoch: 004 Batch: 00025/00094 | Loss: 1060.2410 | CE: 0.2676 | KD: 1059.9734\n",
      "Train Epoch: 004 Batch: 00026/00094 | Loss: 1060.3934 | CE: 0.3588 | KD: 1060.0345\n",
      "Train Epoch: 004 Batch: 00027/00094 | Loss: 1060.3615 | CE: 0.3137 | KD: 1060.0477\n",
      "Train Epoch: 004 Batch: 00028/00094 | Loss: 1060.5653 | CE: 0.4239 | KD: 1060.1414\n",
      "Train Epoch: 004 Batch: 00029/00094 | Loss: 1060.4303 | CE: 0.3763 | KD: 1060.0540\n",
      "Train Epoch: 004 Batch: 00030/00094 | Loss: 1060.3700 | CE: 0.3565 | KD: 1060.0135\n",
      "Train Epoch: 004 Batch: 00031/00094 | Loss: 1060.2491 | CE: 0.3002 | KD: 1059.9490\n",
      "Train Epoch: 004 Batch: 00032/00094 | Loss: 1060.3853 | CE: 0.3755 | KD: 1060.0098\n",
      "Train Epoch: 004 Batch: 00033/00094 | Loss: 1060.4163 | CE: 0.3602 | KD: 1060.0560\n",
      "Train Epoch: 004 Batch: 00034/00094 | Loss: 1060.4609 | CE: 0.3647 | KD: 1060.0962\n",
      "Train Epoch: 004 Batch: 00035/00094 | Loss: 1060.4691 | CE: 0.3950 | KD: 1060.0741\n",
      "Train Epoch: 004 Batch: 00036/00094 | Loss: 1060.5438 | CE: 0.4516 | KD: 1060.0923\n",
      "Train Epoch: 004 Batch: 00037/00094 | Loss: 1060.4370 | CE: 0.3993 | KD: 1060.0377\n",
      "Train Epoch: 004 Batch: 00038/00094 | Loss: 1060.2939 | CE: 0.2997 | KD: 1059.9943\n",
      "Train Epoch: 004 Batch: 00039/00094 | Loss: 1060.4010 | CE: 0.3587 | KD: 1060.0422\n",
      "Train Epoch: 004 Batch: 00040/00094 | Loss: 1060.3938 | CE: 0.3400 | KD: 1060.0538\n",
      "Train Epoch: 004 Batch: 00041/00094 | Loss: 1060.4324 | CE: 0.3708 | KD: 1060.0616\n",
      "Train Epoch: 004 Batch: 00042/00094 | Loss: 1060.3134 | CE: 0.2826 | KD: 1060.0308\n",
      "Train Epoch: 004 Batch: 00043/00094 | Loss: 1060.2762 | CE: 0.3337 | KD: 1059.9425\n",
      "Train Epoch: 004 Batch: 00044/00094 | Loss: 1060.4117 | CE: 0.3833 | KD: 1060.0284\n",
      "Train Epoch: 004 Batch: 00045/00094 | Loss: 1060.4691 | CE: 0.4249 | KD: 1060.0442\n",
      "Train Epoch: 004 Batch: 00046/00094 | Loss: 1060.3706 | CE: 0.2781 | KD: 1060.0925\n",
      "Train Epoch: 004 Batch: 00047/00094 | Loss: 1060.3828 | CE: 0.3315 | KD: 1060.0513\n",
      "Train Epoch: 004 Batch: 00048/00094 | Loss: 1060.2372 | CE: 0.2545 | KD: 1059.9827\n",
      "Train Epoch: 004 Batch: 00049/00094 | Loss: 1060.2859 | CE: 0.2778 | KD: 1060.0081\n",
      "Train Epoch: 004 Batch: 00050/00094 | Loss: 1060.3745 | CE: 0.2947 | KD: 1060.0798\n",
      "Train Epoch: 004 Batch: 00051/00094 | Loss: 1060.3927 | CE: 0.3469 | KD: 1060.0458\n",
      "Train Epoch: 004 Batch: 00052/00094 | Loss: 1060.2976 | CE: 0.2809 | KD: 1060.0167\n",
      "Train Epoch: 004 Batch: 00053/00094 | Loss: 1060.3711 | CE: 0.3420 | KD: 1060.0291\n",
      "Train Epoch: 004 Batch: 00054/00094 | Loss: 1060.4406 | CE: 0.3500 | KD: 1060.0906\n",
      "Train Epoch: 004 Batch: 00055/00094 | Loss: 1060.2802 | CE: 0.2980 | KD: 1059.9822\n",
      "Train Epoch: 004 Batch: 00056/00094 | Loss: 1060.5115 | CE: 0.4258 | KD: 1060.0857\n",
      "Train Epoch: 004 Batch: 00057/00094 | Loss: 1060.4016 | CE: 0.3226 | KD: 1060.0790\n",
      "Train Epoch: 004 Batch: 00058/00094 | Loss: 1060.3137 | CE: 0.2992 | KD: 1060.0145\n",
      "Train Epoch: 004 Batch: 00059/00094 | Loss: 1060.1755 | CE: 0.2316 | KD: 1059.9440\n",
      "Train Epoch: 004 Batch: 00060/00094 | Loss: 1060.3988 | CE: 0.3786 | KD: 1060.0201\n",
      "Train Epoch: 004 Batch: 00061/00094 | Loss: 1060.4072 | CE: 0.3452 | KD: 1060.0620\n",
      "Train Epoch: 004 Batch: 00062/00094 | Loss: 1060.4808 | CE: 0.3619 | KD: 1060.1190\n",
      "Train Epoch: 004 Batch: 00063/00094 | Loss: 1060.3925 | CE: 0.3676 | KD: 1060.0249\n",
      "Train Epoch: 004 Batch: 00064/00094 | Loss: 1060.4365 | CE: 0.3795 | KD: 1060.0570\n",
      "Train Epoch: 004 Batch: 00065/00094 | Loss: 1060.2485 | CE: 0.3185 | KD: 1059.9301\n",
      "Train Epoch: 004 Batch: 00066/00094 | Loss: 1060.2985 | CE: 0.2906 | KD: 1060.0078\n",
      "Train Epoch: 004 Batch: 00067/00094 | Loss: 1060.3674 | CE: 0.3291 | KD: 1060.0383\n",
      "Train Epoch: 004 Batch: 00068/00094 | Loss: 1060.3456 | CE: 0.3274 | KD: 1060.0182\n",
      "Train Epoch: 004 Batch: 00069/00094 | Loss: 1060.4249 | CE: 0.3874 | KD: 1060.0376\n",
      "Train Epoch: 004 Batch: 00070/00094 | Loss: 1060.3915 | CE: 0.3752 | KD: 1060.0162\n",
      "Train Epoch: 004 Batch: 00071/00094 | Loss: 1060.3499 | CE: 0.3571 | KD: 1059.9928\n",
      "Train Epoch: 004 Batch: 00072/00094 | Loss: 1060.4531 | CE: 0.3859 | KD: 1060.0673\n",
      "Train Epoch: 004 Batch: 00073/00094 | Loss: 1060.2925 | CE: 0.3096 | KD: 1059.9829\n",
      "Train Epoch: 004 Batch: 00074/00094 | Loss: 1060.3668 | CE: 0.2980 | KD: 1060.0688\n",
      "Train Epoch: 004 Batch: 00075/00094 | Loss: 1060.4839 | CE: 0.3960 | KD: 1060.0879\n",
      "Train Epoch: 004 Batch: 00076/00094 | Loss: 1060.5497 | CE: 0.4555 | KD: 1060.0942\n",
      "Train Epoch: 004 Batch: 00077/00094 | Loss: 1060.4515 | CE: 0.4036 | KD: 1060.0480\n",
      "Train Epoch: 004 Batch: 00078/00094 | Loss: 1060.4578 | CE: 0.4157 | KD: 1060.0421\n",
      "Train Epoch: 004 Batch: 00079/00094 | Loss: 1060.3232 | CE: 0.3212 | KD: 1060.0021\n",
      "Train Epoch: 004 Batch: 00080/00094 | Loss: 1060.3418 | CE: 0.3473 | KD: 1059.9945\n",
      "Train Epoch: 004 Batch: 00081/00094 | Loss: 1060.3031 | CE: 0.2964 | KD: 1060.0067\n",
      "Train Epoch: 004 Batch: 00082/00094 | Loss: 1060.3444 | CE: 0.2624 | KD: 1060.0819\n",
      "Train Epoch: 004 Batch: 00083/00094 | Loss: 1060.3091 | CE: 0.3251 | KD: 1059.9840\n",
      "Train Epoch: 004 Batch: 00084/00094 | Loss: 1060.4070 | CE: 0.3607 | KD: 1060.0463\n",
      "Train Epoch: 004 Batch: 00085/00094 | Loss: 1060.2859 | CE: 0.2888 | KD: 1059.9971\n",
      "Train Epoch: 004 Batch: 00086/00094 | Loss: 1060.5723 | CE: 0.4651 | KD: 1060.1072\n",
      "Train Epoch: 004 Batch: 00087/00094 | Loss: 1060.3422 | CE: 0.3352 | KD: 1060.0070\n",
      "Train Epoch: 004 Batch: 00088/00094 | Loss: 1060.2557 | CE: 0.2895 | KD: 1059.9663\n",
      "Train Epoch: 004 Batch: 00089/00094 | Loss: 1060.3910 | CE: 0.3628 | KD: 1060.0282\n",
      "Train Epoch: 004 Batch: 00090/00094 | Loss: 1060.4006 | CE: 0.3901 | KD: 1060.0105\n",
      "Train Epoch: 004 Batch: 00091/00094 | Loss: 1060.2765 | CE: 0.2976 | KD: 1059.9789\n",
      "Train Epoch: 004 Batch: 00092/00094 | Loss: 1060.3900 | CE: 0.3904 | KD: 1059.9996\n",
      "Train Epoch: 004 Batch: 00093/00094 | Loss: 1060.3936 | CE: 0.3544 | KD: 1060.0391\n",
      "Train Epoch: 004 Batch: 00094/00094 | Loss: 1060.3928 | CE: 0.3583 | KD: 1060.0345\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3471 | acc:87.4000\n",
      "[VAL Acc] Target: 87.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:0.9654 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8767 | acc:48.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8752 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8178 | acc:52.7038\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9304 | acc:53.2348\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6532 | acc:62.3041\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9360 | acc:54.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4707 | acc:77.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 77.05%\n",
      "[VAL Acc] Avg 59.36%\n",
      "Train Epoch: 005 Batch: 00001/00094 | Loss: 1060.4066 | CE: 0.4269 | KD: 1059.9797\n",
      "Train Epoch: 005 Batch: 00002/00094 | Loss: 1060.3121 | CE: 0.3030 | KD: 1060.0092\n",
      "Train Epoch: 005 Batch: 00003/00094 | Loss: 1060.4020 | CE: 0.3474 | KD: 1060.0546\n",
      "Train Epoch: 005 Batch: 00004/00094 | Loss: 1060.2588 | CE: 0.2973 | KD: 1059.9614\n",
      "Train Epoch: 005 Batch: 00005/00094 | Loss: 1060.3320 | CE: 0.2844 | KD: 1060.0476\n",
      "Train Epoch: 005 Batch: 00006/00094 | Loss: 1060.3418 | CE: 0.3607 | KD: 1059.9811\n",
      "Train Epoch: 005 Batch: 00007/00094 | Loss: 1060.3356 | CE: 0.3217 | KD: 1060.0139\n",
      "Train Epoch: 005 Batch: 00008/00094 | Loss: 1060.2610 | CE: 0.2790 | KD: 1059.9819\n",
      "Train Epoch: 005 Batch: 00009/00094 | Loss: 1060.4546 | CE: 0.4194 | KD: 1060.0352\n",
      "Train Epoch: 005 Batch: 00010/00094 | Loss: 1060.2500 | CE: 0.3050 | KD: 1059.9449\n",
      "Train Epoch: 005 Batch: 00011/00094 | Loss: 1060.4017 | CE: 0.3495 | KD: 1060.0522\n",
      "Train Epoch: 005 Batch: 00012/00094 | Loss: 1060.3689 | CE: 0.3636 | KD: 1060.0052\n",
      "Train Epoch: 005 Batch: 00013/00094 | Loss: 1060.2834 | CE: 0.3156 | KD: 1059.9679\n",
      "Train Epoch: 005 Batch: 00014/00094 | Loss: 1060.2762 | CE: 0.2638 | KD: 1060.0125\n",
      "Train Epoch: 005 Batch: 00015/00094 | Loss: 1060.3336 | CE: 0.3037 | KD: 1060.0299\n",
      "Train Epoch: 005 Batch: 00016/00094 | Loss: 1060.2578 | CE: 0.2512 | KD: 1060.0067\n",
      "Train Epoch: 005 Batch: 00017/00094 | Loss: 1060.3766 | CE: 0.3270 | KD: 1060.0496\n",
      "Train Epoch: 005 Batch: 00018/00094 | Loss: 1060.6348 | CE: 0.4595 | KD: 1060.1753\n",
      "Train Epoch: 005 Batch: 00019/00094 | Loss: 1060.3901 | CE: 0.3697 | KD: 1060.0205\n",
      "Train Epoch: 005 Batch: 00020/00094 | Loss: 1060.2740 | CE: 0.2939 | KD: 1059.9802\n",
      "Train Epoch: 005 Batch: 00021/00094 | Loss: 1060.4036 | CE: 0.3585 | KD: 1060.0450\n",
      "Train Epoch: 005 Batch: 00022/00094 | Loss: 1060.2104 | CE: 0.2726 | KD: 1059.9379\n",
      "Train Epoch: 005 Batch: 00023/00094 | Loss: 1060.4600 | CE: 0.3881 | KD: 1060.0718\n",
      "Train Epoch: 005 Batch: 00024/00094 | Loss: 1060.3541 | CE: 0.3709 | KD: 1059.9833\n",
      "Train Epoch: 005 Batch: 00025/00094 | Loss: 1060.4410 | CE: 0.3742 | KD: 1060.0668\n",
      "Train Epoch: 005 Batch: 00026/00094 | Loss: 1060.3167 | CE: 0.3074 | KD: 1060.0093\n",
      "Train Epoch: 005 Batch: 00027/00094 | Loss: 1060.3931 | CE: 0.3049 | KD: 1060.0881\n",
      "Train Epoch: 005 Batch: 00028/00094 | Loss: 1060.3636 | CE: 0.3227 | KD: 1060.0410\n",
      "Train Epoch: 005 Batch: 00029/00094 | Loss: 1060.3763 | CE: 0.3464 | KD: 1060.0299\n",
      "Train Epoch: 005 Batch: 00030/00094 | Loss: 1060.1797 | CE: 0.2420 | KD: 1059.9377\n",
      "Train Epoch: 005 Batch: 00031/00094 | Loss: 1060.2783 | CE: 0.3126 | KD: 1059.9657\n",
      "Train Epoch: 005 Batch: 00032/00094 | Loss: 1060.3610 | CE: 0.3503 | KD: 1060.0106\n",
      "Train Epoch: 005 Batch: 00033/00094 | Loss: 1060.4296 | CE: 0.3497 | KD: 1060.0800\n",
      "Train Epoch: 005 Batch: 00034/00094 | Loss: 1060.2637 | CE: 0.2677 | KD: 1059.9960\n",
      "Train Epoch: 005 Batch: 00035/00094 | Loss: 1060.4015 | CE: 0.3357 | KD: 1060.0658\n",
      "Train Epoch: 005 Batch: 00036/00094 | Loss: 1060.3442 | CE: 0.2923 | KD: 1060.0519\n",
      "Train Epoch: 005 Batch: 00037/00094 | Loss: 1060.5756 | CE: 0.4199 | KD: 1060.1558\n",
      "Train Epoch: 005 Batch: 00038/00094 | Loss: 1060.5056 | CE: 0.4342 | KD: 1060.0714\n",
      "Train Epoch: 005 Batch: 00039/00094 | Loss: 1060.3884 | CE: 0.3610 | KD: 1060.0275\n",
      "Train Epoch: 005 Batch: 00040/00094 | Loss: 1060.4987 | CE: 0.4088 | KD: 1060.0898\n",
      "Train Epoch: 005 Batch: 00041/00094 | Loss: 1060.4288 | CE: 0.3539 | KD: 1060.0750\n",
      "Train Epoch: 005 Batch: 00042/00094 | Loss: 1060.4115 | CE: 0.3848 | KD: 1060.0267\n",
      "Train Epoch: 005 Batch: 00043/00094 | Loss: 1060.3744 | CE: 0.3461 | KD: 1060.0283\n",
      "Train Epoch: 005 Batch: 00044/00094 | Loss: 1060.3856 | CE: 0.3883 | KD: 1059.9973\n",
      "Train Epoch: 005 Batch: 00045/00094 | Loss: 1060.3376 | CE: 0.3382 | KD: 1059.9995\n",
      "Train Epoch: 005 Batch: 00046/00094 | Loss: 1060.3599 | CE: 0.3172 | KD: 1060.0426\n",
      "Train Epoch: 005 Batch: 00047/00094 | Loss: 1060.2646 | CE: 0.3310 | KD: 1059.9336\n",
      "Train Epoch: 005 Batch: 00048/00094 | Loss: 1060.2059 | CE: 0.2579 | KD: 1059.9481\n",
      "Train Epoch: 005 Batch: 00049/00094 | Loss: 1060.4271 | CE: 0.3432 | KD: 1060.0840\n",
      "Train Epoch: 005 Batch: 00050/00094 | Loss: 1060.5697 | CE: 0.4701 | KD: 1060.0996\n",
      "Train Epoch: 005 Batch: 00051/00094 | Loss: 1060.2151 | CE: 0.2735 | KD: 1059.9415\n",
      "Train Epoch: 005 Batch: 00052/00094 | Loss: 1060.3232 | CE: 0.2906 | KD: 1060.0327\n",
      "Train Epoch: 005 Batch: 00053/00094 | Loss: 1060.4183 | CE: 0.3338 | KD: 1060.0846\n",
      "Train Epoch: 005 Batch: 00054/00094 | Loss: 1060.4170 | CE: 0.3707 | KD: 1060.0463\n",
      "Train Epoch: 005 Batch: 00055/00094 | Loss: 1060.3356 | CE: 0.3400 | KD: 1059.9956\n",
      "Train Epoch: 005 Batch: 00056/00094 | Loss: 1060.4429 | CE: 0.4049 | KD: 1060.0380\n",
      "Train Epoch: 005 Batch: 00057/00094 | Loss: 1060.5443 | CE: 0.4529 | KD: 1060.0914\n",
      "Train Epoch: 005 Batch: 00058/00094 | Loss: 1060.4230 | CE: 0.3661 | KD: 1060.0569\n",
      "Train Epoch: 005 Batch: 00059/00094 | Loss: 1060.3199 | CE: 0.2683 | KD: 1060.0516\n",
      "Train Epoch: 005 Batch: 00060/00094 | Loss: 1060.3416 | CE: 0.3408 | KD: 1060.0007\n",
      "Train Epoch: 005 Batch: 00061/00094 | Loss: 1060.3726 | CE: 0.3526 | KD: 1060.0199\n",
      "Train Epoch: 005 Batch: 00062/00094 | Loss: 1060.3243 | CE: 0.3510 | KD: 1059.9734\n",
      "Train Epoch: 005 Batch: 00063/00094 | Loss: 1060.2849 | CE: 0.2996 | KD: 1059.9854\n",
      "Train Epoch: 005 Batch: 00064/00094 | Loss: 1060.3591 | CE: 0.3333 | KD: 1060.0259\n",
      "Train Epoch: 005 Batch: 00065/00094 | Loss: 1060.3545 | CE: 0.3560 | KD: 1059.9985\n",
      "Train Epoch: 005 Batch: 00066/00094 | Loss: 1060.3751 | CE: 0.3241 | KD: 1060.0510\n",
      "Train Epoch: 005 Batch: 00067/00094 | Loss: 1060.5483 | CE: 0.3924 | KD: 1060.1559\n",
      "Train Epoch: 005 Batch: 00068/00094 | Loss: 1060.4159 | CE: 0.3256 | KD: 1060.0903\n",
      "Train Epoch: 005 Batch: 00069/00094 | Loss: 1060.4139 | CE: 0.3763 | KD: 1060.0377\n",
      "Train Epoch: 005 Batch: 00070/00094 | Loss: 1060.3333 | CE: 0.3225 | KD: 1060.0107\n",
      "Train Epoch: 005 Batch: 00071/00094 | Loss: 1060.2012 | CE: 0.2855 | KD: 1059.9156\n",
      "Train Epoch: 005 Batch: 00072/00094 | Loss: 1060.3109 | CE: 0.2861 | KD: 1060.0249\n",
      "Train Epoch: 005 Batch: 00073/00094 | Loss: 1060.4158 | CE: 0.3403 | KD: 1060.0754\n",
      "Train Epoch: 005 Batch: 00074/00094 | Loss: 1060.5245 | CE: 0.4367 | KD: 1060.0879\n",
      "Train Epoch: 005 Batch: 00075/00094 | Loss: 1060.3561 | CE: 0.3501 | KD: 1060.0060\n",
      "Train Epoch: 005 Batch: 00076/00094 | Loss: 1060.3781 | CE: 0.3319 | KD: 1060.0461\n",
      "Train Epoch: 005 Batch: 00077/00094 | Loss: 1060.2323 | CE: 0.2723 | KD: 1059.9601\n",
      "Train Epoch: 005 Batch: 00078/00094 | Loss: 1060.5277 | CE: 0.4485 | KD: 1060.0791\n",
      "Train Epoch: 005 Batch: 00079/00094 | Loss: 1060.2562 | CE: 0.3099 | KD: 1059.9463\n",
      "Train Epoch: 005 Batch: 00080/00094 | Loss: 1060.3379 | CE: 0.3554 | KD: 1059.9824\n",
      "Train Epoch: 005 Batch: 00081/00094 | Loss: 1060.3820 | CE: 0.3537 | KD: 1060.0282\n",
      "Train Epoch: 005 Batch: 00082/00094 | Loss: 1060.2377 | CE: 0.2661 | KD: 1059.9716\n",
      "Train Epoch: 005 Batch: 00083/00094 | Loss: 1060.4517 | CE: 0.3502 | KD: 1060.1014\n",
      "Train Epoch: 005 Batch: 00084/00094 | Loss: 1060.5245 | CE: 0.4635 | KD: 1060.0610\n",
      "Train Epoch: 005 Batch: 00085/00094 | Loss: 1060.3486 | CE: 0.3549 | KD: 1059.9937\n",
      "Train Epoch: 005 Batch: 00086/00094 | Loss: 1060.4055 | CE: 0.3076 | KD: 1060.0979\n",
      "Train Epoch: 005 Batch: 00087/00094 | Loss: 1060.2440 | CE: 0.2698 | KD: 1059.9742\n",
      "Train Epoch: 005 Batch: 00088/00094 | Loss: 1060.2800 | CE: 0.3249 | KD: 1059.9552\n",
      "Train Epoch: 005 Batch: 00089/00094 | Loss: 1060.3829 | CE: 0.3372 | KD: 1060.0457\n",
      "Train Epoch: 005 Batch: 00090/00094 | Loss: 1060.3352 | CE: 0.3122 | KD: 1060.0231\n",
      "Train Epoch: 005 Batch: 00091/00094 | Loss: 1060.2305 | CE: 0.3027 | KD: 1059.9277\n",
      "Train Epoch: 005 Batch: 00092/00094 | Loss: 1060.2947 | CE: 0.3180 | KD: 1059.9767\n",
      "Train Epoch: 005 Batch: 00093/00094 | Loss: 1060.1470 | CE: 0.2634 | KD: 1059.8835\n",
      "Train Epoch: 005 Batch: 00094/00094 | Loss: 1060.2913 | CE: 0.2885 | KD: 1060.0027\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3395 | acc:86.5500\n",
      "[VAL Acc] Target: 86.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1159 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9590 | acc:46.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 46.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9752 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9488 | acc:51.9592\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7128 | acc:60.9982\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 61.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7214 | acc:58.8558\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 58.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8783 | acc:56.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5111 | acc:72.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 72.95%\n",
      "[VAL Acc] Avg 59.28%\n",
      "Train Epoch: 006 Batch: 00001/00094 | Loss: 1060.5043 | CE: 0.4037 | KD: 1060.1006\n",
      "Train Epoch: 006 Batch: 00002/00094 | Loss: 1060.4523 | CE: 0.3879 | KD: 1060.0643\n",
      "Train Epoch: 006 Batch: 00003/00094 | Loss: 1060.3365 | CE: 0.3205 | KD: 1060.0161\n",
      "Train Epoch: 006 Batch: 00004/00094 | Loss: 1060.2441 | CE: 0.2979 | KD: 1059.9463\n",
      "Train Epoch: 006 Batch: 00005/00094 | Loss: 1060.2937 | CE: 0.3355 | KD: 1059.9583\n",
      "Train Epoch: 006 Batch: 00006/00094 | Loss: 1060.2587 | CE: 0.2533 | KD: 1060.0054\n",
      "Train Epoch: 006 Batch: 00007/00094 | Loss: 1060.1870 | CE: 0.2581 | KD: 1059.9288\n",
      "Train Epoch: 006 Batch: 00008/00094 | Loss: 1060.2490 | CE: 0.2434 | KD: 1060.0056\n",
      "Train Epoch: 006 Batch: 00009/00094 | Loss: 1060.1764 | CE: 0.2604 | KD: 1059.9160\n",
      "Train Epoch: 006 Batch: 00010/00094 | Loss: 1060.4420 | CE: 0.3677 | KD: 1060.0743\n",
      "Train Epoch: 006 Batch: 00011/00094 | Loss: 1060.3270 | CE: 0.3266 | KD: 1060.0005\n",
      "Train Epoch: 006 Batch: 00012/00094 | Loss: 1060.4690 | CE: 0.3750 | KD: 1060.0940\n",
      "Train Epoch: 006 Batch: 00013/00094 | Loss: 1060.4075 | CE: 0.3844 | KD: 1060.0231\n",
      "Train Epoch: 006 Batch: 00014/00094 | Loss: 1060.4614 | CE: 0.4126 | KD: 1060.0488\n",
      "Train Epoch: 006 Batch: 00015/00094 | Loss: 1060.4056 | CE: 0.3435 | KD: 1060.0621\n",
      "Train Epoch: 006 Batch: 00016/00094 | Loss: 1060.3875 | CE: 0.3792 | KD: 1060.0082\n",
      "Train Epoch: 006 Batch: 00017/00094 | Loss: 1060.3213 | CE: 0.3182 | KD: 1060.0031\n",
      "Train Epoch: 006 Batch: 00018/00094 | Loss: 1060.4497 | CE: 0.3498 | KD: 1060.1000\n",
      "Train Epoch: 006 Batch: 00019/00094 | Loss: 1060.3022 | CE: 0.2968 | KD: 1060.0054\n",
      "Train Epoch: 006 Batch: 00020/00094 | Loss: 1060.3665 | CE: 0.3254 | KD: 1060.0410\n",
      "Train Epoch: 006 Batch: 00021/00094 | Loss: 1060.4923 | CE: 0.4419 | KD: 1060.0504\n",
      "Train Epoch: 006 Batch: 00022/00094 | Loss: 1060.4307 | CE: 0.3250 | KD: 1060.1056\n",
      "Train Epoch: 006 Batch: 00023/00094 | Loss: 1060.2676 | CE: 0.2973 | KD: 1059.9702\n",
      "Train Epoch: 006 Batch: 00024/00094 | Loss: 1060.3644 | CE: 0.3156 | KD: 1060.0487\n",
      "Train Epoch: 006 Batch: 00025/00094 | Loss: 1060.3837 | CE: 0.3459 | KD: 1060.0377\n",
      "Train Epoch: 006 Batch: 00026/00094 | Loss: 1060.4027 | CE: 0.3801 | KD: 1060.0226\n",
      "Train Epoch: 006 Batch: 00027/00094 | Loss: 1060.3441 | CE: 0.3367 | KD: 1060.0074\n",
      "Train Epoch: 006 Batch: 00028/00094 | Loss: 1060.3872 | CE: 0.3828 | KD: 1060.0044\n",
      "Train Epoch: 006 Batch: 00029/00094 | Loss: 1060.4633 | CE: 0.3663 | KD: 1060.0969\n",
      "Train Epoch: 006 Batch: 00030/00094 | Loss: 1060.5004 | CE: 0.4357 | KD: 1060.0646\n",
      "Train Epoch: 006 Batch: 00031/00094 | Loss: 1060.4434 | CE: 0.3591 | KD: 1060.0842\n",
      "Train Epoch: 006 Batch: 00032/00094 | Loss: 1060.2922 | CE: 0.2886 | KD: 1060.0037\n",
      "Train Epoch: 006 Batch: 00033/00094 | Loss: 1060.2842 | CE: 0.3190 | KD: 1059.9652\n",
      "Train Epoch: 006 Batch: 00034/00094 | Loss: 1060.2523 | CE: 0.2781 | KD: 1059.9742\n",
      "Train Epoch: 006 Batch: 00035/00094 | Loss: 1060.4232 | CE: 0.3912 | KD: 1060.0320\n",
      "Train Epoch: 006 Batch: 00036/00094 | Loss: 1060.2617 | CE: 0.2809 | KD: 1059.9808\n",
      "Train Epoch: 006 Batch: 00037/00094 | Loss: 1060.3560 | CE: 0.3423 | KD: 1060.0137\n",
      "Train Epoch: 006 Batch: 00038/00094 | Loss: 1060.3438 | CE: 0.3055 | KD: 1060.0382\n",
      "Train Epoch: 006 Batch: 00039/00094 | Loss: 1060.2592 | CE: 0.3025 | KD: 1059.9567\n",
      "Train Epoch: 006 Batch: 00040/00094 | Loss: 1060.4167 | CE: 0.3583 | KD: 1060.0585\n",
      "Train Epoch: 006 Batch: 00041/00094 | Loss: 1060.5110 | CE: 0.4385 | KD: 1060.0725\n",
      "Train Epoch: 006 Batch: 00042/00094 | Loss: 1060.3600 | CE: 0.3509 | KD: 1060.0090\n",
      "Train Epoch: 006 Batch: 00043/00094 | Loss: 1060.2771 | CE: 0.2891 | KD: 1059.9880\n",
      "Train Epoch: 006 Batch: 00044/00094 | Loss: 1060.4005 | CE: 0.3796 | KD: 1060.0209\n",
      "Train Epoch: 006 Batch: 00045/00094 | Loss: 1060.2091 | CE: 0.2405 | KD: 1059.9686\n",
      "Train Epoch: 006 Batch: 00046/00094 | Loss: 1060.4703 | CE: 0.3765 | KD: 1060.0938\n",
      "Train Epoch: 006 Batch: 00047/00094 | Loss: 1060.3047 | CE: 0.2961 | KD: 1060.0085\n",
      "Train Epoch: 006 Batch: 00048/00094 | Loss: 1060.3038 | CE: 0.3093 | KD: 1059.9945\n",
      "Train Epoch: 006 Batch: 00049/00094 | Loss: 1060.3331 | CE: 0.3030 | KD: 1060.0302\n",
      "Train Epoch: 006 Batch: 00050/00094 | Loss: 1060.3883 | CE: 0.3308 | KD: 1060.0575\n",
      "Train Epoch: 006 Batch: 00051/00094 | Loss: 1060.3447 | CE: 0.2999 | KD: 1060.0449\n",
      "Train Epoch: 006 Batch: 00052/00094 | Loss: 1060.3466 | CE: 0.3073 | KD: 1060.0392\n",
      "Train Epoch: 006 Batch: 00053/00094 | Loss: 1060.4739 | CE: 0.4003 | KD: 1060.0735\n",
      "Train Epoch: 006 Batch: 00054/00094 | Loss: 1060.3190 | CE: 0.2906 | KD: 1060.0283\n",
      "Train Epoch: 006 Batch: 00055/00094 | Loss: 1060.2684 | CE: 0.2790 | KD: 1059.9894\n",
      "Train Epoch: 006 Batch: 00056/00094 | Loss: 1060.3784 | CE: 0.3203 | KD: 1060.0581\n",
      "Train Epoch: 006 Batch: 00057/00094 | Loss: 1060.3625 | CE: 0.3250 | KD: 1060.0375\n",
      "Train Epoch: 006 Batch: 00058/00094 | Loss: 1060.3966 | CE: 0.3538 | KD: 1060.0428\n",
      "Train Epoch: 006 Batch: 00059/00094 | Loss: 1060.3604 | CE: 0.3144 | KD: 1060.0460\n",
      "Train Epoch: 006 Batch: 00060/00094 | Loss: 1060.4183 | CE: 0.3621 | KD: 1060.0563\n",
      "Train Epoch: 006 Batch: 00061/00094 | Loss: 1060.4484 | CE: 0.3322 | KD: 1060.1161\n",
      "Train Epoch: 006 Batch: 00062/00094 | Loss: 1060.3983 | CE: 0.3569 | KD: 1060.0414\n",
      "Train Epoch: 006 Batch: 00063/00094 | Loss: 1060.3596 | CE: 0.3529 | KD: 1060.0067\n",
      "Train Epoch: 006 Batch: 00064/00094 | Loss: 1060.3126 | CE: 0.3031 | KD: 1060.0095\n",
      "Train Epoch: 006 Batch: 00065/00094 | Loss: 1060.3630 | CE: 0.3584 | KD: 1060.0046\n",
      "Train Epoch: 006 Batch: 00066/00094 | Loss: 1060.3577 | CE: 0.3496 | KD: 1060.0081\n",
      "Train Epoch: 006 Batch: 00067/00094 | Loss: 1060.4072 | CE: 0.3929 | KD: 1060.0143\n",
      "Train Epoch: 006 Batch: 00068/00094 | Loss: 1060.3937 | CE: 0.3513 | KD: 1060.0424\n",
      "Train Epoch: 006 Batch: 00069/00094 | Loss: 1060.3629 | CE: 0.3354 | KD: 1060.0276\n",
      "Train Epoch: 006 Batch: 00070/00094 | Loss: 1060.4972 | CE: 0.3822 | KD: 1060.1150\n",
      "Train Epoch: 006 Batch: 00071/00094 | Loss: 1060.2614 | CE: 0.3028 | KD: 1059.9586\n",
      "Train Epoch: 006 Batch: 00072/00094 | Loss: 1060.5859 | CE: 0.4753 | KD: 1060.1107\n",
      "Train Epoch: 006 Batch: 00073/00094 | Loss: 1060.2118 | CE: 0.2037 | KD: 1060.0081\n",
      "Train Epoch: 006 Batch: 00074/00094 | Loss: 1060.4724 | CE: 0.4127 | KD: 1060.0597\n",
      "Train Epoch: 006 Batch: 00075/00094 | Loss: 1060.3566 | CE: 0.2932 | KD: 1060.0634\n",
      "Train Epoch: 006 Batch: 00076/00094 | Loss: 1060.3378 | CE: 0.3321 | KD: 1060.0057\n",
      "Train Epoch: 006 Batch: 00077/00094 | Loss: 1060.3531 | CE: 0.3068 | KD: 1060.0463\n",
      "Train Epoch: 006 Batch: 00078/00094 | Loss: 1060.3727 | CE: 0.3108 | KD: 1060.0619\n",
      "Train Epoch: 006 Batch: 00079/00094 | Loss: 1060.3606 | CE: 0.3214 | KD: 1060.0392\n",
      "Train Epoch: 006 Batch: 00080/00094 | Loss: 1060.3033 | CE: 0.3193 | KD: 1059.9841\n",
      "Train Epoch: 006 Batch: 00081/00094 | Loss: 1060.3505 | CE: 0.2784 | KD: 1060.0720\n",
      "Train Epoch: 006 Batch: 00082/00094 | Loss: 1060.2905 | CE: 0.2998 | KD: 1059.9907\n",
      "Train Epoch: 006 Batch: 00083/00094 | Loss: 1060.3026 | CE: 0.2960 | KD: 1060.0066\n",
      "Train Epoch: 006 Batch: 00084/00094 | Loss: 1060.3339 | CE: 0.3121 | KD: 1060.0217\n",
      "Train Epoch: 006 Batch: 00085/00094 | Loss: 1060.3591 | CE: 0.3346 | KD: 1060.0245\n",
      "Train Epoch: 006 Batch: 00086/00094 | Loss: 1060.3926 | CE: 0.3577 | KD: 1060.0349\n",
      "Train Epoch: 006 Batch: 00087/00094 | Loss: 1060.2483 | CE: 0.2977 | KD: 1059.9506\n",
      "Train Epoch: 006 Batch: 00088/00094 | Loss: 1060.2969 | CE: 0.2589 | KD: 1060.0380\n",
      "Train Epoch: 006 Batch: 00089/00094 | Loss: 1060.5063 | CE: 0.3757 | KD: 1060.1307\n",
      "Train Epoch: 006 Batch: 00090/00094 | Loss: 1060.3384 | CE: 0.3164 | KD: 1060.0220\n",
      "Train Epoch: 006 Batch: 00091/00094 | Loss: 1060.4586 | CE: 0.3818 | KD: 1060.0768\n",
      "Train Epoch: 006 Batch: 00092/00094 | Loss: 1060.3380 | CE: 0.3189 | KD: 1060.0190\n",
      "Train Epoch: 006 Batch: 00093/00094 | Loss: 1060.3477 | CE: 0.3592 | KD: 1059.9884\n",
      "Train Epoch: 006 Batch: 00094/00094 | Loss: 1060.3165 | CE: 0.2903 | KD: 1060.0262\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3228 | acc:88.6000\n",
      "[VAL Acc] Target: 88.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0584 | acc:49.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8959 | acc:51.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9293 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8829 | acc:52.3119\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7401 | acc:57.3013\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 57.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6578 | acc:62.8527\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8457 | acc:57.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4769 | acc:75.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.10%\n",
      "[VAL Acc] Avg 60.40%\n",
      "VAL Acc improve from 60.09% to 60.40%\n",
      "Save best model\n",
      "Train Epoch: 007 Batch: 00001/00094 | Loss: 1060.2942 | CE: 0.2681 | KD: 1060.0261\n",
      "Train Epoch: 007 Batch: 00002/00094 | Loss: 1060.2552 | CE: 0.3080 | KD: 1059.9473\n",
      "Train Epoch: 007 Batch: 00003/00094 | Loss: 1060.4502 | CE: 0.3637 | KD: 1060.0864\n",
      "Train Epoch: 007 Batch: 00004/00094 | Loss: 1060.3223 | CE: 0.3250 | KD: 1059.9972\n",
      "Train Epoch: 007 Batch: 00005/00094 | Loss: 1060.3802 | CE: 0.3545 | KD: 1060.0258\n",
      "Train Epoch: 007 Batch: 00006/00094 | Loss: 1060.2932 | CE: 0.3079 | KD: 1059.9852\n",
      "Train Epoch: 007 Batch: 00007/00094 | Loss: 1060.3405 | CE: 0.3513 | KD: 1059.9891\n",
      "Train Epoch: 007 Batch: 00008/00094 | Loss: 1060.3712 | CE: 0.3275 | KD: 1060.0437\n",
      "Train Epoch: 007 Batch: 00009/00094 | Loss: 1060.4292 | CE: 0.3277 | KD: 1060.1014\n",
      "Train Epoch: 007 Batch: 00010/00094 | Loss: 1060.3677 | CE: 0.3511 | KD: 1060.0166\n",
      "Train Epoch: 007 Batch: 00011/00094 | Loss: 1060.3300 | CE: 0.3146 | KD: 1060.0154\n",
      "Train Epoch: 007 Batch: 00012/00094 | Loss: 1060.2719 | CE: 0.2914 | KD: 1059.9805\n",
      "Train Epoch: 007 Batch: 00013/00094 | Loss: 1060.3445 | CE: 0.3228 | KD: 1060.0217\n",
      "Train Epoch: 007 Batch: 00014/00094 | Loss: 1060.3716 | CE: 0.3565 | KD: 1060.0151\n",
      "Train Epoch: 007 Batch: 00015/00094 | Loss: 1060.3219 | CE: 0.2643 | KD: 1060.0576\n",
      "Train Epoch: 007 Batch: 00016/00094 | Loss: 1060.3092 | CE: 0.2985 | KD: 1060.0107\n",
      "Train Epoch: 007 Batch: 00017/00094 | Loss: 1060.4617 | CE: 0.3875 | KD: 1060.0742\n",
      "Train Epoch: 007 Batch: 00018/00094 | Loss: 1060.2173 | CE: 0.2321 | KD: 1059.9852\n",
      "Train Epoch: 007 Batch: 00019/00094 | Loss: 1060.2272 | CE: 0.2559 | KD: 1059.9713\n",
      "Train Epoch: 007 Batch: 00020/00094 | Loss: 1060.2738 | CE: 0.2331 | KD: 1060.0406\n",
      "Train Epoch: 007 Batch: 00021/00094 | Loss: 1060.3208 | CE: 0.2934 | KD: 1060.0273\n",
      "Train Epoch: 007 Batch: 00022/00094 | Loss: 1060.4125 | CE: 0.3643 | KD: 1060.0481\n",
      "Train Epoch: 007 Batch: 00023/00094 | Loss: 1060.4794 | CE: 0.3818 | KD: 1060.0975\n",
      "Train Epoch: 007 Batch: 00024/00094 | Loss: 1060.4104 | CE: 0.3551 | KD: 1060.0553\n",
      "Train Epoch: 007 Batch: 00025/00094 | Loss: 1060.3892 | CE: 0.3158 | KD: 1060.0734\n",
      "Train Epoch: 007 Batch: 00026/00094 | Loss: 1060.3990 | CE: 0.2999 | KD: 1060.0991\n",
      "Train Epoch: 007 Batch: 00027/00094 | Loss: 1060.2927 | CE: 0.2836 | KD: 1060.0092\n",
      "Train Epoch: 007 Batch: 00028/00094 | Loss: 1060.3346 | CE: 0.3074 | KD: 1060.0272\n",
      "Train Epoch: 007 Batch: 00029/00094 | Loss: 1060.3278 | CE: 0.3600 | KD: 1059.9678\n",
      "Train Epoch: 007 Batch: 00030/00094 | Loss: 1060.3722 | CE: 0.3407 | KD: 1060.0315\n",
      "Train Epoch: 007 Batch: 00031/00094 | Loss: 1060.5205 | CE: 0.4119 | KD: 1060.1085\n",
      "Train Epoch: 007 Batch: 00032/00094 | Loss: 1060.4749 | CE: 0.3756 | KD: 1060.0992\n",
      "Train Epoch: 007 Batch: 00033/00094 | Loss: 1060.4045 | CE: 0.3496 | KD: 1060.0549\n",
      "Train Epoch: 007 Batch: 00034/00094 | Loss: 1060.4115 | CE: 0.3581 | KD: 1060.0535\n",
      "Train Epoch: 007 Batch: 00035/00094 | Loss: 1060.2534 | CE: 0.3187 | KD: 1059.9348\n",
      "Train Epoch: 007 Batch: 00036/00094 | Loss: 1060.3011 | CE: 0.3224 | KD: 1059.9788\n",
      "Train Epoch: 007 Batch: 00037/00094 | Loss: 1060.4408 | CE: 0.3901 | KD: 1060.0508\n",
      "Train Epoch: 007 Batch: 00038/00094 | Loss: 1060.3112 | CE: 0.2710 | KD: 1060.0402\n",
      "Train Epoch: 007 Batch: 00039/00094 | Loss: 1060.3844 | CE: 0.2994 | KD: 1060.0850\n",
      "Train Epoch: 007 Batch: 00040/00094 | Loss: 1060.4352 | CE: 0.3441 | KD: 1060.0911\n",
      "Train Epoch: 007 Batch: 00041/00094 | Loss: 1060.4861 | CE: 0.4199 | KD: 1060.0662\n",
      "Train Epoch: 007 Batch: 00042/00094 | Loss: 1060.2990 | CE: 0.2423 | KD: 1060.0566\n",
      "Train Epoch: 007 Batch: 00043/00094 | Loss: 1060.3138 | CE: 0.3761 | KD: 1059.9377\n",
      "Train Epoch: 007 Batch: 00044/00094 | Loss: 1060.3524 | CE: 0.3570 | KD: 1059.9954\n",
      "Train Epoch: 007 Batch: 00045/00094 | Loss: 1060.2533 | CE: 0.2948 | KD: 1059.9585\n",
      "Train Epoch: 007 Batch: 00046/00094 | Loss: 1060.3276 | CE: 0.3111 | KD: 1060.0166\n",
      "Train Epoch: 007 Batch: 00047/00094 | Loss: 1060.5520 | CE: 0.4664 | KD: 1060.0857\n",
      "Train Epoch: 007 Batch: 00048/00094 | Loss: 1060.4969 | CE: 0.3874 | KD: 1060.1095\n",
      "Train Epoch: 007 Batch: 00049/00094 | Loss: 1060.4292 | CE: 0.3485 | KD: 1060.0807\n",
      "Train Epoch: 007 Batch: 00050/00094 | Loss: 1060.3331 | CE: 0.3099 | KD: 1060.0233\n",
      "Train Epoch: 007 Batch: 00051/00094 | Loss: 1060.2546 | CE: 0.2860 | KD: 1059.9686\n",
      "Train Epoch: 007 Batch: 00052/00094 | Loss: 1060.4358 | CE: 0.3771 | KD: 1060.0587\n",
      "Train Epoch: 007 Batch: 00053/00094 | Loss: 1060.1764 | CE: 0.2540 | KD: 1059.9224\n",
      "Train Epoch: 007 Batch: 00054/00094 | Loss: 1060.3684 | CE: 0.3171 | KD: 1060.0513\n",
      "Train Epoch: 007 Batch: 00055/00094 | Loss: 1060.3584 | CE: 0.2785 | KD: 1060.0800\n",
      "Train Epoch: 007 Batch: 00056/00094 | Loss: 1060.2728 | CE: 0.3249 | KD: 1059.9479\n",
      "Train Epoch: 007 Batch: 00057/00094 | Loss: 1060.3812 | CE: 0.3650 | KD: 1060.0162\n",
      "Train Epoch: 007 Batch: 00058/00094 | Loss: 1060.3683 | CE: 0.3342 | KD: 1060.0341\n",
      "Train Epoch: 007 Batch: 00059/00094 | Loss: 1060.4292 | CE: 0.3646 | KD: 1060.0646\n",
      "Train Epoch: 007 Batch: 00060/00094 | Loss: 1060.3328 | CE: 0.3440 | KD: 1059.9888\n",
      "Train Epoch: 007 Batch: 00061/00094 | Loss: 1060.3499 | CE: 0.3139 | KD: 1060.0359\n",
      "Train Epoch: 007 Batch: 00062/00094 | Loss: 1060.3264 | CE: 0.3022 | KD: 1060.0243\n",
      "Train Epoch: 007 Batch: 00063/00094 | Loss: 1060.3420 | CE: 0.3058 | KD: 1060.0363\n",
      "Train Epoch: 007 Batch: 00064/00094 | Loss: 1060.3228 | CE: 0.2984 | KD: 1060.0243\n",
      "Train Epoch: 007 Batch: 00065/00094 | Loss: 1060.3340 | CE: 0.3095 | KD: 1060.0244\n",
      "Train Epoch: 007 Batch: 00066/00094 | Loss: 1060.2173 | CE: 0.2480 | KD: 1059.9692\n",
      "Train Epoch: 007 Batch: 00067/00094 | Loss: 1060.4677 | CE: 0.3909 | KD: 1060.0768\n",
      "Train Epoch: 007 Batch: 00068/00094 | Loss: 1060.3107 | CE: 0.3068 | KD: 1060.0038\n",
      "Train Epoch: 007 Batch: 00069/00094 | Loss: 1060.1926 | CE: 0.2145 | KD: 1059.9781\n",
      "Train Epoch: 007 Batch: 00070/00094 | Loss: 1060.2743 | CE: 0.2844 | KD: 1059.9899\n",
      "Train Epoch: 007 Batch: 00071/00094 | Loss: 1060.3345 | CE: 0.3081 | KD: 1060.0264\n",
      "Train Epoch: 007 Batch: 00072/00094 | Loss: 1060.2811 | CE: 0.3056 | KD: 1059.9756\n",
      "Train Epoch: 007 Batch: 00073/00094 | Loss: 1060.3031 | CE: 0.3046 | KD: 1059.9984\n",
      "Train Epoch: 007 Batch: 00074/00094 | Loss: 1060.3063 | CE: 0.3163 | KD: 1059.9900\n",
      "Train Epoch: 007 Batch: 00075/00094 | Loss: 1060.2391 | CE: 0.2613 | KD: 1059.9778\n",
      "Train Epoch: 007 Batch: 00076/00094 | Loss: 1060.2871 | CE: 0.2621 | KD: 1060.0250\n",
      "Train Epoch: 007 Batch: 00077/00094 | Loss: 1060.3425 | CE: 0.3496 | KD: 1059.9929\n",
      "Train Epoch: 007 Batch: 00078/00094 | Loss: 1060.4513 | CE: 0.3933 | KD: 1060.0580\n",
      "Train Epoch: 007 Batch: 00079/00094 | Loss: 1060.1710 | CE: 0.2263 | KD: 1059.9447\n",
      "Train Epoch: 007 Batch: 00080/00094 | Loss: 1060.3662 | CE: 0.3397 | KD: 1060.0265\n",
      "Train Epoch: 007 Batch: 00081/00094 | Loss: 1060.2395 | CE: 0.2725 | KD: 1059.9670\n",
      "Train Epoch: 007 Batch: 00082/00094 | Loss: 1060.3276 | CE: 0.3092 | KD: 1060.0184\n",
      "Train Epoch: 007 Batch: 00083/00094 | Loss: 1060.5664 | CE: 0.4756 | KD: 1060.0908\n",
      "Train Epoch: 007 Batch: 00084/00094 | Loss: 1060.2809 | CE: 0.3180 | KD: 1059.9629\n",
      "Train Epoch: 007 Batch: 00085/00094 | Loss: 1060.3793 | CE: 0.3521 | KD: 1060.0272\n",
      "Train Epoch: 007 Batch: 00086/00094 | Loss: 1060.3363 | CE: 0.3526 | KD: 1059.9836\n",
      "Train Epoch: 007 Batch: 00087/00094 | Loss: 1060.3627 | CE: 0.3106 | KD: 1060.0521\n",
      "Train Epoch: 007 Batch: 00088/00094 | Loss: 1060.5948 | CE: 0.3939 | KD: 1060.2009\n",
      "Train Epoch: 007 Batch: 00089/00094 | Loss: 1060.4126 | CE: 0.3668 | KD: 1060.0459\n",
      "Train Epoch: 007 Batch: 00090/00094 | Loss: 1060.3267 | CE: 0.3512 | KD: 1059.9755\n",
      "Train Epoch: 007 Batch: 00091/00094 | Loss: 1060.3364 | CE: 0.3572 | KD: 1059.9792\n",
      "Train Epoch: 007 Batch: 00092/00094 | Loss: 1060.2694 | CE: 0.2922 | KD: 1059.9773\n",
      "Train Epoch: 007 Batch: 00093/00094 | Loss: 1060.2970 | CE: 0.3126 | KD: 1059.9844\n",
      "Train Epoch: 007 Batch: 00094/00094 | Loss: 1060.3416 | CE: 0.3140 | KD: 1060.0275\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3331 | acc:87.3000\n",
      "[VAL Acc] Target: 87.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0869 | acc:50.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9636 | acc:48.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9414 | acc:48.2824\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8425 | acc:54.3495\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8807 | acc:54.0665\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6081 | acc:65.2821\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9391 | acc:56.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4579 | acc:75.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.40%\n",
      "[VAL Acc] Avg 60.01%\n",
      "Train Epoch: 008 Batch: 00001/00094 | Loss: 1060.3329 | CE: 0.2889 | KD: 1060.0441\n",
      "Train Epoch: 008 Batch: 00002/00094 | Loss: 1060.5514 | CE: 0.4315 | KD: 1060.1199\n",
      "Train Epoch: 008 Batch: 00003/00094 | Loss: 1060.3252 | CE: 0.3119 | KD: 1060.0133\n",
      "Train Epoch: 008 Batch: 00004/00094 | Loss: 1060.3419 | CE: 0.3343 | KD: 1060.0076\n",
      "Train Epoch: 008 Batch: 00005/00094 | Loss: 1060.3416 | CE: 0.2938 | KD: 1060.0477\n",
      "Train Epoch: 008 Batch: 00006/00094 | Loss: 1060.3350 | CE: 0.2954 | KD: 1060.0396\n",
      "Train Epoch: 008 Batch: 00007/00094 | Loss: 1060.2874 | CE: 0.2900 | KD: 1059.9973\n",
      "Train Epoch: 008 Batch: 00008/00094 | Loss: 1060.3472 | CE: 0.3441 | KD: 1060.0031\n",
      "Train Epoch: 008 Batch: 00009/00094 | Loss: 1060.3385 | CE: 0.3489 | KD: 1059.9896\n",
      "Train Epoch: 008 Batch: 00010/00094 | Loss: 1060.4064 | CE: 0.3753 | KD: 1060.0311\n",
      "Train Epoch: 008 Batch: 00011/00094 | Loss: 1060.3651 | CE: 0.3333 | KD: 1060.0319\n",
      "Train Epoch: 008 Batch: 00012/00094 | Loss: 1060.3407 | CE: 0.3410 | KD: 1059.9996\n",
      "Train Epoch: 008 Batch: 00013/00094 | Loss: 1060.2458 | CE: 0.2821 | KD: 1059.9637\n",
      "Train Epoch: 008 Batch: 00014/00094 | Loss: 1060.3739 | CE: 0.3323 | KD: 1060.0416\n",
      "Train Epoch: 008 Batch: 00015/00094 | Loss: 1060.2792 | CE: 0.3219 | KD: 1059.9573\n",
      "Train Epoch: 008 Batch: 00016/00094 | Loss: 1060.4359 | CE: 0.3849 | KD: 1060.0510\n",
      "Train Epoch: 008 Batch: 00017/00094 | Loss: 1060.2422 | CE: 0.2915 | KD: 1059.9507\n",
      "Train Epoch: 008 Batch: 00018/00094 | Loss: 1060.3531 | CE: 0.3118 | KD: 1060.0414\n",
      "Train Epoch: 008 Batch: 00019/00094 | Loss: 1060.2002 | CE: 0.2399 | KD: 1059.9603\n",
      "Train Epoch: 008 Batch: 00020/00094 | Loss: 1060.2991 | CE: 0.2967 | KD: 1060.0023\n",
      "Train Epoch: 008 Batch: 00021/00094 | Loss: 1060.2841 | CE: 0.3019 | KD: 1059.9822\n",
      "Train Epoch: 008 Batch: 00022/00094 | Loss: 1060.2751 | CE: 0.2718 | KD: 1060.0033\n",
      "Train Epoch: 008 Batch: 00023/00094 | Loss: 1060.3463 | CE: 0.3376 | KD: 1060.0087\n",
      "Train Epoch: 008 Batch: 00024/00094 | Loss: 1060.2627 | CE: 0.2839 | KD: 1059.9789\n",
      "Train Epoch: 008 Batch: 00025/00094 | Loss: 1060.6166 | CE: 0.4699 | KD: 1060.1466\n",
      "Train Epoch: 008 Batch: 00026/00094 | Loss: 1060.4657 | CE: 0.3920 | KD: 1060.0737\n",
      "Train Epoch: 008 Batch: 00027/00094 | Loss: 1060.2904 | CE: 0.3025 | KD: 1059.9879\n",
      "Train Epoch: 008 Batch: 00028/00094 | Loss: 1060.2822 | CE: 0.2868 | KD: 1059.9955\n",
      "Train Epoch: 008 Batch: 00029/00094 | Loss: 1060.2649 | CE: 0.2959 | KD: 1059.9690\n",
      "Train Epoch: 008 Batch: 00030/00094 | Loss: 1060.2761 | CE: 0.3086 | KD: 1059.9675\n",
      "Train Epoch: 008 Batch: 00031/00094 | Loss: 1060.3101 | CE: 0.3443 | KD: 1059.9658\n",
      "Train Epoch: 008 Batch: 00032/00094 | Loss: 1060.3293 | CE: 0.2807 | KD: 1060.0487\n",
      "Train Epoch: 008 Batch: 00033/00094 | Loss: 1060.3342 | CE: 0.3012 | KD: 1060.0331\n",
      "Train Epoch: 008 Batch: 00034/00094 | Loss: 1060.3651 | CE: 0.3307 | KD: 1060.0344\n",
      "Train Epoch: 008 Batch: 00035/00094 | Loss: 1060.5583 | CE: 0.4287 | KD: 1060.1296\n",
      "Train Epoch: 008 Batch: 00036/00094 | Loss: 1060.3794 | CE: 0.3639 | KD: 1060.0155\n",
      "Train Epoch: 008 Batch: 00037/00094 | Loss: 1060.4299 | CE: 0.3401 | KD: 1060.0898\n",
      "Train Epoch: 008 Batch: 00038/00094 | Loss: 1060.2920 | CE: 0.2767 | KD: 1060.0153\n",
      "Train Epoch: 008 Batch: 00039/00094 | Loss: 1060.4908 | CE: 0.4250 | KD: 1060.0658\n",
      "Train Epoch: 008 Batch: 00040/00094 | Loss: 1060.3759 | CE: 0.3299 | KD: 1060.0459\n",
      "Train Epoch: 008 Batch: 00041/00094 | Loss: 1060.3605 | CE: 0.3054 | KD: 1060.0551\n",
      "Train Epoch: 008 Batch: 00042/00094 | Loss: 1060.3031 | CE: 0.2997 | KD: 1060.0033\n",
      "Train Epoch: 008 Batch: 00043/00094 | Loss: 1060.2645 | CE: 0.2814 | KD: 1059.9832\n",
      "Train Epoch: 008 Batch: 00044/00094 | Loss: 1060.3677 | CE: 0.3373 | KD: 1060.0303\n",
      "Train Epoch: 008 Batch: 00045/00094 | Loss: 1060.2794 | CE: 0.3045 | KD: 1059.9750\n",
      "Train Epoch: 008 Batch: 00046/00094 | Loss: 1060.3057 | CE: 0.3335 | KD: 1059.9722\n",
      "Train Epoch: 008 Batch: 00047/00094 | Loss: 1060.1843 | CE: 0.2520 | KD: 1059.9324\n",
      "Train Epoch: 008 Batch: 00048/00094 | Loss: 1060.3491 | CE: 0.3221 | KD: 1060.0270\n",
      "Train Epoch: 008 Batch: 00049/00094 | Loss: 1060.4572 | CE: 0.4217 | KD: 1060.0354\n",
      "Train Epoch: 008 Batch: 00050/00094 | Loss: 1060.3986 | CE: 0.3668 | KD: 1060.0317\n",
      "Train Epoch: 008 Batch: 00051/00094 | Loss: 1060.3031 | CE: 0.3066 | KD: 1059.9966\n",
      "Train Epoch: 008 Batch: 00052/00094 | Loss: 1060.2368 | CE: 0.2783 | KD: 1059.9585\n",
      "Train Epoch: 008 Batch: 00053/00094 | Loss: 1060.4625 | CE: 0.3473 | KD: 1060.1152\n",
      "Train Epoch: 008 Batch: 00054/00094 | Loss: 1060.3900 | CE: 0.3463 | KD: 1060.0437\n",
      "Train Epoch: 008 Batch: 00055/00094 | Loss: 1060.3481 | CE: 0.3675 | KD: 1059.9807\n",
      "Train Epoch: 008 Batch: 00056/00094 | Loss: 1060.3840 | CE: 0.2797 | KD: 1060.1044\n",
      "Train Epoch: 008 Batch: 00057/00094 | Loss: 1060.3636 | CE: 0.3363 | KD: 1060.0273\n",
      "Train Epoch: 008 Batch: 00058/00094 | Loss: 1060.3467 | CE: 0.3060 | KD: 1060.0406\n",
      "Train Epoch: 008 Batch: 00059/00094 | Loss: 1060.2631 | CE: 0.2771 | KD: 1059.9860\n",
      "Train Epoch: 008 Batch: 00060/00094 | Loss: 1060.2632 | CE: 0.2528 | KD: 1060.0104\n",
      "Train Epoch: 008 Batch: 00061/00094 | Loss: 1060.3420 | CE: 0.3133 | KD: 1060.0287\n",
      "Train Epoch: 008 Batch: 00062/00094 | Loss: 1060.2811 | CE: 0.2380 | KD: 1060.0431\n",
      "Train Epoch: 008 Batch: 00063/00094 | Loss: 1060.4337 | CE: 0.3448 | KD: 1060.0889\n",
      "Train Epoch: 008 Batch: 00064/00094 | Loss: 1060.2894 | CE: 0.3085 | KD: 1059.9810\n",
      "Train Epoch: 008 Batch: 00065/00094 | Loss: 1060.3164 | CE: 0.3363 | KD: 1059.9801\n",
      "Train Epoch: 008 Batch: 00066/00094 | Loss: 1060.2147 | CE: 0.2705 | KD: 1059.9442\n",
      "Train Epoch: 008 Batch: 00067/00094 | Loss: 1060.3403 | CE: 0.3248 | KD: 1060.0156\n",
      "Train Epoch: 008 Batch: 00068/00094 | Loss: 1060.2861 | CE: 0.2773 | KD: 1060.0088\n",
      "Train Epoch: 008 Batch: 00069/00094 | Loss: 1060.3146 | CE: 0.2828 | KD: 1060.0317\n",
      "Train Epoch: 008 Batch: 00070/00094 | Loss: 1060.4230 | CE: 0.3110 | KD: 1060.1119\n",
      "Train Epoch: 008 Batch: 00071/00094 | Loss: 1060.3379 | CE: 0.3288 | KD: 1060.0092\n",
      "Train Epoch: 008 Batch: 00072/00094 | Loss: 1060.3400 | CE: 0.3133 | KD: 1060.0267\n",
      "Train Epoch: 008 Batch: 00073/00094 | Loss: 1060.2627 | CE: 0.2860 | KD: 1059.9767\n",
      "Train Epoch: 008 Batch: 00074/00094 | Loss: 1060.3346 | CE: 0.3180 | KD: 1060.0166\n",
      "Train Epoch: 008 Batch: 00075/00094 | Loss: 1060.2811 | CE: 0.2595 | KD: 1060.0216\n",
      "Train Epoch: 008 Batch: 00076/00094 | Loss: 1060.2871 | CE: 0.2877 | KD: 1059.9995\n",
      "Train Epoch: 008 Batch: 00077/00094 | Loss: 1060.3203 | CE: 0.3000 | KD: 1060.0203\n",
      "Train Epoch: 008 Batch: 00078/00094 | Loss: 1060.5010 | CE: 0.3666 | KD: 1060.1344\n",
      "Train Epoch: 008 Batch: 00079/00094 | Loss: 1060.3870 | CE: 0.3078 | KD: 1060.0791\n",
      "Train Epoch: 008 Batch: 00080/00094 | Loss: 1060.3105 | CE: 0.2776 | KD: 1060.0330\n",
      "Train Epoch: 008 Batch: 00081/00094 | Loss: 1060.2994 | CE: 0.2579 | KD: 1060.0415\n",
      "Train Epoch: 008 Batch: 00082/00094 | Loss: 1060.1738 | CE: 0.2294 | KD: 1059.9445\n",
      "Train Epoch: 008 Batch: 00083/00094 | Loss: 1060.4214 | CE: 0.3047 | KD: 1060.1167\n",
      "Train Epoch: 008 Batch: 00084/00094 | Loss: 1060.4370 | CE: 0.3396 | KD: 1060.0974\n",
      "Train Epoch: 008 Batch: 00085/00094 | Loss: 1060.3842 | CE: 0.3041 | KD: 1060.0800\n",
      "Train Epoch: 008 Batch: 00086/00094 | Loss: 1060.3018 | CE: 0.3253 | KD: 1059.9764\n",
      "Train Epoch: 008 Batch: 00087/00094 | Loss: 1060.4506 | CE: 0.3561 | KD: 1060.0945\n",
      "Train Epoch: 008 Batch: 00088/00094 | Loss: 1060.3300 | CE: 0.3280 | KD: 1060.0020\n",
      "Train Epoch: 008 Batch: 00089/00094 | Loss: 1060.2473 | CE: 0.2621 | KD: 1059.9852\n",
      "Train Epoch: 008 Batch: 00090/00094 | Loss: 1060.3047 | CE: 0.2945 | KD: 1060.0103\n",
      "Train Epoch: 008 Batch: 00091/00094 | Loss: 1060.3572 | CE: 0.2988 | KD: 1060.0583\n",
      "Train Epoch: 008 Batch: 00092/00094 | Loss: 1060.3939 | CE: 0.3370 | KD: 1060.0569\n",
      "Train Epoch: 008 Batch: 00093/00094 | Loss: 1060.3584 | CE: 0.3457 | KD: 1060.0127\n",
      "Train Epoch: 008 Batch: 00094/00094 | Loss: 1060.2002 | CE: 0.2474 | KD: 1059.9528\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3263 | acc:88.9000\n",
      "[VAL Acc] Target: 88.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0700 | acc:49.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9435 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9073 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8656 | acc:53.1348\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8713 | acc:54.2514\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6558 | acc:61.9906\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9308 | acc:55.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4461 | acc:77.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 77.60%\n",
      "[VAL Acc] Avg 59.81%\n",
      "Train Epoch: 009 Batch: 00001/00094 | Loss: 1060.3030 | CE: 0.3028 | KD: 1060.0002\n",
      "Train Epoch: 009 Batch: 00002/00094 | Loss: 1060.4539 | CE: 0.3957 | KD: 1060.0582\n",
      "Train Epoch: 009 Batch: 00003/00094 | Loss: 1060.5348 | CE: 0.4678 | KD: 1060.0669\n",
      "Train Epoch: 009 Batch: 00004/00094 | Loss: 1060.2812 | CE: 0.3092 | KD: 1059.9720\n",
      "Train Epoch: 009 Batch: 00005/00094 | Loss: 1060.2487 | CE: 0.2655 | KD: 1059.9832\n",
      "Train Epoch: 009 Batch: 00006/00094 | Loss: 1060.2576 | CE: 0.2843 | KD: 1059.9733\n",
      "Train Epoch: 009 Batch: 00007/00094 | Loss: 1060.3256 | CE: 0.3283 | KD: 1059.9972\n",
      "Train Epoch: 009 Batch: 00008/00094 | Loss: 1060.3119 | CE: 0.3002 | KD: 1060.0117\n",
      "Train Epoch: 009 Batch: 00009/00094 | Loss: 1060.2723 | CE: 0.2755 | KD: 1059.9968\n",
      "Train Epoch: 009 Batch: 00010/00094 | Loss: 1060.2455 | CE: 0.2656 | KD: 1059.9799\n",
      "Train Epoch: 009 Batch: 00011/00094 | Loss: 1060.2896 | CE: 0.3226 | KD: 1059.9669\n",
      "Train Epoch: 009 Batch: 00012/00094 | Loss: 1060.2584 | CE: 0.2747 | KD: 1059.9838\n",
      "Train Epoch: 009 Batch: 00013/00094 | Loss: 1060.3198 | CE: 0.3002 | KD: 1060.0197\n",
      "Train Epoch: 009 Batch: 00014/00094 | Loss: 1060.3262 | CE: 0.3144 | KD: 1060.0117\n",
      "Train Epoch: 009 Batch: 00015/00094 | Loss: 1060.4470 | CE: 0.3674 | KD: 1060.0797\n",
      "Train Epoch: 009 Batch: 00016/00094 | Loss: 1060.2145 | CE: 0.2200 | KD: 1059.9945\n",
      "Train Epoch: 009 Batch: 00017/00094 | Loss: 1060.2697 | CE: 0.2412 | KD: 1060.0284\n",
      "Train Epoch: 009 Batch: 00018/00094 | Loss: 1060.4084 | CE: 0.3645 | KD: 1060.0439\n",
      "Train Epoch: 009 Batch: 00019/00094 | Loss: 1060.2955 | CE: 0.2716 | KD: 1060.0239\n",
      "Train Epoch: 009 Batch: 00020/00094 | Loss: 1060.2506 | CE: 0.3030 | KD: 1059.9476\n",
      "Train Epoch: 009 Batch: 00021/00094 | Loss: 1060.4222 | CE: 0.3412 | KD: 1060.0811\n",
      "Train Epoch: 009 Batch: 00022/00094 | Loss: 1060.2111 | CE: 0.2265 | KD: 1059.9846\n",
      "Train Epoch: 009 Batch: 00023/00094 | Loss: 1060.2432 | CE: 0.2709 | KD: 1059.9723\n",
      "Train Epoch: 009 Batch: 00024/00094 | Loss: 1060.3116 | CE: 0.3015 | KD: 1060.0101\n",
      "Train Epoch: 009 Batch: 00025/00094 | Loss: 1060.3312 | CE: 0.3520 | KD: 1059.9792\n",
      "Train Epoch: 009 Batch: 00026/00094 | Loss: 1060.3032 | CE: 0.2509 | KD: 1060.0522\n",
      "Train Epoch: 009 Batch: 00027/00094 | Loss: 1060.2581 | CE: 0.3092 | KD: 1059.9489\n",
      "Train Epoch: 009 Batch: 00028/00094 | Loss: 1060.3948 | CE: 0.2881 | KD: 1060.1067\n",
      "Train Epoch: 009 Batch: 00029/00094 | Loss: 1060.3929 | CE: 0.3314 | KD: 1060.0615\n",
      "Train Epoch: 009 Batch: 00030/00094 | Loss: 1060.4160 | CE: 0.3467 | KD: 1060.0693\n",
      "Train Epoch: 009 Batch: 00031/00094 | Loss: 1060.1851 | CE: 0.2475 | KD: 1059.9376\n",
      "Train Epoch: 009 Batch: 00032/00094 | Loss: 1060.4868 | CE: 0.3981 | KD: 1060.0887\n",
      "Train Epoch: 009 Batch: 00033/00094 | Loss: 1060.3323 | CE: 0.3101 | KD: 1060.0222\n",
      "Train Epoch: 009 Batch: 00034/00094 | Loss: 1060.4287 | CE: 0.2975 | KD: 1060.1312\n",
      "Train Epoch: 009 Batch: 00035/00094 | Loss: 1060.4360 | CE: 0.3851 | KD: 1060.0509\n",
      "Train Epoch: 009 Batch: 00036/00094 | Loss: 1060.4327 | CE: 0.3469 | KD: 1060.0858\n",
      "Train Epoch: 009 Batch: 00037/00094 | Loss: 1060.3379 | CE: 0.3002 | KD: 1060.0377\n",
      "Train Epoch: 009 Batch: 00038/00094 | Loss: 1060.2485 | CE: 0.2666 | KD: 1059.9819\n",
      "Train Epoch: 009 Batch: 00039/00094 | Loss: 1060.2404 | CE: 0.2816 | KD: 1059.9587\n",
      "Train Epoch: 009 Batch: 00040/00094 | Loss: 1060.2588 | CE: 0.2847 | KD: 1059.9741\n",
      "Train Epoch: 009 Batch: 00041/00094 | Loss: 1060.3657 | CE: 0.3538 | KD: 1060.0120\n",
      "Train Epoch: 009 Batch: 00042/00094 | Loss: 1060.4370 | CE: 0.3799 | KD: 1060.0571\n",
      "Train Epoch: 009 Batch: 00043/00094 | Loss: 1060.3380 | CE: 0.3017 | KD: 1060.0364\n",
      "Train Epoch: 009 Batch: 00044/00094 | Loss: 1060.2937 | CE: 0.3084 | KD: 1059.9854\n",
      "Train Epoch: 009 Batch: 00045/00094 | Loss: 1060.4236 | CE: 0.3450 | KD: 1060.0785\n",
      "Train Epoch: 009 Batch: 00046/00094 | Loss: 1060.2676 | CE: 0.2741 | KD: 1059.9935\n",
      "Train Epoch: 009 Batch: 00047/00094 | Loss: 1060.3076 | CE: 0.2903 | KD: 1060.0173\n",
      "Train Epoch: 009 Batch: 00048/00094 | Loss: 1060.2793 | CE: 0.2993 | KD: 1059.9800\n",
      "Train Epoch: 009 Batch: 00049/00094 | Loss: 1060.3611 | CE: 0.3252 | KD: 1060.0359\n",
      "Train Epoch: 009 Batch: 00050/00094 | Loss: 1060.4902 | CE: 0.4349 | KD: 1060.0553\n",
      "Train Epoch: 009 Batch: 00051/00094 | Loss: 1060.1871 | CE: 0.2452 | KD: 1059.9419\n",
      "Train Epoch: 009 Batch: 00052/00094 | Loss: 1060.3184 | CE: 0.2958 | KD: 1060.0226\n",
      "Train Epoch: 009 Batch: 00053/00094 | Loss: 1060.3065 | CE: 0.2999 | KD: 1060.0066\n",
      "Train Epoch: 009 Batch: 00054/00094 | Loss: 1060.4648 | CE: 0.3795 | KD: 1060.0853\n",
      "Train Epoch: 009 Batch: 00055/00094 | Loss: 1060.4094 | CE: 0.3825 | KD: 1060.0269\n",
      "Train Epoch: 009 Batch: 00056/00094 | Loss: 1060.4675 | CE: 0.3523 | KD: 1060.1152\n",
      "Train Epoch: 009 Batch: 00057/00094 | Loss: 1060.2645 | CE: 0.2675 | KD: 1059.9971\n",
      "Train Epoch: 009 Batch: 00058/00094 | Loss: 1060.1965 | CE: 0.2238 | KD: 1059.9727\n",
      "Train Epoch: 009 Batch: 00059/00094 | Loss: 1060.2263 | CE: 0.2581 | KD: 1059.9681\n",
      "Train Epoch: 009 Batch: 00060/00094 | Loss: 1060.2495 | CE: 0.2461 | KD: 1060.0034\n",
      "Train Epoch: 009 Batch: 00061/00094 | Loss: 1060.3568 | CE: 0.3216 | KD: 1060.0353\n",
      "Train Epoch: 009 Batch: 00062/00094 | Loss: 1060.4305 | CE: 0.3700 | KD: 1060.0605\n",
      "Train Epoch: 009 Batch: 00063/00094 | Loss: 1060.4541 | CE: 0.3618 | KD: 1060.0923\n",
      "Train Epoch: 009 Batch: 00064/00094 | Loss: 1060.4519 | CE: 0.3153 | KD: 1060.1366\n",
      "Train Epoch: 009 Batch: 00065/00094 | Loss: 1060.3297 | CE: 0.3206 | KD: 1060.0090\n",
      "Train Epoch: 009 Batch: 00066/00094 | Loss: 1060.3811 | CE: 0.3526 | KD: 1060.0284\n",
      "Train Epoch: 009 Batch: 00067/00094 | Loss: 1060.3406 | CE: 0.3075 | KD: 1060.0331\n",
      "Train Epoch: 009 Batch: 00068/00094 | Loss: 1060.3735 | CE: 0.3477 | KD: 1060.0259\n",
      "Train Epoch: 009 Batch: 00069/00094 | Loss: 1060.3640 | CE: 0.3421 | KD: 1060.0219\n",
      "Train Epoch: 009 Batch: 00070/00094 | Loss: 1060.4196 | CE: 0.3735 | KD: 1060.0460\n",
      "Train Epoch: 009 Batch: 00071/00094 | Loss: 1060.3334 | CE: 0.3250 | KD: 1060.0083\n",
      "Train Epoch: 009 Batch: 00072/00094 | Loss: 1060.3306 | CE: 0.2990 | KD: 1060.0316\n",
      "Train Epoch: 009 Batch: 00073/00094 | Loss: 1060.4060 | CE: 0.3811 | KD: 1060.0249\n",
      "Train Epoch: 009 Batch: 00074/00094 | Loss: 1060.2737 | CE: 0.2613 | KD: 1060.0125\n",
      "Train Epoch: 009 Batch: 00075/00094 | Loss: 1060.2314 | CE: 0.2390 | KD: 1059.9924\n",
      "Train Epoch: 009 Batch: 00076/00094 | Loss: 1060.3694 | CE: 0.3312 | KD: 1060.0382\n",
      "Train Epoch: 009 Batch: 00077/00094 | Loss: 1060.4269 | CE: 0.3393 | KD: 1060.0876\n",
      "Train Epoch: 009 Batch: 00078/00094 | Loss: 1060.3977 | CE: 0.3666 | KD: 1060.0311\n",
      "Train Epoch: 009 Batch: 00079/00094 | Loss: 1060.2965 | CE: 0.3004 | KD: 1059.9961\n",
      "Train Epoch: 009 Batch: 00080/00094 | Loss: 1060.2931 | CE: 0.2859 | KD: 1060.0072\n",
      "Train Epoch: 009 Batch: 00081/00094 | Loss: 1060.3900 | CE: 0.3072 | KD: 1060.0828\n",
      "Train Epoch: 009 Batch: 00082/00094 | Loss: 1060.4707 | CE: 0.3821 | KD: 1060.0886\n",
      "Train Epoch: 009 Batch: 00083/00094 | Loss: 1060.3827 | CE: 0.3752 | KD: 1060.0074\n",
      "Train Epoch: 009 Batch: 00084/00094 | Loss: 1060.3011 | CE: 0.2813 | KD: 1060.0199\n",
      "Train Epoch: 009 Batch: 00085/00094 | Loss: 1060.2568 | CE: 0.2664 | KD: 1059.9905\n",
      "Train Epoch: 009 Batch: 00086/00094 | Loss: 1060.3801 | CE: 0.3435 | KD: 1060.0366\n",
      "Train Epoch: 009 Batch: 00087/00094 | Loss: 1060.2704 | CE: 0.3009 | KD: 1059.9695\n",
      "Train Epoch: 009 Batch: 00088/00094 | Loss: 1060.4735 | CE: 0.3968 | KD: 1060.0767\n",
      "Train Epoch: 009 Batch: 00089/00094 | Loss: 1060.3906 | CE: 0.3234 | KD: 1060.0673\n",
      "Train Epoch: 009 Batch: 00090/00094 | Loss: 1060.4950 | CE: 0.4138 | KD: 1060.0812\n",
      "Train Epoch: 009 Batch: 00091/00094 | Loss: 1060.4509 | CE: 0.3355 | KD: 1060.1155\n",
      "Train Epoch: 009 Batch: 00092/00094 | Loss: 1060.3801 | CE: 0.3349 | KD: 1060.0453\n",
      "Train Epoch: 009 Batch: 00093/00094 | Loss: 1060.2552 | CE: 0.2921 | KD: 1059.9631\n",
      "Train Epoch: 009 Batch: 00094/00094 | Loss: 1060.3634 | CE: 0.3078 | KD: 1060.0557\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3283 | acc:86.5500\n",
      "[VAL Acc] Target: 86.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1048 | acc:51.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9527 | acc:48.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9464 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8934 | acc:52.8213\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9100 | acc:53.3272\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6647 | acc:61.7163\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9400 | acc:55.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4855 | acc:73.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.75%\n",
      "[VAL Acc] Avg 59.12%\n",
      "Train Epoch: 010 Batch: 00001/00094 | Loss: 1060.5121 | CE: 0.4219 | KD: 1060.0901\n",
      "Train Epoch: 010 Batch: 00002/00094 | Loss: 1060.3938 | CE: 0.3480 | KD: 1060.0458\n",
      "Train Epoch: 010 Batch: 00003/00094 | Loss: 1060.2047 | CE: 0.2707 | KD: 1059.9340\n",
      "Train Epoch: 010 Batch: 00004/00094 | Loss: 1060.3175 | CE: 0.3337 | KD: 1059.9839\n",
      "Train Epoch: 010 Batch: 00005/00094 | Loss: 1060.5210 | CE: 0.4338 | KD: 1060.0873\n",
      "Train Epoch: 010 Batch: 00006/00094 | Loss: 1060.4148 | CE: 0.3681 | KD: 1060.0468\n",
      "Train Epoch: 010 Batch: 00007/00094 | Loss: 1060.4117 | CE: 0.2985 | KD: 1060.1133\n",
      "Train Epoch: 010 Batch: 00008/00094 | Loss: 1060.2816 | CE: 0.3028 | KD: 1059.9789\n",
      "Train Epoch: 010 Batch: 00009/00094 | Loss: 1060.5002 | CE: 0.4043 | KD: 1060.0959\n",
      "Train Epoch: 010 Batch: 00010/00094 | Loss: 1060.3823 | CE: 0.3515 | KD: 1060.0308\n",
      "Train Epoch: 010 Batch: 00011/00094 | Loss: 1060.2609 | CE: 0.2932 | KD: 1059.9677\n",
      "Train Epoch: 010 Batch: 00012/00094 | Loss: 1060.4054 | CE: 0.3512 | KD: 1060.0542\n",
      "Train Epoch: 010 Batch: 00013/00094 | Loss: 1060.2498 | CE: 0.2863 | KD: 1059.9635\n",
      "Train Epoch: 010 Batch: 00014/00094 | Loss: 1060.3447 | CE: 0.2935 | KD: 1060.0513\n",
      "Train Epoch: 010 Batch: 00015/00094 | Loss: 1060.3450 | CE: 0.3501 | KD: 1059.9949\n",
      "Train Epoch: 010 Batch: 00016/00094 | Loss: 1060.3342 | CE: 0.2786 | KD: 1060.0557\n",
      "Train Epoch: 010 Batch: 00017/00094 | Loss: 1060.3489 | CE: 0.3572 | KD: 1059.9916\n",
      "Train Epoch: 010 Batch: 00018/00094 | Loss: 1060.3400 | CE: 0.3367 | KD: 1060.0033\n",
      "Train Epoch: 010 Batch: 00019/00094 | Loss: 1060.3510 | CE: 0.2996 | KD: 1060.0514\n",
      "Train Epoch: 010 Batch: 00020/00094 | Loss: 1060.3236 | CE: 0.3432 | KD: 1059.9805\n",
      "Train Epoch: 010 Batch: 00021/00094 | Loss: 1060.3549 | CE: 0.3250 | KD: 1060.0299\n",
      "Train Epoch: 010 Batch: 00022/00094 | Loss: 1060.2441 | CE: 0.3130 | KD: 1059.9312\n",
      "Train Epoch: 010 Batch: 00023/00094 | Loss: 1060.3601 | CE: 0.3419 | KD: 1060.0182\n",
      "Train Epoch: 010 Batch: 00024/00094 | Loss: 1060.3268 | CE: 0.2847 | KD: 1060.0421\n",
      "Train Epoch: 010 Batch: 00025/00094 | Loss: 1060.3347 | CE: 0.3237 | KD: 1060.0110\n",
      "Train Epoch: 010 Batch: 00026/00094 | Loss: 1060.4218 | CE: 0.3673 | KD: 1060.0544\n",
      "Train Epoch: 010 Batch: 00027/00094 | Loss: 1060.2804 | CE: 0.2665 | KD: 1060.0139\n",
      "Train Epoch: 010 Batch: 00028/00094 | Loss: 1060.2770 | CE: 0.2926 | KD: 1059.9844\n",
      "Train Epoch: 010 Batch: 00029/00094 | Loss: 1060.4498 | CE: 0.3585 | KD: 1060.0913\n",
      "Train Epoch: 010 Batch: 00030/00094 | Loss: 1060.4669 | CE: 0.3496 | KD: 1060.1173\n",
      "Train Epoch: 010 Batch: 00031/00094 | Loss: 1060.2753 | CE: 0.2972 | KD: 1059.9781\n",
      "Train Epoch: 010 Batch: 00032/00094 | Loss: 1060.3223 | CE: 0.3072 | KD: 1060.0150\n",
      "Train Epoch: 010 Batch: 00033/00094 | Loss: 1060.3029 | CE: 0.3042 | KD: 1059.9987\n",
      "Train Epoch: 010 Batch: 00034/00094 | Loss: 1060.3962 | CE: 0.3730 | KD: 1060.0232\n",
      "Train Epoch: 010 Batch: 00035/00094 | Loss: 1060.3832 | CE: 0.3346 | KD: 1060.0486\n",
      "Train Epoch: 010 Batch: 00036/00094 | Loss: 1060.3761 | CE: 0.3367 | KD: 1060.0394\n",
      "Train Epoch: 010 Batch: 00037/00094 | Loss: 1060.3633 | CE: 0.3594 | KD: 1060.0039\n",
      "Train Epoch: 010 Batch: 00038/00094 | Loss: 1060.3075 | CE: 0.3213 | KD: 1059.9862\n",
      "Train Epoch: 010 Batch: 00039/00094 | Loss: 1060.3282 | CE: 0.2829 | KD: 1060.0453\n",
      "Train Epoch: 010 Batch: 00040/00094 | Loss: 1060.3020 | CE: 0.3172 | KD: 1059.9847\n",
      "Train Epoch: 010 Batch: 00041/00094 | Loss: 1060.3381 | CE: 0.2719 | KD: 1060.0662\n",
      "Train Epoch: 010 Batch: 00042/00094 | Loss: 1060.3350 | CE: 0.3127 | KD: 1060.0222\n",
      "Train Epoch: 010 Batch: 00043/00094 | Loss: 1060.3766 | CE: 0.3472 | KD: 1060.0293\n",
      "Train Epoch: 010 Batch: 00044/00094 | Loss: 1060.3770 | CE: 0.3313 | KD: 1060.0457\n",
      "Train Epoch: 010 Batch: 00045/00094 | Loss: 1060.4160 | CE: 0.3608 | KD: 1060.0552\n",
      "Train Epoch: 010 Batch: 00046/00094 | Loss: 1060.3635 | CE: 0.3300 | KD: 1060.0334\n",
      "Train Epoch: 010 Batch: 00047/00094 | Loss: 1060.3933 | CE: 0.3036 | KD: 1060.0897\n",
      "Train Epoch: 010 Batch: 00048/00094 | Loss: 1060.3878 | CE: 0.3285 | KD: 1060.0593\n",
      "Train Epoch: 010 Batch: 00049/00094 | Loss: 1060.3416 | CE: 0.3339 | KD: 1060.0077\n",
      "Train Epoch: 010 Batch: 00050/00094 | Loss: 1060.3965 | CE: 0.2950 | KD: 1060.1014\n",
      "Train Epoch: 010 Batch: 00051/00094 | Loss: 1060.2457 | CE: 0.2998 | KD: 1059.9459\n",
      "Train Epoch: 010 Batch: 00052/00094 | Loss: 1060.2130 | CE: 0.2669 | KD: 1059.9462\n",
      "Train Epoch: 010 Batch: 00053/00094 | Loss: 1060.3123 | CE: 0.3492 | KD: 1059.9630\n",
      "Train Epoch: 010 Batch: 00054/00094 | Loss: 1060.3197 | CE: 0.3195 | KD: 1060.0001\n",
      "Train Epoch: 010 Batch: 00055/00094 | Loss: 1060.3418 | CE: 0.3107 | KD: 1060.0311\n",
      "Train Epoch: 010 Batch: 00056/00094 | Loss: 1060.3986 | CE: 0.3797 | KD: 1060.0189\n",
      "Train Epoch: 010 Batch: 00057/00094 | Loss: 1060.4943 | CE: 0.4220 | KD: 1060.0723\n",
      "Train Epoch: 010 Batch: 00058/00094 | Loss: 1060.3787 | CE: 0.3722 | KD: 1060.0065\n",
      "Train Epoch: 010 Batch: 00059/00094 | Loss: 1060.3701 | CE: 0.3087 | KD: 1060.0614\n",
      "Train Epoch: 010 Batch: 00060/00094 | Loss: 1060.3199 | CE: 0.2975 | KD: 1060.0225\n",
      "Train Epoch: 010 Batch: 00061/00094 | Loss: 1060.4253 | CE: 0.3402 | KD: 1060.0851\n",
      "Train Epoch: 010 Batch: 00062/00094 | Loss: 1060.2723 | CE: 0.2895 | KD: 1059.9829\n",
      "Train Epoch: 010 Batch: 00063/00094 | Loss: 1060.3872 | CE: 0.2840 | KD: 1060.1031\n",
      "Train Epoch: 010 Batch: 00064/00094 | Loss: 1060.3113 | CE: 0.3122 | KD: 1059.9991\n",
      "Train Epoch: 010 Batch: 00065/00094 | Loss: 1060.2266 | CE: 0.2587 | KD: 1059.9679\n",
      "Train Epoch: 010 Batch: 00066/00094 | Loss: 1060.3246 | CE: 0.3015 | KD: 1060.0231\n",
      "Train Epoch: 010 Batch: 00067/00094 | Loss: 1060.2312 | CE: 0.2690 | KD: 1059.9622\n",
      "Train Epoch: 010 Batch: 00068/00094 | Loss: 1060.2714 | CE: 0.3068 | KD: 1059.9646\n",
      "Train Epoch: 010 Batch: 00069/00094 | Loss: 1060.3201 | CE: 0.2967 | KD: 1060.0233\n",
      "Train Epoch: 010 Batch: 00070/00094 | Loss: 1060.1704 | CE: 0.2598 | KD: 1059.9105\n",
      "Train Epoch: 010 Batch: 00071/00094 | Loss: 1060.2963 | CE: 0.3216 | KD: 1059.9746\n",
      "Train Epoch: 010 Batch: 00072/00094 | Loss: 1060.2177 | CE: 0.2337 | KD: 1059.9839\n",
      "Train Epoch: 010 Batch: 00073/00094 | Loss: 1060.5255 | CE: 0.3909 | KD: 1060.1346\n",
      "Train Epoch: 010 Batch: 00074/00094 | Loss: 1060.3455 | CE: 0.2508 | KD: 1060.0946\n",
      "Train Epoch: 010 Batch: 00075/00094 | Loss: 1060.3026 | CE: 0.3082 | KD: 1059.9944\n",
      "Train Epoch: 010 Batch: 00076/00094 | Loss: 1060.2130 | CE: 0.2637 | KD: 1059.9493\n",
      "Train Epoch: 010 Batch: 00077/00094 | Loss: 1060.3093 | CE: 0.3292 | KD: 1059.9802\n",
      "Train Epoch: 010 Batch: 00078/00094 | Loss: 1060.2971 | CE: 0.2662 | KD: 1060.0309\n",
      "Train Epoch: 010 Batch: 00079/00094 | Loss: 1060.3772 | CE: 0.3676 | KD: 1060.0095\n",
      "Train Epoch: 010 Batch: 00080/00094 | Loss: 1060.3831 | CE: 0.3426 | KD: 1060.0404\n",
      "Train Epoch: 010 Batch: 00081/00094 | Loss: 1060.3013 | CE: 0.2871 | KD: 1060.0142\n",
      "Train Epoch: 010 Batch: 00082/00094 | Loss: 1060.3918 | CE: 0.3669 | KD: 1060.0250\n",
      "Train Epoch: 010 Batch: 00083/00094 | Loss: 1060.4022 | CE: 0.3714 | KD: 1060.0309\n",
      "Train Epoch: 010 Batch: 00084/00094 | Loss: 1060.3812 | CE: 0.3519 | KD: 1060.0294\n",
      "Train Epoch: 010 Batch: 00085/00094 | Loss: 1060.4850 | CE: 0.3929 | KD: 1060.0920\n",
      "Train Epoch: 010 Batch: 00086/00094 | Loss: 1060.4600 | CE: 0.3928 | KD: 1060.0673\n",
      "Train Epoch: 010 Batch: 00087/00094 | Loss: 1060.3156 | CE: 0.2971 | KD: 1060.0184\n",
      "Train Epoch: 010 Batch: 00088/00094 | Loss: 1060.3538 | CE: 0.3483 | KD: 1060.0055\n",
      "Train Epoch: 010 Batch: 00089/00094 | Loss: 1060.3163 | CE: 0.3329 | KD: 1059.9834\n",
      "Train Epoch: 010 Batch: 00090/00094 | Loss: 1060.3306 | CE: 0.3086 | KD: 1060.0220\n",
      "Train Epoch: 010 Batch: 00091/00094 | Loss: 1060.3436 | CE: 0.3254 | KD: 1060.0182\n",
      "Train Epoch: 010 Batch: 00092/00094 | Loss: 1060.4155 | CE: 0.3657 | KD: 1060.0498\n",
      "Train Epoch: 010 Batch: 00093/00094 | Loss: 1060.3126 | CE: 0.3353 | KD: 1059.9774\n",
      "Train Epoch: 010 Batch: 00094/00094 | Loss: 1060.4452 | CE: 0.3576 | KD: 1060.0875\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3363 | acc:86.7000\n",
      "[VAL Acc] Target: 86.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0730 | acc:50.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9479 | acc:49.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9406 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8776 | acc:52.7038\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8883 | acc:54.6211\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6688 | acc:61.3245\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.32%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9348 | acc:56.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4696 | acc:74.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.95%\n",
      "[VAL Acc] Avg 59.47%\n",
      "Train Epoch: 011 Batch: 00001/00094 | Loss: 954.4335 | CE: 0.3950 | KD: 1060.0428\n",
      "Train Epoch: 011 Batch: 00002/00094 | Loss: 954.3608 | CE: 0.3516 | KD: 1060.0103\n",
      "Train Epoch: 011 Batch: 00003/00094 | Loss: 954.2735 | CE: 0.2945 | KD: 1059.9767\n",
      "Train Epoch: 011 Batch: 00004/00094 | Loss: 954.2775 | CE: 0.2785 | KD: 1059.9989\n",
      "Train Epoch: 011 Batch: 00005/00094 | Loss: 954.4033 | CE: 0.3221 | KD: 1060.0902\n",
      "Train Epoch: 011 Batch: 00006/00094 | Loss: 954.2946 | CE: 0.3048 | KD: 1059.9888\n",
      "Train Epoch: 011 Batch: 00007/00094 | Loss: 954.4185 | CE: 0.4062 | KD: 1060.0137\n",
      "Train Epoch: 011 Batch: 00008/00094 | Loss: 954.3971 | CE: 0.3446 | KD: 1060.0583\n",
      "Train Epoch: 011 Batch: 00009/00094 | Loss: 954.2729 | CE: 0.3051 | KD: 1059.9644\n",
      "Train Epoch: 011 Batch: 00010/00094 | Loss: 954.4312 | CE: 0.3456 | KD: 1060.0951\n",
      "Train Epoch: 011 Batch: 00011/00094 | Loss: 954.3276 | CE: 0.3178 | KD: 1060.0109\n",
      "Train Epoch: 011 Batch: 00012/00094 | Loss: 954.3333 | CE: 0.3119 | KD: 1060.0238\n",
      "Train Epoch: 011 Batch: 00013/00094 | Loss: 954.4022 | CE: 0.3936 | KD: 1060.0095\n",
      "Train Epoch: 011 Batch: 00014/00094 | Loss: 954.5358 | CE: 0.4086 | KD: 1060.1414\n",
      "Train Epoch: 011 Batch: 00015/00094 | Loss: 954.2547 | CE: 0.2730 | KD: 1059.9797\n",
      "Train Epoch: 011 Batch: 00016/00094 | Loss: 954.2487 | CE: 0.2842 | KD: 1059.9606\n",
      "Train Epoch: 011 Batch: 00017/00094 | Loss: 954.3077 | CE: 0.2993 | KD: 1060.0094\n",
      "Train Epoch: 011 Batch: 00018/00094 | Loss: 954.6341 | CE: 0.4590 | KD: 1060.1946\n",
      "Train Epoch: 011 Batch: 00019/00094 | Loss: 954.3030 | CE: 0.2990 | KD: 1060.0045\n",
      "Train Epoch: 011 Batch: 00020/00094 | Loss: 954.2587 | CE: 0.2839 | KD: 1059.9720\n",
      "Train Epoch: 011 Batch: 00021/00094 | Loss: 954.2644 | CE: 0.3086 | KD: 1059.9509\n",
      "Train Epoch: 011 Batch: 00022/00094 | Loss: 954.4094 | CE: 0.3717 | KD: 1060.0419\n",
      "Train Epoch: 011 Batch: 00023/00094 | Loss: 954.3405 | CE: 0.2819 | KD: 1060.0651\n",
      "Train Epoch: 011 Batch: 00024/00094 | Loss: 954.3654 | CE: 0.3565 | KD: 1060.0099\n",
      "Train Epoch: 011 Batch: 00025/00094 | Loss: 954.4083 | CE: 0.3493 | KD: 1060.0656\n",
      "Train Epoch: 011 Batch: 00026/00094 | Loss: 954.3369 | CE: 0.3464 | KD: 1059.9895\n",
      "Train Epoch: 011 Batch: 00027/00094 | Loss: 954.2755 | CE: 0.2709 | KD: 1060.0051\n",
      "Train Epoch: 011 Batch: 00028/00094 | Loss: 954.3096 | CE: 0.3099 | KD: 1059.9996\n",
      "Train Epoch: 011 Batch: 00029/00094 | Loss: 954.3721 | CE: 0.3588 | KD: 1060.0148\n",
      "Train Epoch: 011 Batch: 00030/00094 | Loss: 954.3743 | CE: 0.3398 | KD: 1060.0383\n",
      "Train Epoch: 011 Batch: 00031/00094 | Loss: 954.3084 | CE: 0.3352 | KD: 1059.9702\n",
      "Train Epoch: 011 Batch: 00032/00094 | Loss: 954.3324 | CE: 0.3052 | KD: 1060.0303\n",
      "Train Epoch: 011 Batch: 00033/00094 | Loss: 954.4130 | CE: 0.3681 | KD: 1060.0498\n",
      "Train Epoch: 011 Batch: 00034/00094 | Loss: 954.2933 | CE: 0.3030 | KD: 1059.9893\n",
      "Train Epoch: 011 Batch: 00035/00094 | Loss: 954.4689 | CE: 0.3769 | KD: 1060.1023\n",
      "Train Epoch: 011 Batch: 00036/00094 | Loss: 954.3874 | CE: 0.3381 | KD: 1060.0548\n",
      "Train Epoch: 011 Batch: 00037/00094 | Loss: 954.4721 | CE: 0.3585 | KD: 1060.1262\n",
      "Train Epoch: 011 Batch: 00038/00094 | Loss: 954.2499 | CE: 0.2725 | KD: 1059.9749\n",
      "Train Epoch: 011 Batch: 00039/00094 | Loss: 954.4019 | CE: 0.3825 | KD: 1060.0216\n",
      "Train Epoch: 011 Batch: 00040/00094 | Loss: 954.2374 | CE: 0.2551 | KD: 1059.9803\n",
      "Train Epoch: 011 Batch: 00041/00094 | Loss: 954.2168 | CE: 0.2486 | KD: 1059.9646\n",
      "Train Epoch: 011 Batch: 00042/00094 | Loss: 954.5240 | CE: 0.4015 | KD: 1060.1362\n",
      "Train Epoch: 011 Batch: 00043/00094 | Loss: 954.3264 | CE: 0.3167 | KD: 1060.0107\n",
      "Train Epoch: 011 Batch: 00044/00094 | Loss: 954.2477 | CE: 0.2864 | KD: 1059.9570\n",
      "Train Epoch: 011 Batch: 00045/00094 | Loss: 954.3967 | CE: 0.3642 | KD: 1060.0361\n",
      "Train Epoch: 011 Batch: 00046/00094 | Loss: 954.2410 | CE: 0.2560 | KD: 1059.9833\n",
      "Train Epoch: 011 Batch: 00047/00094 | Loss: 954.1898 | CE: 0.2414 | KD: 1059.9427\n",
      "Train Epoch: 011 Batch: 00048/00094 | Loss: 954.3965 | CE: 0.3244 | KD: 1060.0801\n",
      "Train Epoch: 011 Batch: 00049/00094 | Loss: 954.2352 | CE: 0.2765 | KD: 1059.9541\n",
      "Train Epoch: 011 Batch: 00050/00094 | Loss: 954.2703 | CE: 0.2682 | KD: 1060.0023\n",
      "Train Epoch: 011 Batch: 00051/00094 | Loss: 954.2132 | CE: 0.2398 | KD: 1059.9705\n",
      "Train Epoch: 011 Batch: 00052/00094 | Loss: 954.4689 | CE: 0.4189 | KD: 1060.0557\n",
      "Train Epoch: 011 Batch: 00053/00094 | Loss: 954.3083 | CE: 0.3057 | KD: 1060.0029\n",
      "Train Epoch: 011 Batch: 00054/00094 | Loss: 954.3130 | CE: 0.2891 | KD: 1060.0266\n",
      "Train Epoch: 011 Batch: 00055/00094 | Loss: 954.3516 | CE: 0.3086 | KD: 1060.0477\n",
      "Train Epoch: 011 Batch: 00056/00094 | Loss: 954.3029 | CE: 0.3030 | KD: 1059.9999\n",
      "Train Epoch: 011 Batch: 00057/00094 | Loss: 954.3262 | CE: 0.3027 | KD: 1060.0261\n",
      "Train Epoch: 011 Batch: 00058/00094 | Loss: 954.3146 | CE: 0.2870 | KD: 1060.0306\n",
      "Train Epoch: 011 Batch: 00059/00094 | Loss: 954.3749 | CE: 0.3078 | KD: 1060.0747\n",
      "Train Epoch: 011 Batch: 00060/00094 | Loss: 954.4155 | CE: 0.3566 | KD: 1060.0656\n",
      "Train Epoch: 011 Batch: 00061/00094 | Loss: 954.2500 | CE: 0.2700 | KD: 1059.9778\n",
      "Train Epoch: 011 Batch: 00062/00094 | Loss: 954.4023 | CE: 0.3688 | KD: 1060.0374\n",
      "Train Epoch: 011 Batch: 00063/00094 | Loss: 954.3043 | CE: 0.2996 | KD: 1060.0052\n",
      "Train Epoch: 011 Batch: 00064/00094 | Loss: 954.3289 | CE: 0.2863 | KD: 1060.0474\n",
      "Train Epoch: 011 Batch: 00065/00094 | Loss: 954.3256 | CE: 0.3315 | KD: 1059.9935\n",
      "Train Epoch: 011 Batch: 00066/00094 | Loss: 954.2299 | CE: 0.2765 | KD: 1059.9484\n",
      "Train Epoch: 011 Batch: 00067/00094 | Loss: 954.2510 | CE: 0.2494 | KD: 1060.0018\n",
      "Train Epoch: 011 Batch: 00068/00094 | Loss: 954.2903 | CE: 0.2705 | KD: 1060.0220\n",
      "Train Epoch: 011 Batch: 00069/00094 | Loss: 954.3989 | CE: 0.3421 | KD: 1060.0631\n",
      "Train Epoch: 011 Batch: 00070/00094 | Loss: 954.2968 | CE: 0.3088 | KD: 1059.9867\n",
      "Train Epoch: 011 Batch: 00071/00094 | Loss: 954.2877 | CE: 0.2578 | KD: 1060.0332\n",
      "Train Epoch: 011 Batch: 00072/00094 | Loss: 954.3237 | CE: 0.3240 | KD: 1059.9998\n",
      "Train Epoch: 011 Batch: 00073/00094 | Loss: 954.4067 | CE: 0.3451 | KD: 1060.0685\n",
      "Train Epoch: 011 Batch: 00074/00094 | Loss: 954.3958 | CE: 0.3703 | KD: 1060.0283\n",
      "Train Epoch: 011 Batch: 00075/00094 | Loss: 954.3464 | CE: 0.2965 | KD: 1060.0554\n",
      "Train Epoch: 011 Batch: 00076/00094 | Loss: 954.2443 | CE: 0.2829 | KD: 1059.9572\n",
      "Train Epoch: 011 Batch: 00077/00094 | Loss: 954.2673 | CE: 0.2771 | KD: 1059.9893\n",
      "Train Epoch: 011 Batch: 00078/00094 | Loss: 954.2391 | CE: 0.2753 | KD: 1059.9597\n",
      "Train Epoch: 011 Batch: 00079/00094 | Loss: 954.3295 | CE: 0.2930 | KD: 1060.0405\n",
      "Train Epoch: 011 Batch: 00080/00094 | Loss: 954.2812 | CE: 0.2895 | KD: 1059.9908\n",
      "Train Epoch: 011 Batch: 00081/00094 | Loss: 954.3857 | CE: 0.3478 | KD: 1060.0421\n",
      "Train Epoch: 011 Batch: 00082/00094 | Loss: 954.3593 | CE: 0.3300 | KD: 1060.0326\n",
      "Train Epoch: 011 Batch: 00083/00094 | Loss: 954.3628 | CE: 0.3272 | KD: 1060.0396\n",
      "Train Epoch: 011 Batch: 00084/00094 | Loss: 954.3747 | CE: 0.3394 | KD: 1060.0393\n",
      "Train Epoch: 011 Batch: 00085/00094 | Loss: 954.1614 | CE: 0.2254 | KD: 1059.9288\n",
      "Train Epoch: 011 Batch: 00086/00094 | Loss: 954.5038 | CE: 0.3758 | KD: 1060.1422\n",
      "Train Epoch: 011 Batch: 00087/00094 | Loss: 954.2831 | CE: 0.3159 | KD: 1059.9636\n",
      "Train Epoch: 011 Batch: 00088/00094 | Loss: 954.4341 | CE: 0.3741 | KD: 1060.0668\n",
      "Train Epoch: 011 Batch: 00089/00094 | Loss: 954.2975 | CE: 0.3353 | KD: 1059.9581\n",
      "Train Epoch: 011 Batch: 00090/00094 | Loss: 954.2962 | CE: 0.3220 | KD: 1059.9713\n",
      "Train Epoch: 011 Batch: 00091/00094 | Loss: 954.3166 | CE: 0.3264 | KD: 1059.9891\n",
      "Train Epoch: 011 Batch: 00092/00094 | Loss: 954.4148 | CE: 0.3208 | KD: 1060.1044\n",
      "Train Epoch: 011 Batch: 00093/00094 | Loss: 954.3938 | CE: 0.3447 | KD: 1060.0546\n",
      "Train Epoch: 011 Batch: 00094/00094 | Loss: 954.4278 | CE: 0.3657 | KD: 1060.0691\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3167 | acc:88.9000\n",
      "[VAL Acc] Target: 88.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1473 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9757 | acc:48.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9785 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9366 | acc:52.5470\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7444 | acc:57.8558\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 57.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7058 | acc:59.7571\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 59.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8444 | acc:57.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5172 | acc:72.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 72.00%\n",
      "[VAL Acc] Avg 59.55%\n",
      "Train Epoch: 012 Batch: 00001/00094 | Loss: 954.3589 | CE: 0.3486 | KD: 1060.0115\n",
      "Train Epoch: 012 Batch: 00002/00094 | Loss: 954.3220 | CE: 0.3003 | KD: 1060.0242\n",
      "Train Epoch: 012 Batch: 00003/00094 | Loss: 954.4525 | CE: 0.3623 | KD: 1060.1002\n",
      "Train Epoch: 012 Batch: 00004/00094 | Loss: 954.3412 | CE: 0.3382 | KD: 1060.0033\n",
      "Train Epoch: 012 Batch: 00005/00094 | Loss: 954.2576 | CE: 0.2844 | KD: 1059.9702\n",
      "Train Epoch: 012 Batch: 00006/00094 | Loss: 954.3391 | CE: 0.3413 | KD: 1059.9976\n",
      "Train Epoch: 012 Batch: 00007/00094 | Loss: 954.2816 | CE: 0.2832 | KD: 1059.9983\n",
      "Train Epoch: 012 Batch: 00008/00094 | Loss: 954.3392 | CE: 0.2956 | KD: 1060.0485\n",
      "Train Epoch: 012 Batch: 00009/00094 | Loss: 954.2930 | CE: 0.2912 | KD: 1060.0021\n",
      "Train Epoch: 012 Batch: 00010/00094 | Loss: 954.2050 | CE: 0.2647 | KD: 1059.9337\n",
      "Train Epoch: 012 Batch: 00011/00094 | Loss: 954.2592 | CE: 0.2772 | KD: 1059.9801\n",
      "Train Epoch: 012 Batch: 00012/00094 | Loss: 954.3585 | CE: 0.3509 | KD: 1060.0084\n",
      "Train Epoch: 012 Batch: 00013/00094 | Loss: 954.3692 | CE: 0.3521 | KD: 1060.0190\n",
      "Train Epoch: 012 Batch: 00014/00094 | Loss: 954.1091 | CE: 0.2284 | KD: 1059.8674\n",
      "Train Epoch: 012 Batch: 00015/00094 | Loss: 954.2797 | CE: 0.2969 | KD: 1059.9810\n",
      "Train Epoch: 012 Batch: 00016/00094 | Loss: 954.3405 | CE: 0.2956 | KD: 1060.0499\n",
      "Train Epoch: 012 Batch: 00017/00094 | Loss: 954.3243 | CE: 0.3045 | KD: 1060.0221\n",
      "Train Epoch: 012 Batch: 00018/00094 | Loss: 954.2601 | CE: 0.2734 | KD: 1059.9852\n",
      "Train Epoch: 012 Batch: 00019/00094 | Loss: 954.3626 | CE: 0.3165 | KD: 1060.0513\n",
      "Train Epoch: 012 Batch: 00020/00094 | Loss: 954.3032 | CE: 0.2525 | KD: 1060.0564\n",
      "Train Epoch: 012 Batch: 00021/00094 | Loss: 954.3638 | CE: 0.3594 | KD: 1060.0049\n",
      "Train Epoch: 012 Batch: 00022/00094 | Loss: 954.4442 | CE: 0.4034 | KD: 1060.0454\n",
      "Train Epoch: 012 Batch: 00023/00094 | Loss: 954.3209 | CE: 0.3374 | KD: 1059.9817\n",
      "Train Epoch: 012 Batch: 00024/00094 | Loss: 954.2291 | CE: 0.2803 | KD: 1059.9431\n",
      "Train Epoch: 012 Batch: 00025/00094 | Loss: 954.3578 | CE: 0.2958 | KD: 1060.0688\n",
      "Train Epoch: 012 Batch: 00026/00094 | Loss: 954.3906 | CE: 0.2931 | KD: 1060.1084\n",
      "Train Epoch: 012 Batch: 00027/00094 | Loss: 954.2620 | CE: 0.2952 | KD: 1059.9631\n",
      "Train Epoch: 012 Batch: 00028/00094 | Loss: 954.3811 | CE: 0.3535 | KD: 1060.0308\n",
      "Train Epoch: 012 Batch: 00029/00094 | Loss: 954.3426 | CE: 0.3233 | KD: 1060.0215\n",
      "Train Epoch: 012 Batch: 00030/00094 | Loss: 954.2874 | CE: 0.2896 | KD: 1059.9976\n",
      "Train Epoch: 012 Batch: 00031/00094 | Loss: 954.2240 | CE: 0.2500 | KD: 1059.9711\n",
      "Train Epoch: 012 Batch: 00032/00094 | Loss: 954.3672 | CE: 0.3487 | KD: 1060.0206\n",
      "Train Epoch: 012 Batch: 00033/00094 | Loss: 954.3306 | CE: 0.2731 | KD: 1060.0640\n",
      "Train Epoch: 012 Batch: 00034/00094 | Loss: 954.4770 | CE: 0.4014 | KD: 1060.0840\n",
      "Train Epoch: 012 Batch: 00035/00094 | Loss: 954.5353 | CE: 0.4508 | KD: 1060.0939\n",
      "Train Epoch: 012 Batch: 00036/00094 | Loss: 954.2503 | CE: 0.2907 | KD: 1059.9552\n",
      "Train Epoch: 012 Batch: 00037/00094 | Loss: 954.2808 | CE: 0.3355 | KD: 1059.9392\n",
      "Train Epoch: 012 Batch: 00038/00094 | Loss: 954.3250 | CE: 0.3145 | KD: 1060.0117\n",
      "Train Epoch: 012 Batch: 00039/00094 | Loss: 954.3196 | CE: 0.3526 | KD: 1059.9634\n",
      "Train Epoch: 012 Batch: 00040/00094 | Loss: 954.3782 | CE: 0.3288 | KD: 1060.0551\n",
      "Train Epoch: 012 Batch: 00041/00094 | Loss: 954.2726 | CE: 0.3003 | KD: 1059.9692\n",
      "Train Epoch: 012 Batch: 00042/00094 | Loss: 954.3656 | CE: 0.3331 | KD: 1060.0361\n",
      "Train Epoch: 012 Batch: 00043/00094 | Loss: 954.3383 | CE: 0.2918 | KD: 1060.0516\n",
      "Train Epoch: 012 Batch: 00044/00094 | Loss: 954.3062 | CE: 0.3131 | KD: 1059.9924\n",
      "Train Epoch: 012 Batch: 00045/00094 | Loss: 954.4735 | CE: 0.3596 | KD: 1060.1266\n",
      "Train Epoch: 012 Batch: 00046/00094 | Loss: 954.2805 | CE: 0.2911 | KD: 1059.9882\n",
      "Train Epoch: 012 Batch: 00047/00094 | Loss: 954.3754 | CE: 0.3252 | KD: 1060.0558\n",
      "Train Epoch: 012 Batch: 00048/00094 | Loss: 954.3659 | CE: 0.3375 | KD: 1060.0316\n",
      "Train Epoch: 012 Batch: 00049/00094 | Loss: 954.2734 | CE: 0.2780 | KD: 1059.9950\n",
      "Train Epoch: 012 Batch: 00050/00094 | Loss: 954.4216 | CE: 0.3292 | KD: 1060.1027\n",
      "Train Epoch: 012 Batch: 00051/00094 | Loss: 954.3472 | CE: 0.3149 | KD: 1060.0359\n",
      "Train Epoch: 012 Batch: 00052/00094 | Loss: 954.3240 | CE: 0.2820 | KD: 1060.0468\n",
      "Train Epoch: 012 Batch: 00053/00094 | Loss: 954.4773 | CE: 0.4384 | KD: 1060.0432\n",
      "Train Epoch: 012 Batch: 00054/00094 | Loss: 954.2809 | CE: 0.2817 | KD: 1059.9991\n",
      "Train Epoch: 012 Batch: 00055/00094 | Loss: 954.3393 | CE: 0.3032 | KD: 1060.0402\n",
      "Train Epoch: 012 Batch: 00056/00094 | Loss: 954.2034 | CE: 0.2730 | KD: 1059.9227\n",
      "Train Epoch: 012 Batch: 00057/00094 | Loss: 954.2188 | CE: 0.2662 | KD: 1059.9473\n",
      "Train Epoch: 012 Batch: 00058/00094 | Loss: 954.4310 | CE: 0.3368 | KD: 1060.1046\n",
      "Train Epoch: 012 Batch: 00059/00094 | Loss: 954.3350 | CE: 0.3283 | KD: 1060.0074\n",
      "Train Epoch: 012 Batch: 00060/00094 | Loss: 954.4170 | CE: 0.3634 | KD: 1060.0596\n",
      "Train Epoch: 012 Batch: 00061/00094 | Loss: 954.4005 | CE: 0.3397 | KD: 1060.0675\n",
      "Train Epoch: 012 Batch: 00062/00094 | Loss: 954.3882 | CE: 0.3462 | KD: 1060.0468\n",
      "Train Epoch: 012 Batch: 00063/00094 | Loss: 954.3796 | CE: 0.3460 | KD: 1060.0374\n",
      "Train Epoch: 012 Batch: 00064/00094 | Loss: 954.4866 | CE: 0.4140 | KD: 1060.0806\n",
      "Train Epoch: 012 Batch: 00065/00094 | Loss: 954.2370 | CE: 0.2548 | KD: 1059.9802\n",
      "Train Epoch: 012 Batch: 00066/00094 | Loss: 954.2862 | CE: 0.2702 | KD: 1060.0177\n",
      "Train Epoch: 012 Batch: 00067/00094 | Loss: 954.3649 | CE: 0.2863 | KD: 1060.0874\n",
      "Train Epoch: 012 Batch: 00068/00094 | Loss: 954.3762 | CE: 0.3483 | KD: 1060.0310\n",
      "Train Epoch: 012 Batch: 00069/00094 | Loss: 954.2303 | CE: 0.2858 | KD: 1059.9384\n",
      "Train Epoch: 012 Batch: 00070/00094 | Loss: 954.5126 | CE: 0.4214 | KD: 1060.1013\n",
      "Train Epoch: 012 Batch: 00071/00094 | Loss: 954.3785 | CE: 0.3558 | KD: 1060.0253\n",
      "Train Epoch: 012 Batch: 00072/00094 | Loss: 954.3101 | CE: 0.2954 | KD: 1060.0164\n",
      "Train Epoch: 012 Batch: 00073/00094 | Loss: 954.3432 | CE: 0.3025 | KD: 1060.0453\n",
      "Train Epoch: 012 Batch: 00074/00094 | Loss: 954.3945 | CE: 0.3189 | KD: 1060.0841\n",
      "Train Epoch: 012 Batch: 00075/00094 | Loss: 954.3525 | CE: 0.3398 | KD: 1060.0140\n",
      "Train Epoch: 012 Batch: 00076/00094 | Loss: 954.2751 | CE: 0.2691 | KD: 1060.0067\n",
      "Train Epoch: 012 Batch: 00077/00094 | Loss: 954.2508 | CE: 0.2703 | KD: 1059.9784\n",
      "Train Epoch: 012 Batch: 00078/00094 | Loss: 954.4271 | CE: 0.3839 | KD: 1060.0480\n",
      "Train Epoch: 012 Batch: 00079/00094 | Loss: 954.2921 | CE: 0.2797 | KD: 1060.0138\n",
      "Train Epoch: 012 Batch: 00080/00094 | Loss: 954.3688 | CE: 0.3435 | KD: 1060.0282\n",
      "Train Epoch: 012 Batch: 00081/00094 | Loss: 954.3284 | CE: 0.3165 | KD: 1060.0132\n",
      "Train Epoch: 012 Batch: 00082/00094 | Loss: 954.3632 | CE: 0.3502 | KD: 1060.0144\n",
      "Train Epoch: 012 Batch: 00083/00094 | Loss: 954.3912 | CE: 0.2927 | KD: 1060.1095\n",
      "Train Epoch: 012 Batch: 00084/00094 | Loss: 954.3671 | CE: 0.3110 | KD: 1060.0623\n",
      "Train Epoch: 012 Batch: 00085/00094 | Loss: 954.3485 | CE: 0.3245 | KD: 1060.0267\n",
      "Train Epoch: 012 Batch: 00086/00094 | Loss: 954.3658 | CE: 0.3294 | KD: 1060.0405\n",
      "Train Epoch: 012 Batch: 00087/00094 | Loss: 954.3818 | CE: 0.3017 | KD: 1060.0891\n",
      "Train Epoch: 012 Batch: 00088/00094 | Loss: 954.3251 | CE: 0.2641 | KD: 1060.0679\n",
      "Train Epoch: 012 Batch: 00089/00094 | Loss: 954.3245 | CE: 0.3053 | KD: 1060.0214\n",
      "Train Epoch: 012 Batch: 00090/00094 | Loss: 954.2927 | CE: 0.2955 | KD: 1059.9968\n",
      "Train Epoch: 012 Batch: 00091/00094 | Loss: 954.3801 | CE: 0.3388 | KD: 1060.0459\n",
      "Train Epoch: 012 Batch: 00092/00094 | Loss: 954.3777 | CE: 0.3473 | KD: 1060.0338\n",
      "Train Epoch: 012 Batch: 00093/00094 | Loss: 954.3042 | CE: 0.3031 | KD: 1060.0012\n",
      "Train Epoch: 012 Batch: 00094/00094 | Loss: 954.4160 | CE: 0.3389 | KD: 1060.0857\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3094 | acc:88.2000\n",
      "[VAL Acc] Target: 88.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0846 | acc:50.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9359 | acc:50.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9245 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8707 | acc:52.8213\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9376 | acc:53.6044\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6344 | acc:63.4013\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9627 | acc:55.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4674 | acc:75.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.60%\n",
      "[VAL Acc] Avg 59.95%\n",
      "Train Epoch: 013 Batch: 00001/00094 | Loss: 954.2812 | CE: 0.2627 | KD: 1060.0205\n",
      "Train Epoch: 013 Batch: 00002/00094 | Loss: 954.3411 | CE: 0.3199 | KD: 1060.0237\n",
      "Train Epoch: 013 Batch: 00003/00094 | Loss: 954.5120 | CE: 0.4095 | KD: 1060.1139\n",
      "Train Epoch: 013 Batch: 00004/00094 | Loss: 954.2877 | CE: 0.3057 | KD: 1059.9800\n",
      "Train Epoch: 013 Batch: 00005/00094 | Loss: 954.3263 | CE: 0.3046 | KD: 1060.0242\n",
      "Train Epoch: 013 Batch: 00006/00094 | Loss: 954.4799 | CE: 0.4278 | KD: 1060.0579\n",
      "Train Epoch: 013 Batch: 00007/00094 | Loss: 954.3015 | CE: 0.3243 | KD: 1059.9746\n",
      "Train Epoch: 013 Batch: 00008/00094 | Loss: 954.3401 | CE: 0.2859 | KD: 1060.0603\n",
      "Train Epoch: 013 Batch: 00009/00094 | Loss: 954.3608 | CE: 0.2954 | KD: 1060.0728\n",
      "Train Epoch: 013 Batch: 00010/00094 | Loss: 954.2948 | CE: 0.2935 | KD: 1060.0015\n",
      "Train Epoch: 013 Batch: 00011/00094 | Loss: 954.3428 | CE: 0.2987 | KD: 1060.0491\n",
      "Train Epoch: 013 Batch: 00012/00094 | Loss: 954.3687 | CE: 0.3404 | KD: 1060.0315\n",
      "Train Epoch: 013 Batch: 00013/00094 | Loss: 954.5432 | CE: 0.4024 | KD: 1060.1565\n",
      "Train Epoch: 013 Batch: 00014/00094 | Loss: 954.2997 | CE: 0.3024 | KD: 1059.9969\n",
      "Train Epoch: 013 Batch: 00015/00094 | Loss: 954.2903 | CE: 0.2949 | KD: 1059.9949\n",
      "Train Epoch: 013 Batch: 00016/00094 | Loss: 954.3813 | CE: 0.3692 | KD: 1060.0135\n",
      "Train Epoch: 013 Batch: 00017/00094 | Loss: 954.2325 | CE: 0.2629 | KD: 1059.9662\n",
      "Train Epoch: 013 Batch: 00018/00094 | Loss: 954.4146 | CE: 0.3790 | KD: 1060.0396\n",
      "Train Epoch: 013 Batch: 00019/00094 | Loss: 954.3562 | CE: 0.3341 | KD: 1060.0247\n",
      "Train Epoch: 013 Batch: 00020/00094 | Loss: 954.2660 | CE: 0.2583 | KD: 1060.0085\n",
      "Train Epoch: 013 Batch: 00021/00094 | Loss: 954.2922 | CE: 0.2581 | KD: 1060.0380\n",
      "Train Epoch: 013 Batch: 00022/00094 | Loss: 954.2333 | CE: 0.2674 | KD: 1059.9623\n",
      "Train Epoch: 013 Batch: 00023/00094 | Loss: 954.5154 | CE: 0.4117 | KD: 1060.1152\n",
      "Train Epoch: 013 Batch: 00024/00094 | Loss: 954.2256 | CE: 0.2541 | KD: 1059.9684\n",
      "Train Epoch: 013 Batch: 00025/00094 | Loss: 954.2882 | CE: 0.3248 | KD: 1059.9594\n",
      "Train Epoch: 013 Batch: 00026/00094 | Loss: 954.3210 | CE: 0.2952 | KD: 1060.0287\n",
      "Train Epoch: 013 Batch: 00027/00094 | Loss: 954.3134 | CE: 0.2739 | KD: 1060.0439\n",
      "Train Epoch: 013 Batch: 00028/00094 | Loss: 954.3663 | CE: 0.2897 | KD: 1060.0851\n",
      "Train Epoch: 013 Batch: 00029/00094 | Loss: 954.2755 | CE: 0.2679 | KD: 1060.0084\n",
      "Train Epoch: 013 Batch: 00030/00094 | Loss: 954.4024 | CE: 0.3481 | KD: 1060.0603\n",
      "Train Epoch: 013 Batch: 00031/00094 | Loss: 954.2646 | CE: 0.2605 | KD: 1060.0045\n",
      "Train Epoch: 013 Batch: 00032/00094 | Loss: 954.2422 | CE: 0.2564 | KD: 1059.9844\n",
      "Train Epoch: 013 Batch: 00033/00094 | Loss: 954.3042 | CE: 0.2995 | KD: 1060.0052\n",
      "Train Epoch: 013 Batch: 00034/00094 | Loss: 954.2569 | CE: 0.2856 | KD: 1059.9681\n",
      "Train Epoch: 013 Batch: 00035/00094 | Loss: 954.2464 | CE: 0.2835 | KD: 1059.9589\n",
      "Train Epoch: 013 Batch: 00036/00094 | Loss: 954.2941 | CE: 0.2618 | KD: 1060.0359\n",
      "Train Epoch: 013 Batch: 00037/00094 | Loss: 954.3690 | CE: 0.3217 | KD: 1060.0525\n",
      "Train Epoch: 013 Batch: 00038/00094 | Loss: 954.2570 | CE: 0.2784 | KD: 1059.9762\n",
      "Train Epoch: 013 Batch: 00039/00094 | Loss: 954.2637 | CE: 0.2851 | KD: 1059.9763\n",
      "Train Epoch: 013 Batch: 00040/00094 | Loss: 954.3568 | CE: 0.3474 | KD: 1060.0105\n",
      "Train Epoch: 013 Batch: 00041/00094 | Loss: 954.4189 | CE: 0.4029 | KD: 1060.0178\n",
      "Train Epoch: 013 Batch: 00042/00094 | Loss: 954.4321 | CE: 0.3761 | KD: 1060.0623\n",
      "Train Epoch: 013 Batch: 00043/00094 | Loss: 954.2252 | CE: 0.2803 | KD: 1059.9388\n",
      "Train Epoch: 013 Batch: 00044/00094 | Loss: 954.4215 | CE: 0.3538 | KD: 1060.0753\n",
      "Train Epoch: 013 Batch: 00045/00094 | Loss: 954.4139 | CE: 0.3683 | KD: 1060.0507\n",
      "Train Epoch: 013 Batch: 00046/00094 | Loss: 954.2742 | CE: 0.3048 | KD: 1059.9661\n",
      "Train Epoch: 013 Batch: 00047/00094 | Loss: 954.2881 | CE: 0.2855 | KD: 1060.0029\n",
      "Train Epoch: 013 Batch: 00048/00094 | Loss: 954.3174 | CE: 0.2822 | KD: 1060.0392\n",
      "Train Epoch: 013 Batch: 00049/00094 | Loss: 954.1838 | CE: 0.2634 | KD: 1059.9116\n",
      "Train Epoch: 013 Batch: 00050/00094 | Loss: 954.2249 | CE: 0.2658 | KD: 1059.9546\n",
      "Train Epoch: 013 Batch: 00051/00094 | Loss: 954.3754 | CE: 0.3144 | KD: 1060.0677\n",
      "Train Epoch: 013 Batch: 00052/00094 | Loss: 954.3958 | CE: 0.3299 | KD: 1060.0732\n",
      "Train Epoch: 013 Batch: 00053/00094 | Loss: 954.2299 | CE: 0.2622 | KD: 1059.9641\n",
      "Train Epoch: 013 Batch: 00054/00094 | Loss: 954.3253 | CE: 0.3089 | KD: 1060.0182\n",
      "Train Epoch: 013 Batch: 00055/00094 | Loss: 954.2323 | CE: 0.3019 | KD: 1059.9227\n",
      "Train Epoch: 013 Batch: 00056/00094 | Loss: 954.3340 | CE: 0.3338 | KD: 1060.0004\n",
      "Train Epoch: 013 Batch: 00057/00094 | Loss: 954.3663 | CE: 0.3558 | KD: 1060.0116\n",
      "Train Epoch: 013 Batch: 00058/00094 | Loss: 954.3517 | CE: 0.2926 | KD: 1060.0657\n",
      "Train Epoch: 013 Batch: 00059/00094 | Loss: 954.3339 | CE: 0.3291 | KD: 1060.0054\n",
      "Train Epoch: 013 Batch: 00060/00094 | Loss: 954.2787 | CE: 0.3086 | KD: 1059.9668\n",
      "Train Epoch: 013 Batch: 00061/00094 | Loss: 954.3416 | CE: 0.3250 | KD: 1060.0184\n",
      "Train Epoch: 013 Batch: 00062/00094 | Loss: 954.5209 | CE: 0.4376 | KD: 1060.0925\n",
      "Train Epoch: 013 Batch: 00063/00094 | Loss: 954.3239 | CE: 0.3201 | KD: 1060.0042\n",
      "Train Epoch: 013 Batch: 00064/00094 | Loss: 954.3813 | CE: 0.3018 | KD: 1060.0884\n",
      "Train Epoch: 013 Batch: 00065/00094 | Loss: 954.3179 | CE: 0.3251 | KD: 1059.9919\n",
      "Train Epoch: 013 Batch: 00066/00094 | Loss: 954.2634 | CE: 0.2631 | KD: 1060.0004\n",
      "Train Epoch: 013 Batch: 00067/00094 | Loss: 954.3303 | CE: 0.3190 | KD: 1060.0126\n",
      "Train Epoch: 013 Batch: 00068/00094 | Loss: 954.2635 | CE: 0.2767 | KD: 1059.9855\n",
      "Train Epoch: 013 Batch: 00069/00094 | Loss: 954.3951 | CE: 0.3516 | KD: 1060.0483\n",
      "Train Epoch: 013 Batch: 00070/00094 | Loss: 954.2001 | CE: 0.2429 | KD: 1059.9525\n",
      "Train Epoch: 013 Batch: 00071/00094 | Loss: 954.3311 | CE: 0.2904 | KD: 1060.0453\n",
      "Train Epoch: 013 Batch: 00072/00094 | Loss: 954.3289 | CE: 0.3364 | KD: 1059.9917\n",
      "Train Epoch: 013 Batch: 00073/00094 | Loss: 954.3021 | CE: 0.2601 | KD: 1060.0468\n",
      "Train Epoch: 013 Batch: 00074/00094 | Loss: 954.2054 | CE: 0.2704 | KD: 1059.9279\n",
      "Train Epoch: 013 Batch: 00075/00094 | Loss: 954.4225 | CE: 0.4155 | KD: 1060.0078\n",
      "Train Epoch: 013 Batch: 00076/00094 | Loss: 954.3162 | CE: 0.3261 | KD: 1059.9890\n",
      "Train Epoch: 013 Batch: 00077/00094 | Loss: 954.2917 | CE: 0.2834 | KD: 1060.0093\n",
      "Train Epoch: 013 Batch: 00078/00094 | Loss: 954.1982 | CE: 0.2585 | KD: 1059.9330\n",
      "Train Epoch: 013 Batch: 00079/00094 | Loss: 954.3038 | CE: 0.3114 | KD: 1059.9915\n",
      "Train Epoch: 013 Batch: 00080/00094 | Loss: 954.2576 | CE: 0.2964 | KD: 1059.9569\n",
      "Train Epoch: 013 Batch: 00081/00094 | Loss: 954.4342 | CE: 0.3636 | KD: 1060.0785\n",
      "Train Epoch: 013 Batch: 00082/00094 | Loss: 954.3192 | CE: 0.3019 | KD: 1060.0193\n",
      "Train Epoch: 013 Batch: 00083/00094 | Loss: 954.3265 | CE: 0.3015 | KD: 1060.0278\n",
      "Train Epoch: 013 Batch: 00084/00094 | Loss: 954.2905 | CE: 0.2945 | KD: 1059.9956\n",
      "Train Epoch: 013 Batch: 00085/00094 | Loss: 954.4622 | CE: 0.3771 | KD: 1060.0945\n",
      "Train Epoch: 013 Batch: 00086/00094 | Loss: 954.3889 | CE: 0.3350 | KD: 1060.0599\n",
      "Train Epoch: 013 Batch: 00087/00094 | Loss: 954.3646 | CE: 0.3567 | KD: 1060.0088\n",
      "Train Epoch: 013 Batch: 00088/00094 | Loss: 954.3744 | CE: 0.2940 | KD: 1060.0894\n",
      "Train Epoch: 013 Batch: 00089/00094 | Loss: 954.3412 | CE: 0.3581 | KD: 1059.9813\n",
      "Train Epoch: 013 Batch: 00090/00094 | Loss: 954.3408 | CE: 0.3146 | KD: 1060.0292\n",
      "Train Epoch: 013 Batch: 00091/00094 | Loss: 954.3441 | CE: 0.3434 | KD: 1060.0007\n",
      "Train Epoch: 013 Batch: 00092/00094 | Loss: 954.2904 | CE: 0.2783 | KD: 1060.0134\n",
      "Train Epoch: 013 Batch: 00093/00094 | Loss: 954.4138 | CE: 0.3668 | KD: 1060.0522\n",
      "Train Epoch: 013 Batch: 00094/00094 | Loss: 954.4393 | CE: 0.4184 | KD: 1060.0232\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3295 | acc:87.2000\n",
      "[VAL Acc] Target: 87.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0743 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9455 | acc:48.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9310 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8682 | acc:52.5078\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8753 | acc:54.8983\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6470 | acc:62.2257\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8952 | acc:56.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4684 | acc:75.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.20%\n",
      "[VAL Acc] Avg 59.47%\n",
      "Train Epoch: 014 Batch: 00001/00094 | Loss: 954.3537 | CE: 0.3413 | KD: 1060.0138\n",
      "Train Epoch: 014 Batch: 00002/00094 | Loss: 954.2836 | CE: 0.3014 | KD: 1059.9802\n",
      "Train Epoch: 014 Batch: 00003/00094 | Loss: 954.3705 | CE: 0.3170 | KD: 1060.0596\n",
      "Train Epoch: 014 Batch: 00004/00094 | Loss: 954.2499 | CE: 0.3055 | KD: 1059.9382\n",
      "Train Epoch: 014 Batch: 00005/00094 | Loss: 954.4007 | CE: 0.3157 | KD: 1060.0945\n",
      "Train Epoch: 014 Batch: 00006/00094 | Loss: 954.2960 | CE: 0.3028 | KD: 1059.9924\n",
      "Train Epoch: 014 Batch: 00007/00094 | Loss: 954.3207 | CE: 0.3356 | KD: 1059.9835\n",
      "Train Epoch: 014 Batch: 00008/00094 | Loss: 954.3556 | CE: 0.3102 | KD: 1060.0504\n",
      "Train Epoch: 014 Batch: 00009/00094 | Loss: 954.2950 | CE: 0.3197 | KD: 1059.9727\n",
      "Train Epoch: 014 Batch: 00010/00094 | Loss: 954.4131 | CE: 0.3698 | KD: 1060.0482\n",
      "Train Epoch: 014 Batch: 00011/00094 | Loss: 954.2824 | CE: 0.3057 | KD: 1059.9741\n",
      "Train Epoch: 014 Batch: 00012/00094 | Loss: 954.3565 | CE: 0.3290 | KD: 1060.0305\n",
      "Train Epoch: 014 Batch: 00013/00094 | Loss: 954.1797 | CE: 0.2382 | KD: 1059.9351\n",
      "Train Epoch: 014 Batch: 00014/00094 | Loss: 954.2836 | CE: 0.3035 | KD: 1059.9779\n",
      "Train Epoch: 014 Batch: 00015/00094 | Loss: 954.2595 | CE: 0.2637 | KD: 1059.9954\n",
      "Train Epoch: 014 Batch: 00016/00094 | Loss: 954.3870 | CE: 0.3229 | KD: 1060.0713\n",
      "Train Epoch: 014 Batch: 00017/00094 | Loss: 954.3621 | CE: 0.3663 | KD: 1059.9954\n",
      "Train Epoch: 014 Batch: 00018/00094 | Loss: 954.4349 | CE: 0.3547 | KD: 1060.0892\n",
      "Train Epoch: 014 Batch: 00019/00094 | Loss: 954.5297 | CE: 0.3467 | KD: 1060.2032\n",
      "Train Epoch: 014 Batch: 00020/00094 | Loss: 954.3462 | CE: 0.3242 | KD: 1060.0244\n",
      "Train Epoch: 014 Batch: 00021/00094 | Loss: 954.3069 | CE: 0.2577 | KD: 1060.0547\n",
      "Train Epoch: 014 Batch: 00022/00094 | Loss: 954.2894 | CE: 0.2534 | KD: 1060.0400\n",
      "Train Epoch: 014 Batch: 00023/00094 | Loss: 954.3756 | CE: 0.3461 | KD: 1060.0328\n",
      "Train Epoch: 014 Batch: 00024/00094 | Loss: 954.3073 | CE: 0.3258 | KD: 1059.9795\n",
      "Train Epoch: 014 Batch: 00025/00094 | Loss: 954.2203 | CE: 0.2717 | KD: 1059.9430\n",
      "Train Epoch: 014 Batch: 00026/00094 | Loss: 954.2408 | CE: 0.2376 | KD: 1060.0035\n",
      "Train Epoch: 014 Batch: 00027/00094 | Loss: 954.2779 | CE: 0.3015 | KD: 1059.9738\n",
      "Train Epoch: 014 Batch: 00028/00094 | Loss: 954.2532 | CE: 0.2900 | KD: 1059.9591\n",
      "Train Epoch: 014 Batch: 00029/00094 | Loss: 954.3020 | CE: 0.3054 | KD: 1059.9962\n",
      "Train Epoch: 014 Batch: 00030/00094 | Loss: 954.3452 | CE: 0.3434 | KD: 1060.0021\n",
      "Train Epoch: 014 Batch: 00031/00094 | Loss: 954.4814 | CE: 0.3440 | KD: 1060.1526\n",
      "Train Epoch: 014 Batch: 00032/00094 | Loss: 954.4036 | CE: 0.3551 | KD: 1060.0540\n",
      "Train Epoch: 014 Batch: 00033/00094 | Loss: 954.2230 | CE: 0.2397 | KD: 1059.9814\n",
      "Train Epoch: 014 Batch: 00034/00094 | Loss: 954.2938 | CE: 0.2902 | KD: 1060.0040\n",
      "Train Epoch: 014 Batch: 00035/00094 | Loss: 954.4088 | CE: 0.3585 | KD: 1060.0559\n",
      "Train Epoch: 014 Batch: 00036/00094 | Loss: 954.3195 | CE: 0.3345 | KD: 1059.9834\n",
      "Train Epoch: 014 Batch: 00037/00094 | Loss: 954.4944 | CE: 0.4108 | KD: 1060.0929\n",
      "Train Epoch: 014 Batch: 00038/00094 | Loss: 954.4293 | CE: 0.3752 | KD: 1060.0601\n",
      "Train Epoch: 014 Batch: 00039/00094 | Loss: 954.3649 | CE: 0.3538 | KD: 1060.0123\n",
      "Train Epoch: 014 Batch: 00040/00094 | Loss: 954.2422 | CE: 0.2713 | KD: 1059.9677\n",
      "Train Epoch: 014 Batch: 00041/00094 | Loss: 954.2806 | CE: 0.2838 | KD: 1059.9965\n",
      "Train Epoch: 014 Batch: 00042/00094 | Loss: 954.4232 | CE: 0.3692 | KD: 1060.0601\n",
      "Train Epoch: 014 Batch: 00043/00094 | Loss: 954.4285 | CE: 0.3506 | KD: 1060.0865\n",
      "Train Epoch: 014 Batch: 00044/00094 | Loss: 954.2404 | CE: 0.2583 | KD: 1059.9802\n",
      "Train Epoch: 014 Batch: 00045/00094 | Loss: 954.4006 | CE: 0.3659 | KD: 1060.0386\n",
      "Train Epoch: 014 Batch: 00046/00094 | Loss: 954.2919 | CE: 0.2877 | KD: 1060.0046\n",
      "Train Epoch: 014 Batch: 00047/00094 | Loss: 954.2122 | CE: 0.2755 | KD: 1059.9297\n",
      "Train Epoch: 014 Batch: 00048/00094 | Loss: 954.2238 | CE: 0.2682 | KD: 1059.9507\n",
      "Train Epoch: 014 Batch: 00049/00094 | Loss: 954.3257 | CE: 0.3032 | KD: 1060.0250\n",
      "Train Epoch: 014 Batch: 00050/00094 | Loss: 954.2831 | CE: 0.3087 | KD: 1059.9716\n",
      "Train Epoch: 014 Batch: 00051/00094 | Loss: 954.2986 | CE: 0.2890 | KD: 1060.0107\n",
      "Train Epoch: 014 Batch: 00052/00094 | Loss: 954.4903 | CE: 0.3568 | KD: 1060.1483\n",
      "Train Epoch: 014 Batch: 00053/00094 | Loss: 954.2939 | CE: 0.2960 | KD: 1059.9978\n",
      "Train Epoch: 014 Batch: 00054/00094 | Loss: 954.3624 | CE: 0.3154 | KD: 1060.0522\n",
      "Train Epoch: 014 Batch: 00055/00094 | Loss: 954.1403 | CE: 0.2314 | KD: 1059.8988\n",
      "Train Epoch: 014 Batch: 00056/00094 | Loss: 954.4655 | CE: 0.3918 | KD: 1060.0819\n",
      "Train Epoch: 014 Batch: 00057/00094 | Loss: 954.4735 | CE: 0.3956 | KD: 1060.0865\n",
      "Train Epoch: 014 Batch: 00058/00094 | Loss: 954.1797 | CE: 0.2196 | KD: 1059.9558\n",
      "Train Epoch: 014 Batch: 00059/00094 | Loss: 954.3303 | CE: 0.3142 | KD: 1060.0179\n",
      "Train Epoch: 014 Batch: 00060/00094 | Loss: 954.3491 | CE: 0.3328 | KD: 1060.0182\n",
      "Train Epoch: 014 Batch: 00061/00094 | Loss: 954.3082 | CE: 0.3036 | KD: 1060.0051\n",
      "Train Epoch: 014 Batch: 00062/00094 | Loss: 954.3875 | CE: 0.3635 | KD: 1060.0267\n",
      "Train Epoch: 014 Batch: 00063/00094 | Loss: 954.2846 | CE: 0.2783 | KD: 1060.0071\n",
      "Train Epoch: 014 Batch: 00064/00094 | Loss: 954.2580 | CE: 0.2558 | KD: 1060.0024\n",
      "Train Epoch: 014 Batch: 00065/00094 | Loss: 954.3275 | CE: 0.3245 | KD: 1060.0033\n",
      "Train Epoch: 014 Batch: 00066/00094 | Loss: 954.3274 | CE: 0.2957 | KD: 1060.0353\n",
      "Train Epoch: 014 Batch: 00067/00094 | Loss: 954.3068 | CE: 0.3266 | KD: 1059.9779\n",
      "Train Epoch: 014 Batch: 00068/00094 | Loss: 954.3563 | CE: 0.2919 | KD: 1060.0717\n",
      "Train Epoch: 014 Batch: 00069/00094 | Loss: 954.3734 | CE: 0.3333 | KD: 1060.0446\n",
      "Train Epoch: 014 Batch: 00070/00094 | Loss: 954.2260 | CE: 0.2518 | KD: 1059.9713\n",
      "Train Epoch: 014 Batch: 00071/00094 | Loss: 954.2819 | CE: 0.3108 | KD: 1059.9679\n",
      "Train Epoch: 014 Batch: 00072/00094 | Loss: 954.3303 | CE: 0.2873 | KD: 1060.0477\n",
      "Train Epoch: 014 Batch: 00073/00094 | Loss: 954.4338 | CE: 0.3974 | KD: 1060.0405\n",
      "Train Epoch: 014 Batch: 00074/00094 | Loss: 954.4632 | CE: 0.4257 | KD: 1060.0417\n",
      "Train Epoch: 014 Batch: 00075/00094 | Loss: 954.3270 | CE: 0.2876 | KD: 1060.0438\n",
      "Train Epoch: 014 Batch: 00076/00094 | Loss: 954.2711 | CE: 0.2777 | KD: 1059.9927\n",
      "Train Epoch: 014 Batch: 00077/00094 | Loss: 954.2884 | CE: 0.3035 | KD: 1059.9833\n",
      "Train Epoch: 014 Batch: 00078/00094 | Loss: 954.3038 | CE: 0.3202 | KD: 1059.9818\n",
      "Train Epoch: 014 Batch: 00079/00094 | Loss: 954.3865 | CE: 0.3422 | KD: 1060.0492\n",
      "Train Epoch: 014 Batch: 00080/00094 | Loss: 954.3503 | CE: 0.2613 | KD: 1060.0989\n",
      "Train Epoch: 014 Batch: 00081/00094 | Loss: 954.3325 | CE: 0.3261 | KD: 1060.0071\n",
      "Train Epoch: 014 Batch: 00082/00094 | Loss: 954.3221 | CE: 0.2703 | KD: 1060.0576\n",
      "Train Epoch: 014 Batch: 00083/00094 | Loss: 954.3865 | CE: 0.3677 | KD: 1060.0210\n",
      "Train Epoch: 014 Batch: 00084/00094 | Loss: 954.3984 | CE: 0.3461 | KD: 1060.0581\n",
      "Train Epoch: 014 Batch: 00085/00094 | Loss: 954.1912 | CE: 0.2348 | KD: 1059.9515\n",
      "Train Epoch: 014 Batch: 00086/00094 | Loss: 954.4131 | CE: 0.3759 | KD: 1060.0414\n",
      "Train Epoch: 014 Batch: 00087/00094 | Loss: 954.2885 | CE: 0.2723 | KD: 1060.0179\n",
      "Train Epoch: 014 Batch: 00088/00094 | Loss: 954.2603 | CE: 0.2468 | KD: 1060.0150\n",
      "Train Epoch: 014 Batch: 00089/00094 | Loss: 954.3598 | CE: 0.3537 | KD: 1060.0067\n",
      "Train Epoch: 014 Batch: 00090/00094 | Loss: 954.3430 | CE: 0.2990 | KD: 1060.0490\n",
      "Train Epoch: 014 Batch: 00091/00094 | Loss: 954.2965 | CE: 0.3173 | KD: 1059.9769\n",
      "Train Epoch: 014 Batch: 00092/00094 | Loss: 954.3910 | CE: 0.3313 | KD: 1060.0663\n",
      "Train Epoch: 014 Batch: 00093/00094 | Loss: 954.2690 | CE: 0.2847 | KD: 1059.9825\n",
      "Train Epoch: 014 Batch: 00094/00094 | Loss: 954.3989 | CE: 0.3278 | KD: 1060.0790\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3163 | acc:88.7500\n",
      "[VAL Acc] Target: 88.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0629 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9280 | acc:48.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9557 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8735 | acc:52.1552\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8383 | acc:55.9150\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.91%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6096 | acc:65.5564\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8968 | acc:57.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4706 | acc:75.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.25%\n",
      "[VAL Acc] Avg 60.18%\n",
      "Train Epoch: 015 Batch: 00001/00094 | Loss: 858.9145 | CE: 0.2456 | KD: 1060.0850\n",
      "Train Epoch: 015 Batch: 00002/00094 | Loss: 858.8624 | CE: 0.2325 | KD: 1060.0369\n",
      "Train Epoch: 015 Batch: 00003/00094 | Loss: 858.9100 | CE: 0.3123 | KD: 1059.9972\n",
      "Train Epoch: 015 Batch: 00004/00094 | Loss: 858.9821 | CE: 0.3056 | KD: 1060.0944\n",
      "Train Epoch: 015 Batch: 00005/00094 | Loss: 859.1276 | CE: 0.4213 | KD: 1060.1312\n",
      "Train Epoch: 015 Batch: 00006/00094 | Loss: 859.0419 | CE: 0.3544 | KD: 1060.1079\n",
      "Train Epoch: 015 Batch: 00007/00094 | Loss: 859.0369 | CE: 0.3827 | KD: 1060.0669\n",
      "Train Epoch: 015 Batch: 00008/00094 | Loss: 858.9428 | CE: 0.3699 | KD: 1059.9666\n",
      "Train Epoch: 015 Batch: 00009/00094 | Loss: 858.9379 | CE: 0.3385 | KD: 1059.9993\n",
      "Train Epoch: 015 Batch: 00010/00094 | Loss: 858.9672 | CE: 0.3485 | KD: 1060.0231\n",
      "Train Epoch: 015 Batch: 00011/00094 | Loss: 858.9340 | CE: 0.3087 | KD: 1060.0312\n",
      "Train Epoch: 015 Batch: 00012/00094 | Loss: 858.9366 | CE: 0.3161 | KD: 1060.0253\n",
      "Train Epoch: 015 Batch: 00013/00094 | Loss: 858.9235 | CE: 0.2969 | KD: 1060.0327\n",
      "Train Epoch: 015 Batch: 00014/00094 | Loss: 858.8850 | CE: 0.2882 | KD: 1059.9961\n",
      "Train Epoch: 015 Batch: 00015/00094 | Loss: 858.9378 | CE: 0.3302 | KD: 1060.0094\n",
      "Train Epoch: 015 Batch: 00016/00094 | Loss: 858.8579 | CE: 0.2829 | KD: 1059.9691\n",
      "Train Epoch: 015 Batch: 00017/00094 | Loss: 859.0384 | CE: 0.3347 | KD: 1060.1281\n",
      "Train Epoch: 015 Batch: 00018/00094 | Loss: 858.8116 | CE: 0.2796 | KD: 1059.9161\n",
      "Train Epoch: 015 Batch: 00019/00094 | Loss: 858.8649 | CE: 0.2678 | KD: 1059.9965\n",
      "Train Epoch: 015 Batch: 00020/00094 | Loss: 858.9691 | CE: 0.3292 | KD: 1060.0492\n",
      "Train Epoch: 015 Batch: 00021/00094 | Loss: 858.8763 | CE: 0.2956 | KD: 1059.9762\n",
      "Train Epoch: 015 Batch: 00022/00094 | Loss: 858.8828 | CE: 0.2899 | KD: 1059.9912\n",
      "Train Epoch: 015 Batch: 00023/00094 | Loss: 858.9494 | CE: 0.3305 | KD: 1060.0233\n",
      "Train Epoch: 015 Batch: 00024/00094 | Loss: 858.9023 | CE: 0.2943 | KD: 1060.0099\n",
      "Train Epoch: 015 Batch: 00025/00094 | Loss: 858.8004 | CE: 0.2352 | KD: 1059.9570\n",
      "Train Epoch: 015 Batch: 00026/00094 | Loss: 858.9357 | CE: 0.3153 | KD: 1060.0251\n",
      "Train Epoch: 015 Batch: 00027/00094 | Loss: 858.9417 | CE: 0.3473 | KD: 1059.9932\n",
      "Train Epoch: 015 Batch: 00028/00094 | Loss: 858.9137 | CE: 0.3431 | KD: 1059.9637\n",
      "Train Epoch: 015 Batch: 00029/00094 | Loss: 859.0153 | CE: 0.3184 | KD: 1060.1196\n",
      "Train Epoch: 015 Batch: 00030/00094 | Loss: 858.9115 | CE: 0.3007 | KD: 1060.0133\n",
      "Train Epoch: 015 Batch: 00031/00094 | Loss: 858.9615 | CE: 0.2819 | KD: 1060.0983\n",
      "Train Epoch: 015 Batch: 00032/00094 | Loss: 858.9557 | CE: 0.3160 | KD: 1060.0491\n",
      "Train Epoch: 015 Batch: 00033/00094 | Loss: 858.9255 | CE: 0.3372 | KD: 1059.9855\n",
      "Train Epoch: 015 Batch: 00034/00094 | Loss: 858.8817 | CE: 0.2870 | KD: 1059.9934\n",
      "Train Epoch: 015 Batch: 00035/00094 | Loss: 858.9323 | CE: 0.2842 | KD: 1060.0594\n",
      "Train Epoch: 015 Batch: 00036/00094 | Loss: 858.9269 | CE: 0.3014 | KD: 1060.0315\n",
      "Train Epoch: 015 Batch: 00037/00094 | Loss: 858.9650 | CE: 0.3231 | KD: 1060.0518\n",
      "Train Epoch: 015 Batch: 00038/00094 | Loss: 858.9754 | CE: 0.3212 | KD: 1060.0669\n",
      "Train Epoch: 015 Batch: 00039/00094 | Loss: 858.8232 | CE: 0.2361 | KD: 1059.9841\n",
      "Train Epoch: 015 Batch: 00040/00094 | Loss: 858.8834 | CE: 0.2774 | KD: 1060.0074\n",
      "Train Epoch: 015 Batch: 00041/00094 | Loss: 859.0036 | CE: 0.3492 | KD: 1060.0671\n",
      "Train Epoch: 015 Batch: 00042/00094 | Loss: 858.8687 | CE: 0.2504 | KD: 1060.0225\n",
      "Train Epoch: 015 Batch: 00043/00094 | Loss: 858.8253 | CE: 0.2736 | KD: 1059.9404\n",
      "Train Epoch: 015 Batch: 00044/00094 | Loss: 859.0295 | CE: 0.3507 | KD: 1060.0973\n",
      "Train Epoch: 015 Batch: 00045/00094 | Loss: 858.8577 | CE: 0.2761 | KD: 1059.9772\n",
      "Train Epoch: 015 Batch: 00046/00094 | Loss: 858.9658 | CE: 0.3263 | KD: 1060.0487\n",
      "Train Epoch: 015 Batch: 00047/00094 | Loss: 858.9612 | CE: 0.3368 | KD: 1060.0302\n",
      "Train Epoch: 015 Batch: 00048/00094 | Loss: 858.8499 | CE: 0.2924 | KD: 1059.9475\n",
      "Train Epoch: 015 Batch: 00049/00094 | Loss: 858.9788 | CE: 0.3400 | KD: 1060.0480\n",
      "Train Epoch: 015 Batch: 00050/00094 | Loss: 858.9197 | CE: 0.2825 | KD: 1060.0459\n",
      "Train Epoch: 015 Batch: 00051/00094 | Loss: 858.8920 | CE: 0.2943 | KD: 1059.9971\n",
      "Train Epoch: 015 Batch: 00052/00094 | Loss: 858.9052 | CE: 0.3078 | KD: 1059.9967\n",
      "Train Epoch: 015 Batch: 00053/00094 | Loss: 858.9019 | CE: 0.2947 | KD: 1060.0088\n",
      "Train Epoch: 015 Batch: 00054/00094 | Loss: 858.9111 | CE: 0.3120 | KD: 1059.9989\n",
      "Train Epoch: 015 Batch: 00055/00094 | Loss: 858.9155 | CE: 0.2857 | KD: 1060.0369\n",
      "Train Epoch: 015 Batch: 00056/00094 | Loss: 858.9028 | CE: 0.2977 | KD: 1060.0062\n",
      "Train Epoch: 015 Batch: 00057/00094 | Loss: 859.0096 | CE: 0.3575 | KD: 1060.0643\n",
      "Train Epoch: 015 Batch: 00058/00094 | Loss: 858.8884 | CE: 0.2908 | KD: 1059.9969\n",
      "Train Epoch: 015 Batch: 00059/00094 | Loss: 858.9194 | CE: 0.3001 | KD: 1060.0238\n",
      "Train Epoch: 015 Batch: 00060/00094 | Loss: 858.9024 | CE: 0.3218 | KD: 1059.9761\n",
      "Train Epoch: 015 Batch: 00061/00094 | Loss: 858.9716 | CE: 0.3537 | KD: 1060.0221\n",
      "Train Epoch: 015 Batch: 00062/00094 | Loss: 859.0576 | CE: 0.3877 | KD: 1060.0862\n",
      "Train Epoch: 015 Batch: 00063/00094 | Loss: 858.9904 | CE: 0.3779 | KD: 1060.0154\n",
      "Train Epoch: 015 Batch: 00064/00094 | Loss: 858.8148 | CE: 0.2416 | KD: 1059.9668\n",
      "Train Epoch: 015 Batch: 00065/00094 | Loss: 859.0441 | CE: 0.3738 | KD: 1060.0868\n",
      "Train Epoch: 015 Batch: 00066/00094 | Loss: 858.9140 | CE: 0.3435 | KD: 1059.9636\n",
      "Train Epoch: 015 Batch: 00067/00094 | Loss: 858.9077 | CE: 0.3215 | KD: 1059.9829\n",
      "Train Epoch: 015 Batch: 00068/00094 | Loss: 858.8928 | CE: 0.3003 | KD: 1059.9907\n",
      "Train Epoch: 015 Batch: 00069/00094 | Loss: 859.0264 | CE: 0.3464 | KD: 1060.0988\n",
      "Train Epoch: 015 Batch: 00070/00094 | Loss: 858.8438 | CE: 0.2558 | KD: 1059.9852\n",
      "Train Epoch: 015 Batch: 00071/00094 | Loss: 859.0101 | CE: 0.3507 | KD: 1060.0734\n",
      "Train Epoch: 015 Batch: 00072/00094 | Loss: 858.8528 | CE: 0.2422 | KD: 1060.0132\n",
      "Train Epoch: 015 Batch: 00073/00094 | Loss: 858.8763 | CE: 0.2768 | KD: 1059.9994\n",
      "Train Epoch: 015 Batch: 00074/00094 | Loss: 858.9152 | CE: 0.2772 | KD: 1060.0469\n",
      "Train Epoch: 015 Batch: 00075/00094 | Loss: 859.1161 | CE: 0.4291 | KD: 1060.1074\n",
      "Train Epoch: 015 Batch: 00076/00094 | Loss: 858.9332 | CE: 0.3342 | KD: 1059.9987\n",
      "Train Epoch: 015 Batch: 00077/00094 | Loss: 858.9836 | CE: 0.2917 | KD: 1060.1135\n",
      "Train Epoch: 015 Batch: 00078/00094 | Loss: 858.9853 | CE: 0.3702 | KD: 1060.0186\n",
      "Train Epoch: 015 Batch: 00079/00094 | Loss: 858.9509 | CE: 0.3125 | KD: 1060.0474\n",
      "Train Epoch: 015 Batch: 00080/00094 | Loss: 858.9373 | CE: 0.3079 | KD: 1060.0363\n",
      "Train Epoch: 015 Batch: 00081/00094 | Loss: 858.9773 | CE: 0.3420 | KD: 1060.0436\n",
      "Train Epoch: 015 Batch: 00082/00094 | Loss: 858.9656 | CE: 0.3430 | KD: 1060.0278\n",
      "Train Epoch: 015 Batch: 00083/00094 | Loss: 858.8765 | CE: 0.2744 | KD: 1060.0027\n",
      "Train Epoch: 015 Batch: 00084/00094 | Loss: 859.0068 | CE: 0.3850 | KD: 1060.0269\n",
      "Train Epoch: 015 Batch: 00085/00094 | Loss: 858.8757 | CE: 0.3090 | KD: 1059.9589\n",
      "Train Epoch: 015 Batch: 00086/00094 | Loss: 858.9099 | CE: 0.3140 | KD: 1059.9950\n",
      "Train Epoch: 015 Batch: 00087/00094 | Loss: 858.9942 | CE: 0.3474 | KD: 1060.0579\n",
      "Train Epoch: 015 Batch: 00088/00094 | Loss: 859.0786 | CE: 0.3531 | KD: 1060.1549\n",
      "Train Epoch: 015 Batch: 00089/00094 | Loss: 858.8286 | CE: 0.2501 | KD: 1059.9735\n",
      "Train Epoch: 015 Batch: 00090/00094 | Loss: 858.9072 | CE: 0.2912 | KD: 1060.0197\n",
      "Train Epoch: 015 Batch: 00091/00094 | Loss: 859.0048 | CE: 0.3672 | KD: 1060.0464\n",
      "Train Epoch: 015 Batch: 00092/00094 | Loss: 858.9446 | CE: 0.3235 | KD: 1060.0260\n",
      "Train Epoch: 015 Batch: 00093/00094 | Loss: 858.8411 | CE: 0.2753 | KD: 1059.9578\n",
      "Train Epoch: 015 Batch: 00094/00094 | Loss: 858.9838 | CE: 0.3586 | KD: 1060.0311\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3145 | acc:89.0000\n",
      "[VAL Acc] Target: 89.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0872 | acc:50.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9629 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9568 | acc:47.9008\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8875 | acc:53.9577\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8128 | acc:56.9316\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 56.93%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6756 | acc:61.7163\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8715 | acc:58.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4985 | acc:73.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.35%\n",
      "[VAL Acc] Avg 60.09%\n",
      "Train Epoch: 016 Batch: 00001/00094 | Loss: 858.8925 | CE: 0.2676 | KD: 1060.0306\n",
      "Train Epoch: 016 Batch: 00002/00094 | Loss: 858.9331 | CE: 0.3261 | KD: 1060.0087\n",
      "Train Epoch: 016 Batch: 00003/00094 | Loss: 858.8516 | CE: 0.2375 | KD: 1060.0175\n",
      "Train Epoch: 016 Batch: 00004/00094 | Loss: 858.9404 | CE: 0.2929 | KD: 1060.0586\n",
      "Train Epoch: 016 Batch: 00005/00094 | Loss: 858.9225 | CE: 0.3126 | KD: 1060.0122\n",
      "Train Epoch: 016 Batch: 00006/00094 | Loss: 858.8933 | CE: 0.2966 | KD: 1059.9960\n",
      "Train Epoch: 016 Batch: 00007/00094 | Loss: 858.8807 | CE: 0.3043 | KD: 1059.9708\n",
      "Train Epoch: 016 Batch: 00008/00094 | Loss: 858.9380 | CE: 0.2795 | KD: 1060.0721\n",
      "Train Epoch: 016 Batch: 00009/00094 | Loss: 859.0154 | CE: 0.3300 | KD: 1060.1055\n",
      "Train Epoch: 016 Batch: 00010/00094 | Loss: 859.0123 | CE: 0.3543 | KD: 1060.0717\n",
      "Train Epoch: 016 Batch: 00011/00094 | Loss: 858.8435 | CE: 0.2728 | KD: 1059.9639\n",
      "Train Epoch: 016 Batch: 00012/00094 | Loss: 858.9507 | CE: 0.3232 | KD: 1060.0339\n",
      "Train Epoch: 016 Batch: 00013/00094 | Loss: 858.7485 | CE: 0.2068 | KD: 1059.9281\n",
      "Train Epoch: 016 Batch: 00014/00094 | Loss: 858.9860 | CE: 0.2927 | KD: 1060.1151\n",
      "Train Epoch: 016 Batch: 00015/00094 | Loss: 858.8795 | CE: 0.2719 | KD: 1060.0093\n",
      "Train Epoch: 016 Batch: 00016/00094 | Loss: 858.8812 | CE: 0.2715 | KD: 1060.0118\n",
      "Train Epoch: 016 Batch: 00017/00094 | Loss: 859.0049 | CE: 0.3704 | KD: 1060.0426\n",
      "Train Epoch: 016 Batch: 00018/00094 | Loss: 858.9899 | CE: 0.3911 | KD: 1059.9985\n",
      "Train Epoch: 016 Batch: 00019/00094 | Loss: 859.2030 | CE: 0.4701 | KD: 1060.1641\n",
      "Train Epoch: 016 Batch: 00020/00094 | Loss: 858.8547 | CE: 0.2801 | KD: 1059.9686\n",
      "Train Epoch: 016 Batch: 00021/00094 | Loss: 858.9213 | CE: 0.2846 | KD: 1060.0454\n",
      "Train Epoch: 016 Batch: 00022/00094 | Loss: 858.8217 | CE: 0.2634 | KD: 1059.9486\n",
      "Train Epoch: 016 Batch: 00023/00094 | Loss: 858.9193 | CE: 0.2781 | KD: 1060.0509\n",
      "Train Epoch: 016 Batch: 00024/00094 | Loss: 858.9246 | CE: 0.2955 | KD: 1060.0360\n",
      "Train Epoch: 016 Batch: 00025/00094 | Loss: 859.0303 | CE: 0.3379 | KD: 1060.1140\n",
      "Train Epoch: 016 Batch: 00026/00094 | Loss: 858.7968 | CE: 0.2415 | KD: 1059.9447\n",
      "Train Epoch: 016 Batch: 00027/00094 | Loss: 859.0001 | CE: 0.3549 | KD: 1060.0559\n",
      "Train Epoch: 016 Batch: 00028/00094 | Loss: 858.8103 | CE: 0.2365 | KD: 1059.9677\n",
      "Train Epoch: 016 Batch: 00029/00094 | Loss: 858.9896 | CE: 0.3433 | KD: 1060.0571\n",
      "Train Epoch: 016 Batch: 00030/00094 | Loss: 859.0070 | CE: 0.3695 | KD: 1060.0464\n",
      "Train Epoch: 016 Batch: 00031/00094 | Loss: 858.9725 | CE: 0.3479 | KD: 1060.0304\n",
      "Train Epoch: 016 Batch: 00032/00094 | Loss: 858.8880 | CE: 0.3082 | KD: 1059.9751\n",
      "Train Epoch: 016 Batch: 00033/00094 | Loss: 858.9545 | CE: 0.3315 | KD: 1060.0283\n",
      "Train Epoch: 016 Batch: 00034/00094 | Loss: 858.7910 | CE: 0.2379 | KD: 1059.9420\n",
      "Train Epoch: 016 Batch: 00035/00094 | Loss: 859.0375 | CE: 0.3685 | KD: 1060.0852\n",
      "Train Epoch: 016 Batch: 00036/00094 | Loss: 858.9871 | CE: 0.3549 | KD: 1060.0398\n",
      "Train Epoch: 016 Batch: 00037/00094 | Loss: 858.9365 | CE: 0.2605 | KD: 1060.0939\n",
      "Train Epoch: 016 Batch: 00038/00094 | Loss: 858.7957 | CE: 0.2371 | KD: 1059.9489\n",
      "Train Epoch: 016 Batch: 00039/00094 | Loss: 858.9816 | CE: 0.3653 | KD: 1060.0201\n",
      "Train Epoch: 016 Batch: 00040/00094 | Loss: 858.9677 | CE: 0.3544 | KD: 1060.0164\n",
      "Train Epoch: 016 Batch: 00041/00094 | Loss: 858.9737 | CE: 0.3508 | KD: 1060.0282\n",
      "Train Epoch: 016 Batch: 00042/00094 | Loss: 858.8979 | CE: 0.2831 | KD: 1060.0183\n",
      "Train Epoch: 016 Batch: 00043/00094 | Loss: 858.9471 | CE: 0.3316 | KD: 1060.0192\n",
      "Train Epoch: 016 Batch: 00044/00094 | Loss: 858.9266 | CE: 0.3118 | KD: 1060.0182\n",
      "Train Epoch: 016 Batch: 00045/00094 | Loss: 858.8978 | CE: 0.2743 | KD: 1060.0291\n",
      "Train Epoch: 016 Batch: 00046/00094 | Loss: 858.8335 | CE: 0.2618 | KD: 1059.9651\n",
      "Train Epoch: 016 Batch: 00047/00094 | Loss: 858.9907 | CE: 0.2788 | KD: 1060.1381\n",
      "Train Epoch: 016 Batch: 00048/00094 | Loss: 859.0256 | CE: 0.3377 | KD: 1060.1085\n",
      "Train Epoch: 016 Batch: 00049/00094 | Loss: 859.0390 | CE: 0.3802 | KD: 1060.0726\n",
      "Train Epoch: 016 Batch: 00050/00094 | Loss: 858.9903 | CE: 0.3163 | KD: 1060.0913\n",
      "Train Epoch: 016 Batch: 00051/00094 | Loss: 858.9758 | CE: 0.3061 | KD: 1060.0861\n",
      "Train Epoch: 016 Batch: 00052/00094 | Loss: 858.8821 | CE: 0.2507 | KD: 1060.0387\n",
      "Train Epoch: 016 Batch: 00053/00094 | Loss: 858.9597 | CE: 0.3231 | KD: 1060.0452\n",
      "Train Epoch: 016 Batch: 00054/00094 | Loss: 858.8760 | CE: 0.2809 | KD: 1059.9940\n",
      "Train Epoch: 016 Batch: 00055/00094 | Loss: 858.8152 | CE: 0.2711 | KD: 1059.9310\n",
      "Train Epoch: 016 Batch: 00056/00094 | Loss: 859.1047 | CE: 0.4038 | KD: 1060.1246\n",
      "Train Epoch: 016 Batch: 00057/00094 | Loss: 858.9092 | CE: 0.3164 | KD: 1059.9911\n",
      "Train Epoch: 016 Batch: 00058/00094 | Loss: 858.9858 | CE: 0.3501 | KD: 1060.0441\n",
      "Train Epoch: 016 Batch: 00059/00094 | Loss: 858.9114 | CE: 0.2723 | KD: 1060.0482\n",
      "Train Epoch: 016 Batch: 00060/00094 | Loss: 858.9031 | CE: 0.3056 | KD: 1059.9968\n",
      "Train Epoch: 016 Batch: 00061/00094 | Loss: 858.9194 | CE: 0.2747 | KD: 1060.0553\n",
      "Train Epoch: 016 Batch: 00062/00094 | Loss: 858.8481 | CE: 0.2935 | KD: 1059.9440\n",
      "Train Epoch: 016 Batch: 00063/00094 | Loss: 858.8281 | CE: 0.2503 | KD: 1059.9725\n",
      "Train Epoch: 016 Batch: 00064/00094 | Loss: 858.8912 | CE: 0.3105 | KD: 1059.9762\n",
      "Train Epoch: 016 Batch: 00065/00094 | Loss: 858.9946 | CE: 0.3470 | KD: 1060.0587\n",
      "Train Epoch: 016 Batch: 00066/00094 | Loss: 858.9011 | CE: 0.2817 | KD: 1060.0239\n",
      "Train Epoch: 016 Batch: 00067/00094 | Loss: 858.7965 | CE: 0.2568 | KD: 1059.9255\n",
      "Train Epoch: 016 Batch: 00068/00094 | Loss: 859.1030 | CE: 0.4419 | KD: 1060.0754\n",
      "Train Epoch: 016 Batch: 00069/00094 | Loss: 858.9382 | CE: 0.3373 | KD: 1060.0011\n",
      "Train Epoch: 016 Batch: 00070/00094 | Loss: 858.8755 | CE: 0.2766 | KD: 1059.9987\n",
      "Train Epoch: 016 Batch: 00071/00094 | Loss: 858.9381 | CE: 0.2957 | KD: 1060.0524\n",
      "Train Epoch: 016 Batch: 00072/00094 | Loss: 858.8349 | CE: 0.2677 | KD: 1059.9595\n",
      "Train Epoch: 016 Batch: 00073/00094 | Loss: 858.8979 | CE: 0.3050 | KD: 1059.9912\n",
      "Train Epoch: 016 Batch: 00074/00094 | Loss: 858.9109 | CE: 0.2976 | KD: 1060.0165\n",
      "Train Epoch: 016 Batch: 00075/00094 | Loss: 858.9809 | CE: 0.3408 | KD: 1060.0496\n",
      "Train Epoch: 016 Batch: 00076/00094 | Loss: 858.9373 | CE: 0.3169 | KD: 1060.0251\n",
      "Train Epoch: 016 Batch: 00077/00094 | Loss: 858.9708 | CE: 0.3726 | KD: 1059.9977\n",
      "Train Epoch: 016 Batch: 00078/00094 | Loss: 858.8922 | CE: 0.2809 | KD: 1060.0139\n",
      "Train Epoch: 016 Batch: 00079/00094 | Loss: 858.9597 | CE: 0.2773 | KD: 1060.1017\n",
      "Train Epoch: 016 Batch: 00080/00094 | Loss: 858.8621 | CE: 0.2503 | KD: 1060.0145\n",
      "Train Epoch: 016 Batch: 00081/00094 | Loss: 858.8928 | CE: 0.2921 | KD: 1060.0009\n",
      "Train Epoch: 016 Batch: 00082/00094 | Loss: 858.9838 | CE: 0.2911 | KD: 1060.1144\n",
      "Train Epoch: 016 Batch: 00083/00094 | Loss: 858.9307 | CE: 0.3089 | KD: 1060.0269\n",
      "Train Epoch: 016 Batch: 00084/00094 | Loss: 858.9987 | CE: 0.3327 | KD: 1060.0814\n",
      "Train Epoch: 016 Batch: 00085/00094 | Loss: 858.9555 | CE: 0.3438 | KD: 1060.0144\n",
      "Train Epoch: 016 Batch: 00086/00094 | Loss: 859.0084 | CE: 0.3203 | KD: 1060.1089\n",
      "Train Epoch: 016 Batch: 00087/00094 | Loss: 858.9122 | CE: 0.2370 | KD: 1060.0928\n",
      "Train Epoch: 016 Batch: 00088/00094 | Loss: 859.0575 | CE: 0.4003 | KD: 1060.0707\n",
      "Train Epoch: 016 Batch: 00089/00094 | Loss: 858.8239 | CE: 0.2860 | KD: 1059.9233\n",
      "Train Epoch: 016 Batch: 00090/00094 | Loss: 858.9984 | CE: 0.3383 | KD: 1060.0742\n",
      "Train Epoch: 016 Batch: 00091/00094 | Loss: 859.0060 | CE: 0.3530 | KD: 1060.0656\n",
      "Train Epoch: 016 Batch: 00092/00094 | Loss: 858.9451 | CE: 0.3110 | KD: 1060.0421\n",
      "Train Epoch: 016 Batch: 00093/00094 | Loss: 858.8726 | CE: 0.2750 | KD: 1059.9969\n",
      "Train Epoch: 016 Batch: 00094/00094 | Loss: 858.8156 | CE: 0.2388 | KD: 1059.9713\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3103 | acc:88.4500\n",
      "[VAL Acc] Target: 88.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1688 | acc:50.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9960 | acc:50.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0143 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8880 | acc:52.4295\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9174 | acc:53.6044\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6295 | acc:64.3809\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9582 | acc:55.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5033 | acc:73.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.10%\n",
      "[VAL Acc] Avg 59.84%\n",
      "Train Epoch: 017 Batch: 00001/00094 | Loss: 858.9083 | CE: 0.2990 | KD: 1060.0116\n",
      "Train Epoch: 017 Batch: 00002/00094 | Loss: 858.8227 | CE: 0.2298 | KD: 1059.9912\n",
      "Train Epoch: 017 Batch: 00003/00094 | Loss: 858.8351 | CE: 0.2634 | KD: 1059.9650\n",
      "Train Epoch: 017 Batch: 00004/00094 | Loss: 858.7951 | CE: 0.2279 | KD: 1059.9595\n",
      "Train Epoch: 017 Batch: 00005/00094 | Loss: 858.9097 | CE: 0.2878 | KD: 1060.0270\n",
      "Train Epoch: 017 Batch: 00006/00094 | Loss: 858.8210 | CE: 0.2438 | KD: 1059.9719\n",
      "Train Epoch: 017 Batch: 00007/00094 | Loss: 858.8990 | CE: 0.2826 | KD: 1060.0203\n",
      "Train Epoch: 017 Batch: 00008/00094 | Loss: 858.9690 | CE: 0.2938 | KD: 1060.0928\n",
      "Train Epoch: 017 Batch: 00009/00094 | Loss: 858.8563 | CE: 0.2655 | KD: 1059.9886\n",
      "Train Epoch: 017 Batch: 00010/00094 | Loss: 859.0336 | CE: 0.3668 | KD: 1060.0824\n",
      "Train Epoch: 017 Batch: 00011/00094 | Loss: 859.1160 | CE: 0.4147 | KD: 1060.1251\n",
      "Train Epoch: 017 Batch: 00012/00094 | Loss: 858.8940 | CE: 0.3130 | KD: 1059.9766\n",
      "Train Epoch: 017 Batch: 00013/00094 | Loss: 858.9838 | CE: 0.3443 | KD: 1060.0487\n",
      "Train Epoch: 017 Batch: 00014/00094 | Loss: 858.9881 | CE: 0.3607 | KD: 1060.0337\n",
      "Train Epoch: 017 Batch: 00015/00094 | Loss: 858.9175 | CE: 0.3053 | KD: 1060.0151\n",
      "Train Epoch: 017 Batch: 00016/00094 | Loss: 858.9469 | CE: 0.3177 | KD: 1060.0360\n",
      "Train Epoch: 017 Batch: 00017/00094 | Loss: 858.9604 | CE: 0.3357 | KD: 1060.0306\n",
      "Train Epoch: 017 Batch: 00018/00094 | Loss: 859.0467 | CE: 0.3428 | KD: 1060.1283\n",
      "Train Epoch: 017 Batch: 00019/00094 | Loss: 858.8821 | CE: 0.2724 | KD: 1060.0121\n",
      "Train Epoch: 017 Batch: 00020/00094 | Loss: 859.0040 | CE: 0.3379 | KD: 1060.0815\n",
      "Train Epoch: 017 Batch: 00021/00094 | Loss: 858.8445 | CE: 0.2557 | KD: 1059.9861\n",
      "Train Epoch: 017 Batch: 00022/00094 | Loss: 859.0418 | CE: 0.3518 | KD: 1060.1112\n",
      "Train Epoch: 017 Batch: 00023/00094 | Loss: 858.8558 | CE: 0.2594 | KD: 1059.9956\n",
      "Train Epoch: 017 Batch: 00024/00094 | Loss: 858.8703 | CE: 0.2637 | KD: 1060.0082\n",
      "Train Epoch: 017 Batch: 00025/00094 | Loss: 859.0030 | CE: 0.3533 | KD: 1060.0613\n",
      "Train Epoch: 017 Batch: 00026/00094 | Loss: 858.8635 | CE: 0.2942 | KD: 1059.9620\n",
      "Train Epoch: 017 Batch: 00027/00094 | Loss: 858.9504 | CE: 0.2646 | KD: 1060.1060\n",
      "Train Epoch: 017 Batch: 00028/00094 | Loss: 859.0323 | CE: 0.3437 | KD: 1060.1094\n",
      "Train Epoch: 017 Batch: 00029/00094 | Loss: 858.8753 | CE: 0.3065 | KD: 1059.9614\n",
      "Train Epoch: 017 Batch: 00030/00094 | Loss: 858.9717 | CE: 0.3269 | KD: 1060.0553\n",
      "Train Epoch: 017 Batch: 00031/00094 | Loss: 858.9164 | CE: 0.2694 | KD: 1060.0580\n",
      "Train Epoch: 017 Batch: 00032/00094 | Loss: 859.0398 | CE: 0.3994 | KD: 1060.0499\n",
      "Train Epoch: 017 Batch: 00033/00094 | Loss: 859.0754 | CE: 0.3224 | KD: 1060.1888\n",
      "Train Epoch: 017 Batch: 00034/00094 | Loss: 858.9705 | CE: 0.3638 | KD: 1060.0082\n",
      "Train Epoch: 017 Batch: 00035/00094 | Loss: 858.8456 | CE: 0.2350 | KD: 1060.0132\n",
      "Train Epoch: 017 Batch: 00036/00094 | Loss: 859.0521 | CE: 0.3377 | KD: 1060.1412\n",
      "Train Epoch: 017 Batch: 00037/00094 | Loss: 858.8758 | CE: 0.2930 | KD: 1059.9788\n",
      "Train Epoch: 017 Batch: 00038/00094 | Loss: 858.8530 | CE: 0.2598 | KD: 1059.9916\n",
      "Train Epoch: 017 Batch: 00039/00094 | Loss: 858.9241 | CE: 0.3230 | KD: 1060.0013\n",
      "Train Epoch: 017 Batch: 00040/00094 | Loss: 858.8685 | CE: 0.2563 | KD: 1060.0151\n",
      "Train Epoch: 017 Batch: 00041/00094 | Loss: 858.9548 | CE: 0.2683 | KD: 1060.1068\n",
      "Train Epoch: 017 Batch: 00042/00094 | Loss: 858.9355 | CE: 0.2767 | KD: 1060.0726\n",
      "Train Epoch: 017 Batch: 00043/00094 | Loss: 858.9658 | CE: 0.3285 | KD: 1060.0460\n",
      "Train Epoch: 017 Batch: 00044/00094 | Loss: 858.8381 | CE: 0.2444 | KD: 1059.9922\n",
      "Train Epoch: 017 Batch: 00045/00094 | Loss: 858.9449 | CE: 0.3100 | KD: 1060.0431\n",
      "Train Epoch: 017 Batch: 00046/00094 | Loss: 858.8922 | CE: 0.2727 | KD: 1060.0240\n",
      "Train Epoch: 017 Batch: 00047/00094 | Loss: 858.9440 | CE: 0.3540 | KD: 1059.9877\n",
      "Train Epoch: 017 Batch: 00048/00094 | Loss: 858.8356 | CE: 0.2362 | KD: 1059.9994\n",
      "Train Epoch: 017 Batch: 00049/00094 | Loss: 858.8953 | CE: 0.2770 | KD: 1060.0226\n",
      "Train Epoch: 017 Batch: 00050/00094 | Loss: 858.8951 | CE: 0.2880 | KD: 1060.0088\n",
      "Train Epoch: 017 Batch: 00051/00094 | Loss: 858.9627 | CE: 0.2966 | KD: 1060.0817\n",
      "Train Epoch: 017 Batch: 00052/00094 | Loss: 858.9189 | CE: 0.2923 | KD: 1060.0328\n",
      "Train Epoch: 017 Batch: 00053/00094 | Loss: 858.8254 | CE: 0.2366 | KD: 1059.9861\n",
      "Train Epoch: 017 Batch: 00054/00094 | Loss: 858.9226 | CE: 0.2982 | KD: 1060.0300\n",
      "Train Epoch: 017 Batch: 00055/00094 | Loss: 858.9604 | CE: 0.3298 | KD: 1060.0377\n",
      "Train Epoch: 017 Batch: 00056/00094 | Loss: 859.0634 | CE: 0.3816 | KD: 1060.1010\n",
      "Train Epoch: 017 Batch: 00057/00094 | Loss: 858.8461 | CE: 0.2750 | KD: 1059.9644\n",
      "Train Epoch: 017 Batch: 00058/00094 | Loss: 858.9106 | CE: 0.3084 | KD: 1060.0028\n",
      "Train Epoch: 017 Batch: 00059/00094 | Loss: 858.8565 | CE: 0.2836 | KD: 1059.9666\n",
      "Train Epoch: 017 Batch: 00060/00094 | Loss: 858.8411 | CE: 0.2397 | KD: 1060.0017\n",
      "Train Epoch: 017 Batch: 00061/00094 | Loss: 858.8502 | CE: 0.2284 | KD: 1060.0269\n",
      "Train Epoch: 017 Batch: 00062/00094 | Loss: 858.9008 | CE: 0.2634 | KD: 1060.0463\n",
      "Train Epoch: 017 Batch: 00063/00094 | Loss: 858.9079 | CE: 0.2887 | KD: 1060.0238\n",
      "Train Epoch: 017 Batch: 00064/00094 | Loss: 858.8622 | CE: 0.2953 | KD: 1059.9592\n",
      "Train Epoch: 017 Batch: 00065/00094 | Loss: 858.8058 | CE: 0.2485 | KD: 1059.9474\n",
      "Train Epoch: 017 Batch: 00066/00094 | Loss: 858.9692 | CE: 0.3297 | KD: 1060.0487\n",
      "Train Epoch: 017 Batch: 00067/00094 | Loss: 858.8154 | CE: 0.2367 | KD: 1059.9736\n",
      "Train Epoch: 017 Batch: 00068/00094 | Loss: 858.8646 | CE: 0.2862 | KD: 1059.9733\n",
      "Train Epoch: 017 Batch: 00069/00094 | Loss: 858.9752 | CE: 0.2985 | KD: 1060.0946\n",
      "Train Epoch: 017 Batch: 00070/00094 | Loss: 859.0836 | CE: 0.4092 | KD: 1060.0918\n",
      "Train Epoch: 017 Batch: 00071/00094 | Loss: 858.9641 | CE: 0.3346 | KD: 1060.0364\n",
      "Train Epoch: 017 Batch: 00072/00094 | Loss: 858.9117 | CE: 0.3132 | KD: 1059.9982\n",
      "Train Epoch: 017 Batch: 00073/00094 | Loss: 859.1711 | CE: 0.4763 | KD: 1060.1171\n",
      "Train Epoch: 017 Batch: 00074/00094 | Loss: 858.8583 | CE: 0.2810 | KD: 1059.9720\n",
      "Train Epoch: 017 Batch: 00075/00094 | Loss: 858.9012 | CE: 0.3242 | KD: 1059.9716\n",
      "Train Epoch: 017 Batch: 00076/00094 | Loss: 859.0262 | CE: 0.2793 | KD: 1060.1814\n",
      "Train Epoch: 017 Batch: 00077/00094 | Loss: 858.9791 | CE: 0.3070 | KD: 1060.0890\n",
      "Train Epoch: 017 Batch: 00078/00094 | Loss: 858.9450 | CE: 0.2857 | KD: 1060.0732\n",
      "Train Epoch: 017 Batch: 00079/00094 | Loss: 859.0894 | CE: 0.3824 | KD: 1060.1321\n",
      "Train Epoch: 017 Batch: 00080/00094 | Loss: 858.8452 | CE: 0.2667 | KD: 1059.9734\n",
      "Train Epoch: 017 Batch: 00081/00094 | Loss: 858.8661 | CE: 0.2671 | KD: 1059.9988\n",
      "Train Epoch: 017 Batch: 00082/00094 | Loss: 859.0847 | CE: 0.3651 | KD: 1060.1477\n",
      "Train Epoch: 017 Batch: 00083/00094 | Loss: 858.9605 | CE: 0.3199 | KD: 1060.0502\n",
      "Train Epoch: 017 Batch: 00084/00094 | Loss: 858.9013 | CE: 0.2402 | KD: 1060.0754\n",
      "Train Epoch: 017 Batch: 00085/00094 | Loss: 858.9432 | CE: 0.2837 | KD: 1060.0735\n",
      "Train Epoch: 017 Batch: 00086/00094 | Loss: 858.9266 | CE: 0.2728 | KD: 1060.0665\n",
      "Train Epoch: 017 Batch: 00087/00094 | Loss: 858.9633 | CE: 0.3416 | KD: 1060.0267\n",
      "Train Epoch: 017 Batch: 00088/00094 | Loss: 858.9846 | CE: 0.3625 | KD: 1060.0272\n",
      "Train Epoch: 017 Batch: 00089/00094 | Loss: 858.8713 | CE: 0.2712 | KD: 1060.0001\n",
      "Train Epoch: 017 Batch: 00090/00094 | Loss: 858.8979 | CE: 0.2830 | KD: 1060.0184\n",
      "Train Epoch: 017 Batch: 00091/00094 | Loss: 858.9890 | CE: 0.3759 | KD: 1060.0162\n",
      "Train Epoch: 017 Batch: 00092/00094 | Loss: 859.0554 | CE: 0.3971 | KD: 1060.0720\n",
      "Train Epoch: 017 Batch: 00093/00094 | Loss: 858.7878 | CE: 0.2296 | KD: 1059.9485\n",
      "Train Epoch: 017 Batch: 00094/00094 | Loss: 859.0360 | CE: 0.3692 | KD: 1060.0825\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3292 | acc:86.6500\n",
      "[VAL Acc] Target: 86.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2237 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0456 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0600 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9569 | acc:52.4687\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9722 | acc:53.6969\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7106 | acc:61.3636\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.36%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9305 | acc:56.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5518 | acc:69.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 69.10%\n",
      "[VAL Acc] Avg 58.69%\n",
      "Train Epoch: 018 Batch: 00001/00094 | Loss: 858.9638 | CE: 0.3177 | KD: 1060.0569\n",
      "Train Epoch: 018 Batch: 00002/00094 | Loss: 859.0609 | CE: 0.3963 | KD: 1060.0797\n",
      "Train Epoch: 018 Batch: 00003/00094 | Loss: 858.8643 | CE: 0.2657 | KD: 1059.9983\n",
      "Train Epoch: 018 Batch: 00004/00094 | Loss: 858.8743 | CE: 0.2902 | KD: 1059.9803\n",
      "Train Epoch: 018 Batch: 00005/00094 | Loss: 859.0981 | CE: 0.4373 | KD: 1060.0751\n",
      "Train Epoch: 018 Batch: 00006/00094 | Loss: 858.9810 | CE: 0.3045 | KD: 1060.0944\n",
      "Train Epoch: 018 Batch: 00007/00094 | Loss: 858.8501 | CE: 0.2610 | KD: 1059.9866\n",
      "Train Epoch: 018 Batch: 00008/00094 | Loss: 859.0599 | CE: 0.3549 | KD: 1060.1296\n",
      "Train Epoch: 018 Batch: 00009/00094 | Loss: 858.9830 | CE: 0.3386 | KD: 1060.0549\n",
      "Train Epoch: 018 Batch: 00010/00094 | Loss: 858.9289 | CE: 0.2975 | KD: 1060.0387\n",
      "Train Epoch: 018 Batch: 00011/00094 | Loss: 858.8485 | CE: 0.2482 | KD: 1060.0004\n",
      "Train Epoch: 018 Batch: 00012/00094 | Loss: 858.9559 | CE: 0.2931 | KD: 1060.0775\n",
      "Train Epoch: 018 Batch: 00013/00094 | Loss: 858.9343 | CE: 0.3135 | KD: 1060.0258\n",
      "Train Epoch: 018 Batch: 00014/00094 | Loss: 858.9582 | CE: 0.3619 | KD: 1059.9954\n",
      "Train Epoch: 018 Batch: 00015/00094 | Loss: 858.9312 | CE: 0.2814 | KD: 1060.0614\n",
      "Train Epoch: 018 Batch: 00016/00094 | Loss: 858.9005 | CE: 0.2787 | KD: 1060.0270\n",
      "Train Epoch: 018 Batch: 00017/00094 | Loss: 858.9017 | CE: 0.3242 | KD: 1059.9722\n",
      "Train Epoch: 018 Batch: 00018/00094 | Loss: 858.9883 | CE: 0.2941 | KD: 1060.1163\n",
      "Train Epoch: 018 Batch: 00019/00094 | Loss: 858.8836 | CE: 0.2863 | KD: 1059.9967\n",
      "Train Epoch: 018 Batch: 00020/00094 | Loss: 858.9178 | CE: 0.2984 | KD: 1060.0240\n",
      "Train Epoch: 018 Batch: 00021/00094 | Loss: 859.0880 | CE: 0.3897 | KD: 1060.1213\n",
      "Train Epoch: 018 Batch: 00022/00094 | Loss: 859.0309 | CE: 0.3428 | KD: 1060.1089\n",
      "Train Epoch: 018 Batch: 00023/00094 | Loss: 858.9115 | CE: 0.2873 | KD: 1060.0299\n",
      "Train Epoch: 018 Batch: 00024/00094 | Loss: 858.9634 | CE: 0.2828 | KD: 1060.0996\n",
      "Train Epoch: 018 Batch: 00025/00094 | Loss: 858.9249 | CE: 0.2953 | KD: 1060.0365\n",
      "Train Epoch: 018 Batch: 00026/00094 | Loss: 858.9125 | CE: 0.3013 | KD: 1060.0139\n",
      "Train Epoch: 018 Batch: 00027/00094 | Loss: 858.9689 | CE: 0.3685 | KD: 1060.0005\n",
      "Train Epoch: 018 Batch: 00028/00094 | Loss: 858.9565 | CE: 0.3536 | KD: 1060.0035\n",
      "Train Epoch: 018 Batch: 00029/00094 | Loss: 858.8141 | CE: 0.2419 | KD: 1059.9657\n",
      "Train Epoch: 018 Batch: 00030/00094 | Loss: 858.7859 | CE: 0.2383 | KD: 1059.9353\n",
      "Train Epoch: 018 Batch: 00031/00094 | Loss: 858.8173 | CE: 0.2297 | KD: 1059.9847\n",
      "Train Epoch: 018 Batch: 00032/00094 | Loss: 858.9059 | CE: 0.2979 | KD: 1060.0099\n",
      "Train Epoch: 018 Batch: 00033/00094 | Loss: 858.9161 | CE: 0.2775 | KD: 1060.0476\n",
      "Train Epoch: 018 Batch: 00034/00094 | Loss: 858.9642 | CE: 0.3344 | KD: 1060.0369\n",
      "Train Epoch: 018 Batch: 00035/00094 | Loss: 858.9589 | CE: 0.3308 | KD: 1060.0347\n",
      "Train Epoch: 018 Batch: 00036/00094 | Loss: 859.0424 | CE: 0.3544 | KD: 1060.1086\n",
      "Train Epoch: 018 Batch: 00037/00094 | Loss: 858.9257 | CE: 0.3212 | KD: 1060.0056\n",
      "Train Epoch: 018 Batch: 00038/00094 | Loss: 858.9650 | CE: 0.3387 | KD: 1060.0326\n",
      "Train Epoch: 018 Batch: 00039/00094 | Loss: 858.9371 | CE: 0.2999 | KD: 1060.0459\n",
      "Train Epoch: 018 Batch: 00040/00094 | Loss: 859.0026 | CE: 0.3430 | KD: 1060.0735\n",
      "Train Epoch: 018 Batch: 00041/00094 | Loss: 858.8468 | CE: 0.2588 | KD: 1059.9851\n",
      "Train Epoch: 018 Batch: 00042/00094 | Loss: 858.8323 | CE: 0.2396 | KD: 1059.9910\n",
      "Train Epoch: 018 Batch: 00043/00094 | Loss: 858.9437 | CE: 0.3281 | KD: 1060.0193\n",
      "Train Epoch: 018 Batch: 00044/00094 | Loss: 858.8815 | CE: 0.3080 | KD: 1059.9672\n",
      "Train Epoch: 018 Batch: 00045/00094 | Loss: 858.9194 | CE: 0.2973 | KD: 1060.0273\n",
      "Train Epoch: 018 Batch: 00046/00094 | Loss: 858.8672 | CE: 0.2821 | KD: 1059.9816\n",
      "Train Epoch: 018 Batch: 00047/00094 | Loss: 858.9596 | CE: 0.3115 | KD: 1060.0594\n",
      "Train Epoch: 018 Batch: 00048/00094 | Loss: 858.7797 | CE: 0.2392 | KD: 1059.9265\n",
      "Train Epoch: 018 Batch: 00049/00094 | Loss: 859.0565 | CE: 0.4130 | KD: 1060.0537\n",
      "Train Epoch: 018 Batch: 00050/00094 | Loss: 858.9844 | CE: 0.3459 | KD: 1060.0475\n",
      "Train Epoch: 018 Batch: 00051/00094 | Loss: 858.9960 | CE: 0.3425 | KD: 1060.0660\n",
      "Train Epoch: 018 Batch: 00052/00094 | Loss: 858.8463 | CE: 0.2780 | KD: 1059.9608\n",
      "Train Epoch: 018 Batch: 00053/00094 | Loss: 858.8787 | CE: 0.3061 | KD: 1059.9663\n",
      "Train Epoch: 018 Batch: 00054/00094 | Loss: 858.8257 | CE: 0.2398 | KD: 1059.9827\n",
      "Train Epoch: 018 Batch: 00055/00094 | Loss: 858.9980 | CE: 0.3936 | KD: 1060.0055\n",
      "Train Epoch: 018 Batch: 00056/00094 | Loss: 858.8248 | CE: 0.1927 | KD: 1060.0397\n",
      "Train Epoch: 018 Batch: 00057/00094 | Loss: 858.9782 | CE: 0.3169 | KD: 1060.0757\n",
      "Train Epoch: 018 Batch: 00058/00094 | Loss: 859.0552 | CE: 0.3919 | KD: 1060.0781\n",
      "Train Epoch: 018 Batch: 00059/00094 | Loss: 859.0309 | CE: 0.3459 | KD: 1060.1049\n",
      "Train Epoch: 018 Batch: 00060/00094 | Loss: 858.9929 | CE: 0.3279 | KD: 1060.0802\n",
      "Train Epoch: 018 Batch: 00061/00094 | Loss: 859.0278 | CE: 0.3801 | KD: 1060.0590\n",
      "Train Epoch: 018 Batch: 00062/00094 | Loss: 858.9757 | CE: 0.3403 | KD: 1060.0437\n",
      "Train Epoch: 018 Batch: 00063/00094 | Loss: 858.8523 | CE: 0.2316 | KD: 1060.0255\n",
      "Train Epoch: 018 Batch: 00064/00094 | Loss: 859.0464 | CE: 0.3605 | KD: 1060.1061\n",
      "Train Epoch: 018 Batch: 00065/00094 | Loss: 858.8127 | CE: 0.2312 | KD: 1059.9772\n",
      "Train Epoch: 018 Batch: 00066/00094 | Loss: 859.0970 | CE: 0.3942 | KD: 1060.1268\n",
      "Train Epoch: 018 Batch: 00067/00094 | Loss: 858.8445 | CE: 0.2585 | KD: 1059.9828\n",
      "Train Epoch: 018 Batch: 00068/00094 | Loss: 858.9828 | CE: 0.3595 | KD: 1060.0288\n",
      "Train Epoch: 018 Batch: 00069/00094 | Loss: 858.9373 | CE: 0.3121 | KD: 1060.0311\n",
      "Train Epoch: 018 Batch: 00070/00094 | Loss: 858.8729 | CE: 0.2563 | KD: 1060.0205\n",
      "Train Epoch: 018 Batch: 00071/00094 | Loss: 858.8157 | CE: 0.2344 | KD: 1059.9768\n",
      "Train Epoch: 018 Batch: 00072/00094 | Loss: 859.0339 | CE: 0.3477 | KD: 1060.1064\n",
      "Train Epoch: 018 Batch: 00073/00094 | Loss: 858.8750 | CE: 0.2797 | KD: 1059.9941\n",
      "Train Epoch: 018 Batch: 00074/00094 | Loss: 859.0355 | CE: 0.3629 | KD: 1060.0896\n",
      "Train Epoch: 018 Batch: 00075/00094 | Loss: 858.9485 | CE: 0.2942 | KD: 1060.0670\n",
      "Train Epoch: 018 Batch: 00076/00094 | Loss: 858.8702 | CE: 0.2644 | KD: 1060.0072\n",
      "Train Epoch: 018 Batch: 00077/00094 | Loss: 858.8260 | CE: 0.2609 | KD: 1059.9569\n",
      "Train Epoch: 018 Batch: 00078/00094 | Loss: 858.7855 | CE: 0.2114 | KD: 1059.9680\n",
      "Train Epoch: 018 Batch: 00079/00094 | Loss: 858.8436 | CE: 0.2350 | KD: 1060.0106\n",
      "Train Epoch: 018 Batch: 00080/00094 | Loss: 858.9837 | CE: 0.3262 | KD: 1060.0710\n",
      "Train Epoch: 018 Batch: 00081/00094 | Loss: 859.0208 | CE: 0.3690 | KD: 1060.0638\n",
      "Train Epoch: 018 Batch: 00082/00094 | Loss: 859.0165 | CE: 0.3363 | KD: 1060.0989\n",
      "Train Epoch: 018 Batch: 00083/00094 | Loss: 858.8591 | CE: 0.2378 | KD: 1060.0264\n",
      "Train Epoch: 018 Batch: 00084/00094 | Loss: 858.9982 | CE: 0.3649 | KD: 1060.0411\n",
      "Train Epoch: 018 Batch: 00085/00094 | Loss: 858.9165 | CE: 0.2902 | KD: 1060.0325\n",
      "Train Epoch: 018 Batch: 00086/00094 | Loss: 858.8837 | CE: 0.3080 | KD: 1059.9700\n",
      "Train Epoch: 018 Batch: 00087/00094 | Loss: 859.1555 | CE: 0.4441 | KD: 1060.1376\n",
      "Train Epoch: 018 Batch: 00088/00094 | Loss: 858.9656 | CE: 0.2883 | KD: 1060.0953\n",
      "Train Epoch: 018 Batch: 00089/00094 | Loss: 858.9234 | CE: 0.3281 | KD: 1059.9943\n",
      "Train Epoch: 018 Batch: 00090/00094 | Loss: 858.9422 | CE: 0.3156 | KD: 1060.0328\n",
      "Train Epoch: 018 Batch: 00091/00094 | Loss: 858.9379 | CE: 0.3190 | KD: 1060.0233\n",
      "Train Epoch: 018 Batch: 00092/00094 | Loss: 859.0386 | CE: 0.3808 | KD: 1060.0713\n",
      "Train Epoch: 018 Batch: 00093/00094 | Loss: 858.9760 | CE: 0.3345 | KD: 1060.0513\n",
      "Train Epoch: 018 Batch: 00094/00094 | Loss: 858.9139 | CE: 0.2914 | KD: 1060.0277\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3133 | acc:87.7000\n",
      "[VAL Acc] Target: 87.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1896 | acc:50.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0262 | acc:49.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0610 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9396 | acc:53.0172\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9528 | acc:53.8817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7216 | acc:60.0705\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 60.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9682 | acc:55.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5409 | acc:71.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 71.50%\n",
      "[VAL Acc] Avg 58.73%\n",
      "Train Epoch: 019 Batch: 00001/00094 | Loss: 773.0311 | CE: 0.2957 | KD: 1059.9938\n",
      "Train Epoch: 019 Batch: 00002/00094 | Loss: 773.0058 | CE: 0.2409 | KD: 1060.0342\n",
      "Train Epoch: 019 Batch: 00003/00094 | Loss: 772.9625 | CE: 0.2528 | KD: 1059.9584\n",
      "Train Epoch: 019 Batch: 00004/00094 | Loss: 773.0373 | CE: 0.2740 | KD: 1060.0320\n",
      "Train Epoch: 019 Batch: 00005/00094 | Loss: 772.9903 | CE: 0.2625 | KD: 1059.9833\n",
      "Train Epoch: 019 Batch: 00006/00094 | Loss: 772.9745 | CE: 0.2177 | KD: 1060.0232\n",
      "Train Epoch: 019 Batch: 00007/00094 | Loss: 773.1083 | CE: 0.2682 | KD: 1060.1375\n",
      "Train Epoch: 019 Batch: 00008/00094 | Loss: 773.0208 | CE: 0.2680 | KD: 1060.0176\n",
      "Train Epoch: 019 Batch: 00009/00094 | Loss: 773.1261 | CE: 0.3195 | KD: 1060.0914\n",
      "Train Epoch: 019 Batch: 00010/00094 | Loss: 772.9774 | CE: 0.2714 | KD: 1059.9534\n",
      "Train Epoch: 019 Batch: 00011/00094 | Loss: 773.0731 | CE: 0.3241 | KD: 1060.0125\n",
      "Train Epoch: 019 Batch: 00012/00094 | Loss: 773.0932 | CE: 0.3038 | KD: 1060.0679\n",
      "Train Epoch: 019 Batch: 00013/00094 | Loss: 773.0049 | CE: 0.2281 | KD: 1060.0505\n",
      "Train Epoch: 019 Batch: 00014/00094 | Loss: 773.0264 | CE: 0.2718 | KD: 1060.0201\n",
      "Train Epoch: 019 Batch: 00015/00094 | Loss: 772.9518 | CE: 0.2237 | KD: 1059.9838\n",
      "Train Epoch: 019 Batch: 00016/00094 | Loss: 773.1068 | CE: 0.3271 | KD: 1060.0544\n",
      "Train Epoch: 019 Batch: 00017/00094 | Loss: 773.0049 | CE: 0.2902 | KD: 1059.9653\n",
      "Train Epoch: 019 Batch: 00018/00094 | Loss: 773.0089 | CE: 0.2913 | KD: 1059.9692\n",
      "Train Epoch: 019 Batch: 00019/00094 | Loss: 773.1318 | CE: 0.3339 | KD: 1060.0796\n",
      "Train Epoch: 019 Batch: 00020/00094 | Loss: 773.0058 | CE: 0.2501 | KD: 1060.0216\n",
      "Train Epoch: 019 Batch: 00021/00094 | Loss: 773.1028 | CE: 0.3105 | KD: 1060.0718\n",
      "Train Epoch: 019 Batch: 00022/00094 | Loss: 773.1075 | CE: 0.3143 | KD: 1060.0731\n",
      "Train Epoch: 019 Batch: 00023/00094 | Loss: 773.0597 | CE: 0.3231 | KD: 1059.9954\n",
      "Train Epoch: 019 Batch: 00024/00094 | Loss: 772.9883 | CE: 0.2714 | KD: 1059.9683\n",
      "Train Epoch: 019 Batch: 00025/00094 | Loss: 772.9236 | CE: 0.1906 | KD: 1059.9906\n",
      "Train Epoch: 019 Batch: 00026/00094 | Loss: 773.0975 | CE: 0.2936 | KD: 1060.0876\n",
      "Train Epoch: 019 Batch: 00027/00094 | Loss: 772.9992 | CE: 0.2413 | KD: 1060.0245\n",
      "Train Epoch: 019 Batch: 00028/00094 | Loss: 773.1705 | CE: 0.4003 | KD: 1060.0415\n",
      "Train Epoch: 019 Batch: 00029/00094 | Loss: 772.9227 | CE: 0.2408 | KD: 1059.9204\n",
      "Train Epoch: 019 Batch: 00030/00094 | Loss: 773.0859 | CE: 0.3341 | KD: 1060.0162\n",
      "Train Epoch: 019 Batch: 00031/00094 | Loss: 773.0042 | CE: 0.2772 | KD: 1059.9822\n",
      "Train Epoch: 019 Batch: 00032/00094 | Loss: 773.0121 | CE: 0.2816 | KD: 1059.9871\n",
      "Train Epoch: 019 Batch: 00033/00094 | Loss: 772.9806 | CE: 0.2674 | KD: 1059.9633\n",
      "Train Epoch: 019 Batch: 00034/00094 | Loss: 772.9855 | CE: 0.2539 | KD: 1059.9885\n",
      "Train Epoch: 019 Batch: 00035/00094 | Loss: 773.0972 | CE: 0.3103 | KD: 1060.0643\n",
      "Train Epoch: 019 Batch: 00036/00094 | Loss: 773.1635 | CE: 0.3586 | KD: 1060.0890\n",
      "Train Epoch: 019 Batch: 00037/00094 | Loss: 773.1471 | CE: 0.3612 | KD: 1060.0631\n",
      "Train Epoch: 019 Batch: 00038/00094 | Loss: 773.1717 | CE: 0.3683 | KD: 1060.0870\n",
      "Train Epoch: 019 Batch: 00039/00094 | Loss: 773.0429 | CE: 0.2984 | KD: 1060.0062\n",
      "Train Epoch: 019 Batch: 00040/00094 | Loss: 773.1935 | CE: 0.3756 | KD: 1060.1069\n",
      "Train Epoch: 019 Batch: 00041/00094 | Loss: 773.0273 | CE: 0.2539 | KD: 1060.0459\n",
      "Train Epoch: 019 Batch: 00042/00094 | Loss: 773.1315 | CE: 0.3260 | KD: 1060.0900\n",
      "Train Epoch: 019 Batch: 00043/00094 | Loss: 773.0601 | CE: 0.3102 | KD: 1060.0137\n",
      "Train Epoch: 019 Batch: 00044/00094 | Loss: 773.0350 | CE: 0.2967 | KD: 1059.9977\n",
      "Train Epoch: 019 Batch: 00045/00094 | Loss: 773.1070 | CE: 0.3219 | KD: 1060.0619\n",
      "Train Epoch: 019 Batch: 00046/00094 | Loss: 773.0608 | CE: 0.2677 | KD: 1060.0729\n",
      "Train Epoch: 019 Batch: 00047/00094 | Loss: 773.1215 | CE: 0.3430 | KD: 1060.0529\n",
      "Train Epoch: 019 Batch: 00048/00094 | Loss: 773.0826 | CE: 0.2909 | KD: 1060.0709\n",
      "Train Epoch: 019 Batch: 00049/00094 | Loss: 772.9866 | CE: 0.2302 | KD: 1060.0226\n",
      "Train Epoch: 019 Batch: 00050/00094 | Loss: 773.2021 | CE: 0.3775 | KD: 1060.1162\n",
      "Train Epoch: 019 Batch: 00051/00094 | Loss: 772.9917 | CE: 0.2306 | KD: 1060.0289\n",
      "Train Epoch: 019 Batch: 00052/00094 | Loss: 773.0289 | CE: 0.2887 | KD: 1060.0002\n",
      "Train Epoch: 019 Batch: 00053/00094 | Loss: 773.0231 | CE: 0.2793 | KD: 1060.0052\n",
      "Train Epoch: 019 Batch: 00054/00094 | Loss: 773.1660 | CE: 0.3984 | KD: 1060.0378\n",
      "Train Epoch: 019 Batch: 00055/00094 | Loss: 773.0268 | CE: 0.3258 | KD: 1059.9464\n",
      "Train Epoch: 019 Batch: 00056/00094 | Loss: 773.1000 | CE: 0.3546 | KD: 1060.0076\n",
      "Train Epoch: 019 Batch: 00057/00094 | Loss: 772.9955 | CE: 0.2564 | KD: 1059.9988\n",
      "Train Epoch: 019 Batch: 00058/00094 | Loss: 773.0253 | CE: 0.2054 | KD: 1060.1097\n",
      "Train Epoch: 019 Batch: 00059/00094 | Loss: 773.1423 | CE: 0.2802 | KD: 1060.1675\n",
      "Train Epoch: 019 Batch: 00060/00094 | Loss: 773.0522 | CE: 0.2891 | KD: 1060.0317\n",
      "Train Epoch: 019 Batch: 00061/00094 | Loss: 773.0476 | CE: 0.3053 | KD: 1060.0032\n",
      "Train Epoch: 019 Batch: 00062/00094 | Loss: 773.1030 | CE: 0.3452 | KD: 1060.0244\n",
      "Train Epoch: 019 Batch: 00063/00094 | Loss: 773.0328 | CE: 0.2312 | KD: 1060.0846\n",
      "Train Epoch: 019 Batch: 00064/00094 | Loss: 773.0832 | CE: 0.3108 | KD: 1060.0446\n",
      "Train Epoch: 019 Batch: 00065/00094 | Loss: 772.9890 | CE: 0.2680 | KD: 1059.9740\n",
      "Train Epoch: 019 Batch: 00066/00094 | Loss: 773.0983 | CE: 0.3105 | KD: 1060.0657\n",
      "Train Epoch: 019 Batch: 00067/00094 | Loss: 773.1069 | CE: 0.3071 | KD: 1060.0820\n",
      "Train Epoch: 019 Batch: 00068/00094 | Loss: 772.9042 | CE: 0.2142 | KD: 1059.9315\n",
      "Train Epoch: 019 Batch: 00069/00094 | Loss: 773.0416 | CE: 0.2665 | KD: 1060.0481\n",
      "Train Epoch: 019 Batch: 00070/00094 | Loss: 772.9856 | CE: 0.2406 | KD: 1060.0070\n",
      "Train Epoch: 019 Batch: 00071/00094 | Loss: 773.0381 | CE: 0.2687 | KD: 1060.0404\n",
      "Train Epoch: 019 Batch: 00072/00094 | Loss: 773.0471 | CE: 0.2883 | KD: 1060.0258\n",
      "Train Epoch: 019 Batch: 00073/00094 | Loss: 773.1876 | CE: 0.3781 | KD: 1060.0953\n",
      "Train Epoch: 019 Batch: 00074/00094 | Loss: 773.0943 | CE: 0.2786 | KD: 1060.1039\n",
      "Train Epoch: 019 Batch: 00075/00094 | Loss: 773.0607 | CE: 0.2940 | KD: 1060.0366\n",
      "Train Epoch: 019 Batch: 00076/00094 | Loss: 773.1146 | CE: 0.3135 | KD: 1060.0839\n",
      "Train Epoch: 019 Batch: 00077/00094 | Loss: 772.9680 | CE: 0.2595 | KD: 1059.9568\n",
      "Train Epoch: 019 Batch: 00078/00094 | Loss: 773.1804 | CE: 0.3482 | KD: 1060.1265\n",
      "Train Epoch: 019 Batch: 00079/00094 | Loss: 773.1011 | CE: 0.3083 | KD: 1060.0725\n",
      "Train Epoch: 019 Batch: 00080/00094 | Loss: 773.0440 | CE: 0.3232 | KD: 1059.9738\n",
      "Train Epoch: 019 Batch: 00081/00094 | Loss: 773.0014 | CE: 0.2496 | KD: 1060.0162\n",
      "Train Epoch: 019 Batch: 00082/00094 | Loss: 773.0172 | CE: 0.2765 | KD: 1060.0010\n",
      "Train Epoch: 019 Batch: 00083/00094 | Loss: 772.9938 | CE: 0.2425 | KD: 1060.0155\n",
      "Train Epoch: 019 Batch: 00084/00094 | Loss: 773.3246 | CE: 0.4552 | KD: 1060.1775\n",
      "Train Epoch: 019 Batch: 00085/00094 | Loss: 773.1772 | CE: 0.3718 | KD: 1060.0898\n",
      "Train Epoch: 019 Batch: 00086/00094 | Loss: 773.1218 | CE: 0.3159 | KD: 1060.0905\n",
      "Train Epoch: 019 Batch: 00087/00094 | Loss: 772.9746 | CE: 0.2763 | KD: 1059.9429\n",
      "Train Epoch: 019 Batch: 00088/00094 | Loss: 773.0032 | CE: 0.2274 | KD: 1060.0491\n",
      "Train Epoch: 019 Batch: 00089/00094 | Loss: 772.9684 | CE: 0.2497 | KD: 1059.9708\n",
      "Train Epoch: 019 Batch: 00090/00094 | Loss: 773.0608 | CE: 0.2884 | KD: 1060.0444\n",
      "Train Epoch: 019 Batch: 00091/00094 | Loss: 773.2125 | CE: 0.4083 | KD: 1060.0880\n",
      "Train Epoch: 019 Batch: 00092/00094 | Loss: 773.0648 | CE: 0.3015 | KD: 1060.0321\n",
      "Train Epoch: 019 Batch: 00093/00094 | Loss: 773.0226 | CE: 0.2524 | KD: 1060.0415\n",
      "Train Epoch: 019 Batch: 00094/00094 | Loss: 772.9541 | CE: 0.2288 | KD: 1059.9799\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3091 | acc:89.3000\n",
      "[VAL Acc] Target: 89.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0910 | acc:49.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9739 | acc:47.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9633 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7982 | acc:53.7618\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8536 | acc:55.3604\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.36%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5956 | acc:66.7712\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.77%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9933 | acc:54.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4332 | acc:78.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 78.25%\n",
      "[VAL Acc] Avg 60.52%\n",
      "VAL Acc improve from 60.40% to 60.52%\n",
      "Save best model\n",
      "Train Epoch: 020 Batch: 00001/00094 | Loss: 773.1395 | CE: 0.3924 | KD: 1060.0096\n",
      "Train Epoch: 020 Batch: 00002/00094 | Loss: 773.0803 | CE: 0.3041 | KD: 1060.0497\n",
      "Train Epoch: 020 Batch: 00003/00094 | Loss: 773.2210 | CE: 0.3955 | KD: 1060.1174\n",
      "Train Epoch: 020 Batch: 00004/00094 | Loss: 773.0891 | CE: 0.2968 | KD: 1060.0718\n",
      "Train Epoch: 020 Batch: 00005/00094 | Loss: 773.0251 | CE: 0.3166 | KD: 1059.9568\n",
      "Train Epoch: 020 Batch: 00006/00094 | Loss: 773.1411 | CE: 0.3653 | KD: 1060.0492\n",
      "Train Epoch: 020 Batch: 00007/00094 | Loss: 773.1079 | CE: 0.3096 | KD: 1060.0801\n",
      "Train Epoch: 020 Batch: 00008/00094 | Loss: 772.9805 | CE: 0.2376 | KD: 1060.0040\n",
      "Train Epoch: 020 Batch: 00009/00094 | Loss: 773.1457 | CE: 0.3533 | KD: 1060.0719\n",
      "Train Epoch: 020 Batch: 00010/00094 | Loss: 773.0146 | CE: 0.2574 | KD: 1060.0237\n",
      "Train Epoch: 020 Batch: 00011/00094 | Loss: 773.1447 | CE: 0.3542 | KD: 1060.0692\n",
      "Train Epoch: 020 Batch: 00012/00094 | Loss: 773.0597 | CE: 0.3131 | KD: 1060.0090\n",
      "Train Epoch: 020 Batch: 00013/00094 | Loss: 773.1920 | CE: 0.3940 | KD: 1060.0796\n",
      "Train Epoch: 020 Batch: 00014/00094 | Loss: 773.1356 | CE: 0.3376 | KD: 1060.0797\n",
      "Train Epoch: 020 Batch: 00015/00094 | Loss: 773.0927 | CE: 0.3133 | KD: 1060.0541\n",
      "Train Epoch: 020 Batch: 00016/00094 | Loss: 773.0029 | CE: 0.2562 | KD: 1060.0092\n",
      "Train Epoch: 020 Batch: 00017/00094 | Loss: 773.1075 | CE: 0.2833 | KD: 1060.1156\n",
      "Train Epoch: 020 Batch: 00018/00094 | Loss: 773.0427 | CE: 0.2983 | KD: 1060.0061\n",
      "Train Epoch: 020 Batch: 00019/00094 | Loss: 773.1556 | CE: 0.3038 | KD: 1060.1534\n",
      "Train Epoch: 020 Batch: 00020/00094 | Loss: 773.0262 | CE: 0.3024 | KD: 1059.9778\n",
      "Train Epoch: 020 Batch: 00021/00094 | Loss: 773.1582 | CE: 0.3374 | KD: 1060.1108\n",
      "Train Epoch: 020 Batch: 00022/00094 | Loss: 773.0555 | CE: 0.2533 | KD: 1060.0853\n",
      "Train Epoch: 020 Batch: 00023/00094 | Loss: 772.9549 | CE: 0.2280 | KD: 1059.9821\n",
      "Train Epoch: 020 Batch: 00024/00094 | Loss: 773.1090 | CE: 0.2930 | KD: 1060.1042\n",
      "Train Epoch: 020 Batch: 00025/00094 | Loss: 773.0029 | CE: 0.2566 | KD: 1060.0087\n",
      "Train Epoch: 020 Batch: 00026/00094 | Loss: 773.0289 | CE: 0.2782 | KD: 1060.0146\n",
      "Train Epoch: 020 Batch: 00027/00094 | Loss: 773.0349 | CE: 0.3110 | KD: 1059.9780\n",
      "Train Epoch: 020 Batch: 00028/00094 | Loss: 773.0746 | CE: 0.2905 | KD: 1060.0605\n",
      "Train Epoch: 020 Batch: 00029/00094 | Loss: 773.1025 | CE: 0.3128 | KD: 1060.0684\n",
      "Train Epoch: 020 Batch: 00030/00094 | Loss: 773.0782 | CE: 0.2925 | KD: 1060.0627\n",
      "Train Epoch: 020 Batch: 00031/00094 | Loss: 773.1293 | CE: 0.3378 | KD: 1060.0708\n",
      "Train Epoch: 020 Batch: 00032/00094 | Loss: 773.0233 | CE: 0.2950 | KD: 1059.9840\n",
      "Train Epoch: 020 Batch: 00033/00094 | Loss: 772.9837 | CE: 0.2447 | KD: 1059.9988\n",
      "Train Epoch: 020 Batch: 00034/00094 | Loss: 773.1293 | CE: 0.2777 | KD: 1060.1531\n",
      "Train Epoch: 020 Batch: 00035/00094 | Loss: 773.0092 | CE: 0.2648 | KD: 1060.0060\n",
      "Train Epoch: 020 Batch: 00036/00094 | Loss: 773.0122 | CE: 0.2576 | KD: 1060.0201\n",
      "Train Epoch: 020 Batch: 00037/00094 | Loss: 772.9954 | CE: 0.2706 | KD: 1059.9792\n",
      "Train Epoch: 020 Batch: 00038/00094 | Loss: 773.0165 | CE: 0.2421 | KD: 1060.0472\n",
      "Train Epoch: 020 Batch: 00039/00094 | Loss: 773.0766 | CE: 0.2724 | KD: 1060.0881\n",
      "Train Epoch: 020 Batch: 00040/00094 | Loss: 772.9331 | CE: 0.2096 | KD: 1059.9774\n",
      "Train Epoch: 020 Batch: 00041/00094 | Loss: 773.0128 | CE: 0.2887 | KD: 1059.9781\n",
      "Train Epoch: 020 Batch: 00042/00094 | Loss: 772.9332 | CE: 0.2164 | KD: 1059.9683\n",
      "Train Epoch: 020 Batch: 00043/00094 | Loss: 773.0756 | CE: 0.3060 | KD: 1060.0406\n",
      "Train Epoch: 020 Batch: 00044/00094 | Loss: 772.9904 | CE: 0.2483 | KD: 1060.0028\n",
      "Train Epoch: 020 Batch: 00045/00094 | Loss: 773.0388 | CE: 0.2560 | KD: 1060.0587\n",
      "Train Epoch: 020 Batch: 00046/00094 | Loss: 772.9844 | CE: 0.2258 | KD: 1060.0255\n",
      "Train Epoch: 020 Batch: 00047/00094 | Loss: 772.9749 | CE: 0.2221 | KD: 1060.0176\n",
      "Train Epoch: 020 Batch: 00048/00094 | Loss: 772.9138 | CE: 0.1946 | KD: 1059.9714\n",
      "Train Epoch: 020 Batch: 00049/00094 | Loss: 773.1174 | CE: 0.3140 | KD: 1060.0869\n",
      "Train Epoch: 020 Batch: 00050/00094 | Loss: 773.0777 | CE: 0.2946 | KD: 1060.0592\n",
      "Train Epoch: 020 Batch: 00051/00094 | Loss: 773.1974 | CE: 0.3140 | KD: 1060.1968\n",
      "Train Epoch: 020 Batch: 00052/00094 | Loss: 773.1241 | CE: 0.3166 | KD: 1060.0925\n",
      "Train Epoch: 020 Batch: 00053/00094 | Loss: 773.0274 | CE: 0.2825 | KD: 1060.0067\n",
      "Train Epoch: 020 Batch: 00054/00094 | Loss: 773.0833 | CE: 0.3340 | KD: 1060.0128\n",
      "Train Epoch: 020 Batch: 00055/00094 | Loss: 773.1079 | CE: 0.3030 | KD: 1060.0890\n",
      "Train Epoch: 020 Batch: 00056/00094 | Loss: 773.0487 | CE: 0.3126 | KD: 1059.9946\n",
      "Train Epoch: 020 Batch: 00057/00094 | Loss: 773.0029 | CE: 0.2903 | KD: 1059.9624\n",
      "Train Epoch: 020 Batch: 00058/00094 | Loss: 773.0684 | CE: 0.3025 | KD: 1060.0355\n",
      "Train Epoch: 020 Batch: 00059/00094 | Loss: 773.0192 | CE: 0.2626 | KD: 1060.0228\n",
      "Train Epoch: 020 Batch: 00060/00094 | Loss: 773.1179 | CE: 0.3206 | KD: 1060.0786\n",
      "Train Epoch: 020 Batch: 00061/00094 | Loss: 773.0530 | CE: 0.3150 | KD: 1059.9973\n",
      "Train Epoch: 020 Batch: 00062/00094 | Loss: 773.1297 | CE: 0.3400 | KD: 1060.0682\n",
      "Train Epoch: 020 Batch: 00063/00094 | Loss: 772.9891 | CE: 0.2627 | KD: 1059.9814\n",
      "Train Epoch: 020 Batch: 00064/00094 | Loss: 773.0405 | CE: 0.2641 | KD: 1060.0499\n",
      "Train Epoch: 020 Batch: 00065/00094 | Loss: 773.0333 | CE: 0.2916 | KD: 1060.0023\n",
      "Train Epoch: 020 Batch: 00066/00094 | Loss: 773.0585 | CE: 0.3043 | KD: 1060.0195\n",
      "Train Epoch: 020 Batch: 00067/00094 | Loss: 773.0813 | CE: 0.2909 | KD: 1060.0692\n",
      "Train Epoch: 020 Batch: 00068/00094 | Loss: 773.1354 | CE: 0.3691 | KD: 1060.0361\n",
      "Train Epoch: 020 Batch: 00069/00094 | Loss: 773.0876 | CE: 0.2877 | KD: 1060.0823\n",
      "Train Epoch: 020 Batch: 00070/00094 | Loss: 773.0861 | CE: 0.3206 | KD: 1060.0350\n",
      "Train Epoch: 020 Batch: 00071/00094 | Loss: 773.0722 | CE: 0.2555 | KD: 1060.1053\n",
      "Train Epoch: 020 Batch: 00072/00094 | Loss: 773.1349 | CE: 0.3615 | KD: 1060.0459\n",
      "Train Epoch: 020 Batch: 00073/00094 | Loss: 773.2213 | CE: 0.4031 | KD: 1060.1073\n",
      "Train Epoch: 020 Batch: 00074/00094 | Loss: 773.0959 | CE: 0.3524 | KD: 1060.0050\n",
      "Train Epoch: 020 Batch: 00075/00094 | Loss: 773.1133 | CE: 0.3280 | KD: 1060.0621\n",
      "Train Epoch: 020 Batch: 00076/00094 | Loss: 773.0761 | CE: 0.3156 | KD: 1060.0282\n",
      "Train Epoch: 020 Batch: 00077/00094 | Loss: 773.1124 | CE: 0.3267 | KD: 1060.0627\n",
      "Train Epoch: 020 Batch: 00078/00094 | Loss: 773.1259 | CE: 0.3389 | KD: 1060.0645\n",
      "Train Epoch: 020 Batch: 00079/00094 | Loss: 773.0189 | CE: 0.2843 | KD: 1059.9927\n",
      "Train Epoch: 020 Batch: 00080/00094 | Loss: 772.9827 | CE: 0.2753 | KD: 1059.9553\n",
      "Train Epoch: 020 Batch: 00081/00094 | Loss: 773.1531 | CE: 0.3439 | KD: 1060.0950\n",
      "Train Epoch: 020 Batch: 00082/00094 | Loss: 773.0652 | CE: 0.3184 | KD: 1060.0094\n",
      "Train Epoch: 020 Batch: 00083/00094 | Loss: 773.0489 | CE: 0.2770 | KD: 1060.0438\n",
      "Train Epoch: 020 Batch: 00084/00094 | Loss: 773.0084 | CE: 0.2906 | KD: 1059.9696\n",
      "Train Epoch: 020 Batch: 00085/00094 | Loss: 773.0834 | CE: 0.3380 | KD: 1060.0074\n",
      "Train Epoch: 020 Batch: 00086/00094 | Loss: 773.1276 | CE: 0.3346 | KD: 1060.0729\n",
      "Train Epoch: 020 Batch: 00087/00094 | Loss: 773.1588 | CE: 0.3445 | KD: 1060.1019\n",
      "Train Epoch: 020 Batch: 00088/00094 | Loss: 772.9433 | CE: 0.2480 | KD: 1059.9387\n",
      "Train Epoch: 020 Batch: 00089/00094 | Loss: 773.0776 | CE: 0.3061 | KD: 1060.0432\n",
      "Train Epoch: 020 Batch: 00090/00094 | Loss: 773.0119 | CE: 0.2791 | KD: 1059.9901\n",
      "Train Epoch: 020 Batch: 00091/00094 | Loss: 773.0527 | CE: 0.3012 | KD: 1060.0157\n",
      "Train Epoch: 020 Batch: 00092/00094 | Loss: 773.0659 | CE: 0.2899 | KD: 1060.0494\n",
      "Train Epoch: 020 Batch: 00093/00094 | Loss: 773.0784 | CE: 0.2857 | KD: 1060.0723\n",
      "Train Epoch: 020 Batch: 00094/00094 | Loss: 773.2954 | CE: 0.4866 | KD: 1060.0945\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3100 | acc:89.0500\n",
      "[VAL Acc] Target: 89.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:0.9942 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8804 | acc:48.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9517 | acc:46.3740\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8107 | acc:52.9389\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9257 | acc:53.5120\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6136 | acc:66.1834\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9795 | acc:55.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4489 | acc:76.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.40%\n",
      "[VAL Acc] Avg 59.84%\n",
      "Train Epoch: 021 Batch: 00001/00094 | Loss: 773.1500 | CE: 0.3771 | KD: 1060.0452\n",
      "Train Epoch: 021 Batch: 00002/00094 | Loss: 773.0692 | CE: 0.2859 | KD: 1060.0593\n",
      "Train Epoch: 021 Batch: 00003/00094 | Loss: 773.0815 | CE: 0.2633 | KD: 1060.1073\n",
      "Train Epoch: 021 Batch: 00004/00094 | Loss: 773.1890 | CE: 0.2990 | KD: 1060.2059\n",
      "Train Epoch: 021 Batch: 00005/00094 | Loss: 773.0051 | CE: 0.2431 | KD: 1060.0303\n",
      "Train Epoch: 021 Batch: 00006/00094 | Loss: 773.1160 | CE: 0.3333 | KD: 1060.0587\n",
      "Train Epoch: 021 Batch: 00007/00094 | Loss: 772.9467 | CE: 0.2395 | KD: 1059.9550\n",
      "Train Epoch: 021 Batch: 00008/00094 | Loss: 773.0125 | CE: 0.2541 | KD: 1060.0254\n",
      "Train Epoch: 021 Batch: 00009/00094 | Loss: 773.0188 | CE: 0.2465 | KD: 1060.0444\n",
      "Train Epoch: 021 Batch: 00010/00094 | Loss: 773.1927 | CE: 0.3757 | KD: 1060.1056\n",
      "Train Epoch: 021 Batch: 00011/00094 | Loss: 772.9199 | CE: 0.2242 | KD: 1059.9392\n",
      "Train Epoch: 021 Batch: 00012/00094 | Loss: 773.0910 | CE: 0.2975 | KD: 1060.0735\n",
      "Train Epoch: 021 Batch: 00013/00094 | Loss: 773.0652 | CE: 0.2965 | KD: 1060.0396\n",
      "Train Epoch: 021 Batch: 00014/00094 | Loss: 773.1229 | CE: 0.3284 | KD: 1060.0748\n",
      "Train Epoch: 021 Batch: 00015/00094 | Loss: 773.1085 | CE: 0.3253 | KD: 1060.0592\n",
      "Train Epoch: 021 Batch: 00016/00094 | Loss: 773.1249 | CE: 0.3362 | KD: 1060.0668\n",
      "Train Epoch: 021 Batch: 00017/00094 | Loss: 773.0095 | CE: 0.2850 | KD: 1059.9789\n",
      "Train Epoch: 021 Batch: 00018/00094 | Loss: 772.9609 | CE: 0.2487 | KD: 1059.9620\n",
      "Train Epoch: 021 Batch: 00019/00094 | Loss: 772.9830 | CE: 0.2248 | KD: 1060.0250\n",
      "Train Epoch: 021 Batch: 00020/00094 | Loss: 772.8899 | CE: 0.1874 | KD: 1059.9486\n",
      "Train Epoch: 021 Batch: 00021/00094 | Loss: 773.0886 | CE: 0.3117 | KD: 1060.0507\n",
      "Train Epoch: 021 Batch: 00022/00094 | Loss: 773.0507 | CE: 0.2814 | KD: 1060.0402\n",
      "Train Epoch: 021 Batch: 00023/00094 | Loss: 773.0753 | CE: 0.2583 | KD: 1060.1057\n",
      "Train Epoch: 021 Batch: 00024/00094 | Loss: 773.1117 | CE: 0.2587 | KD: 1060.1550\n",
      "Train Epoch: 021 Batch: 00025/00094 | Loss: 773.1169 | CE: 0.3215 | KD: 1060.0760\n",
      "Train Epoch: 021 Batch: 00026/00094 | Loss: 773.1560 | CE: 0.3699 | KD: 1060.0634\n",
      "Train Epoch: 021 Batch: 00027/00094 | Loss: 773.1274 | CE: 0.3383 | KD: 1060.0675\n",
      "Train Epoch: 021 Batch: 00028/00094 | Loss: 773.1094 | CE: 0.3494 | KD: 1060.0275\n",
      "Train Epoch: 021 Batch: 00029/00094 | Loss: 772.9037 | CE: 0.1977 | KD: 1059.9535\n",
      "Train Epoch: 021 Batch: 00030/00094 | Loss: 772.9038 | CE: 0.2117 | KD: 1059.9343\n",
      "Train Epoch: 021 Batch: 00031/00094 | Loss: 773.0579 | CE: 0.2979 | KD: 1060.0273\n",
      "Train Epoch: 021 Batch: 00032/00094 | Loss: 773.0039 | CE: 0.2618 | KD: 1060.0029\n",
      "Train Epoch: 021 Batch: 00033/00094 | Loss: 772.9587 | CE: 0.2263 | KD: 1059.9896\n",
      "Train Epoch: 021 Batch: 00034/00094 | Loss: 773.1017 | CE: 0.3434 | KD: 1060.0253\n",
      "Train Epoch: 021 Batch: 00035/00094 | Loss: 773.1286 | CE: 0.3093 | KD: 1060.1089\n",
      "Train Epoch: 021 Batch: 00036/00094 | Loss: 773.0201 | CE: 0.3059 | KD: 1059.9647\n",
      "Train Epoch: 021 Batch: 00037/00094 | Loss: 773.0222 | CE: 0.2584 | KD: 1060.0327\n",
      "Train Epoch: 021 Batch: 00038/00094 | Loss: 772.9832 | CE: 0.2298 | KD: 1060.0184\n",
      "Train Epoch: 021 Batch: 00039/00094 | Loss: 773.0565 | CE: 0.2856 | KD: 1060.0424\n",
      "Train Epoch: 021 Batch: 00040/00094 | Loss: 773.0749 | CE: 0.2956 | KD: 1060.0538\n",
      "Train Epoch: 021 Batch: 00041/00094 | Loss: 773.0944 | CE: 0.2572 | KD: 1060.1333\n",
      "Train Epoch: 021 Batch: 00042/00094 | Loss: 773.0527 | CE: 0.2807 | KD: 1060.0438\n",
      "Train Epoch: 021 Batch: 00043/00094 | Loss: 773.0247 | CE: 0.2532 | KD: 1060.0432\n",
      "Train Epoch: 021 Batch: 00044/00094 | Loss: 772.8850 | CE: 0.1821 | KD: 1059.9492\n",
      "Train Epoch: 021 Batch: 00045/00094 | Loss: 773.0753 | CE: 0.2932 | KD: 1060.0579\n",
      "Train Epoch: 021 Batch: 00046/00094 | Loss: 773.0063 | CE: 0.2525 | KD: 1060.0190\n",
      "Train Epoch: 021 Batch: 00047/00094 | Loss: 773.0599 | CE: 0.2377 | KD: 1060.1128\n",
      "Train Epoch: 021 Batch: 00048/00094 | Loss: 773.0565 | CE: 0.2828 | KD: 1060.0463\n",
      "Train Epoch: 021 Batch: 00049/00094 | Loss: 772.9972 | CE: 0.2738 | KD: 1059.9773\n",
      "Train Epoch: 021 Batch: 00050/00094 | Loss: 772.9914 | CE: 0.2207 | KD: 1060.0421\n",
      "Train Epoch: 021 Batch: 00051/00094 | Loss: 773.1512 | CE: 0.3252 | KD: 1060.1179\n",
      "Train Epoch: 021 Batch: 00052/00094 | Loss: 772.9661 | CE: 0.2196 | KD: 1060.0089\n",
      "Train Epoch: 021 Batch: 00053/00094 | Loss: 772.9636 | CE: 0.2593 | KD: 1059.9512\n",
      "Train Epoch: 021 Batch: 00054/00094 | Loss: 773.0549 | CE: 0.2840 | KD: 1060.0424\n",
      "Train Epoch: 021 Batch: 00055/00094 | Loss: 773.0328 | CE: 0.2901 | KD: 1060.0038\n",
      "Train Epoch: 021 Batch: 00056/00094 | Loss: 773.2447 | CE: 0.4502 | KD: 1060.0748\n",
      "Train Epoch: 021 Batch: 00057/00094 | Loss: 773.0006 | CE: 0.2519 | KD: 1060.0120\n",
      "Train Epoch: 021 Batch: 00058/00094 | Loss: 773.0084 | CE: 0.2752 | KD: 1059.9907\n",
      "Train Epoch: 021 Batch: 00059/00094 | Loss: 773.1169 | CE: 0.3123 | KD: 1060.0887\n",
      "Train Epoch: 021 Batch: 00060/00094 | Loss: 773.0853 | CE: 0.3208 | KD: 1060.0336\n",
      "Train Epoch: 021 Batch: 00061/00094 | Loss: 772.9542 | CE: 0.2351 | KD: 1059.9713\n",
      "Train Epoch: 021 Batch: 00062/00094 | Loss: 773.1076 | CE: 0.3307 | KD: 1060.0508\n",
      "Train Epoch: 021 Batch: 00063/00094 | Loss: 773.1436 | CE: 0.3462 | KD: 1060.0786\n",
      "Train Epoch: 021 Batch: 00064/00094 | Loss: 772.9523 | CE: 0.2136 | KD: 1059.9983\n",
      "Train Epoch: 021 Batch: 00065/00094 | Loss: 773.0629 | CE: 0.2930 | KD: 1060.0410\n",
      "Train Epoch: 021 Batch: 00066/00094 | Loss: 773.0963 | CE: 0.3016 | KD: 1060.0751\n",
      "Train Epoch: 021 Batch: 00067/00094 | Loss: 773.3564 | CE: 0.4920 | KD: 1060.1708\n",
      "Train Epoch: 021 Batch: 00068/00094 | Loss: 772.9099 | CE: 0.2218 | KD: 1059.9288\n",
      "Train Epoch: 021 Batch: 00069/00094 | Loss: 772.9196 | CE: 0.2241 | KD: 1059.9390\n",
      "Train Epoch: 021 Batch: 00070/00094 | Loss: 773.0202 | CE: 0.2341 | KD: 1060.0634\n",
      "Train Epoch: 021 Batch: 00071/00094 | Loss: 772.9527 | CE: 0.2226 | KD: 1059.9865\n",
      "Train Epoch: 021 Batch: 00072/00094 | Loss: 772.9739 | CE: 0.2677 | KD: 1059.9536\n",
      "Train Epoch: 021 Batch: 00073/00094 | Loss: 772.9933 | CE: 0.2744 | KD: 1059.9712\n",
      "Train Epoch: 021 Batch: 00074/00094 | Loss: 773.0311 | CE: 0.2890 | KD: 1060.0029\n",
      "Train Epoch: 021 Batch: 00075/00094 | Loss: 773.1124 | CE: 0.2820 | KD: 1060.1240\n",
      "Train Epoch: 021 Batch: 00076/00094 | Loss: 773.0247 | CE: 0.2718 | KD: 1060.0177\n",
      "Train Epoch: 021 Batch: 00077/00094 | Loss: 773.0270 | CE: 0.2536 | KD: 1060.0459\n",
      "Train Epoch: 021 Batch: 00078/00094 | Loss: 773.0428 | CE: 0.2701 | KD: 1060.0449\n",
      "Train Epoch: 021 Batch: 00079/00094 | Loss: 773.0726 | CE: 0.3083 | KD: 1060.0333\n",
      "Train Epoch: 021 Batch: 00080/00094 | Loss: 773.1610 | CE: 0.3304 | KD: 1060.1244\n",
      "Train Epoch: 021 Batch: 00081/00094 | Loss: 773.0409 | CE: 0.2821 | KD: 1060.0259\n",
      "Train Epoch: 021 Batch: 00082/00094 | Loss: 773.1576 | CE: 0.3546 | KD: 1060.0865\n",
      "Train Epoch: 021 Batch: 00083/00094 | Loss: 773.0253 | CE: 0.2766 | KD: 1060.0121\n",
      "Train Epoch: 021 Batch: 00084/00094 | Loss: 772.9437 | CE: 0.2539 | KD: 1059.9312\n",
      "Train Epoch: 021 Batch: 00085/00094 | Loss: 773.0640 | CE: 0.2912 | KD: 1060.0450\n",
      "Train Epoch: 021 Batch: 00086/00094 | Loss: 773.0958 | CE: 0.3527 | KD: 1060.0043\n",
      "Train Epoch: 021 Batch: 00087/00094 | Loss: 773.2038 | CE: 0.3270 | KD: 1060.1876\n",
      "Train Epoch: 021 Batch: 00088/00094 | Loss: 773.1174 | CE: 0.3264 | KD: 1060.0699\n",
      "Train Epoch: 021 Batch: 00089/00094 | Loss: 773.0612 | CE: 0.3209 | KD: 1060.0005\n",
      "Train Epoch: 021 Batch: 00090/00094 | Loss: 772.9135 | CE: 0.2288 | KD: 1059.9242\n",
      "Train Epoch: 021 Batch: 00091/00094 | Loss: 773.0177 | CE: 0.2331 | KD: 1060.0612\n",
      "Train Epoch: 021 Batch: 00092/00094 | Loss: 773.2091 | CE: 0.3758 | KD: 1060.1279\n",
      "Train Epoch: 021 Batch: 00093/00094 | Loss: 773.0324 | CE: 0.2750 | KD: 1060.0239\n",
      "Train Epoch: 021 Batch: 00094/00094 | Loss: 772.9046 | CE: 0.1775 | KD: 1059.9823\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2912 | acc:88.9000\n",
      "[VAL Acc] Target: 88.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2060 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0480 | acc:48.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1124 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8929 | acc:53.3699\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7687 | acc:57.5786\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 57.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5852 | acc:67.7508\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8779 | acc:57.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5451 | acc:70.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 70.95%\n",
      "[VAL Acc] Avg 60.41%\n",
      "Train Epoch: 022 Batch: 00001/00094 | Loss: 773.0054 | CE: 0.2498 | KD: 1060.0214\n",
      "Train Epoch: 022 Batch: 00002/00094 | Loss: 772.9763 | CE: 0.2314 | KD: 1060.0068\n",
      "Train Epoch: 022 Batch: 00003/00094 | Loss: 773.2059 | CE: 0.3862 | KD: 1060.1094\n",
      "Train Epoch: 022 Batch: 00004/00094 | Loss: 773.1458 | CE: 0.3307 | KD: 1060.1030\n",
      "Train Epoch: 022 Batch: 00005/00094 | Loss: 773.1021 | CE: 0.3107 | KD: 1060.0706\n",
      "Train Epoch: 022 Batch: 00006/00094 | Loss: 772.9642 | CE: 0.2445 | KD: 1059.9722\n",
      "Train Epoch: 022 Batch: 00007/00094 | Loss: 773.0826 | CE: 0.3144 | KD: 1060.0388\n",
      "Train Epoch: 022 Batch: 00008/00094 | Loss: 772.9864 | CE: 0.2406 | KD: 1060.0079\n",
      "Train Epoch: 022 Batch: 00009/00094 | Loss: 773.0670 | CE: 0.2971 | KD: 1060.0411\n",
      "Train Epoch: 022 Batch: 00010/00094 | Loss: 773.0238 | CE: 0.2714 | KD: 1060.0171\n",
      "Train Epoch: 022 Batch: 00011/00094 | Loss: 773.1193 | CE: 0.3460 | KD: 1060.0458\n",
      "Train Epoch: 022 Batch: 00012/00094 | Loss: 773.0522 | CE: 0.2229 | KD: 1060.1227\n",
      "Train Epoch: 022 Batch: 00013/00094 | Loss: 773.0024 | CE: 0.2498 | KD: 1060.0173\n",
      "Train Epoch: 022 Batch: 00014/00094 | Loss: 773.1588 | CE: 0.3455 | KD: 1060.1005\n",
      "Train Epoch: 022 Batch: 00015/00094 | Loss: 773.0609 | CE: 0.3201 | KD: 1060.0011\n",
      "Train Epoch: 022 Batch: 00016/00094 | Loss: 773.0652 | CE: 0.2819 | KD: 1060.0594\n",
      "Train Epoch: 022 Batch: 00017/00094 | Loss: 773.0552 | CE: 0.3139 | KD: 1060.0017\n",
      "Train Epoch: 022 Batch: 00018/00094 | Loss: 773.1581 | CE: 0.3939 | KD: 1060.0332\n",
      "Train Epoch: 022 Batch: 00019/00094 | Loss: 773.0982 | CE: 0.3135 | KD: 1060.0614\n",
      "Train Epoch: 022 Batch: 00020/00094 | Loss: 773.0189 | CE: 0.2485 | KD: 1060.0417\n",
      "Train Epoch: 022 Batch: 00021/00094 | Loss: 773.0006 | CE: 0.2379 | KD: 1060.0312\n",
      "Train Epoch: 022 Batch: 00022/00094 | Loss: 773.0007 | CE: 0.2881 | KD: 1059.9624\n",
      "Train Epoch: 022 Batch: 00023/00094 | Loss: 773.1389 | CE: 0.3273 | KD: 1060.0983\n",
      "Train Epoch: 022 Batch: 00024/00094 | Loss: 773.0873 | CE: 0.2810 | KD: 1060.0909\n",
      "Train Epoch: 022 Batch: 00025/00094 | Loss: 773.0215 | CE: 0.2883 | KD: 1059.9908\n",
      "Train Epoch: 022 Batch: 00026/00094 | Loss: 772.9645 | CE: 0.2462 | KD: 1059.9703\n",
      "Train Epoch: 022 Batch: 00027/00094 | Loss: 773.0383 | CE: 0.2598 | KD: 1060.0530\n",
      "Train Epoch: 022 Batch: 00028/00094 | Loss: 773.1439 | CE: 0.3724 | KD: 1060.0432\n",
      "Train Epoch: 022 Batch: 00029/00094 | Loss: 773.1104 | CE: 0.3009 | KD: 1060.0953\n",
      "Train Epoch: 022 Batch: 00030/00094 | Loss: 773.0307 | CE: 0.2651 | KD: 1060.0352\n",
      "Train Epoch: 022 Batch: 00031/00094 | Loss: 773.2275 | CE: 0.4058 | KD: 1060.1121\n",
      "Train Epoch: 022 Batch: 00032/00094 | Loss: 773.1450 | CE: 0.3285 | KD: 1060.1049\n",
      "Train Epoch: 022 Batch: 00033/00094 | Loss: 773.0654 | CE: 0.2661 | KD: 1060.0813\n",
      "Train Epoch: 022 Batch: 00034/00094 | Loss: 773.0854 | CE: 0.2815 | KD: 1060.0878\n",
      "Train Epoch: 022 Batch: 00035/00094 | Loss: 773.0085 | CE: 0.2498 | KD: 1060.0258\n",
      "Train Epoch: 022 Batch: 00036/00094 | Loss: 773.1547 | CE: 0.3232 | KD: 1060.1255\n",
      "Train Epoch: 022 Batch: 00037/00094 | Loss: 773.0466 | CE: 0.3296 | KD: 1059.9685\n",
      "Train Epoch: 022 Batch: 00038/00094 | Loss: 773.0140 | CE: 0.2796 | KD: 1059.9923\n",
      "Train Epoch: 022 Batch: 00039/00094 | Loss: 772.9877 | CE: 0.2791 | KD: 1059.9570\n",
      "Train Epoch: 022 Batch: 00040/00094 | Loss: 773.1019 | CE: 0.3227 | KD: 1060.0537\n",
      "Train Epoch: 022 Batch: 00041/00094 | Loss: 773.0259 | CE: 0.2332 | KD: 1060.0724\n",
      "Train Epoch: 022 Batch: 00042/00094 | Loss: 773.0904 | CE: 0.2788 | KD: 1060.0983\n",
      "Train Epoch: 022 Batch: 00043/00094 | Loss: 772.9776 | CE: 0.2272 | KD: 1060.0143\n",
      "Train Epoch: 022 Batch: 00044/00094 | Loss: 773.0640 | CE: 0.2822 | KD: 1060.0574\n",
      "Train Epoch: 022 Batch: 00045/00094 | Loss: 772.9598 | CE: 0.2153 | KD: 1060.0063\n",
      "Train Epoch: 022 Batch: 00046/00094 | Loss: 773.0453 | CE: 0.2767 | KD: 1060.0392\n",
      "Train Epoch: 022 Batch: 00047/00094 | Loss: 773.1879 | CE: 0.3270 | KD: 1060.1659\n",
      "Train Epoch: 022 Batch: 00048/00094 | Loss: 773.0237 | CE: 0.2901 | KD: 1059.9912\n",
      "Train Epoch: 022 Batch: 00049/00094 | Loss: 772.9629 | CE: 0.2429 | KD: 1059.9725\n",
      "Train Epoch: 022 Batch: 00050/00094 | Loss: 773.0930 | CE: 0.3241 | KD: 1060.0397\n",
      "Train Epoch: 022 Batch: 00051/00094 | Loss: 772.9167 | CE: 0.2316 | KD: 1059.9248\n",
      "Train Epoch: 022 Batch: 00052/00094 | Loss: 773.0621 | CE: 0.3324 | KD: 1059.9861\n",
      "Train Epoch: 022 Batch: 00053/00094 | Loss: 773.0491 | CE: 0.2691 | KD: 1060.0549\n",
      "Train Epoch: 022 Batch: 00054/00094 | Loss: 773.0853 | CE: 0.3280 | KD: 1060.0238\n",
      "Train Epoch: 022 Batch: 00055/00094 | Loss: 773.0101 | CE: 0.2790 | KD: 1059.9879\n",
      "Train Epoch: 022 Batch: 00056/00094 | Loss: 773.0363 | CE: 0.2801 | KD: 1060.0223\n",
      "Train Epoch: 022 Batch: 00057/00094 | Loss: 773.0817 | CE: 0.3009 | KD: 1060.0560\n",
      "Train Epoch: 022 Batch: 00058/00094 | Loss: 772.9804 | CE: 0.2299 | KD: 1060.0145\n",
      "Train Epoch: 022 Batch: 00059/00094 | Loss: 773.0853 | CE: 0.3435 | KD: 1060.0026\n",
      "Train Epoch: 022 Batch: 00060/00094 | Loss: 773.0430 | CE: 0.2931 | KD: 1060.0135\n",
      "Train Epoch: 022 Batch: 00061/00094 | Loss: 772.9915 | CE: 0.2295 | KD: 1060.0303\n",
      "Train Epoch: 022 Batch: 00062/00094 | Loss: 772.9431 | CE: 0.2289 | KD: 1059.9646\n",
      "Train Epoch: 022 Batch: 00063/00094 | Loss: 772.9452 | CE: 0.2388 | KD: 1059.9539\n",
      "Train Epoch: 022 Batch: 00064/00094 | Loss: 773.0515 | CE: 0.2378 | KD: 1060.1011\n",
      "Train Epoch: 022 Batch: 00065/00094 | Loss: 773.0372 | CE: 0.2491 | KD: 1060.0659\n",
      "Train Epoch: 022 Batch: 00066/00094 | Loss: 773.0203 | CE: 0.3065 | KD: 1059.9640\n",
      "Train Epoch: 022 Batch: 00067/00094 | Loss: 773.0378 | CE: 0.2921 | KD: 1060.0079\n",
      "Train Epoch: 022 Batch: 00068/00094 | Loss: 772.9440 | CE: 0.1940 | KD: 1060.0138\n",
      "Train Epoch: 022 Batch: 00069/00094 | Loss: 773.0577 | CE: 0.2819 | KD: 1060.0492\n",
      "Train Epoch: 022 Batch: 00070/00094 | Loss: 773.0635 | CE: 0.2895 | KD: 1060.0466\n",
      "Train Epoch: 022 Batch: 00071/00094 | Loss: 772.9900 | CE: 0.2494 | KD: 1060.0010\n",
      "Train Epoch: 022 Batch: 00072/00094 | Loss: 773.1091 | CE: 0.3348 | KD: 1060.0471\n",
      "Train Epoch: 022 Batch: 00073/00094 | Loss: 773.1328 | CE: 0.3139 | KD: 1060.1084\n",
      "Train Epoch: 022 Batch: 00074/00094 | Loss: 772.9230 | CE: 0.1861 | KD: 1059.9958\n",
      "Train Epoch: 022 Batch: 00075/00094 | Loss: 773.0880 | CE: 0.3197 | KD: 1060.0388\n",
      "Train Epoch: 022 Batch: 00076/00094 | Loss: 773.0620 | CE: 0.2989 | KD: 1060.0317\n",
      "Train Epoch: 022 Batch: 00077/00094 | Loss: 773.0077 | CE: 0.2756 | KD: 1059.9891\n",
      "Train Epoch: 022 Batch: 00078/00094 | Loss: 773.0214 | CE: 0.2580 | KD: 1060.0321\n",
      "Train Epoch: 022 Batch: 00079/00094 | Loss: 773.1515 | CE: 0.3450 | KD: 1060.0912\n",
      "Train Epoch: 022 Batch: 00080/00094 | Loss: 773.0398 | CE: 0.2945 | KD: 1060.0073\n",
      "Train Epoch: 022 Batch: 00081/00094 | Loss: 773.0146 | CE: 0.2443 | KD: 1060.0416\n",
      "Train Epoch: 022 Batch: 00082/00094 | Loss: 773.0538 | CE: 0.2849 | KD: 1060.0397\n",
      "Train Epoch: 022 Batch: 00083/00094 | Loss: 773.0988 | CE: 0.3317 | KD: 1060.0372\n",
      "Train Epoch: 022 Batch: 00084/00094 | Loss: 773.0890 | CE: 0.2702 | KD: 1060.1082\n",
      "Train Epoch: 022 Batch: 00085/00094 | Loss: 773.0827 | CE: 0.3120 | KD: 1060.0422\n",
      "Train Epoch: 022 Batch: 00086/00094 | Loss: 773.0489 | CE: 0.2495 | KD: 1060.0815\n",
      "Train Epoch: 022 Batch: 00087/00094 | Loss: 772.9553 | CE: 0.2170 | KD: 1059.9977\n",
      "Train Epoch: 022 Batch: 00088/00094 | Loss: 773.0063 | CE: 0.2717 | KD: 1059.9926\n",
      "Train Epoch: 022 Batch: 00089/00094 | Loss: 773.1029 | CE: 0.3247 | KD: 1060.0525\n",
      "Train Epoch: 022 Batch: 00090/00094 | Loss: 773.1707 | CE: 0.3201 | KD: 1060.1519\n",
      "Train Epoch: 022 Batch: 00091/00094 | Loss: 773.0993 | CE: 0.2879 | KD: 1060.0979\n",
      "Train Epoch: 022 Batch: 00092/00094 | Loss: 773.0455 | CE: 0.2987 | KD: 1060.0094\n",
      "Train Epoch: 022 Batch: 00093/00094 | Loss: 772.8616 | CE: 0.1756 | KD: 1059.9260\n",
      "Train Epoch: 022 Batch: 00094/00094 | Loss: 773.1279 | CE: 0.3004 | KD: 1060.1200\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2943 | acc:90.0500\n",
      "[VAL Acc] Target: 90.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0111 | acc:49.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9020 | acc:49.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9549 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7487 | acc:56.0737\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9866 | acc:53.4196\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5555 | acc:70.7680\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.77%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0626 | acc:53.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 53.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4654 | acc:76.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.25%\n",
      "[VAL Acc] Avg 60.91%\n",
      "VAL Acc improve from 60.52% to 60.91%\n",
      "Save best model\n",
      "Train Epoch: 023 Batch: 00001/00094 | Loss: 773.0262 | CE: 0.2798 | KD: 1060.0088\n",
      "Train Epoch: 023 Batch: 00002/00094 | Loss: 773.0313 | CE: 0.2615 | KD: 1060.0409\n",
      "Train Epoch: 023 Batch: 00003/00094 | Loss: 773.0146 | CE: 0.2742 | KD: 1060.0006\n",
      "Train Epoch: 023 Batch: 00004/00094 | Loss: 773.0585 | CE: 0.2630 | KD: 1060.0762\n",
      "Train Epoch: 023 Batch: 00005/00094 | Loss: 773.0274 | CE: 0.2585 | KD: 1060.0397\n",
      "Train Epoch: 023 Batch: 00006/00094 | Loss: 773.0443 | CE: 0.2475 | KD: 1060.0779\n",
      "Train Epoch: 023 Batch: 00007/00094 | Loss: 773.1904 | CE: 0.3805 | KD: 1060.0959\n",
      "Train Epoch: 023 Batch: 00008/00094 | Loss: 772.9800 | CE: 0.2355 | KD: 1060.0062\n",
      "Train Epoch: 023 Batch: 00009/00094 | Loss: 773.1126 | CE: 0.3329 | KD: 1060.0546\n",
      "Train Epoch: 023 Batch: 00010/00094 | Loss: 772.9065 | CE: 0.1942 | KD: 1059.9619\n",
      "Train Epoch: 023 Batch: 00011/00094 | Loss: 773.0785 | CE: 0.3183 | KD: 1060.0277\n",
      "Train Epoch: 023 Batch: 00012/00094 | Loss: 773.0918 | CE: 0.2769 | KD: 1060.1028\n",
      "Train Epoch: 023 Batch: 00013/00094 | Loss: 773.0428 | CE: 0.2789 | KD: 1060.0328\n",
      "Train Epoch: 023 Batch: 00014/00094 | Loss: 773.0752 | CE: 0.3041 | KD: 1060.0426\n",
      "Train Epoch: 023 Batch: 00015/00094 | Loss: 773.1230 | CE: 0.3053 | KD: 1060.1066\n",
      "Train Epoch: 023 Batch: 00016/00094 | Loss: 773.1454 | CE: 0.2857 | KD: 1060.1643\n",
      "Train Epoch: 023 Batch: 00017/00094 | Loss: 773.0463 | CE: 0.2770 | KD: 1060.0403\n",
      "Train Epoch: 023 Batch: 00018/00094 | Loss: 773.0438 | CE: 0.2709 | KD: 1060.0452\n",
      "Train Epoch: 023 Batch: 00019/00094 | Loss: 773.0872 | CE: 0.2876 | KD: 1060.0818\n",
      "Train Epoch: 023 Batch: 00020/00094 | Loss: 773.0252 | CE: 0.2736 | KD: 1060.0160\n",
      "Train Epoch: 023 Batch: 00021/00094 | Loss: 773.0217 | CE: 0.2598 | KD: 1060.0300\n",
      "Train Epoch: 023 Batch: 00022/00094 | Loss: 773.0045 | CE: 0.2400 | KD: 1060.0337\n",
      "Train Epoch: 023 Batch: 00023/00094 | Loss: 773.0791 | CE: 0.3225 | KD: 1060.0228\n",
      "Train Epoch: 023 Batch: 00024/00094 | Loss: 773.1335 | CE: 0.3832 | KD: 1060.0143\n",
      "Train Epoch: 023 Batch: 00025/00094 | Loss: 773.0399 | CE: 0.2859 | KD: 1060.0193\n",
      "Train Epoch: 023 Batch: 00026/00094 | Loss: 773.0700 | CE: 0.2788 | KD: 1060.0703\n",
      "Train Epoch: 023 Batch: 00027/00094 | Loss: 773.0806 | CE: 0.2991 | KD: 1060.0570\n",
      "Train Epoch: 023 Batch: 00028/00094 | Loss: 772.9758 | CE: 0.2300 | KD: 1060.0081\n",
      "Train Epoch: 023 Batch: 00029/00094 | Loss: 772.9611 | CE: 0.2212 | KD: 1059.9998\n",
      "Train Epoch: 023 Batch: 00030/00094 | Loss: 773.0428 | CE: 0.2703 | KD: 1060.0446\n",
      "Train Epoch: 023 Batch: 00031/00094 | Loss: 773.0891 | CE: 0.3050 | KD: 1060.0605\n",
      "Train Epoch: 023 Batch: 00032/00094 | Loss: 773.0509 | CE: 0.2595 | KD: 1060.0706\n",
      "Train Epoch: 023 Batch: 00033/00094 | Loss: 773.0166 | CE: 0.2610 | KD: 1060.0215\n",
      "Train Epoch: 023 Batch: 00034/00094 | Loss: 773.0281 | CE: 0.2414 | KD: 1060.0641\n",
      "Train Epoch: 023 Batch: 00035/00094 | Loss: 773.0748 | CE: 0.2503 | KD: 1060.1161\n",
      "Train Epoch: 023 Batch: 00036/00094 | Loss: 772.9106 | CE: 0.2092 | KD: 1059.9470\n",
      "Train Epoch: 023 Batch: 00037/00094 | Loss: 773.1688 | CE: 0.3409 | KD: 1060.1206\n",
      "Train Epoch: 023 Batch: 00038/00094 | Loss: 773.0803 | CE: 0.3156 | KD: 1060.0338\n",
      "Train Epoch: 023 Batch: 00039/00094 | Loss: 772.9620 | CE: 0.2301 | KD: 1059.9889\n",
      "Train Epoch: 023 Batch: 00040/00094 | Loss: 773.0989 | CE: 0.3002 | KD: 1060.0806\n",
      "Train Epoch: 023 Batch: 00041/00094 | Loss: 773.0538 | CE: 0.2568 | KD: 1060.0781\n",
      "Train Epoch: 023 Batch: 00042/00094 | Loss: 773.0708 | CE: 0.2940 | KD: 1060.0505\n",
      "Train Epoch: 023 Batch: 00043/00094 | Loss: 773.0355 | CE: 0.2576 | KD: 1060.0521\n",
      "Train Epoch: 023 Batch: 00044/00094 | Loss: 773.0760 | CE: 0.3266 | KD: 1060.0129\n",
      "Train Epoch: 023 Batch: 00045/00094 | Loss: 773.1646 | CE: 0.3458 | KD: 1060.1082\n",
      "Train Epoch: 023 Batch: 00046/00094 | Loss: 772.9694 | CE: 0.2154 | KD: 1060.0192\n",
      "Train Epoch: 023 Batch: 00047/00094 | Loss: 773.0872 | CE: 0.3357 | KD: 1060.0157\n",
      "Train Epoch: 023 Batch: 00048/00094 | Loss: 773.1381 | CE: 0.3506 | KD: 1060.0651\n",
      "Train Epoch: 023 Batch: 00049/00094 | Loss: 773.1911 | CE: 0.3671 | KD: 1060.1152\n",
      "Train Epoch: 023 Batch: 00050/00094 | Loss: 773.1342 | CE: 0.3570 | KD: 1060.0510\n",
      "Train Epoch: 023 Batch: 00051/00094 | Loss: 773.2007 | CE: 0.3195 | KD: 1060.1937\n",
      "Train Epoch: 023 Batch: 00052/00094 | Loss: 773.0528 | CE: 0.2547 | KD: 1060.0797\n",
      "Train Epoch: 023 Batch: 00053/00094 | Loss: 772.9896 | CE: 0.2230 | KD: 1060.0365\n",
      "Train Epoch: 023 Batch: 00054/00094 | Loss: 772.9066 | CE: 0.1904 | KD: 1059.9673\n",
      "Train Epoch: 023 Batch: 00055/00094 | Loss: 772.9524 | CE: 0.2203 | KD: 1059.9891\n",
      "Train Epoch: 023 Batch: 00056/00094 | Loss: 772.8929 | CE: 0.2028 | KD: 1059.9316\n",
      "Train Epoch: 023 Batch: 00057/00094 | Loss: 773.0016 | CE: 0.2499 | KD: 1060.0160\n",
      "Train Epoch: 023 Batch: 00058/00094 | Loss: 773.0428 | CE: 0.2931 | KD: 1060.0134\n",
      "Train Epoch: 023 Batch: 00059/00094 | Loss: 773.0704 | CE: 0.2958 | KD: 1060.0476\n",
      "Train Epoch: 023 Batch: 00060/00094 | Loss: 773.0209 | CE: 0.2895 | KD: 1059.9883\n",
      "Train Epoch: 023 Batch: 00061/00094 | Loss: 773.0752 | CE: 0.2965 | KD: 1060.0531\n",
      "Train Epoch: 023 Batch: 00062/00094 | Loss: 773.0858 | CE: 0.2869 | KD: 1060.0808\n",
      "Train Epoch: 023 Batch: 00063/00094 | Loss: 773.0892 | CE: 0.3442 | KD: 1060.0070\n",
      "Train Epoch: 023 Batch: 00064/00094 | Loss: 773.1089 | CE: 0.2953 | KD: 1060.1011\n",
      "Train Epoch: 023 Batch: 00065/00094 | Loss: 773.0850 | CE: 0.3197 | KD: 1060.0347\n",
      "Train Epoch: 023 Batch: 00066/00094 | Loss: 772.9835 | CE: 0.2173 | KD: 1060.0360\n",
      "Train Epoch: 023 Batch: 00067/00094 | Loss: 772.9722 | CE: 0.2402 | KD: 1059.9890\n",
      "Train Epoch: 023 Batch: 00068/00094 | Loss: 773.1654 | CE: 0.3368 | KD: 1060.1217\n",
      "Train Epoch: 023 Batch: 00069/00094 | Loss: 773.0570 | CE: 0.2849 | KD: 1060.0441\n",
      "Train Epoch: 023 Batch: 00070/00094 | Loss: 773.0159 | CE: 0.2967 | KD: 1059.9716\n",
      "Train Epoch: 023 Batch: 00071/00094 | Loss: 773.0800 | CE: 0.3333 | KD: 1060.0093\n",
      "Train Epoch: 023 Batch: 00072/00094 | Loss: 772.9819 | CE: 0.2683 | KD: 1059.9637\n",
      "Train Epoch: 023 Batch: 00073/00094 | Loss: 773.0964 | CE: 0.2480 | KD: 1060.1487\n",
      "Train Epoch: 023 Batch: 00074/00094 | Loss: 773.1147 | CE: 0.2612 | KD: 1060.1558\n",
      "Train Epoch: 023 Batch: 00075/00094 | Loss: 772.9656 | CE: 0.2038 | KD: 1060.0299\n",
      "Train Epoch: 023 Batch: 00076/00094 | Loss: 772.9993 | CE: 0.2745 | KD: 1059.9792\n",
      "Train Epoch: 023 Batch: 00077/00094 | Loss: 773.1955 | CE: 0.3718 | KD: 1060.1149\n",
      "Train Epoch: 023 Batch: 00078/00094 | Loss: 772.9744 | CE: 0.2414 | KD: 1059.9905\n",
      "Train Epoch: 023 Batch: 00079/00094 | Loss: 773.0576 | CE: 0.3010 | KD: 1060.0228\n",
      "Train Epoch: 023 Batch: 00080/00094 | Loss: 772.9871 | CE: 0.2570 | KD: 1059.9865\n",
      "Train Epoch: 023 Batch: 00081/00094 | Loss: 773.0443 | CE: 0.2690 | KD: 1060.0483\n",
      "Train Epoch: 023 Batch: 00082/00094 | Loss: 773.1793 | CE: 0.3342 | KD: 1060.1442\n",
      "Train Epoch: 023 Batch: 00083/00094 | Loss: 773.1233 | CE: 0.2552 | KD: 1060.1757\n",
      "Train Epoch: 023 Batch: 00084/00094 | Loss: 773.1384 | CE: 0.2841 | KD: 1060.1569\n",
      "Train Epoch: 023 Batch: 00085/00094 | Loss: 772.9201 | CE: 0.2258 | KD: 1059.9374\n",
      "Train Epoch: 023 Batch: 00086/00094 | Loss: 772.9810 | CE: 0.2182 | KD: 1060.0314\n",
      "Train Epoch: 023 Batch: 00087/00094 | Loss: 772.9078 | CE: 0.2062 | KD: 1059.9474\n",
      "Train Epoch: 023 Batch: 00088/00094 | Loss: 773.0791 | CE: 0.2913 | KD: 1060.0656\n",
      "Train Epoch: 023 Batch: 00089/00094 | Loss: 772.9633 | CE: 0.2128 | KD: 1060.0145\n",
      "Train Epoch: 023 Batch: 00090/00094 | Loss: 773.0364 | CE: 0.2841 | KD: 1060.0168\n",
      "Train Epoch: 023 Batch: 00091/00094 | Loss: 772.9559 | CE: 0.2300 | KD: 1059.9807\n",
      "Train Epoch: 023 Batch: 00092/00094 | Loss: 772.9681 | CE: 0.2417 | KD: 1059.9813\n",
      "Train Epoch: 023 Batch: 00093/00094 | Loss: 772.9477 | CE: 0.2417 | KD: 1059.9534\n",
      "Train Epoch: 023 Batch: 00094/00094 | Loss: 773.0526 | CE: 0.3119 | KD: 1060.0010\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2776 | acc:89.9000\n",
      "[VAL Acc] Target: 89.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0638 | acc:50.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9394 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9308 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7358 | acc:56.6614\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9188 | acc:55.4529\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5582 | acc:71.0423\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9830 | acc:55.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4697 | acc:75.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.35%\n",
      "[VAL Acc] Avg 61.47%\n",
      "VAL Acc improve from 60.91% to 61.47%\n",
      "Save best model\n",
      "Train Epoch: 024 Batch: 00001/00094 | Loss: 773.0651 | CE: 0.2833 | KD: 1060.0574\n",
      "Train Epoch: 024 Batch: 00002/00094 | Loss: 773.0006 | CE: 0.2338 | KD: 1060.0369\n",
      "Train Epoch: 024 Batch: 00003/00094 | Loss: 772.9122 | CE: 0.2116 | KD: 1059.9459\n",
      "Train Epoch: 024 Batch: 00004/00094 | Loss: 772.9972 | CE: 0.2821 | KD: 1059.9659\n",
      "Train Epoch: 024 Batch: 00005/00094 | Loss: 773.0134 | CE: 0.2401 | KD: 1060.0458\n",
      "Train Epoch: 024 Batch: 00006/00094 | Loss: 773.0208 | CE: 0.2739 | KD: 1060.0095\n",
      "Train Epoch: 024 Batch: 00007/00094 | Loss: 773.1681 | CE: 0.3434 | KD: 1060.1162\n",
      "Train Epoch: 024 Batch: 00008/00094 | Loss: 772.9914 | CE: 0.2434 | KD: 1060.0111\n",
      "Train Epoch: 024 Batch: 00009/00094 | Loss: 772.9864 | CE: 0.2369 | KD: 1060.0131\n",
      "Train Epoch: 024 Batch: 00010/00094 | Loss: 772.9861 | CE: 0.2594 | KD: 1059.9819\n",
      "Train Epoch: 024 Batch: 00011/00094 | Loss: 773.0422 | CE: 0.2713 | KD: 1060.0425\n",
      "Train Epoch: 024 Batch: 00012/00094 | Loss: 773.0067 | CE: 0.2819 | KD: 1059.9791\n",
      "Train Epoch: 024 Batch: 00013/00094 | Loss: 772.9417 | CE: 0.2496 | KD: 1059.9343\n",
      "Train Epoch: 024 Batch: 00014/00094 | Loss: 772.9732 | CE: 0.2600 | KD: 1059.9633\n",
      "Train Epoch: 024 Batch: 00015/00094 | Loss: 773.0510 | CE: 0.2420 | KD: 1060.0946\n",
      "Train Epoch: 024 Batch: 00016/00094 | Loss: 773.0224 | CE: 0.2587 | KD: 1060.0325\n",
      "Train Epoch: 024 Batch: 00017/00094 | Loss: 773.0461 | CE: 0.2952 | KD: 1060.0151\n",
      "Train Epoch: 024 Batch: 00018/00094 | Loss: 772.9321 | CE: 0.2240 | KD: 1059.9562\n",
      "Train Epoch: 024 Batch: 00019/00094 | Loss: 773.0611 | CE: 0.2797 | KD: 1060.0569\n",
      "Train Epoch: 024 Batch: 00020/00094 | Loss: 772.9222 | CE: 0.2103 | KD: 1059.9615\n",
      "Train Epoch: 024 Batch: 00021/00094 | Loss: 772.9403 | CE: 0.2432 | KD: 1059.9412\n",
      "Train Epoch: 024 Batch: 00022/00094 | Loss: 773.2016 | CE: 0.3511 | KD: 1060.1516\n",
      "Train Epoch: 024 Batch: 00023/00094 | Loss: 773.0120 | CE: 0.2564 | KD: 1060.0215\n",
      "Train Epoch: 024 Batch: 00024/00094 | Loss: 773.0115 | CE: 0.2720 | KD: 1059.9993\n",
      "Train Epoch: 024 Batch: 00025/00094 | Loss: 772.9575 | CE: 0.2460 | KD: 1059.9609\n",
      "Train Epoch: 024 Batch: 00026/00094 | Loss: 772.9518 | CE: 0.2395 | KD: 1059.9620\n",
      "Train Epoch: 024 Batch: 00027/00094 | Loss: 773.0789 | CE: 0.3208 | KD: 1060.0249\n",
      "Train Epoch: 024 Batch: 00028/00094 | Loss: 772.9691 | CE: 0.2393 | KD: 1059.9860\n",
      "Train Epoch: 024 Batch: 00029/00094 | Loss: 773.1396 | CE: 0.3658 | KD: 1060.0464\n",
      "Train Epoch: 024 Batch: 00030/00094 | Loss: 773.0981 | CE: 0.2708 | KD: 1060.1198\n",
      "Train Epoch: 024 Batch: 00031/00094 | Loss: 773.1151 | CE: 0.2973 | KD: 1060.1067\n",
      "Train Epoch: 024 Batch: 00032/00094 | Loss: 773.1823 | CE: 0.3653 | KD: 1060.1057\n",
      "Train Epoch: 024 Batch: 00033/00094 | Loss: 773.1118 | CE: 0.3616 | KD: 1060.0140\n",
      "Train Epoch: 024 Batch: 00034/00094 | Loss: 773.0641 | CE: 0.3301 | KD: 1059.9919\n",
      "Train Epoch: 024 Batch: 00035/00094 | Loss: 773.0328 | CE: 0.2539 | KD: 1060.0535\n",
      "Train Epoch: 024 Batch: 00036/00094 | Loss: 773.1658 | CE: 0.3039 | KD: 1060.1674\n",
      "Train Epoch: 024 Batch: 00037/00094 | Loss: 772.9837 | CE: 0.2434 | KD: 1060.0005\n",
      "Train Epoch: 024 Batch: 00038/00094 | Loss: 773.0692 | CE: 0.2781 | KD: 1060.0702\n",
      "Train Epoch: 024 Batch: 00039/00094 | Loss: 773.0509 | CE: 0.2669 | KD: 1060.0604\n",
      "Train Epoch: 024 Batch: 00040/00094 | Loss: 773.0099 | CE: 0.2553 | KD: 1060.0201\n",
      "Train Epoch: 024 Batch: 00041/00094 | Loss: 773.0671 | CE: 0.3097 | KD: 1060.0238\n",
      "Train Epoch: 024 Batch: 00042/00094 | Loss: 772.9354 | CE: 0.2004 | KD: 1059.9932\n",
      "Train Epoch: 024 Batch: 00043/00094 | Loss: 773.0208 | CE: 0.2633 | KD: 1060.0242\n",
      "Train Epoch: 024 Batch: 00044/00094 | Loss: 773.0217 | CE: 0.2703 | KD: 1060.0156\n",
      "Train Epoch: 024 Batch: 00045/00094 | Loss: 773.0312 | CE: 0.2337 | KD: 1060.0790\n",
      "Train Epoch: 024 Batch: 00046/00094 | Loss: 772.9207 | CE: 0.2256 | KD: 1059.9384\n",
      "Train Epoch: 024 Batch: 00047/00094 | Loss: 772.9636 | CE: 0.2166 | KD: 1060.0096\n",
      "Train Epoch: 024 Batch: 00048/00094 | Loss: 773.0234 | CE: 0.2780 | KD: 1060.0074\n",
      "Train Epoch: 024 Batch: 00049/00094 | Loss: 773.0691 | CE: 0.2733 | KD: 1060.0765\n",
      "Train Epoch: 024 Batch: 00050/00094 | Loss: 773.0087 | CE: 0.2183 | KD: 1060.0692\n",
      "Train Epoch: 024 Batch: 00051/00094 | Loss: 773.0414 | CE: 0.2586 | KD: 1060.0588\n",
      "Train Epoch: 024 Batch: 00052/00094 | Loss: 773.2415 | CE: 0.4172 | KD: 1060.1156\n",
      "Train Epoch: 024 Batch: 00053/00094 | Loss: 773.0229 | CE: 0.2689 | KD: 1060.0192\n",
      "Train Epoch: 024 Batch: 00054/00094 | Loss: 772.9810 | CE: 0.2549 | KD: 1059.9810\n",
      "Train Epoch: 024 Batch: 00055/00094 | Loss: 772.9622 | CE: 0.1975 | KD: 1060.0338\n",
      "Train Epoch: 024 Batch: 00056/00094 | Loss: 773.1357 | CE: 0.3063 | KD: 1060.1228\n",
      "Train Epoch: 024 Batch: 00057/00094 | Loss: 773.0056 | CE: 0.2679 | KD: 1059.9969\n",
      "Train Epoch: 024 Batch: 00058/00094 | Loss: 772.9280 | CE: 0.2027 | KD: 1059.9799\n",
      "Train Epoch: 024 Batch: 00059/00094 | Loss: 773.0406 | CE: 0.2678 | KD: 1060.0450\n",
      "Train Epoch: 024 Batch: 00060/00094 | Loss: 772.9951 | CE: 0.2498 | KD: 1060.0073\n",
      "Train Epoch: 024 Batch: 00061/00094 | Loss: 773.0084 | CE: 0.2517 | KD: 1060.0229\n",
      "Train Epoch: 024 Batch: 00062/00094 | Loss: 773.0739 | CE: 0.2888 | KD: 1060.0619\n",
      "Train Epoch: 024 Batch: 00063/00094 | Loss: 773.0326 | CE: 0.2332 | KD: 1060.0814\n",
      "Train Epoch: 024 Batch: 00064/00094 | Loss: 773.0515 | CE: 0.2163 | KD: 1060.1306\n",
      "Train Epoch: 024 Batch: 00065/00094 | Loss: 773.0739 | CE: 0.2625 | KD: 1060.0980\n",
      "Train Epoch: 024 Batch: 00066/00094 | Loss: 773.0740 | CE: 0.2718 | KD: 1060.0853\n",
      "Train Epoch: 024 Batch: 00067/00094 | Loss: 772.9647 | CE: 0.2568 | KD: 1059.9561\n",
      "Train Epoch: 024 Batch: 00068/00094 | Loss: 773.0372 | CE: 0.2874 | KD: 1060.0135\n",
      "Train Epoch: 024 Batch: 00069/00094 | Loss: 773.0612 | CE: 0.3118 | KD: 1060.0128\n",
      "Train Epoch: 024 Batch: 00070/00094 | Loss: 772.9387 | CE: 0.2152 | KD: 1059.9774\n",
      "Train Epoch: 024 Batch: 00071/00094 | Loss: 773.0834 | CE: 0.3258 | KD: 1060.0242\n",
      "Train Epoch: 024 Batch: 00072/00094 | Loss: 772.9431 | CE: 0.2070 | KD: 1059.9946\n",
      "Train Epoch: 024 Batch: 00073/00094 | Loss: 773.0081 | CE: 0.2590 | KD: 1060.0126\n",
      "Train Epoch: 024 Batch: 00074/00094 | Loss: 772.9769 | CE: 0.2210 | KD: 1060.0219\n",
      "Train Epoch: 024 Batch: 00075/00094 | Loss: 773.1355 | CE: 0.2986 | KD: 1060.1329\n",
      "Train Epoch: 024 Batch: 00076/00094 | Loss: 773.2524 | CE: 0.4443 | KD: 1060.0935\n",
      "Train Epoch: 024 Batch: 00077/00094 | Loss: 773.0349 | CE: 0.2530 | KD: 1060.0576\n",
      "Train Epoch: 024 Batch: 00078/00094 | Loss: 773.0318 | CE: 0.2498 | KD: 1060.0577\n",
      "Train Epoch: 024 Batch: 00079/00094 | Loss: 772.9970 | CE: 0.2432 | KD: 1060.0189\n",
      "Train Epoch: 024 Batch: 00080/00094 | Loss: 773.0861 | CE: 0.2569 | KD: 1060.1223\n",
      "Train Epoch: 024 Batch: 00081/00094 | Loss: 773.0101 | CE: 0.2646 | KD: 1060.0076\n",
      "Train Epoch: 024 Batch: 00082/00094 | Loss: 773.0600 | CE: 0.3397 | KD: 1059.9730\n",
      "Train Epoch: 024 Batch: 00083/00094 | Loss: 773.1038 | CE: 0.3189 | KD: 1060.0618\n",
      "Train Epoch: 024 Batch: 00084/00094 | Loss: 773.1055 | CE: 0.2732 | KD: 1060.1267\n",
      "Train Epoch: 024 Batch: 00085/00094 | Loss: 773.1317 | CE: 0.3065 | KD: 1060.1169\n",
      "Train Epoch: 024 Batch: 00086/00094 | Loss: 773.1614 | CE: 0.3269 | KD: 1060.1296\n",
      "Train Epoch: 024 Batch: 00087/00094 | Loss: 773.1931 | CE: 0.3849 | KD: 1060.0936\n",
      "Train Epoch: 024 Batch: 00088/00094 | Loss: 773.0760 | CE: 0.3204 | KD: 1060.0215\n",
      "Train Epoch: 024 Batch: 00089/00094 | Loss: 773.2545 | CE: 0.4189 | KD: 1060.1311\n",
      "Train Epoch: 024 Batch: 00090/00094 | Loss: 773.0658 | CE: 0.2736 | KD: 1060.0717\n",
      "Train Epoch: 024 Batch: 00091/00094 | Loss: 773.0464 | CE: 0.3099 | KD: 1059.9954\n",
      "Train Epoch: 024 Batch: 00092/00094 | Loss: 773.0331 | CE: 0.2442 | KD: 1060.0671\n",
      "Train Epoch: 024 Batch: 00093/00094 | Loss: 772.9996 | CE: 0.2565 | KD: 1060.0043\n",
      "Train Epoch: 024 Batch: 00094/00094 | Loss: 773.0046 | CE: 0.2830 | KD: 1059.9747\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2902 | acc:89.6000\n",
      "[VAL Acc] Target: 89.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1336 | acc:50.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0069 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0276 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7857 | acc:55.1332\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7646 | acc:59.2421\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 59.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5722 | acc:70.2978\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8942 | acc:56.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4842 | acc:74.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.95%\n",
      "[VAL Acc] Avg 61.44%\n",
      "Train Epoch: 025 Batch: 00001/00094 | Loss: 773.0928 | CE: 0.3055 | KD: 1060.0648\n",
      "Train Epoch: 025 Batch: 00002/00094 | Loss: 772.9805 | CE: 0.2553 | KD: 1059.9797\n",
      "Train Epoch: 025 Batch: 00003/00094 | Loss: 772.9676 | CE: 0.2254 | KD: 1060.0031\n",
      "Train Epoch: 025 Batch: 00004/00094 | Loss: 773.0469 | CE: 0.2391 | KD: 1060.0930\n",
      "Train Epoch: 025 Batch: 00005/00094 | Loss: 772.9649 | CE: 0.2543 | KD: 1059.9597\n",
      "Train Epoch: 025 Batch: 00006/00094 | Loss: 773.0526 | CE: 0.2634 | KD: 1060.0675\n",
      "Train Epoch: 025 Batch: 00007/00094 | Loss: 772.9858 | CE: 0.2224 | KD: 1060.0321\n",
      "Train Epoch: 025 Batch: 00008/00094 | Loss: 773.0663 | CE: 0.2701 | KD: 1060.0771\n",
      "Train Epoch: 025 Batch: 00009/00094 | Loss: 773.0679 | CE: 0.2661 | KD: 1060.0847\n",
      "Train Epoch: 025 Batch: 00010/00094 | Loss: 773.0341 | CE: 0.2591 | KD: 1060.0480\n",
      "Train Epoch: 025 Batch: 00011/00094 | Loss: 772.9584 | CE: 0.2075 | KD: 1060.0149\n",
      "Train Epoch: 025 Batch: 00012/00094 | Loss: 772.9787 | CE: 0.2150 | KD: 1060.0325\n",
      "Train Epoch: 025 Batch: 00013/00094 | Loss: 772.9641 | CE: 0.2293 | KD: 1059.9928\n",
      "Train Epoch: 025 Batch: 00014/00094 | Loss: 773.0277 | CE: 0.2549 | KD: 1060.0450\n",
      "Train Epoch: 025 Batch: 00015/00094 | Loss: 773.0244 | CE: 0.2470 | KD: 1060.0513\n",
      "Train Epoch: 025 Batch: 00016/00094 | Loss: 773.0644 | CE: 0.2851 | KD: 1060.0540\n",
      "Train Epoch: 025 Batch: 00017/00094 | Loss: 772.9626 | CE: 0.1933 | KD: 1060.0402\n",
      "Train Epoch: 025 Batch: 00018/00094 | Loss: 772.9862 | CE: 0.2489 | KD: 1059.9965\n",
      "Train Epoch: 025 Batch: 00019/00094 | Loss: 772.9219 | CE: 0.1984 | KD: 1059.9774\n",
      "Train Epoch: 025 Batch: 00020/00094 | Loss: 772.9845 | CE: 0.2501 | KD: 1059.9924\n",
      "Train Epoch: 025 Batch: 00021/00094 | Loss: 773.0364 | CE: 0.2685 | KD: 1060.0383\n",
      "Train Epoch: 025 Batch: 00022/00094 | Loss: 772.9855 | CE: 0.2221 | KD: 1060.0322\n",
      "Train Epoch: 025 Batch: 00023/00094 | Loss: 773.0950 | CE: 0.3040 | KD: 1060.0699\n",
      "Train Epoch: 025 Batch: 00024/00094 | Loss: 773.0708 | CE: 0.2778 | KD: 1060.0728\n",
      "Train Epoch: 025 Batch: 00025/00094 | Loss: 773.1423 | CE: 0.3217 | KD: 1060.1106\n",
      "Train Epoch: 025 Batch: 00026/00094 | Loss: 773.1310 | CE: 0.3317 | KD: 1060.0814\n",
      "Train Epoch: 025 Batch: 00027/00094 | Loss: 772.9938 | CE: 0.2620 | KD: 1059.9889\n",
      "Train Epoch: 025 Batch: 00028/00094 | Loss: 773.0366 | CE: 0.2763 | KD: 1060.0278\n",
      "Train Epoch: 025 Batch: 00029/00094 | Loss: 773.0073 | CE: 0.2736 | KD: 1059.9913\n",
      "Train Epoch: 025 Batch: 00030/00094 | Loss: 773.0203 | CE: 0.2480 | KD: 1060.0444\n",
      "Train Epoch: 025 Batch: 00031/00094 | Loss: 773.1395 | CE: 0.3259 | KD: 1060.1011\n",
      "Train Epoch: 025 Batch: 00032/00094 | Loss: 772.9411 | CE: 0.1875 | KD: 1060.0187\n",
      "Train Epoch: 025 Batch: 00033/00094 | Loss: 773.1233 | CE: 0.3013 | KD: 1060.1125\n",
      "Train Epoch: 025 Batch: 00034/00094 | Loss: 773.0300 | CE: 0.2400 | KD: 1060.0685\n",
      "Train Epoch: 025 Batch: 00035/00094 | Loss: 773.0826 | CE: 0.2681 | KD: 1060.1023\n",
      "Train Epoch: 025 Batch: 00036/00094 | Loss: 773.0089 | CE: 0.2842 | KD: 1059.9790\n",
      "Train Epoch: 025 Batch: 00037/00094 | Loss: 773.0085 | CE: 0.2372 | KD: 1060.0430\n",
      "Train Epoch: 025 Batch: 00038/00094 | Loss: 773.0091 | CE: 0.2353 | KD: 1060.0465\n",
      "Train Epoch: 025 Batch: 00039/00094 | Loss: 773.1094 | CE: 0.2836 | KD: 1060.1178\n",
      "Train Epoch: 025 Batch: 00040/00094 | Loss: 773.0370 | CE: 0.2819 | KD: 1060.0209\n",
      "Train Epoch: 025 Batch: 00041/00094 | Loss: 773.0586 | CE: 0.2843 | KD: 1060.0471\n",
      "Train Epoch: 025 Batch: 00042/00094 | Loss: 772.9959 | CE: 0.2581 | KD: 1059.9969\n",
      "Train Epoch: 025 Batch: 00043/00094 | Loss: 773.1342 | CE: 0.3142 | KD: 1060.1097\n",
      "Train Epoch: 025 Batch: 00044/00094 | Loss: 772.9316 | CE: 0.2228 | KD: 1059.9573\n",
      "Train Epoch: 025 Batch: 00045/00094 | Loss: 772.9169 | CE: 0.1951 | KD: 1059.9750\n",
      "Train Epoch: 025 Batch: 00046/00094 | Loss: 772.9366 | CE: 0.2020 | KD: 1059.9927\n",
      "Train Epoch: 025 Batch: 00047/00094 | Loss: 772.9406 | CE: 0.1997 | KD: 1060.0011\n",
      "Train Epoch: 025 Batch: 00048/00094 | Loss: 772.9894 | CE: 0.2058 | KD: 1060.0598\n",
      "Train Epoch: 025 Batch: 00049/00094 | Loss: 773.0884 | CE: 0.3316 | KD: 1060.0232\n",
      "Train Epoch: 025 Batch: 00050/00094 | Loss: 773.0349 | CE: 0.3047 | KD: 1059.9866\n",
      "Train Epoch: 025 Batch: 00051/00094 | Loss: 773.0613 | CE: 0.2404 | KD: 1060.1110\n",
      "Train Epoch: 025 Batch: 00052/00094 | Loss: 772.9660 | CE: 0.2384 | KD: 1059.9830\n",
      "Train Epoch: 025 Batch: 00053/00094 | Loss: 773.0496 | CE: 0.3051 | KD: 1060.0061\n",
      "Train Epoch: 025 Batch: 00054/00094 | Loss: 773.0199 | CE: 0.2642 | KD: 1060.0216\n",
      "Train Epoch: 025 Batch: 00055/00094 | Loss: 773.0239 | CE: 0.2826 | KD: 1060.0018\n",
      "Train Epoch: 025 Batch: 00056/00094 | Loss: 773.0681 | CE: 0.2780 | KD: 1060.0687\n",
      "Train Epoch: 025 Batch: 00057/00094 | Loss: 773.0276 | CE: 0.2834 | KD: 1060.0057\n",
      "Train Epoch: 025 Batch: 00058/00094 | Loss: 772.9792 | CE: 0.2413 | KD: 1059.9971\n",
      "Train Epoch: 025 Batch: 00059/00094 | Loss: 773.1004 | CE: 0.3101 | KD: 1060.0691\n",
      "Train Epoch: 025 Batch: 00060/00094 | Loss: 773.0329 | CE: 0.2740 | KD: 1060.0259\n",
      "Train Epoch: 025 Batch: 00061/00094 | Loss: 773.1240 | CE: 0.3105 | KD: 1060.1008\n",
      "Train Epoch: 025 Batch: 00062/00094 | Loss: 773.0192 | CE: 0.2714 | KD: 1060.0109\n",
      "Train Epoch: 025 Batch: 00063/00094 | Loss: 773.0004 | CE: 0.2514 | KD: 1060.0125\n",
      "Train Epoch: 025 Batch: 00064/00094 | Loss: 773.0396 | CE: 0.3048 | KD: 1059.9929\n",
      "Train Epoch: 025 Batch: 00065/00094 | Loss: 772.9869 | CE: 0.2429 | KD: 1060.0056\n",
      "Train Epoch: 025 Batch: 00066/00094 | Loss: 773.0364 | CE: 0.3021 | KD: 1059.9923\n",
      "Train Epoch: 025 Batch: 00067/00094 | Loss: 773.0285 | CE: 0.2370 | KD: 1060.0708\n",
      "Train Epoch: 025 Batch: 00068/00094 | Loss: 773.0959 | CE: 0.3299 | KD: 1060.0359\n",
      "Train Epoch: 025 Batch: 00069/00094 | Loss: 773.0014 | CE: 0.2731 | KD: 1059.9840\n",
      "Train Epoch: 025 Batch: 00070/00094 | Loss: 773.0165 | CE: 0.2151 | KD: 1060.0844\n",
      "Train Epoch: 025 Batch: 00071/00094 | Loss: 773.0135 | CE: 0.2210 | KD: 1060.0721\n",
      "Train Epoch: 025 Batch: 00072/00094 | Loss: 773.0684 | CE: 0.3096 | KD: 1060.0259\n",
      "Train Epoch: 025 Batch: 00073/00094 | Loss: 773.0548 | CE: 0.2639 | KD: 1060.0699\n",
      "Train Epoch: 025 Batch: 00074/00094 | Loss: 772.9716 | CE: 0.2250 | KD: 1060.0092\n",
      "Train Epoch: 025 Batch: 00075/00094 | Loss: 773.0142 | CE: 0.2897 | KD: 1059.9788\n",
      "Train Epoch: 025 Batch: 00076/00094 | Loss: 773.0943 | CE: 0.3040 | KD: 1060.0691\n",
      "Train Epoch: 025 Batch: 00077/00094 | Loss: 773.0055 | CE: 0.2796 | KD: 1059.9806\n",
      "Train Epoch: 025 Batch: 00078/00094 | Loss: 773.0347 | CE: 0.2495 | KD: 1060.0620\n",
      "Train Epoch: 025 Batch: 00079/00094 | Loss: 773.0104 | CE: 0.2758 | KD: 1059.9927\n",
      "Train Epoch: 025 Batch: 00080/00094 | Loss: 772.9791 | CE: 0.2343 | KD: 1060.0067\n",
      "Train Epoch: 025 Batch: 00081/00094 | Loss: 772.9511 | CE: 0.2010 | KD: 1060.0140\n",
      "Train Epoch: 025 Batch: 00082/00094 | Loss: 773.0748 | CE: 0.2815 | KD: 1060.0730\n",
      "Train Epoch: 025 Batch: 00083/00094 | Loss: 772.9542 | CE: 0.2379 | KD: 1059.9675\n",
      "Train Epoch: 025 Batch: 00084/00094 | Loss: 773.1008 | CE: 0.3283 | KD: 1060.0446\n",
      "Train Epoch: 025 Batch: 00085/00094 | Loss: 773.1491 | CE: 0.3290 | KD: 1060.1099\n",
      "Train Epoch: 025 Batch: 00086/00094 | Loss: 772.9895 | CE: 0.2121 | KD: 1060.0514\n",
      "Train Epoch: 025 Batch: 00087/00094 | Loss: 773.0449 | CE: 0.2384 | KD: 1060.0913\n",
      "Train Epoch: 025 Batch: 00088/00094 | Loss: 773.0248 | CE: 0.2438 | KD: 1060.0563\n",
      "Train Epoch: 025 Batch: 00089/00094 | Loss: 772.9520 | CE: 0.2163 | KD: 1059.9941\n",
      "Train Epoch: 025 Batch: 00090/00094 | Loss: 773.1122 | CE: 0.2771 | KD: 1060.1305\n",
      "Train Epoch: 025 Batch: 00091/00094 | Loss: 773.1583 | CE: 0.3326 | KD: 1060.1177\n",
      "Train Epoch: 025 Batch: 00092/00094 | Loss: 773.0081 | CE: 0.2850 | KD: 1059.9769\n",
      "Train Epoch: 025 Batch: 00093/00094 | Loss: 773.1218 | CE: 0.3192 | KD: 1060.0859\n",
      "Train Epoch: 025 Batch: 00094/00094 | Loss: 773.1411 | CE: 0.2724 | KD: 1060.1765\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2827 | acc:90.3000\n",
      "[VAL Acc] Target: 90.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2292 | acc:49.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0495 | acc:48.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1017 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7719 | acc:58.9734\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.97%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9248 | acc:53.6969\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5167 | acc:75.4310\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9715 | acc:54.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5070 | acc:73.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.00%\n",
      "[VAL Acc] Avg 61.48%\n",
      "VAL Acc improve from 61.47% to 61.48%\n",
      "Save best model\n",
      "Train Epoch: 026 Batch: 00001/00094 | Loss: 773.1149 | CE: 0.2861 | KD: 1060.1218\n",
      "Train Epoch: 026 Batch: 00002/00094 | Loss: 773.0126 | CE: 0.2540 | KD: 1060.0256\n",
      "Train Epoch: 026 Batch: 00003/00094 | Loss: 772.9989 | CE: 0.2576 | KD: 1060.0018\n",
      "Train Epoch: 026 Batch: 00004/00094 | Loss: 773.0718 | CE: 0.2998 | KD: 1060.0439\n",
      "Train Epoch: 026 Batch: 00005/00094 | Loss: 773.0952 | CE: 0.3147 | KD: 1060.0555\n",
      "Train Epoch: 026 Batch: 00006/00094 | Loss: 772.9930 | CE: 0.2326 | KD: 1060.0280\n",
      "Train Epoch: 026 Batch: 00007/00094 | Loss: 773.0766 | CE: 0.2874 | KD: 1060.0675\n",
      "Train Epoch: 026 Batch: 00008/00094 | Loss: 773.1079 | CE: 0.2972 | KD: 1060.0970\n",
      "Train Epoch: 026 Batch: 00009/00094 | Loss: 773.0775 | CE: 0.3049 | KD: 1060.0447\n",
      "Train Epoch: 026 Batch: 00010/00094 | Loss: 773.0112 | CE: 0.2818 | KD: 1059.9855\n",
      "Train Epoch: 026 Batch: 00011/00094 | Loss: 772.9609 | CE: 0.2198 | KD: 1060.0016\n",
      "Train Epoch: 026 Batch: 00012/00094 | Loss: 773.0091 | CE: 0.2484 | KD: 1060.0284\n",
      "Train Epoch: 026 Batch: 00013/00094 | Loss: 773.0237 | CE: 0.2463 | KD: 1060.0514\n",
      "Train Epoch: 026 Batch: 00014/00094 | Loss: 773.0176 | CE: 0.2342 | KD: 1060.0596\n",
      "Train Epoch: 026 Batch: 00015/00094 | Loss: 772.9515 | CE: 0.2503 | KD: 1059.9469\n",
      "Train Epoch: 026 Batch: 00016/00094 | Loss: 773.0160 | CE: 0.2528 | KD: 1060.0319\n",
      "Train Epoch: 026 Batch: 00017/00094 | Loss: 773.0284 | CE: 0.2889 | KD: 1059.9994\n",
      "Train Epoch: 026 Batch: 00018/00094 | Loss: 772.9974 | CE: 0.2266 | KD: 1060.0424\n",
      "Train Epoch: 026 Batch: 00019/00094 | Loss: 773.0608 | CE: 0.2797 | KD: 1060.0564\n",
      "Train Epoch: 026 Batch: 00020/00094 | Loss: 772.9880 | CE: 0.2107 | KD: 1060.0513\n",
      "Train Epoch: 026 Batch: 00021/00094 | Loss: 773.0191 | CE: 0.2625 | KD: 1060.0228\n",
      "Train Epoch: 026 Batch: 00022/00094 | Loss: 772.9359 | CE: 0.2243 | KD: 1059.9611\n",
      "Train Epoch: 026 Batch: 00023/00094 | Loss: 772.9578 | CE: 0.2349 | KD: 1059.9766\n",
      "Train Epoch: 026 Batch: 00024/00094 | Loss: 773.0775 | CE: 0.2808 | KD: 1060.0778\n",
      "Train Epoch: 026 Batch: 00025/00094 | Loss: 772.9661 | CE: 0.2058 | KD: 1060.0280\n",
      "Train Epoch: 026 Batch: 00026/00094 | Loss: 773.0527 | CE: 0.2896 | KD: 1060.0317\n",
      "Train Epoch: 026 Batch: 00027/00094 | Loss: 773.0176 | CE: 0.2531 | KD: 1060.0336\n",
      "Train Epoch: 026 Batch: 00028/00094 | Loss: 773.0068 | CE: 0.2485 | KD: 1060.0251\n",
      "Train Epoch: 026 Batch: 00029/00094 | Loss: 773.0376 | CE: 0.2912 | KD: 1060.0088\n",
      "Train Epoch: 026 Batch: 00030/00094 | Loss: 773.0457 | CE: 0.3202 | KD: 1059.9802\n",
      "Train Epoch: 026 Batch: 00031/00094 | Loss: 773.0474 | CE: 0.3250 | KD: 1059.9760\n",
      "Train Epoch: 026 Batch: 00032/00094 | Loss: 772.9756 | CE: 0.2409 | KD: 1059.9928\n",
      "Train Epoch: 026 Batch: 00033/00094 | Loss: 773.0027 | CE: 0.2775 | KD: 1059.9797\n",
      "Train Epoch: 026 Batch: 00034/00094 | Loss: 772.9167 | CE: 0.2223 | KD: 1059.9374\n",
      "Train Epoch: 026 Batch: 00035/00094 | Loss: 773.0120 | CE: 0.2233 | KD: 1060.0668\n",
      "Train Epoch: 026 Batch: 00036/00094 | Loss: 773.0469 | CE: 0.2557 | KD: 1060.0703\n",
      "Train Epoch: 026 Batch: 00037/00094 | Loss: 772.9294 | CE: 0.2107 | KD: 1059.9707\n",
      "Train Epoch: 026 Batch: 00038/00094 | Loss: 772.9802 | CE: 0.2354 | KD: 1060.0066\n",
      "Train Epoch: 026 Batch: 00039/00094 | Loss: 772.9802 | CE: 0.2197 | KD: 1060.0281\n",
      "Train Epoch: 026 Batch: 00040/00094 | Loss: 773.0890 | CE: 0.2977 | KD: 1060.0704\n",
      "Train Epoch: 026 Batch: 00041/00094 | Loss: 773.0314 | CE: 0.2720 | KD: 1060.0266\n",
      "Train Epoch: 026 Batch: 00042/00094 | Loss: 772.9568 | CE: 0.2419 | KD: 1059.9656\n",
      "Train Epoch: 026 Batch: 00043/00094 | Loss: 773.0152 | CE: 0.2579 | KD: 1060.0237\n",
      "Train Epoch: 026 Batch: 00044/00094 | Loss: 773.0125 | CE: 0.2515 | KD: 1060.0288\n",
      "Train Epoch: 026 Batch: 00045/00094 | Loss: 773.0447 | CE: 0.2464 | KD: 1060.0800\n",
      "Train Epoch: 026 Batch: 00046/00094 | Loss: 773.0912 | CE: 0.2927 | KD: 1060.0802\n",
      "Train Epoch: 026 Batch: 00047/00094 | Loss: 773.0345 | CE: 0.2527 | KD: 1060.0574\n",
      "Train Epoch: 026 Batch: 00048/00094 | Loss: 773.0305 | CE: 0.2213 | KD: 1060.0950\n",
      "Train Epoch: 026 Batch: 00049/00094 | Loss: 773.0508 | CE: 0.2250 | KD: 1060.1178\n",
      "Train Epoch: 026 Batch: 00050/00094 | Loss: 773.0331 | CE: 0.2309 | KD: 1060.0854\n",
      "Train Epoch: 026 Batch: 00051/00094 | Loss: 772.9318 | CE: 0.2348 | KD: 1059.9410\n",
      "Train Epoch: 026 Batch: 00052/00094 | Loss: 772.9613 | CE: 0.2169 | KD: 1060.0061\n",
      "Train Epoch: 026 Batch: 00053/00094 | Loss: 772.9733 | CE: 0.2621 | KD: 1059.9606\n",
      "Train Epoch: 026 Batch: 00054/00094 | Loss: 772.9508 | CE: 0.2393 | KD: 1059.9609\n",
      "Train Epoch: 026 Batch: 00055/00094 | Loss: 773.1508 | CE: 0.3061 | KD: 1060.1438\n",
      "Train Epoch: 026 Batch: 00056/00094 | Loss: 772.9916 | CE: 0.2378 | KD: 1060.0190\n",
      "Train Epoch: 026 Batch: 00057/00094 | Loss: 773.0486 | CE: 0.2942 | KD: 1060.0199\n",
      "Train Epoch: 026 Batch: 00058/00094 | Loss: 773.0983 | CE: 0.3169 | KD: 1060.0568\n",
      "Train Epoch: 026 Batch: 00059/00094 | Loss: 772.9995 | CE: 0.2311 | KD: 1060.0389\n",
      "Train Epoch: 026 Batch: 00060/00094 | Loss: 773.0952 | CE: 0.3091 | KD: 1060.0632\n",
      "Train Epoch: 026 Batch: 00061/00094 | Loss: 773.0335 | CE: 0.2338 | KD: 1060.0819\n",
      "Train Epoch: 026 Batch: 00062/00094 | Loss: 772.9649 | CE: 0.2054 | KD: 1060.0267\n",
      "Train Epoch: 026 Batch: 00063/00094 | Loss: 773.0127 | CE: 0.2431 | KD: 1060.0405\n",
      "Train Epoch: 026 Batch: 00064/00094 | Loss: 773.0392 | CE: 0.2471 | KD: 1060.0715\n",
      "Train Epoch: 026 Batch: 00065/00094 | Loss: 772.8905 | CE: 0.1766 | KD: 1059.9642\n",
      "Train Epoch: 026 Batch: 00066/00094 | Loss: 772.9593 | CE: 0.2169 | KD: 1060.0033\n",
      "Train Epoch: 026 Batch: 00067/00094 | Loss: 773.0763 | CE: 0.2976 | KD: 1060.0531\n",
      "Train Epoch: 026 Batch: 00068/00094 | Loss: 773.0463 | CE: 0.2676 | KD: 1060.0531\n",
      "Train Epoch: 026 Batch: 00069/00094 | Loss: 773.0802 | CE: 0.2523 | KD: 1060.1206\n",
      "Train Epoch: 026 Batch: 00070/00094 | Loss: 773.0705 | CE: 0.2729 | KD: 1060.0791\n",
      "Train Epoch: 026 Batch: 00071/00094 | Loss: 773.0095 | CE: 0.2375 | KD: 1060.0438\n",
      "Train Epoch: 026 Batch: 00072/00094 | Loss: 773.0842 | CE: 0.3013 | KD: 1060.0588\n",
      "Train Epoch: 026 Batch: 00073/00094 | Loss: 772.8841 | CE: 0.2020 | KD: 1059.9205\n",
      "Train Epoch: 026 Batch: 00074/00094 | Loss: 773.2043 | CE: 0.3650 | KD: 1060.1362\n",
      "Train Epoch: 026 Batch: 00075/00094 | Loss: 773.0828 | CE: 0.2964 | KD: 1060.0637\n",
      "Train Epoch: 026 Batch: 00076/00094 | Loss: 773.0473 | CE: 0.2912 | KD: 1060.0222\n",
      "Train Epoch: 026 Batch: 00077/00094 | Loss: 773.3206 | CE: 0.5044 | KD: 1060.1045\n",
      "Train Epoch: 026 Batch: 00078/00094 | Loss: 773.0240 | CE: 0.2183 | KD: 1060.0901\n",
      "Train Epoch: 026 Batch: 00079/00094 | Loss: 772.9637 | CE: 0.2190 | KD: 1060.0065\n",
      "Train Epoch: 026 Batch: 00080/00094 | Loss: 773.0134 | CE: 0.2587 | KD: 1060.0203\n",
      "Train Epoch: 026 Batch: 00081/00094 | Loss: 773.0556 | CE: 0.2530 | KD: 1060.0859\n",
      "Train Epoch: 026 Batch: 00082/00094 | Loss: 772.9330 | CE: 0.2272 | KD: 1059.9531\n",
      "Train Epoch: 026 Batch: 00083/00094 | Loss: 773.1369 | CE: 0.2759 | KD: 1060.1660\n",
      "Train Epoch: 026 Batch: 00084/00094 | Loss: 772.9835 | CE: 0.2620 | KD: 1059.9747\n",
      "Train Epoch: 026 Batch: 00085/00094 | Loss: 772.9595 | CE: 0.2659 | KD: 1059.9364\n",
      "Train Epoch: 026 Batch: 00086/00094 | Loss: 773.0356 | CE: 0.2818 | KD: 1060.0190\n",
      "Train Epoch: 026 Batch: 00087/00094 | Loss: 772.9904 | CE: 0.2497 | KD: 1060.0009\n",
      "Train Epoch: 026 Batch: 00088/00094 | Loss: 773.0193 | CE: 0.2547 | KD: 1060.0338\n",
      "Train Epoch: 026 Batch: 00089/00094 | Loss: 773.0959 | CE: 0.3252 | KD: 1060.0422\n",
      "Train Epoch: 026 Batch: 00090/00094 | Loss: 773.0561 | CE: 0.2937 | KD: 1060.0308\n",
      "Train Epoch: 026 Batch: 00091/00094 | Loss: 772.9276 | CE: 0.2009 | KD: 1059.9818\n",
      "Train Epoch: 026 Batch: 00092/00094 | Loss: 773.0170 | CE: 0.2620 | KD: 1060.0206\n",
      "Train Epoch: 026 Batch: 00093/00094 | Loss: 772.9479 | CE: 0.2211 | KD: 1059.9821\n",
      "Train Epoch: 026 Batch: 00094/00094 | Loss: 773.0001 | CE: 0.2569 | KD: 1060.0043\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2817 | acc:90.4500\n",
      "[VAL Acc] Target: 90.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1038 | acc:50.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9534 | acc:49.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9982 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7185 | acc:60.1881\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9015 | acc:54.8983\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5082 | acc:75.4310\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9196 | acc:56.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4510 | acc:75.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.65%\n",
      "[VAL Acc] Avg 62.61%\n",
      "VAL Acc improve from 61.48% to 62.61%\n",
      "Save best model\n",
      "Train Epoch: 027 Batch: 00001/00094 | Loss: 772.9574 | CE: 0.2212 | KD: 1059.9949\n",
      "Train Epoch: 027 Batch: 00002/00094 | Loss: 772.9982 | CE: 0.2184 | KD: 1060.0548\n",
      "Train Epoch: 027 Batch: 00003/00094 | Loss: 772.9157 | CE: 0.2259 | KD: 1059.9313\n",
      "Train Epoch: 027 Batch: 00004/00094 | Loss: 772.9339 | CE: 0.1961 | KD: 1059.9969\n",
      "Train Epoch: 027 Batch: 00005/00094 | Loss: 772.9782 | CE: 0.2611 | KD: 1059.9688\n",
      "Train Epoch: 027 Batch: 00006/00094 | Loss: 772.8985 | CE: 0.1913 | KD: 1059.9550\n",
      "Train Epoch: 027 Batch: 00007/00094 | Loss: 772.9473 | CE: 0.2190 | KD: 1059.9840\n",
      "Train Epoch: 027 Batch: 00008/00094 | Loss: 772.9836 | CE: 0.2443 | KD: 1059.9991\n",
      "Train Epoch: 027 Batch: 00009/00094 | Loss: 772.9849 | CE: 0.2395 | KD: 1060.0074\n",
      "Train Epoch: 027 Batch: 00010/00094 | Loss: 773.0456 | CE: 0.2446 | KD: 1060.0837\n",
      "Train Epoch: 027 Batch: 00011/00094 | Loss: 772.9665 | CE: 0.2236 | KD: 1060.0040\n",
      "Train Epoch: 027 Batch: 00012/00094 | Loss: 773.0853 | CE: 0.2973 | KD: 1060.0658\n",
      "Train Epoch: 027 Batch: 00013/00094 | Loss: 772.9292 | CE: 0.2148 | KD: 1059.9648\n",
      "Train Epoch: 027 Batch: 00014/00094 | Loss: 773.0210 | CE: 0.2516 | KD: 1060.0404\n",
      "Train Epoch: 027 Batch: 00015/00094 | Loss: 773.0115 | CE: 0.2586 | KD: 1060.0177\n",
      "Train Epoch: 027 Batch: 00016/00094 | Loss: 773.0659 | CE: 0.3002 | KD: 1060.0353\n",
      "Train Epoch: 027 Batch: 00017/00094 | Loss: 773.0325 | CE: 0.2506 | KD: 1060.0576\n",
      "Train Epoch: 027 Batch: 00018/00094 | Loss: 773.0690 | CE: 0.2757 | KD: 1060.0731\n",
      "Train Epoch: 027 Batch: 00019/00094 | Loss: 772.9352 | CE: 0.2131 | KD: 1059.9755\n",
      "Train Epoch: 027 Batch: 00020/00094 | Loss: 772.9417 | CE: 0.2113 | KD: 1059.9869\n",
      "Train Epoch: 027 Batch: 00021/00094 | Loss: 773.0389 | CE: 0.2638 | KD: 1060.0482\n",
      "Train Epoch: 027 Batch: 00022/00094 | Loss: 773.0476 | CE: 0.2958 | KD: 1060.0162\n",
      "Train Epoch: 027 Batch: 00023/00094 | Loss: 772.9892 | CE: 0.2713 | KD: 1059.9696\n",
      "Train Epoch: 027 Batch: 00024/00094 | Loss: 772.9691 | CE: 0.2296 | KD: 1059.9994\n",
      "Train Epoch: 027 Batch: 00025/00094 | Loss: 773.0109 | CE: 0.2670 | KD: 1060.0054\n",
      "Train Epoch: 027 Batch: 00026/00094 | Loss: 773.0675 | CE: 0.2947 | KD: 1060.0450\n",
      "Train Epoch: 027 Batch: 00027/00094 | Loss: 773.0194 | CE: 0.2706 | KD: 1060.0121\n",
      "Train Epoch: 027 Batch: 00028/00094 | Loss: 772.9833 | CE: 0.2135 | KD: 1060.0409\n",
      "Train Epoch: 027 Batch: 00029/00094 | Loss: 772.9355 | CE: 0.2162 | KD: 1059.9717\n",
      "Train Epoch: 027 Batch: 00030/00094 | Loss: 773.0322 | CE: 0.2801 | KD: 1060.0165\n",
      "Train Epoch: 027 Batch: 00031/00094 | Loss: 772.9407 | CE: 0.2094 | KD: 1059.9882\n",
      "Train Epoch: 027 Batch: 00032/00094 | Loss: 772.9677 | CE: 0.2061 | KD: 1060.0297\n",
      "Train Epoch: 027 Batch: 00033/00094 | Loss: 772.9623 | CE: 0.2125 | KD: 1060.0134\n",
      "Train Epoch: 027 Batch: 00034/00094 | Loss: 773.0177 | CE: 0.3063 | KD: 1059.9608\n",
      "Train Epoch: 027 Batch: 00035/00094 | Loss: 773.0889 | CE: 0.2754 | KD: 1060.1007\n",
      "Train Epoch: 027 Batch: 00036/00094 | Loss: 773.0513 | CE: 0.2845 | KD: 1060.0367\n",
      "Train Epoch: 027 Batch: 00037/00094 | Loss: 773.0684 | CE: 0.2390 | KD: 1060.1227\n",
      "Train Epoch: 027 Batch: 00038/00094 | Loss: 773.1227 | CE: 0.3067 | KD: 1060.1042\n",
      "Train Epoch: 027 Batch: 00039/00094 | Loss: 772.8832 | CE: 0.1889 | KD: 1059.9374\n",
      "Train Epoch: 027 Batch: 00040/00094 | Loss: 773.0875 | CE: 0.2758 | KD: 1060.0984\n",
      "Train Epoch: 027 Batch: 00041/00094 | Loss: 773.0428 | CE: 0.2853 | KD: 1060.0242\n",
      "Train Epoch: 027 Batch: 00042/00094 | Loss: 773.0487 | CE: 0.2892 | KD: 1060.0269\n",
      "Train Epoch: 027 Batch: 00043/00094 | Loss: 773.0463 | CE: 0.2828 | KD: 1060.0322\n",
      "Train Epoch: 027 Batch: 00044/00094 | Loss: 773.1119 | CE: 0.3147 | KD: 1060.0785\n",
      "Train Epoch: 027 Batch: 00045/00094 | Loss: 773.1058 | CE: 0.3423 | KD: 1060.0322\n",
      "Train Epoch: 027 Batch: 00046/00094 | Loss: 772.9465 | CE: 0.2157 | KD: 1059.9873\n",
      "Train Epoch: 027 Batch: 00047/00094 | Loss: 773.0118 | CE: 0.2436 | KD: 1060.0388\n",
      "Train Epoch: 027 Batch: 00048/00094 | Loss: 773.1584 | CE: 0.3048 | KD: 1060.1559\n",
      "Train Epoch: 027 Batch: 00049/00094 | Loss: 773.1375 | CE: 0.3655 | KD: 1060.0438\n",
      "Train Epoch: 027 Batch: 00050/00094 | Loss: 772.9821 | CE: 0.2182 | KD: 1060.0328\n",
      "Train Epoch: 027 Batch: 00051/00094 | Loss: 772.9722 | CE: 0.2442 | KD: 1059.9835\n",
      "Train Epoch: 027 Batch: 00052/00094 | Loss: 773.0052 | CE: 0.2121 | KD: 1060.0729\n",
      "Train Epoch: 027 Batch: 00053/00094 | Loss: 772.9946 | CE: 0.2493 | KD: 1060.0073\n",
      "Train Epoch: 027 Batch: 00054/00094 | Loss: 773.0106 | CE: 0.2739 | KD: 1059.9956\n",
      "Train Epoch: 027 Batch: 00055/00094 | Loss: 773.1047 | CE: 0.3042 | KD: 1060.0831\n",
      "Train Epoch: 027 Batch: 00056/00094 | Loss: 773.0046 | CE: 0.2357 | KD: 1060.0397\n",
      "Train Epoch: 027 Batch: 00057/00094 | Loss: 772.9328 | CE: 0.2409 | KD: 1059.9340\n",
      "Train Epoch: 027 Batch: 00058/00094 | Loss: 773.1004 | CE: 0.2806 | KD: 1060.1095\n",
      "Train Epoch: 027 Batch: 00059/00094 | Loss: 772.9517 | CE: 0.2253 | KD: 1059.9813\n",
      "Train Epoch: 027 Batch: 00060/00094 | Loss: 773.0269 | CE: 0.2890 | KD: 1059.9972\n",
      "Train Epoch: 027 Batch: 00061/00094 | Loss: 773.1098 | CE: 0.3168 | KD: 1060.0728\n",
      "Train Epoch: 027 Batch: 00062/00094 | Loss: 773.0731 | CE: 0.3202 | KD: 1060.0177\n",
      "Train Epoch: 027 Batch: 00063/00094 | Loss: 772.9753 | CE: 0.2160 | KD: 1060.0265\n",
      "Train Epoch: 027 Batch: 00064/00094 | Loss: 773.0151 | CE: 0.2625 | KD: 1060.0172\n",
      "Train Epoch: 027 Batch: 00065/00094 | Loss: 772.9752 | CE: 0.2097 | KD: 1060.0350\n",
      "Train Epoch: 027 Batch: 00066/00094 | Loss: 772.9968 | CE: 0.2561 | KD: 1060.0011\n",
      "Train Epoch: 027 Batch: 00067/00094 | Loss: 773.0869 | CE: 0.2437 | KD: 1060.1417\n",
      "Train Epoch: 027 Batch: 00068/00094 | Loss: 773.0287 | CE: 0.2485 | KD: 1060.0552\n",
      "Train Epoch: 027 Batch: 00069/00094 | Loss: 772.9637 | CE: 0.2198 | KD: 1060.0054\n",
      "Train Epoch: 027 Batch: 00070/00094 | Loss: 773.0790 | CE: 0.2899 | KD: 1060.0675\n",
      "Train Epoch: 027 Batch: 00071/00094 | Loss: 773.0327 | CE: 0.2492 | KD: 1060.0597\n",
      "Train Epoch: 027 Batch: 00072/00094 | Loss: 773.1207 | CE: 0.2998 | KD: 1060.1111\n",
      "Train Epoch: 027 Batch: 00073/00094 | Loss: 773.0364 | CE: 0.2786 | KD: 1060.0244\n",
      "Train Epoch: 027 Batch: 00074/00094 | Loss: 773.0032 | CE: 0.2502 | KD: 1060.0179\n",
      "Train Epoch: 027 Batch: 00075/00094 | Loss: 773.0242 | CE: 0.2665 | KD: 1060.0243\n",
      "Train Epoch: 027 Batch: 00076/00094 | Loss: 773.0953 | CE: 0.3129 | KD: 1060.0581\n",
      "Train Epoch: 027 Batch: 00077/00094 | Loss: 773.0212 | CE: 0.2674 | KD: 1060.0189\n",
      "Train Epoch: 027 Batch: 00078/00094 | Loss: 773.0641 | CE: 0.3096 | KD: 1060.0200\n",
      "Train Epoch: 027 Batch: 00079/00094 | Loss: 773.0030 | CE: 0.2717 | KD: 1059.9880\n",
      "Train Epoch: 027 Batch: 00080/00094 | Loss: 773.1439 | CE: 0.2979 | KD: 1060.1453\n",
      "Train Epoch: 027 Batch: 00081/00094 | Loss: 773.1578 | CE: 0.3717 | KD: 1060.0632\n",
      "Train Epoch: 027 Batch: 00082/00094 | Loss: 772.9695 | CE: 0.2116 | KD: 1060.0247\n",
      "Train Epoch: 027 Batch: 00083/00094 | Loss: 772.9896 | CE: 0.2834 | KD: 1059.9536\n",
      "Train Epoch: 027 Batch: 00084/00094 | Loss: 772.9188 | CE: 0.1964 | KD: 1059.9760\n",
      "Train Epoch: 027 Batch: 00085/00094 | Loss: 773.1899 | CE: 0.3613 | KD: 1060.1216\n",
      "Train Epoch: 027 Batch: 00086/00094 | Loss: 772.8806 | CE: 0.2129 | KD: 1059.9009\n",
      "Train Epoch: 027 Batch: 00087/00094 | Loss: 772.9478 | CE: 0.2340 | KD: 1059.9640\n",
      "Train Epoch: 027 Batch: 00088/00094 | Loss: 773.0179 | CE: 0.2631 | KD: 1060.0204\n",
      "Train Epoch: 027 Batch: 00089/00094 | Loss: 772.9958 | CE: 0.2589 | KD: 1059.9958\n",
      "Train Epoch: 027 Batch: 00090/00094 | Loss: 772.9201 | CE: 0.1984 | KD: 1059.9750\n",
      "Train Epoch: 027 Batch: 00091/00094 | Loss: 772.9623 | CE: 0.2031 | KD: 1060.0264\n",
      "Train Epoch: 027 Batch: 00092/00094 | Loss: 773.0954 | CE: 0.3087 | KD: 1060.0640\n",
      "Train Epoch: 027 Batch: 00093/00094 | Loss: 773.0200 | CE: 0.2431 | KD: 1060.0505\n",
      "Train Epoch: 027 Batch: 00094/00094 | Loss: 772.9507 | CE: 0.2595 | KD: 1059.9331\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2712 | acc:90.9500\n",
      "[VAL Acc] Target: 90.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1418 | acc:50.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0050 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0159 | acc:50.9542\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7668 | acc:57.6803\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9240 | acc:54.6211\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5379 | acc:73.5893\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.59%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9451 | acc:56.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4570 | acc:76.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.65%\n",
      "[VAL Acc] Avg 62.17%\n",
      "Train Epoch: 028 Batch: 00001/00094 | Loss: 773.0443 | CE: 0.2744 | KD: 1060.0410\n",
      "Train Epoch: 028 Batch: 00002/00094 | Loss: 772.9635 | CE: 0.2056 | KD: 1060.0245\n",
      "Train Epoch: 028 Batch: 00003/00094 | Loss: 772.9258 | CE: 0.1941 | KD: 1059.9886\n",
      "Train Epoch: 028 Batch: 00004/00094 | Loss: 772.9618 | CE: 0.2121 | KD: 1060.0133\n",
      "Train Epoch: 028 Batch: 00005/00094 | Loss: 772.9777 | CE: 0.2345 | KD: 1060.0044\n",
      "Train Epoch: 028 Batch: 00006/00094 | Loss: 773.0158 | CE: 0.1998 | KD: 1060.1042\n",
      "Train Epoch: 028 Batch: 00007/00094 | Loss: 773.0782 | CE: 0.2703 | KD: 1060.0931\n",
      "Train Epoch: 028 Batch: 00008/00094 | Loss: 772.9700 | CE: 0.2076 | KD: 1060.0308\n",
      "Train Epoch: 028 Batch: 00009/00094 | Loss: 773.0190 | CE: 0.2709 | KD: 1060.0112\n",
      "Train Epoch: 028 Batch: 00010/00094 | Loss: 773.0458 | CE: 0.2880 | KD: 1060.0244\n",
      "Train Epoch: 028 Batch: 00011/00094 | Loss: 772.9821 | CE: 0.2591 | KD: 1059.9767\n",
      "Train Epoch: 028 Batch: 00012/00094 | Loss: 773.0886 | CE: 0.3140 | KD: 1060.0476\n",
      "Train Epoch: 028 Batch: 00013/00094 | Loss: 772.9522 | CE: 0.2289 | KD: 1059.9771\n",
      "Train Epoch: 028 Batch: 00014/00094 | Loss: 772.9886 | CE: 0.2136 | KD: 1060.0480\n",
      "Train Epoch: 028 Batch: 00015/00094 | Loss: 772.9999 | CE: 0.2613 | KD: 1059.9982\n",
      "Train Epoch: 028 Batch: 00016/00094 | Loss: 773.0020 | CE: 0.2185 | KD: 1060.0597\n",
      "Train Epoch: 028 Batch: 00017/00094 | Loss: 772.9807 | CE: 0.2771 | KD: 1059.9502\n",
      "Train Epoch: 028 Batch: 00018/00094 | Loss: 772.9387 | CE: 0.2395 | KD: 1059.9441\n",
      "Train Epoch: 028 Batch: 00019/00094 | Loss: 773.0498 | CE: 0.2596 | KD: 1060.0690\n",
      "Train Epoch: 028 Batch: 00020/00094 | Loss: 773.0762 | CE: 0.2707 | KD: 1060.0898\n",
      "Train Epoch: 028 Batch: 00021/00094 | Loss: 773.0000 | CE: 0.2431 | KD: 1060.0232\n",
      "Train Epoch: 028 Batch: 00022/00094 | Loss: 773.1306 | CE: 0.3090 | KD: 1060.1121\n",
      "Train Epoch: 028 Batch: 00023/00094 | Loss: 772.9650 | CE: 0.2265 | KD: 1059.9979\n",
      "Train Epoch: 028 Batch: 00024/00094 | Loss: 773.0489 | CE: 0.2665 | KD: 1060.0581\n",
      "Train Epoch: 028 Batch: 00025/00094 | Loss: 773.0436 | CE: 0.2564 | KD: 1060.0649\n",
      "Train Epoch: 028 Batch: 00026/00094 | Loss: 773.0714 | CE: 0.3063 | KD: 1060.0344\n",
      "Train Epoch: 028 Batch: 00027/00094 | Loss: 773.0554 | CE: 0.2880 | KD: 1060.0376\n",
      "Train Epoch: 028 Batch: 00028/00094 | Loss: 773.0997 | CE: 0.2896 | KD: 1060.0963\n",
      "Train Epoch: 028 Batch: 00029/00094 | Loss: 772.9558 | CE: 0.2531 | KD: 1059.9489\n",
      "Train Epoch: 028 Batch: 00030/00094 | Loss: 772.9587 | CE: 0.2638 | KD: 1059.9381\n",
      "Train Epoch: 028 Batch: 00031/00094 | Loss: 773.0003 | CE: 0.2537 | KD: 1060.0092\n",
      "Train Epoch: 028 Batch: 00032/00094 | Loss: 773.0098 | CE: 0.2696 | KD: 1060.0002\n",
      "Train Epoch: 028 Batch: 00033/00094 | Loss: 772.9411 | CE: 0.2357 | KD: 1059.9525\n",
      "Train Epoch: 028 Batch: 00034/00094 | Loss: 772.9868 | CE: 0.2499 | KD: 1059.9957\n",
      "Train Epoch: 028 Batch: 00035/00094 | Loss: 772.9916 | CE: 0.2265 | KD: 1060.0344\n",
      "Train Epoch: 028 Batch: 00036/00094 | Loss: 772.9777 | CE: 0.2406 | KD: 1059.9960\n",
      "Train Epoch: 028 Batch: 00037/00094 | Loss: 773.0196 | CE: 0.2660 | KD: 1060.0187\n",
      "Train Epoch: 028 Batch: 00038/00094 | Loss: 773.0502 | CE: 0.2588 | KD: 1060.0704\n",
      "Train Epoch: 028 Batch: 00039/00094 | Loss: 773.1312 | CE: 0.2587 | KD: 1060.1819\n",
      "Train Epoch: 028 Batch: 00040/00094 | Loss: 773.0502 | CE: 0.3073 | KD: 1060.0042\n",
      "Train Epoch: 028 Batch: 00041/00094 | Loss: 772.9567 | CE: 0.2388 | KD: 1059.9697\n",
      "Train Epoch: 028 Batch: 00042/00094 | Loss: 773.1141 | CE: 0.3036 | KD: 1060.0968\n",
      "Train Epoch: 028 Batch: 00043/00094 | Loss: 772.8809 | CE: 0.1932 | KD: 1059.9282\n",
      "Train Epoch: 028 Batch: 00044/00094 | Loss: 772.9606 | CE: 0.2158 | KD: 1060.0066\n",
      "Train Epoch: 028 Batch: 00045/00094 | Loss: 772.9587 | CE: 0.2366 | KD: 1059.9755\n",
      "Train Epoch: 028 Batch: 00046/00094 | Loss: 772.9843 | CE: 0.2199 | KD: 1060.0336\n",
      "Train Epoch: 028 Batch: 00047/00094 | Loss: 773.0251 | CE: 0.2436 | KD: 1060.0570\n",
      "Train Epoch: 028 Batch: 00048/00094 | Loss: 772.9475 | CE: 0.2278 | KD: 1059.9722\n",
      "Train Epoch: 028 Batch: 00049/00094 | Loss: 772.9736 | CE: 0.2552 | KD: 1059.9705\n",
      "Train Epoch: 028 Batch: 00050/00094 | Loss: 772.9890 | CE: 0.2338 | KD: 1060.0208\n",
      "Train Epoch: 028 Batch: 00051/00094 | Loss: 773.0527 | CE: 0.2840 | KD: 1060.0394\n",
      "Train Epoch: 028 Batch: 00052/00094 | Loss: 773.0016 | CE: 0.2856 | KD: 1059.9672\n",
      "Train Epoch: 028 Batch: 00053/00094 | Loss: 773.1428 | CE: 0.3192 | KD: 1060.1146\n",
      "Train Epoch: 028 Batch: 00054/00094 | Loss: 773.0825 | CE: 0.3347 | KD: 1060.0106\n",
      "Train Epoch: 028 Batch: 00055/00094 | Loss: 773.0499 | CE: 0.2815 | KD: 1060.0391\n",
      "Train Epoch: 028 Batch: 00056/00094 | Loss: 772.9904 | CE: 0.2528 | KD: 1059.9967\n",
      "Train Epoch: 028 Batch: 00057/00094 | Loss: 772.9248 | CE: 0.2221 | KD: 1059.9489\n",
      "Train Epoch: 028 Batch: 00058/00094 | Loss: 773.0427 | CE: 0.2395 | KD: 1060.0868\n",
      "Train Epoch: 028 Batch: 00059/00094 | Loss: 772.9772 | CE: 0.2388 | KD: 1059.9979\n",
      "Train Epoch: 028 Batch: 00060/00094 | Loss: 773.1127 | CE: 0.3063 | KD: 1060.0912\n",
      "Train Epoch: 028 Batch: 00061/00094 | Loss: 773.0733 | CE: 0.2581 | KD: 1060.1031\n",
      "Train Epoch: 028 Batch: 00062/00094 | Loss: 773.0275 | CE: 0.2638 | KD: 1060.0325\n",
      "Train Epoch: 028 Batch: 00063/00094 | Loss: 773.0177 | CE: 0.2583 | KD: 1060.0266\n",
      "Train Epoch: 028 Batch: 00064/00094 | Loss: 773.0987 | CE: 0.3090 | KD: 1060.0682\n",
      "Train Epoch: 028 Batch: 00065/00094 | Loss: 773.0918 | CE: 0.3163 | KD: 1060.0487\n",
      "Train Epoch: 028 Batch: 00066/00094 | Loss: 772.9943 | CE: 0.2560 | KD: 1059.9977\n",
      "Train Epoch: 028 Batch: 00067/00094 | Loss: 772.9311 | CE: 0.2276 | KD: 1059.9501\n",
      "Train Epoch: 028 Batch: 00068/00094 | Loss: 773.0031 | CE: 0.2509 | KD: 1060.0168\n",
      "Train Epoch: 028 Batch: 00069/00094 | Loss: 773.0362 | CE: 0.2979 | KD: 1059.9978\n",
      "Train Epoch: 028 Batch: 00070/00094 | Loss: 773.1816 | CE: 0.2983 | KD: 1060.1965\n",
      "Train Epoch: 028 Batch: 00071/00094 | Loss: 773.0102 | CE: 0.2445 | KD: 1060.0353\n",
      "Train Epoch: 028 Batch: 00072/00094 | Loss: 772.9333 | CE: 0.2171 | KD: 1059.9675\n",
      "Train Epoch: 028 Batch: 00073/00094 | Loss: 772.9931 | CE: 0.2506 | KD: 1060.0034\n",
      "Train Epoch: 028 Batch: 00074/00094 | Loss: 773.0541 | CE: 0.2724 | KD: 1060.0574\n",
      "Train Epoch: 028 Batch: 00075/00094 | Loss: 773.0092 | CE: 0.2291 | KD: 1060.0551\n",
      "Train Epoch: 028 Batch: 00076/00094 | Loss: 773.0478 | CE: 0.2714 | KD: 1060.0500\n",
      "Train Epoch: 028 Batch: 00077/00094 | Loss: 773.1607 | CE: 0.3540 | KD: 1060.0916\n",
      "Train Epoch: 028 Batch: 00078/00094 | Loss: 773.0505 | CE: 0.2543 | KD: 1060.0771\n",
      "Train Epoch: 028 Batch: 00079/00094 | Loss: 772.9653 | CE: 0.2172 | KD: 1060.0111\n",
      "Train Epoch: 028 Batch: 00080/00094 | Loss: 773.0353 | CE: 0.2800 | KD: 1060.0211\n",
      "Train Epoch: 028 Batch: 00081/00094 | Loss: 773.1635 | CE: 0.3569 | KD: 1060.0914\n",
      "Train Epoch: 028 Batch: 00082/00094 | Loss: 772.9773 | CE: 0.2181 | KD: 1060.0264\n",
      "Train Epoch: 028 Batch: 00083/00094 | Loss: 772.9532 | CE: 0.1920 | KD: 1060.0291\n",
      "Train Epoch: 028 Batch: 00084/00094 | Loss: 773.1073 | CE: 0.3280 | KD: 1060.0540\n",
      "Train Epoch: 028 Batch: 00085/00094 | Loss: 773.0241 | CE: 0.2572 | KD: 1060.0370\n",
      "Train Epoch: 028 Batch: 00086/00094 | Loss: 773.0479 | CE: 0.2787 | KD: 1060.0400\n",
      "Train Epoch: 028 Batch: 00087/00094 | Loss: 773.0927 | CE: 0.2683 | KD: 1060.1158\n",
      "Train Epoch: 028 Batch: 00088/00094 | Loss: 772.9201 | CE: 0.1900 | KD: 1059.9866\n",
      "Train Epoch: 028 Batch: 00089/00094 | Loss: 773.0177 | CE: 0.2322 | KD: 1060.0625\n",
      "Train Epoch: 028 Batch: 00090/00094 | Loss: 773.0226 | CE: 0.2662 | KD: 1060.0225\n",
      "Train Epoch: 028 Batch: 00091/00094 | Loss: 772.9641 | CE: 0.2751 | KD: 1059.9301\n",
      "Train Epoch: 028 Batch: 00092/00094 | Loss: 773.0191 | CE: 0.2538 | KD: 1060.0347\n",
      "Train Epoch: 028 Batch: 00093/00094 | Loss: 772.9855 | CE: 0.2389 | KD: 1060.0090\n",
      "Train Epoch: 028 Batch: 00094/00094 | Loss: 772.9833 | CE: 0.2424 | KD: 1060.0012\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2820 | acc:90.3000\n",
      "[VAL Acc] Target: 90.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1001 | acc:50.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9817 | acc:48.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0185 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7264 | acc:59.6003\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8894 | acc:54.3438\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5141 | acc:75.1176\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9453 | acc:55.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4731 | acc:74.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.70%\n",
      "[VAL Acc] Avg 61.90%\n",
      "Train Epoch: 029 Batch: 00001/00094 | Loss: 773.0242 | CE: 0.2864 | KD: 1059.9971\n",
      "Train Epoch: 029 Batch: 00002/00094 | Loss: 772.9406 | CE: 0.2079 | KD: 1059.9900\n",
      "Train Epoch: 029 Batch: 00003/00094 | Loss: 772.9490 | CE: 0.2561 | KD: 1059.9354\n",
      "Train Epoch: 029 Batch: 00004/00094 | Loss: 773.0446 | CE: 0.2966 | KD: 1060.0110\n",
      "Train Epoch: 029 Batch: 00005/00094 | Loss: 773.0766 | CE: 0.3290 | KD: 1060.0105\n",
      "Train Epoch: 029 Batch: 00006/00094 | Loss: 773.0532 | CE: 0.3023 | KD: 1060.0150\n",
      "Train Epoch: 029 Batch: 00007/00094 | Loss: 772.9297 | CE: 0.2172 | KD: 1059.9624\n",
      "Train Epoch: 029 Batch: 00008/00094 | Loss: 772.9763 | CE: 0.2118 | KD: 1060.0336\n",
      "Train Epoch: 029 Batch: 00009/00094 | Loss: 773.0129 | CE: 0.2742 | KD: 1059.9983\n",
      "Train Epoch: 029 Batch: 00010/00094 | Loss: 773.1296 | CE: 0.3128 | KD: 1060.1053\n",
      "Train Epoch: 029 Batch: 00011/00094 | Loss: 773.0616 | CE: 0.3216 | KD: 1060.0000\n",
      "Train Epoch: 029 Batch: 00012/00094 | Loss: 772.9764 | CE: 0.2527 | KD: 1059.9777\n",
      "Train Epoch: 029 Batch: 00013/00094 | Loss: 773.1929 | CE: 0.3936 | KD: 1060.0813\n",
      "Train Epoch: 029 Batch: 00014/00094 | Loss: 772.9497 | CE: 0.2479 | KD: 1059.9476\n",
      "Train Epoch: 029 Batch: 00015/00094 | Loss: 773.1556 | CE: 0.3474 | KD: 1060.0936\n",
      "Train Epoch: 029 Batch: 00016/00094 | Loss: 773.0468 | CE: 0.2514 | KD: 1060.0760\n",
      "Train Epoch: 029 Batch: 00017/00094 | Loss: 772.9955 | CE: 0.2388 | KD: 1060.0229\n",
      "Train Epoch: 029 Batch: 00018/00094 | Loss: 772.9624 | CE: 0.2296 | KD: 1059.9902\n",
      "Train Epoch: 029 Batch: 00019/00094 | Loss: 773.0359 | CE: 0.2861 | KD: 1060.0134\n",
      "Train Epoch: 029 Batch: 00020/00094 | Loss: 772.9248 | CE: 0.1925 | KD: 1059.9894\n",
      "Train Epoch: 029 Batch: 00021/00094 | Loss: 773.1406 | CE: 0.2964 | KD: 1060.1431\n",
      "Train Epoch: 029 Batch: 00022/00094 | Loss: 773.0037 | CE: 0.2454 | KD: 1060.0250\n",
      "Train Epoch: 029 Batch: 00023/00094 | Loss: 773.0941 | CE: 0.3052 | KD: 1060.0671\n",
      "Train Epoch: 029 Batch: 00024/00094 | Loss: 772.8875 | CE: 0.1960 | KD: 1059.9335\n",
      "Train Epoch: 029 Batch: 00025/00094 | Loss: 773.0243 | CE: 0.2646 | KD: 1060.0271\n",
      "Train Epoch: 029 Batch: 00026/00094 | Loss: 772.9026 | CE: 0.1731 | KD: 1059.9857\n",
      "Train Epoch: 029 Batch: 00027/00094 | Loss: 773.0647 | CE: 0.2836 | KD: 1060.0564\n",
      "Train Epoch: 029 Batch: 00028/00094 | Loss: 773.0249 | CE: 0.2378 | KD: 1060.0647\n",
      "Train Epoch: 029 Batch: 00029/00094 | Loss: 773.0108 | CE: 0.2743 | KD: 1059.9952\n",
      "Train Epoch: 029 Batch: 00030/00094 | Loss: 773.1397 | CE: 0.2924 | KD: 1060.1472\n",
      "Train Epoch: 029 Batch: 00031/00094 | Loss: 772.9430 | CE: 0.2286 | KD: 1059.9650\n",
      "Train Epoch: 029 Batch: 00032/00094 | Loss: 773.0469 | CE: 0.2431 | KD: 1060.0874\n",
      "Train Epoch: 029 Batch: 00033/00094 | Loss: 772.9889 | CE: 0.2432 | KD: 1060.0078\n",
      "Train Epoch: 029 Batch: 00034/00094 | Loss: 773.0095 | CE: 0.2505 | KD: 1060.0261\n",
      "Train Epoch: 029 Batch: 00035/00094 | Loss: 773.0660 | CE: 0.2547 | KD: 1060.0980\n",
      "Train Epoch: 029 Batch: 00036/00094 | Loss: 772.9457 | CE: 0.2056 | KD: 1060.0002\n",
      "Train Epoch: 029 Batch: 00037/00094 | Loss: 773.0955 | CE: 0.2729 | KD: 1060.1133\n",
      "Train Epoch: 029 Batch: 00038/00094 | Loss: 773.0817 | CE: 0.2958 | KD: 1060.0630\n",
      "Train Epoch: 029 Batch: 00039/00094 | Loss: 773.0104 | CE: 0.2410 | KD: 1060.0404\n",
      "Train Epoch: 029 Batch: 00040/00094 | Loss: 773.0051 | CE: 0.2233 | KD: 1060.0575\n",
      "Train Epoch: 029 Batch: 00041/00094 | Loss: 773.1254 | CE: 0.3298 | KD: 1060.0763\n",
      "Train Epoch: 029 Batch: 00042/00094 | Loss: 773.1505 | CE: 0.3120 | KD: 1060.1351\n",
      "Train Epoch: 029 Batch: 00043/00094 | Loss: 772.9517 | CE: 0.2014 | KD: 1060.0142\n",
      "Train Epoch: 029 Batch: 00044/00094 | Loss: 772.9910 | CE: 0.2179 | KD: 1060.0453\n",
      "Train Epoch: 029 Batch: 00045/00094 | Loss: 773.0491 | CE: 0.2988 | KD: 1060.0142\n",
      "Train Epoch: 029 Batch: 00046/00094 | Loss: 773.0395 | CE: 0.2736 | KD: 1060.0356\n",
      "Train Epoch: 029 Batch: 00047/00094 | Loss: 773.1409 | CE: 0.3046 | KD: 1060.1322\n",
      "Train Epoch: 029 Batch: 00048/00094 | Loss: 773.0305 | CE: 0.2415 | KD: 1060.0674\n",
      "Train Epoch: 029 Batch: 00049/00094 | Loss: 772.9597 | CE: 0.2094 | KD: 1060.0142\n",
      "Train Epoch: 029 Batch: 00050/00094 | Loss: 773.1074 | CE: 0.3150 | KD: 1060.0720\n",
      "Train Epoch: 029 Batch: 00051/00094 | Loss: 773.0608 | CE: 0.2647 | KD: 1060.0770\n",
      "Train Epoch: 029 Batch: 00052/00094 | Loss: 773.0933 | CE: 0.3028 | KD: 1060.0695\n",
      "Train Epoch: 029 Batch: 00053/00094 | Loss: 772.9977 | CE: 0.2133 | KD: 1060.0610\n",
      "Train Epoch: 029 Batch: 00054/00094 | Loss: 772.9140 | CE: 0.2106 | KD: 1059.9498\n",
      "Train Epoch: 029 Batch: 00055/00094 | Loss: 773.0133 | CE: 0.2719 | KD: 1060.0020\n",
      "Train Epoch: 029 Batch: 00056/00094 | Loss: 773.0068 | CE: 0.2540 | KD: 1060.0176\n",
      "Train Epoch: 029 Batch: 00057/00094 | Loss: 773.0140 | CE: 0.2081 | KD: 1060.0905\n",
      "Train Epoch: 029 Batch: 00058/00094 | Loss: 773.0021 | CE: 0.2291 | KD: 1060.0453\n",
      "Train Epoch: 029 Batch: 00059/00094 | Loss: 772.9937 | CE: 0.2306 | KD: 1060.0317\n",
      "Train Epoch: 029 Batch: 00060/00094 | Loss: 772.9705 | CE: 0.2065 | KD: 1060.0330\n",
      "Train Epoch: 029 Batch: 00061/00094 | Loss: 773.0613 | CE: 0.2719 | KD: 1060.0679\n",
      "Train Epoch: 029 Batch: 00062/00094 | Loss: 772.9066 | CE: 0.2236 | KD: 1059.9218\n",
      "Train Epoch: 029 Batch: 00063/00094 | Loss: 773.1339 | CE: 0.3044 | KD: 1060.1227\n",
      "Train Epoch: 029 Batch: 00064/00094 | Loss: 772.8806 | CE: 0.1806 | KD: 1059.9451\n",
      "Train Epoch: 029 Batch: 00065/00094 | Loss: 772.9326 | CE: 0.2479 | KD: 1059.9241\n",
      "Train Epoch: 029 Batch: 00066/00094 | Loss: 773.0307 | CE: 0.2545 | KD: 1060.0497\n",
      "Train Epoch: 029 Batch: 00067/00094 | Loss: 773.0311 | CE: 0.2652 | KD: 1060.0356\n",
      "Train Epoch: 029 Batch: 00068/00094 | Loss: 773.0271 | CE: 0.2867 | KD: 1060.0005\n",
      "Train Epoch: 029 Batch: 00069/00094 | Loss: 773.1334 | CE: 0.3006 | KD: 1060.1273\n",
      "Train Epoch: 029 Batch: 00070/00094 | Loss: 773.0621 | CE: 0.2949 | KD: 1060.0374\n",
      "Train Epoch: 029 Batch: 00071/00094 | Loss: 773.0359 | CE: 0.2704 | KD: 1060.0350\n",
      "Train Epoch: 029 Batch: 00072/00094 | Loss: 773.1069 | CE: 0.3164 | KD: 1060.0693\n",
      "Train Epoch: 029 Batch: 00073/00094 | Loss: 773.0368 | CE: 0.2450 | KD: 1060.0710\n",
      "Train Epoch: 029 Batch: 00074/00094 | Loss: 773.0003 | CE: 0.2492 | KD: 1060.0153\n",
      "Train Epoch: 029 Batch: 00075/00094 | Loss: 773.0557 | CE: 0.2729 | KD: 1060.0588\n",
      "Train Epoch: 029 Batch: 00076/00094 | Loss: 772.9891 | CE: 0.2365 | KD: 1060.0173\n",
      "Train Epoch: 029 Batch: 00077/00094 | Loss: 773.0583 | CE: 0.3005 | KD: 1060.0245\n",
      "Train Epoch: 029 Batch: 00078/00094 | Loss: 773.0103 | CE: 0.2464 | KD: 1060.0328\n",
      "Train Epoch: 029 Batch: 00079/00094 | Loss: 773.0445 | CE: 0.2713 | KD: 1060.0455\n",
      "Train Epoch: 029 Batch: 00080/00094 | Loss: 773.0311 | CE: 0.2621 | KD: 1060.0398\n",
      "Train Epoch: 029 Batch: 00081/00094 | Loss: 773.0496 | CE: 0.2800 | KD: 1060.0406\n",
      "Train Epoch: 029 Batch: 00082/00094 | Loss: 773.0151 | CE: 0.2203 | KD: 1060.0753\n",
      "Train Epoch: 029 Batch: 00083/00094 | Loss: 772.9895 | CE: 0.2218 | KD: 1060.0380\n",
      "Train Epoch: 029 Batch: 00084/00094 | Loss: 772.9725 | CE: 0.2054 | KD: 1060.0372\n",
      "Train Epoch: 029 Batch: 00085/00094 | Loss: 773.0016 | CE: 0.2410 | KD: 1060.0283\n",
      "Train Epoch: 029 Batch: 00086/00094 | Loss: 772.9746 | CE: 0.2402 | KD: 1059.9923\n",
      "Train Epoch: 029 Batch: 00087/00094 | Loss: 773.0045 | CE: 0.2513 | KD: 1060.0182\n",
      "Train Epoch: 029 Batch: 00088/00094 | Loss: 772.9794 | CE: 0.2272 | KD: 1060.0167\n",
      "Train Epoch: 029 Batch: 00089/00094 | Loss: 773.0425 | CE: 0.3019 | KD: 1060.0009\n",
      "Train Epoch: 029 Batch: 00090/00094 | Loss: 772.9261 | CE: 0.2237 | KD: 1059.9485\n",
      "Train Epoch: 029 Batch: 00091/00094 | Loss: 773.0542 | CE: 0.2868 | KD: 1060.0377\n",
      "Train Epoch: 029 Batch: 00092/00094 | Loss: 772.9706 | CE: 0.2140 | KD: 1060.0228\n",
      "Train Epoch: 029 Batch: 00093/00094 | Loss: 773.1160 | CE: 0.2717 | KD: 1060.1431\n",
      "Train Epoch: 029 Batch: 00094/00094 | Loss: 772.9614 | CE: 0.2374 | KD: 1059.9780\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2719 | acc:90.6000\n",
      "[VAL Acc] Target: 90.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1452 | acc:49.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9722 | acc:48.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0389 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7367 | acc:59.2476\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8838 | acc:54.5287\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5091 | acc:76.0580\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9680 | acc:55.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4629 | acc:75.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.75%\n",
      "[VAL Acc] Avg 62.14%\n",
      "Train Epoch: 030 Batch: 00001/00094 | Loss: 772.9750 | CE: 0.2376 | KD: 1059.9965\n",
      "Train Epoch: 030 Batch: 00002/00094 | Loss: 773.0379 | CE: 0.2703 | KD: 1060.0380\n",
      "Train Epoch: 030 Batch: 00003/00094 | Loss: 772.9551 | CE: 0.2179 | KD: 1059.9962\n",
      "Train Epoch: 030 Batch: 00004/00094 | Loss: 772.9969 | CE: 0.2531 | KD: 1060.0052\n",
      "Train Epoch: 030 Batch: 00005/00094 | Loss: 773.0465 | CE: 0.2857 | KD: 1060.0286\n",
      "Train Epoch: 030 Batch: 00006/00094 | Loss: 773.0504 | CE: 0.2891 | KD: 1060.0292\n",
      "Train Epoch: 030 Batch: 00007/00094 | Loss: 773.1041 | CE: 0.3248 | KD: 1060.0538\n",
      "Train Epoch: 030 Batch: 00008/00094 | Loss: 773.0844 | CE: 0.3070 | KD: 1060.0513\n",
      "Train Epoch: 030 Batch: 00009/00094 | Loss: 773.1340 | CE: 0.3096 | KD: 1060.1157\n",
      "Train Epoch: 030 Batch: 00010/00094 | Loss: 772.8845 | CE: 0.1833 | KD: 1059.9468\n",
      "Train Epoch: 030 Batch: 00011/00094 | Loss: 772.9408 | CE: 0.2191 | KD: 1059.9750\n",
      "Train Epoch: 030 Batch: 00012/00094 | Loss: 772.9767 | CE: 0.2435 | KD: 1059.9907\n",
      "Train Epoch: 030 Batch: 00013/00094 | Loss: 772.9293 | CE: 0.2354 | KD: 1059.9368\n",
      "Train Epoch: 030 Batch: 00014/00094 | Loss: 773.1785 | CE: 0.3685 | KD: 1060.0959\n",
      "Train Epoch: 030 Batch: 00015/00094 | Loss: 772.9465 | CE: 0.2230 | KD: 1059.9774\n",
      "Train Epoch: 030 Batch: 00016/00094 | Loss: 773.1239 | CE: 0.3116 | KD: 1060.0991\n",
      "Train Epoch: 030 Batch: 00017/00094 | Loss: 772.9426 | CE: 0.2107 | KD: 1059.9889\n",
      "Train Epoch: 030 Batch: 00018/00094 | Loss: 773.0624 | CE: 0.3050 | KD: 1060.0238\n",
      "Train Epoch: 030 Batch: 00019/00094 | Loss: 772.9553 | CE: 0.2271 | KD: 1059.9839\n",
      "Train Epoch: 030 Batch: 00020/00094 | Loss: 773.0567 | CE: 0.2817 | KD: 1060.0481\n",
      "Train Epoch: 030 Batch: 00021/00094 | Loss: 773.1219 | CE: 0.3215 | KD: 1060.0829\n",
      "Train Epoch: 030 Batch: 00022/00094 | Loss: 773.0136 | CE: 0.2657 | KD: 1060.0109\n",
      "Train Epoch: 030 Batch: 00023/00094 | Loss: 772.9343 | CE: 0.2193 | KD: 1059.9658\n",
      "Train Epoch: 030 Batch: 00024/00094 | Loss: 773.1040 | CE: 0.3347 | KD: 1060.0403\n",
      "Train Epoch: 030 Batch: 00025/00094 | Loss: 773.0399 | CE: 0.2060 | KD: 1060.1289\n",
      "Train Epoch: 030 Batch: 00026/00094 | Loss: 773.0112 | CE: 0.2372 | KD: 1060.0466\n",
      "Train Epoch: 030 Batch: 00027/00094 | Loss: 772.9842 | CE: 0.2468 | KD: 1059.9965\n",
      "Train Epoch: 030 Batch: 00028/00094 | Loss: 773.0790 | CE: 0.2737 | KD: 1060.0897\n",
      "Train Epoch: 030 Batch: 00029/00094 | Loss: 772.9769 | CE: 0.2428 | KD: 1059.9919\n",
      "Train Epoch: 030 Batch: 00030/00094 | Loss: 773.0089 | CE: 0.2469 | KD: 1060.0303\n",
      "Train Epoch: 030 Batch: 00031/00094 | Loss: 773.0103 | CE: 0.2364 | KD: 1060.0464\n",
      "Train Epoch: 030 Batch: 00032/00094 | Loss: 772.9636 | CE: 0.1934 | KD: 1060.0414\n",
      "Train Epoch: 030 Batch: 00033/00094 | Loss: 772.9985 | CE: 0.2260 | KD: 1060.0447\n",
      "Train Epoch: 030 Batch: 00034/00094 | Loss: 772.9000 | CE: 0.1758 | KD: 1059.9783\n",
      "Train Epoch: 030 Batch: 00035/00094 | Loss: 773.0050 | CE: 0.2594 | KD: 1060.0077\n",
      "Train Epoch: 030 Batch: 00036/00094 | Loss: 772.9258 | CE: 0.1900 | KD: 1059.9943\n",
      "Train Epoch: 030 Batch: 00037/00094 | Loss: 773.0115 | CE: 0.2148 | KD: 1060.0780\n",
      "Train Epoch: 030 Batch: 00038/00094 | Loss: 773.0479 | CE: 0.3045 | KD: 1060.0048\n",
      "Train Epoch: 030 Batch: 00039/00094 | Loss: 773.1920 | CE: 0.3873 | KD: 1060.0889\n",
      "Train Epoch: 030 Batch: 00040/00094 | Loss: 772.8574 | CE: 0.1795 | KD: 1059.9149\n",
      "Train Epoch: 030 Batch: 00041/00094 | Loss: 773.0056 | CE: 0.2763 | KD: 1059.9855\n",
      "Train Epoch: 030 Batch: 00042/00094 | Loss: 773.0318 | CE: 0.2665 | KD: 1060.0348\n",
      "Train Epoch: 030 Batch: 00043/00094 | Loss: 773.0254 | CE: 0.2539 | KD: 1060.0432\n",
      "Train Epoch: 030 Batch: 00044/00094 | Loss: 772.9954 | CE: 0.2209 | KD: 1060.0472\n",
      "Train Epoch: 030 Batch: 00045/00094 | Loss: 773.0854 | CE: 0.2458 | KD: 1060.1366\n",
      "Train Epoch: 030 Batch: 00046/00094 | Loss: 772.9755 | CE: 0.2108 | KD: 1060.0338\n",
      "Train Epoch: 030 Batch: 00047/00094 | Loss: 772.9904 | CE: 0.2327 | KD: 1060.0243\n",
      "Train Epoch: 030 Batch: 00048/00094 | Loss: 773.0817 | CE: 0.3230 | KD: 1060.0258\n",
      "Train Epoch: 030 Batch: 00049/00094 | Loss: 773.1093 | CE: 0.3376 | KD: 1060.0436\n",
      "Train Epoch: 030 Batch: 00050/00094 | Loss: 773.0472 | CE: 0.2645 | KD: 1060.0586\n",
      "Train Epoch: 030 Batch: 00051/00094 | Loss: 772.9499 | CE: 0.2293 | KD: 1059.9735\n",
      "Train Epoch: 030 Batch: 00052/00094 | Loss: 773.1401 | CE: 0.3274 | KD: 1060.0997\n",
      "Train Epoch: 030 Batch: 00053/00094 | Loss: 773.0037 | CE: 0.2657 | KD: 1059.9973\n",
      "Train Epoch: 030 Batch: 00054/00094 | Loss: 773.0309 | CE: 0.2582 | KD: 1060.0449\n",
      "Train Epoch: 030 Batch: 00055/00094 | Loss: 772.9659 | CE: 0.2469 | KD: 1059.9713\n",
      "Train Epoch: 030 Batch: 00056/00094 | Loss: 772.9719 | CE: 0.2424 | KD: 1059.9857\n",
      "Train Epoch: 030 Batch: 00057/00094 | Loss: 772.9950 | CE: 0.2649 | KD: 1059.9865\n",
      "Train Epoch: 030 Batch: 00058/00094 | Loss: 772.9606 | CE: 0.2182 | KD: 1060.0033\n",
      "Train Epoch: 030 Batch: 00059/00094 | Loss: 773.0521 | CE: 0.2361 | KD: 1060.1044\n",
      "Train Epoch: 030 Batch: 00060/00094 | Loss: 773.1072 | CE: 0.3107 | KD: 1060.0776\n",
      "Train Epoch: 030 Batch: 00061/00094 | Loss: 773.1047 | CE: 0.3223 | KD: 1060.0581\n",
      "Train Epoch: 030 Batch: 00062/00094 | Loss: 772.9836 | CE: 0.2432 | KD: 1060.0006\n",
      "Train Epoch: 030 Batch: 00063/00094 | Loss: 773.0303 | CE: 0.2457 | KD: 1060.0612\n",
      "Train Epoch: 030 Batch: 00064/00094 | Loss: 773.0685 | CE: 0.2582 | KD: 1060.0964\n",
      "Train Epoch: 030 Batch: 00065/00094 | Loss: 773.0768 | CE: 0.2805 | KD: 1060.0774\n",
      "Train Epoch: 030 Batch: 00066/00094 | Loss: 772.9110 | CE: 0.2044 | KD: 1059.9542\n",
      "Train Epoch: 030 Batch: 00067/00094 | Loss: 773.0196 | CE: 0.2669 | KD: 1060.0175\n",
      "Train Epoch: 030 Batch: 00068/00094 | Loss: 773.0436 | CE: 0.2540 | KD: 1060.0681\n",
      "Train Epoch: 030 Batch: 00069/00094 | Loss: 773.0159 | CE: 0.2271 | KD: 1060.0669\n",
      "Train Epoch: 030 Batch: 00070/00094 | Loss: 773.0852 | CE: 0.2787 | KD: 1060.0913\n",
      "Train Epoch: 030 Batch: 00071/00094 | Loss: 772.9767 | CE: 0.2233 | KD: 1060.0184\n",
      "Train Epoch: 030 Batch: 00072/00094 | Loss: 772.9878 | CE: 0.2544 | KD: 1059.9910\n",
      "Train Epoch: 030 Batch: 00073/00094 | Loss: 773.0974 | CE: 0.3430 | KD: 1060.0199\n",
      "Train Epoch: 030 Batch: 00074/00094 | Loss: 773.0046 | CE: 0.2540 | KD: 1060.0145\n",
      "Train Epoch: 030 Batch: 00075/00094 | Loss: 773.0842 | CE: 0.3052 | KD: 1060.0536\n",
      "Train Epoch: 030 Batch: 00076/00094 | Loss: 773.0034 | CE: 0.2377 | KD: 1060.0353\n",
      "Train Epoch: 030 Batch: 00077/00094 | Loss: 773.1119 | CE: 0.3327 | KD: 1060.0538\n",
      "Train Epoch: 030 Batch: 00078/00094 | Loss: 772.9297 | CE: 0.2256 | KD: 1059.9508\n",
      "Train Epoch: 030 Batch: 00079/00094 | Loss: 772.9724 | CE: 0.2172 | KD: 1060.0208\n",
      "Train Epoch: 030 Batch: 00080/00094 | Loss: 772.8607 | CE: 0.1821 | KD: 1059.9159\n",
      "Train Epoch: 030 Batch: 00081/00094 | Loss: 772.9417 | CE: 0.2202 | KD: 1059.9747\n",
      "Train Epoch: 030 Batch: 00082/00094 | Loss: 773.0236 | CE: 0.2293 | KD: 1060.0746\n",
      "Train Epoch: 030 Batch: 00083/00094 | Loss: 773.0389 | CE: 0.2525 | KD: 1060.0637\n",
      "Train Epoch: 030 Batch: 00084/00094 | Loss: 773.0223 | CE: 0.2679 | KD: 1060.0198\n",
      "Train Epoch: 030 Batch: 00085/00094 | Loss: 773.0486 | CE: 0.2552 | KD: 1060.0734\n",
      "Train Epoch: 030 Batch: 00086/00094 | Loss: 773.0187 | CE: 0.2566 | KD: 1060.0304\n",
      "Train Epoch: 030 Batch: 00087/00094 | Loss: 772.9011 | CE: 0.1799 | KD: 1059.9744\n",
      "Train Epoch: 030 Batch: 00088/00094 | Loss: 773.0026 | CE: 0.2256 | KD: 1060.0508\n",
      "Train Epoch: 030 Batch: 00089/00094 | Loss: 772.9945 | CE: 0.2426 | KD: 1060.0164\n",
      "Train Epoch: 030 Batch: 00090/00094 | Loss: 773.1197 | CE: 0.3412 | KD: 1060.0529\n",
      "Train Epoch: 030 Batch: 00091/00094 | Loss: 772.9143 | CE: 0.2199 | KD: 1059.9375\n",
      "Train Epoch: 030 Batch: 00092/00094 | Loss: 772.9366 | CE: 0.2084 | KD: 1059.9839\n",
      "Train Epoch: 030 Batch: 00093/00094 | Loss: 773.0759 | CE: 0.2825 | KD: 1060.0732\n",
      "Train Epoch: 030 Batch: 00094/00094 | Loss: 773.1052 | CE: 0.3071 | KD: 1060.0798\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2796 | acc:90.4500\n",
      "[VAL Acc] Target: 90.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1528 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0091 | acc:49.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0170 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7861 | acc:56.5439\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8329 | acc:56.3771\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 56.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5414 | acc:72.6097\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9316 | acc:56.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4899 | acc:72.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 72.80%\n",
      "[VAL Acc] Avg 61.61%\n",
      "Train Epoch: 031 Batch: 00001/00094 | Loss: 695.7158 | CE: 0.2602 | KD: 1059.9843\n",
      "Train Epoch: 031 Batch: 00002/00094 | Loss: 695.7256 | CE: 0.2248 | KD: 1060.0531\n",
      "Train Epoch: 031 Batch: 00003/00094 | Loss: 695.6830 | CE: 0.2347 | KD: 1059.9730\n",
      "Train Epoch: 031 Batch: 00004/00094 | Loss: 695.7424 | CE: 0.2774 | KD: 1059.9987\n",
      "Train Epoch: 031 Batch: 00005/00094 | Loss: 695.7739 | CE: 0.2432 | KD: 1060.0986\n",
      "Train Epoch: 031 Batch: 00006/00094 | Loss: 695.7523 | CE: 0.2558 | KD: 1060.0466\n",
      "Train Epoch: 031 Batch: 00007/00094 | Loss: 695.7205 | CE: 0.2825 | KD: 1059.9573\n",
      "Train Epoch: 031 Batch: 00008/00094 | Loss: 695.7603 | CE: 0.3031 | KD: 1059.9866\n",
      "Train Epoch: 031 Batch: 00009/00094 | Loss: 695.6982 | CE: 0.1970 | KD: 1060.0538\n",
      "Train Epoch: 031 Batch: 00010/00094 | Loss: 695.8475 | CE: 0.3083 | KD: 1060.1115\n",
      "Train Epoch: 031 Batch: 00011/00094 | Loss: 695.7356 | CE: 0.2401 | KD: 1060.0450\n",
      "Train Epoch: 031 Batch: 00012/00094 | Loss: 695.8126 | CE: 0.2979 | KD: 1060.0742\n",
      "Train Epoch: 031 Batch: 00013/00094 | Loss: 695.7060 | CE: 0.2238 | KD: 1060.0248\n",
      "Train Epoch: 031 Batch: 00014/00094 | Loss: 695.7487 | CE: 0.2593 | KD: 1060.0358\n",
      "Train Epoch: 031 Batch: 00015/00094 | Loss: 695.9773 | CE: 0.3998 | KD: 1060.1699\n",
      "Train Epoch: 031 Batch: 00016/00094 | Loss: 695.7446 | CE: 0.2336 | KD: 1060.0685\n",
      "Train Epoch: 031 Batch: 00017/00094 | Loss: 695.6583 | CE: 0.2186 | KD: 1059.9600\n",
      "Train Epoch: 031 Batch: 00018/00094 | Loss: 695.7237 | CE: 0.2674 | KD: 1059.9854\n",
      "Train Epoch: 031 Batch: 00019/00094 | Loss: 695.7117 | CE: 0.2408 | KD: 1060.0076\n",
      "Train Epoch: 031 Batch: 00020/00094 | Loss: 695.7606 | CE: 0.2534 | KD: 1060.0629\n",
      "Train Epoch: 031 Batch: 00021/00094 | Loss: 695.7020 | CE: 0.2280 | KD: 1060.0123\n",
      "Train Epoch: 031 Batch: 00022/00094 | Loss: 695.6722 | CE: 0.2123 | KD: 1059.9908\n",
      "Train Epoch: 031 Batch: 00023/00094 | Loss: 695.7730 | CE: 0.3140 | KD: 1059.9894\n",
      "Train Epoch: 031 Batch: 00024/00094 | Loss: 695.7330 | CE: 0.2780 | KD: 1059.9834\n",
      "Train Epoch: 031 Batch: 00025/00094 | Loss: 695.7081 | CE: 0.2269 | KD: 1060.0233\n",
      "Train Epoch: 031 Batch: 00026/00094 | Loss: 695.7571 | CE: 0.2560 | KD: 1060.0536\n",
      "Train Epoch: 031 Batch: 00027/00094 | Loss: 695.7311 | CE: 0.2514 | KD: 1060.0210\n",
      "Train Epoch: 031 Batch: 00028/00094 | Loss: 695.7381 | CE: 0.2560 | KD: 1060.0245\n",
      "Train Epoch: 031 Batch: 00029/00094 | Loss: 695.8628 | CE: 0.3088 | KD: 1060.1342\n",
      "Train Epoch: 031 Batch: 00030/00094 | Loss: 695.6926 | CE: 0.2143 | KD: 1060.0188\n",
      "Train Epoch: 031 Batch: 00031/00094 | Loss: 695.7502 | CE: 0.2912 | KD: 1059.9893\n",
      "Train Epoch: 031 Batch: 00032/00094 | Loss: 695.7171 | CE: 0.2524 | KD: 1059.9980\n",
      "Train Epoch: 031 Batch: 00033/00094 | Loss: 695.7412 | CE: 0.2595 | KD: 1060.0240\n",
      "Train Epoch: 031 Batch: 00034/00094 | Loss: 695.7397 | CE: 0.2715 | KD: 1060.0034\n",
      "Train Epoch: 031 Batch: 00035/00094 | Loss: 695.7567 | CE: 0.2438 | KD: 1060.0714\n",
      "Train Epoch: 031 Batch: 00036/00094 | Loss: 695.7772 | CE: 0.2636 | KD: 1060.0725\n",
      "Train Epoch: 031 Batch: 00037/00094 | Loss: 695.7474 | CE: 0.2614 | KD: 1060.0305\n",
      "Train Epoch: 031 Batch: 00038/00094 | Loss: 695.6155 | CE: 0.1851 | KD: 1059.9458\n",
      "Train Epoch: 031 Batch: 00039/00094 | Loss: 695.7800 | CE: 0.2906 | KD: 1060.0356\n",
      "Train Epoch: 031 Batch: 00040/00094 | Loss: 695.8342 | CE: 0.3223 | KD: 1060.0701\n",
      "Train Epoch: 031 Batch: 00041/00094 | Loss: 695.8768 | CE: 0.3433 | KD: 1060.1029\n",
      "Train Epoch: 031 Batch: 00042/00094 | Loss: 695.7495 | CE: 0.2583 | KD: 1060.0385\n",
      "Train Epoch: 031 Batch: 00043/00094 | Loss: 695.6963 | CE: 0.2360 | KD: 1059.9913\n",
      "Train Epoch: 031 Batch: 00044/00094 | Loss: 695.7041 | CE: 0.2473 | KD: 1059.9860\n",
      "Train Epoch: 031 Batch: 00045/00094 | Loss: 695.7957 | CE: 0.2746 | KD: 1060.0840\n",
      "Train Epoch: 031 Batch: 00046/00094 | Loss: 695.6535 | CE: 0.2187 | KD: 1059.9525\n",
      "Train Epoch: 031 Batch: 00047/00094 | Loss: 695.7512 | CE: 0.2327 | KD: 1060.0800\n",
      "Train Epoch: 031 Batch: 00048/00094 | Loss: 695.7938 | CE: 0.2946 | KD: 1060.0507\n",
      "Train Epoch: 031 Batch: 00049/00094 | Loss: 695.6473 | CE: 0.1873 | KD: 1059.9908\n",
      "Train Epoch: 031 Batch: 00050/00094 | Loss: 695.8069 | CE: 0.3218 | KD: 1060.0293\n",
      "Train Epoch: 031 Batch: 00051/00094 | Loss: 695.7045 | CE: 0.2307 | KD: 1060.0118\n",
      "Train Epoch: 031 Batch: 00052/00094 | Loss: 695.8154 | CE: 0.2677 | KD: 1060.1246\n",
      "Train Epoch: 031 Batch: 00053/00094 | Loss: 695.7578 | CE: 0.2911 | KD: 1060.0011\n",
      "Train Epoch: 031 Batch: 00054/00094 | Loss: 695.8019 | CE: 0.2592 | KD: 1060.1169\n",
      "Train Epoch: 031 Batch: 00055/00094 | Loss: 695.7125 | CE: 0.2388 | KD: 1060.0117\n",
      "Train Epoch: 031 Batch: 00056/00094 | Loss: 695.6542 | CE: 0.2140 | KD: 1059.9608\n",
      "Train Epoch: 031 Batch: 00057/00094 | Loss: 695.7380 | CE: 0.2998 | KD: 1059.9576\n",
      "Train Epoch: 031 Batch: 00058/00094 | Loss: 695.7877 | CE: 0.2795 | KD: 1060.0643\n",
      "Train Epoch: 031 Batch: 00059/00094 | Loss: 695.7178 | CE: 0.2551 | KD: 1059.9950\n",
      "Train Epoch: 031 Batch: 00060/00094 | Loss: 695.7408 | CE: 0.2233 | KD: 1060.0785\n",
      "Train Epoch: 031 Batch: 00061/00094 | Loss: 695.6832 | CE: 0.2282 | KD: 1059.9833\n",
      "Train Epoch: 031 Batch: 00062/00094 | Loss: 695.7159 | CE: 0.2303 | KD: 1060.0299\n",
      "Train Epoch: 031 Batch: 00063/00094 | Loss: 695.6688 | CE: 0.2215 | KD: 1059.9716\n",
      "Train Epoch: 031 Batch: 00064/00094 | Loss: 695.8583 | CE: 0.2957 | KD: 1060.1472\n",
      "Train Epoch: 031 Batch: 00065/00094 | Loss: 695.6694 | CE: 0.2422 | KD: 1059.9408\n",
      "Train Epoch: 031 Batch: 00066/00094 | Loss: 695.8602 | CE: 0.3302 | KD: 1060.0977\n",
      "Train Epoch: 031 Batch: 00067/00094 | Loss: 695.7377 | CE: 0.2339 | KD: 1060.0576\n",
      "Train Epoch: 031 Batch: 00068/00094 | Loss: 695.7764 | CE: 0.2684 | KD: 1060.0640\n",
      "Train Epoch: 031 Batch: 00069/00094 | Loss: 695.6540 | CE: 0.2025 | KD: 1059.9779\n",
      "Train Epoch: 031 Batch: 00070/00094 | Loss: 695.6252 | CE: 0.2172 | KD: 1059.9116\n",
      "Train Epoch: 031 Batch: 00071/00094 | Loss: 695.6747 | CE: 0.2069 | KD: 1060.0028\n",
      "Train Epoch: 031 Batch: 00072/00094 | Loss: 695.7223 | CE: 0.2828 | KD: 1059.9597\n",
      "Train Epoch: 031 Batch: 00073/00094 | Loss: 695.6762 | CE: 0.2520 | KD: 1059.9364\n",
      "Train Epoch: 031 Batch: 00074/00094 | Loss: 695.6989 | CE: 0.2183 | KD: 1060.0223\n",
      "Train Epoch: 031 Batch: 00075/00094 | Loss: 695.7346 | CE: 0.2704 | KD: 1059.9973\n",
      "Train Epoch: 031 Batch: 00076/00094 | Loss: 695.6408 | CE: 0.2011 | KD: 1059.9600\n",
      "Train Epoch: 031 Batch: 00077/00094 | Loss: 695.6959 | CE: 0.2383 | KD: 1059.9872\n",
      "Train Epoch: 031 Batch: 00078/00094 | Loss: 695.8156 | CE: 0.2983 | KD: 1060.0782\n",
      "Train Epoch: 031 Batch: 00079/00094 | Loss: 695.6794 | CE: 0.2345 | KD: 1059.9679\n",
      "Train Epoch: 031 Batch: 00080/00094 | Loss: 695.6733 | CE: 0.2233 | KD: 1059.9757\n",
      "Train Epoch: 031 Batch: 00081/00094 | Loss: 695.7640 | CE: 0.2555 | KD: 1060.0648\n",
      "Train Epoch: 031 Batch: 00082/00094 | Loss: 695.7449 | CE: 0.2857 | KD: 1059.9897\n",
      "Train Epoch: 031 Batch: 00083/00094 | Loss: 695.8850 | CE: 0.3207 | KD: 1060.1499\n",
      "Train Epoch: 031 Batch: 00084/00094 | Loss: 695.6785 | CE: 0.2157 | KD: 1059.9952\n",
      "Train Epoch: 031 Batch: 00085/00094 | Loss: 695.7678 | CE: 0.2282 | KD: 1060.1122\n",
      "Train Epoch: 031 Batch: 00086/00094 | Loss: 695.7289 | CE: 0.2261 | KD: 1060.0562\n",
      "Train Epoch: 031 Batch: 00087/00094 | Loss: 695.7095 | CE: 0.2452 | KD: 1059.9974\n",
      "Train Epoch: 031 Batch: 00088/00094 | Loss: 695.7769 | CE: 0.2700 | KD: 1060.0624\n",
      "Train Epoch: 031 Batch: 00089/00094 | Loss: 695.8240 | CE: 0.3457 | KD: 1060.0188\n",
      "Train Epoch: 031 Batch: 00090/00094 | Loss: 695.7882 | CE: 0.3011 | KD: 1060.0322\n",
      "Train Epoch: 031 Batch: 00091/00094 | Loss: 695.7968 | CE: 0.2923 | KD: 1060.0587\n",
      "Train Epoch: 031 Batch: 00092/00094 | Loss: 695.7084 | CE: 0.2356 | KD: 1060.0104\n",
      "Train Epoch: 031 Batch: 00093/00094 | Loss: 695.7614 | CE: 0.2352 | KD: 1060.0918\n",
      "Train Epoch: 031 Batch: 00094/00094 | Loss: 695.8430 | CE: 0.3305 | KD: 1060.0709\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2800 | acc:90.3500\n",
      "[VAL Acc] Target: 90.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1562 | acc:49.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9840 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0369 | acc:50.1908\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7632 | acc:57.0925\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9003 | acc:54.1590\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5572 | acc:71.6301\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9557 | acc:55.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4723 | acc:74.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.80%\n",
      "[VAL Acc] Avg 61.41%\n",
      "Train Epoch: 032 Batch: 00001/00094 | Loss: 695.7431 | CE: 0.2842 | KD: 1059.9893\n",
      "Train Epoch: 032 Batch: 00002/00094 | Loss: 695.7011 | CE: 0.2246 | KD: 1060.0161\n",
      "Train Epoch: 032 Batch: 00003/00094 | Loss: 695.7292 | CE: 0.2670 | KD: 1059.9943\n",
      "Train Epoch: 032 Batch: 00004/00094 | Loss: 695.6722 | CE: 0.2082 | KD: 1059.9969\n",
      "Train Epoch: 032 Batch: 00005/00094 | Loss: 695.6653 | CE: 0.2153 | KD: 1059.9756\n",
      "Train Epoch: 032 Batch: 00006/00094 | Loss: 695.7172 | CE: 0.2334 | KD: 1060.0271\n",
      "Train Epoch: 032 Batch: 00007/00094 | Loss: 695.7792 | CE: 0.3185 | KD: 1059.9918\n",
      "Train Epoch: 032 Batch: 00008/00094 | Loss: 695.6551 | CE: 0.1998 | KD: 1059.9836\n",
      "Train Epoch: 032 Batch: 00009/00094 | Loss: 695.7136 | CE: 0.2577 | KD: 1059.9846\n",
      "Train Epoch: 032 Batch: 00010/00094 | Loss: 695.6965 | CE: 0.2256 | KD: 1060.0076\n",
      "Train Epoch: 032 Batch: 00011/00094 | Loss: 695.7728 | CE: 0.2695 | KD: 1060.0569\n",
      "Train Epoch: 032 Batch: 00012/00094 | Loss: 695.6642 | CE: 0.2202 | KD: 1059.9667\n",
      "Train Epoch: 032 Batch: 00013/00094 | Loss: 695.6707 | CE: 0.1959 | KD: 1060.0134\n",
      "Train Epoch: 032 Batch: 00014/00094 | Loss: 695.8519 | CE: 0.2624 | KD: 1060.1884\n",
      "Train Epoch: 032 Batch: 00015/00094 | Loss: 695.6642 | CE: 0.2325 | KD: 1059.9476\n",
      "Train Epoch: 032 Batch: 00016/00094 | Loss: 695.7822 | CE: 0.2640 | KD: 1060.0795\n",
      "Train Epoch: 032 Batch: 00017/00094 | Loss: 695.7045 | CE: 0.2289 | KD: 1060.0148\n",
      "Train Epoch: 032 Batch: 00018/00094 | Loss: 695.7510 | CE: 0.2599 | KD: 1060.0382\n",
      "Train Epoch: 032 Batch: 00019/00094 | Loss: 695.7286 | CE: 0.1930 | KD: 1060.1062\n",
      "Train Epoch: 032 Batch: 00020/00094 | Loss: 695.7407 | CE: 0.2811 | KD: 1059.9902\n",
      "Train Epoch: 032 Batch: 00021/00094 | Loss: 695.6782 | CE: 0.2075 | KD: 1060.0072\n",
      "Train Epoch: 032 Batch: 00022/00094 | Loss: 695.6455 | CE: 0.1866 | KD: 1059.9893\n",
      "Train Epoch: 032 Batch: 00023/00094 | Loss: 695.7217 | CE: 0.2527 | KD: 1060.0045\n",
      "Train Epoch: 032 Batch: 00024/00094 | Loss: 695.7806 | CE: 0.2690 | KD: 1060.0696\n",
      "Train Epoch: 032 Batch: 00025/00094 | Loss: 695.7086 | CE: 0.2442 | KD: 1059.9976\n",
      "Train Epoch: 032 Batch: 00026/00094 | Loss: 695.6492 | CE: 0.1865 | KD: 1059.9950\n",
      "Train Epoch: 032 Batch: 00027/00094 | Loss: 695.7393 | CE: 0.2453 | KD: 1060.0426\n",
      "Train Epoch: 032 Batch: 00028/00094 | Loss: 695.8351 | CE: 0.2953 | KD: 1060.1124\n",
      "Train Epoch: 032 Batch: 00029/00094 | Loss: 695.7113 | CE: 0.2395 | KD: 1060.0089\n",
      "Train Epoch: 032 Batch: 00030/00094 | Loss: 695.8295 | CE: 0.2892 | KD: 1060.1132\n",
      "Train Epoch: 032 Batch: 00031/00094 | Loss: 695.7157 | CE: 0.2462 | KD: 1060.0054\n",
      "Train Epoch: 032 Batch: 00032/00094 | Loss: 695.7742 | CE: 0.2866 | KD: 1060.0330\n",
      "Train Epoch: 032 Batch: 00033/00094 | Loss: 695.7386 | CE: 0.2808 | KD: 1059.9875\n",
      "Train Epoch: 032 Batch: 00034/00094 | Loss: 695.6744 | CE: 0.2085 | KD: 1060.0000\n",
      "Train Epoch: 032 Batch: 00035/00094 | Loss: 695.7963 | CE: 0.2632 | KD: 1060.1022\n",
      "Train Epoch: 032 Batch: 00036/00094 | Loss: 695.7368 | CE: 0.2482 | KD: 1060.0344\n",
      "Train Epoch: 032 Batch: 00037/00094 | Loss: 695.9407 | CE: 0.3851 | KD: 1060.1365\n",
      "Train Epoch: 032 Batch: 00038/00094 | Loss: 695.7218 | CE: 0.2341 | KD: 1060.0331\n",
      "Train Epoch: 032 Batch: 00039/00094 | Loss: 695.7167 | CE: 0.2256 | KD: 1060.0383\n",
      "Train Epoch: 032 Batch: 00040/00094 | Loss: 695.7000 | CE: 0.2472 | KD: 1059.9800\n",
      "Train Epoch: 032 Batch: 00041/00094 | Loss: 695.7758 | CE: 0.2689 | KD: 1060.0623\n",
      "Train Epoch: 032 Batch: 00042/00094 | Loss: 695.7695 | CE: 0.2878 | KD: 1060.0239\n",
      "Train Epoch: 032 Batch: 00043/00094 | Loss: 695.8519 | CE: 0.3435 | KD: 1060.0646\n",
      "Train Epoch: 032 Batch: 00044/00094 | Loss: 695.7474 | CE: 0.2664 | KD: 1060.0229\n",
      "Train Epoch: 032 Batch: 00045/00094 | Loss: 695.7357 | CE: 0.2532 | KD: 1060.0250\n",
      "Train Epoch: 032 Batch: 00046/00094 | Loss: 695.8431 | CE: 0.2854 | KD: 1060.1399\n",
      "Train Epoch: 032 Batch: 00047/00094 | Loss: 695.7657 | CE: 0.2728 | KD: 1060.0411\n",
      "Train Epoch: 032 Batch: 00048/00094 | Loss: 695.7090 | CE: 0.2206 | KD: 1060.0342\n",
      "Train Epoch: 032 Batch: 00049/00094 | Loss: 695.8613 | CE: 0.3215 | KD: 1060.1125\n",
      "Train Epoch: 032 Batch: 00050/00094 | Loss: 695.7003 | CE: 0.2083 | KD: 1060.0397\n",
      "Train Epoch: 032 Batch: 00051/00094 | Loss: 695.6761 | CE: 0.2513 | KD: 1059.9371\n",
      "Train Epoch: 032 Batch: 00052/00094 | Loss: 695.8018 | CE: 0.3043 | KD: 1060.0480\n",
      "Train Epoch: 032 Batch: 00053/00094 | Loss: 695.8215 | CE: 0.3061 | KD: 1060.0752\n",
      "Train Epoch: 032 Batch: 00054/00094 | Loss: 695.8213 | CE: 0.2723 | KD: 1060.1266\n",
      "Train Epoch: 032 Batch: 00055/00094 | Loss: 695.7490 | CE: 0.2502 | KD: 1060.0499\n",
      "Train Epoch: 032 Batch: 00056/00094 | Loss: 695.7333 | CE: 0.2565 | KD: 1060.0165\n",
      "Train Epoch: 032 Batch: 00057/00094 | Loss: 695.7037 | CE: 0.1998 | KD: 1060.0577\n",
      "Train Epoch: 032 Batch: 00058/00094 | Loss: 695.7902 | CE: 0.2497 | KD: 1060.1136\n",
      "Train Epoch: 032 Batch: 00059/00094 | Loss: 695.7469 | CE: 0.2342 | KD: 1060.0712\n",
      "Train Epoch: 032 Batch: 00060/00094 | Loss: 695.7687 | CE: 0.2858 | KD: 1060.0258\n",
      "Train Epoch: 032 Batch: 00061/00094 | Loss: 695.7521 | CE: 0.2497 | KD: 1060.0555\n",
      "Train Epoch: 032 Batch: 00062/00094 | Loss: 695.8854 | CE: 0.3626 | KD: 1060.0867\n",
      "Train Epoch: 032 Batch: 00063/00094 | Loss: 695.7205 | CE: 0.2418 | KD: 1060.0193\n",
      "Train Epoch: 032 Batch: 00064/00094 | Loss: 695.7206 | CE: 0.2427 | KD: 1060.0182\n",
      "Train Epoch: 032 Batch: 00065/00094 | Loss: 695.8664 | CE: 0.3152 | KD: 1060.1299\n",
      "Train Epoch: 032 Batch: 00066/00094 | Loss: 695.8163 | CE: 0.3283 | KD: 1060.0336\n",
      "Train Epoch: 032 Batch: 00067/00094 | Loss: 695.7499 | CE: 0.2215 | KD: 1060.0951\n",
      "Train Epoch: 032 Batch: 00068/00094 | Loss: 695.7404 | CE: 0.2707 | KD: 1060.0055\n",
      "Train Epoch: 032 Batch: 00069/00094 | Loss: 695.7715 | CE: 0.2991 | KD: 1060.0098\n",
      "Train Epoch: 032 Batch: 00070/00094 | Loss: 695.7527 | CE: 0.2625 | KD: 1060.0370\n",
      "Train Epoch: 032 Batch: 00071/00094 | Loss: 695.6743 | CE: 0.2225 | KD: 1059.9783\n",
      "Train Epoch: 032 Batch: 00072/00094 | Loss: 695.6849 | CE: 0.2283 | KD: 1059.9857\n",
      "Train Epoch: 032 Batch: 00073/00094 | Loss: 695.7521 | CE: 0.2577 | KD: 1060.0432\n",
      "Train Epoch: 032 Batch: 00074/00094 | Loss: 695.6592 | CE: 0.2001 | KD: 1059.9895\n",
      "Train Epoch: 032 Batch: 00075/00094 | Loss: 695.7850 | CE: 0.2798 | KD: 1060.0598\n",
      "Train Epoch: 032 Batch: 00076/00094 | Loss: 695.7893 | CE: 0.3016 | KD: 1060.0332\n",
      "Train Epoch: 032 Batch: 00077/00094 | Loss: 695.6885 | CE: 0.2544 | KD: 1059.9514\n",
      "Train Epoch: 032 Batch: 00078/00094 | Loss: 695.8568 | CE: 0.3143 | KD: 1060.1167\n",
      "Train Epoch: 032 Batch: 00079/00094 | Loss: 695.8624 | CE: 0.3442 | KD: 1060.0796\n",
      "Train Epoch: 032 Batch: 00080/00094 | Loss: 695.7183 | CE: 0.2673 | KD: 1059.9772\n",
      "Train Epoch: 032 Batch: 00081/00094 | Loss: 695.6737 | CE: 0.1968 | KD: 1060.0166\n",
      "Train Epoch: 032 Batch: 00082/00094 | Loss: 695.7512 | CE: 0.2783 | KD: 1060.0105\n",
      "Train Epoch: 032 Batch: 00083/00094 | Loss: 695.7710 | CE: 0.2702 | KD: 1060.0531\n",
      "Train Epoch: 032 Batch: 00084/00094 | Loss: 695.7603 | CE: 0.2434 | KD: 1060.0776\n",
      "Train Epoch: 032 Batch: 00085/00094 | Loss: 695.7309 | CE: 0.2384 | KD: 1060.0405\n",
      "Train Epoch: 032 Batch: 00086/00094 | Loss: 695.7352 | CE: 0.2876 | KD: 1059.9719\n",
      "Train Epoch: 032 Batch: 00087/00094 | Loss: 695.7473 | CE: 0.2392 | KD: 1060.0643\n",
      "Train Epoch: 032 Batch: 00088/00094 | Loss: 695.8234 | CE: 0.2997 | KD: 1060.0880\n",
      "Train Epoch: 032 Batch: 00089/00094 | Loss: 695.7712 | CE: 0.2366 | KD: 1060.1047\n",
      "Train Epoch: 032 Batch: 00090/00094 | Loss: 695.7253 | CE: 0.2633 | KD: 1059.9939\n",
      "Train Epoch: 032 Batch: 00091/00094 | Loss: 695.6566 | CE: 0.2151 | KD: 1059.9625\n",
      "Train Epoch: 032 Batch: 00092/00094 | Loss: 695.7414 | CE: 0.2736 | KD: 1060.0028\n",
      "Train Epoch: 032 Batch: 00093/00094 | Loss: 695.7604 | CE: 0.2949 | KD: 1059.9993\n",
      "Train Epoch: 032 Batch: 00094/00094 | Loss: 695.7730 | CE: 0.2407 | KD: 1060.1012\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2768 | acc:89.9500\n",
      "[VAL Acc] Target: 89.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1438 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0065 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0244 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7585 | acc:58.3464\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8570 | acc:55.7301\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.73%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5281 | acc:74.2947\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.29%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9307 | acc:55.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4790 | acc:74.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.50%\n",
      "[VAL Acc] Avg 61.78%\n",
      "Train Epoch: 033 Batch: 00001/00094 | Loss: 695.6954 | CE: 0.2137 | KD: 1060.0240\n",
      "Train Epoch: 033 Batch: 00002/00094 | Loss: 695.7911 | CE: 0.2811 | KD: 1060.0670\n",
      "Train Epoch: 033 Batch: 00003/00094 | Loss: 695.7916 | CE: 0.2647 | KD: 1060.0928\n",
      "Train Epoch: 033 Batch: 00004/00094 | Loss: 695.6321 | CE: 0.2009 | KD: 1059.9469\n",
      "Train Epoch: 033 Batch: 00005/00094 | Loss: 695.7353 | CE: 0.2676 | KD: 1060.0026\n",
      "Train Epoch: 033 Batch: 00006/00094 | Loss: 695.6997 | CE: 0.2262 | KD: 1060.0115\n",
      "Train Epoch: 033 Batch: 00007/00094 | Loss: 695.7923 | CE: 0.3067 | KD: 1060.0300\n",
      "Train Epoch: 033 Batch: 00008/00094 | Loss: 695.9067 | CE: 0.3600 | KD: 1060.1230\n",
      "Train Epoch: 033 Batch: 00009/00094 | Loss: 695.6767 | CE: 0.2199 | KD: 1059.9860\n",
      "Train Epoch: 033 Batch: 00010/00094 | Loss: 695.8680 | CE: 0.3279 | KD: 1060.1130\n",
      "Train Epoch: 033 Batch: 00011/00094 | Loss: 695.8694 | CE: 0.3414 | KD: 1060.0945\n",
      "Train Epoch: 033 Batch: 00012/00094 | Loss: 695.6599 | CE: 0.2169 | KD: 1059.9648\n",
      "Train Epoch: 033 Batch: 00013/00094 | Loss: 695.6821 | CE: 0.2504 | KD: 1059.9478\n",
      "Train Epoch: 033 Batch: 00014/00094 | Loss: 695.6635 | CE: 0.2224 | KD: 1059.9620\n",
      "Train Epoch: 033 Batch: 00015/00094 | Loss: 695.7939 | CE: 0.2706 | KD: 1060.0873\n",
      "Train Epoch: 033 Batch: 00016/00094 | Loss: 695.7084 | CE: 0.2374 | KD: 1060.0076\n",
      "Train Epoch: 033 Batch: 00017/00094 | Loss: 695.7766 | CE: 0.2799 | KD: 1060.0469\n",
      "Train Epoch: 033 Batch: 00018/00094 | Loss: 695.8169 | CE: 0.3049 | KD: 1060.0702\n",
      "Train Epoch: 033 Batch: 00019/00094 | Loss: 695.7205 | CE: 0.2750 | KD: 1059.9689\n",
      "Train Epoch: 033 Batch: 00020/00094 | Loss: 695.6451 | CE: 0.2187 | KD: 1059.9397\n",
      "Train Epoch: 033 Batch: 00021/00094 | Loss: 695.7723 | CE: 0.3053 | KD: 1060.0015\n",
      "Train Epoch: 033 Batch: 00022/00094 | Loss: 695.7710 | CE: 0.2784 | KD: 1060.0405\n",
      "Train Epoch: 033 Batch: 00023/00094 | Loss: 695.6887 | CE: 0.2385 | KD: 1059.9760\n",
      "Train Epoch: 033 Batch: 00024/00094 | Loss: 695.8285 | CE: 0.3147 | KD: 1060.0729\n",
      "Train Epoch: 033 Batch: 00025/00094 | Loss: 695.6983 | CE: 0.2150 | KD: 1060.0264\n",
      "Train Epoch: 033 Batch: 00026/00094 | Loss: 695.7518 | CE: 0.2737 | KD: 1060.0184\n",
      "Train Epoch: 033 Batch: 00027/00094 | Loss: 695.7947 | CE: 0.2861 | KD: 1060.0649\n",
      "Train Epoch: 033 Batch: 00028/00094 | Loss: 695.7438 | CE: 0.2478 | KD: 1060.0458\n",
      "Train Epoch: 033 Batch: 00029/00094 | Loss: 695.7675 | CE: 0.2495 | KD: 1060.0792\n",
      "Train Epoch: 033 Batch: 00030/00094 | Loss: 695.7621 | CE: 0.2731 | KD: 1060.0350\n",
      "Train Epoch: 033 Batch: 00031/00094 | Loss: 695.7225 | CE: 0.2384 | KD: 1060.0276\n",
      "Train Epoch: 033 Batch: 00032/00094 | Loss: 695.7354 | CE: 0.2487 | KD: 1060.0315\n",
      "Train Epoch: 033 Batch: 00033/00094 | Loss: 695.6522 | CE: 0.2078 | KD: 1059.9670\n",
      "Train Epoch: 033 Batch: 00034/00094 | Loss: 695.6395 | CE: 0.2071 | KD: 1059.9489\n",
      "Train Epoch: 033 Batch: 00035/00094 | Loss: 695.7794 | CE: 0.2701 | KD: 1060.0659\n",
      "Train Epoch: 033 Batch: 00036/00094 | Loss: 695.7478 | CE: 0.2578 | KD: 1060.0366\n",
      "Train Epoch: 033 Batch: 00037/00094 | Loss: 695.6813 | CE: 0.2010 | KD: 1060.0219\n",
      "Train Epoch: 033 Batch: 00038/00094 | Loss: 695.8727 | CE: 0.3052 | KD: 1060.1548\n",
      "Train Epoch: 033 Batch: 00039/00094 | Loss: 695.6382 | CE: 0.2345 | KD: 1059.9050\n",
      "Train Epoch: 033 Batch: 00040/00094 | Loss: 695.7276 | CE: 0.2198 | KD: 1060.0637\n",
      "Train Epoch: 033 Batch: 00041/00094 | Loss: 695.7986 | CE: 0.2720 | KD: 1060.0925\n",
      "Train Epoch: 033 Batch: 00042/00094 | Loss: 695.6377 | CE: 0.2203 | KD: 1059.9259\n",
      "Train Epoch: 033 Batch: 00043/00094 | Loss: 695.7070 | CE: 0.2496 | KD: 1059.9871\n",
      "Train Epoch: 033 Batch: 00044/00094 | Loss: 695.8334 | CE: 0.3057 | KD: 1060.0940\n",
      "Train Epoch: 033 Batch: 00045/00094 | Loss: 695.7602 | CE: 0.2465 | KD: 1060.0728\n",
      "Train Epoch: 033 Batch: 00046/00094 | Loss: 695.7902 | CE: 0.2839 | KD: 1060.0615\n",
      "Train Epoch: 033 Batch: 00047/00094 | Loss: 695.8538 | CE: 0.3254 | KD: 1060.0951\n",
      "Train Epoch: 033 Batch: 00048/00094 | Loss: 695.7729 | CE: 0.2854 | KD: 1060.0328\n",
      "Train Epoch: 033 Batch: 00049/00094 | Loss: 695.8857 | CE: 0.3655 | KD: 1060.0825\n",
      "Train Epoch: 033 Batch: 00050/00094 | Loss: 695.7444 | CE: 0.2881 | KD: 1059.9854\n",
      "Train Epoch: 033 Batch: 00051/00094 | Loss: 695.6976 | CE: 0.2021 | KD: 1060.0452\n",
      "Train Epoch: 033 Batch: 00052/00094 | Loss: 695.8044 | CE: 0.2850 | KD: 1060.0814\n",
      "Train Epoch: 033 Batch: 00053/00094 | Loss: 695.7578 | CE: 0.2613 | KD: 1060.0464\n",
      "Train Epoch: 033 Batch: 00054/00094 | Loss: 695.8346 | CE: 0.2684 | KD: 1060.1528\n",
      "Train Epoch: 033 Batch: 00055/00094 | Loss: 695.7369 | CE: 0.3052 | KD: 1059.9478\n",
      "Train Epoch: 033 Batch: 00056/00094 | Loss: 695.7419 | CE: 0.2703 | KD: 1060.0087\n",
      "Train Epoch: 033 Batch: 00057/00094 | Loss: 695.8391 | CE: 0.3466 | KD: 1060.0405\n",
      "Train Epoch: 033 Batch: 00058/00094 | Loss: 695.7858 | CE: 0.2446 | KD: 1060.1147\n",
      "Train Epoch: 033 Batch: 00059/00094 | Loss: 695.7550 | CE: 0.2415 | KD: 1060.0724\n",
      "Train Epoch: 033 Batch: 00060/00094 | Loss: 695.8217 | CE: 0.2682 | KD: 1060.1333\n",
      "Train Epoch: 033 Batch: 00061/00094 | Loss: 695.7874 | CE: 0.2689 | KD: 1060.0800\n",
      "Train Epoch: 033 Batch: 00062/00094 | Loss: 695.7333 | CE: 0.2388 | KD: 1060.0435\n",
      "Train Epoch: 033 Batch: 00063/00094 | Loss: 695.6569 | CE: 0.2051 | KD: 1059.9784\n",
      "Train Epoch: 033 Batch: 00064/00094 | Loss: 695.6877 | CE: 0.2254 | KD: 1059.9944\n",
      "Train Epoch: 033 Batch: 00065/00094 | Loss: 695.8190 | CE: 0.3049 | KD: 1060.0735\n",
      "Train Epoch: 033 Batch: 00066/00094 | Loss: 695.7622 | CE: 0.2619 | KD: 1060.0524\n",
      "Train Epoch: 033 Batch: 00067/00094 | Loss: 695.6654 | CE: 0.2137 | KD: 1059.9783\n",
      "Train Epoch: 033 Batch: 00068/00094 | Loss: 695.8678 | CE: 0.3408 | KD: 1060.0930\n",
      "Train Epoch: 033 Batch: 00069/00094 | Loss: 695.7078 | CE: 0.2350 | KD: 1060.0105\n",
      "Train Epoch: 033 Batch: 00070/00094 | Loss: 695.7620 | CE: 0.2756 | KD: 1060.0310\n",
      "Train Epoch: 033 Batch: 00071/00094 | Loss: 695.8145 | CE: 0.3005 | KD: 1060.0731\n",
      "Train Epoch: 033 Batch: 00072/00094 | Loss: 695.7708 | CE: 0.2824 | KD: 1060.0341\n",
      "Train Epoch: 033 Batch: 00073/00094 | Loss: 695.6463 | CE: 0.1810 | KD: 1059.9990\n",
      "Train Epoch: 033 Batch: 00074/00094 | Loss: 695.8677 | CE: 0.2854 | KD: 1060.1774\n",
      "Train Epoch: 033 Batch: 00075/00094 | Loss: 695.6502 | CE: 0.2002 | KD: 1059.9757\n",
      "Train Epoch: 033 Batch: 00076/00094 | Loss: 695.8383 | CE: 0.3205 | KD: 1060.0790\n",
      "Train Epoch: 033 Batch: 00077/00094 | Loss: 695.7531 | CE: 0.2722 | KD: 1060.0227\n",
      "Train Epoch: 033 Batch: 00078/00094 | Loss: 695.7463 | CE: 0.2756 | KD: 1060.0071\n",
      "Train Epoch: 033 Batch: 00079/00094 | Loss: 695.7939 | CE: 0.2881 | KD: 1060.0607\n",
      "Train Epoch: 033 Batch: 00080/00094 | Loss: 695.7787 | CE: 0.2754 | KD: 1060.0569\n",
      "Train Epoch: 033 Batch: 00081/00094 | Loss: 695.6832 | CE: 0.2312 | KD: 1059.9786\n",
      "Train Epoch: 033 Batch: 00082/00094 | Loss: 695.6627 | CE: 0.1882 | KD: 1060.0129\n",
      "Train Epoch: 033 Batch: 00083/00094 | Loss: 695.7646 | CE: 0.2290 | KD: 1060.1062\n",
      "Train Epoch: 033 Batch: 00084/00094 | Loss: 695.7770 | CE: 0.3138 | KD: 1059.9957\n",
      "Train Epoch: 033 Batch: 00085/00094 | Loss: 695.6693 | CE: 0.2432 | KD: 1059.9392\n",
      "Train Epoch: 033 Batch: 00086/00094 | Loss: 695.6457 | CE: 0.2099 | KD: 1059.9540\n",
      "Train Epoch: 033 Batch: 00087/00094 | Loss: 695.7325 | CE: 0.2368 | KD: 1060.0454\n",
      "Train Epoch: 033 Batch: 00088/00094 | Loss: 695.7301 | CE: 0.2575 | KD: 1060.0101\n",
      "Train Epoch: 033 Batch: 00089/00094 | Loss: 695.7628 | CE: 0.2515 | KD: 1060.0691\n",
      "Train Epoch: 033 Batch: 00090/00094 | Loss: 695.6779 | CE: 0.2298 | KD: 1059.9727\n",
      "Train Epoch: 033 Batch: 00091/00094 | Loss: 695.6462 | CE: 0.2005 | KD: 1059.9691\n",
      "Train Epoch: 033 Batch: 00092/00094 | Loss: 695.7198 | CE: 0.2299 | KD: 1060.0364\n",
      "Train Epoch: 033 Batch: 00093/00094 | Loss: 695.6708 | CE: 0.2019 | KD: 1060.0045\n",
      "Train Epoch: 033 Batch: 00094/00094 | Loss: 695.8001 | CE: 0.3109 | KD: 1060.0354\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2855 | acc:90.2500\n",
      "[VAL Acc] Target: 90.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1075 | acc:50.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9757 | acc:48.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0080 | acc:50.5725\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7179 | acc:59.4044\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8684 | acc:56.2847\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 56.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5221 | acc:75.3527\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9688 | acc:54.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4522 | acc:75.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.80%\n",
      "[VAL Acc] Avg 62.40%\n",
      "Train Epoch: 034 Batch: 00001/00094 | Loss: 695.7127 | CE: 0.2266 | KD: 1060.0308\n",
      "Train Epoch: 034 Batch: 00002/00094 | Loss: 695.6694 | CE: 0.2354 | KD: 1059.9512\n",
      "Train Epoch: 034 Batch: 00003/00094 | Loss: 695.7540 | CE: 0.2731 | KD: 1060.0227\n",
      "Train Epoch: 034 Batch: 00004/00094 | Loss: 695.7405 | CE: 0.2439 | KD: 1060.0466\n",
      "Train Epoch: 034 Batch: 00005/00094 | Loss: 695.7984 | CE: 0.2864 | KD: 1060.0702\n",
      "Train Epoch: 034 Batch: 00006/00094 | Loss: 695.7240 | CE: 0.2370 | KD: 1060.0321\n",
      "Train Epoch: 034 Batch: 00007/00094 | Loss: 695.8472 | CE: 0.2836 | KD: 1060.1487\n",
      "Train Epoch: 034 Batch: 00008/00094 | Loss: 695.6828 | CE: 0.2192 | KD: 1059.9963\n",
      "Train Epoch: 034 Batch: 00009/00094 | Loss: 695.6967 | CE: 0.2247 | KD: 1060.0092\n",
      "Train Epoch: 034 Batch: 00010/00094 | Loss: 695.8618 | CE: 0.2971 | KD: 1060.1505\n",
      "Train Epoch: 034 Batch: 00011/00094 | Loss: 695.7163 | CE: 0.2577 | KD: 1059.9889\n",
      "Train Epoch: 034 Batch: 00012/00094 | Loss: 695.7422 | CE: 0.2638 | KD: 1060.0190\n",
      "Train Epoch: 034 Batch: 00013/00094 | Loss: 695.7771 | CE: 0.2912 | KD: 1060.0304\n",
      "Train Epoch: 034 Batch: 00014/00094 | Loss: 695.7961 | CE: 0.3065 | KD: 1060.0361\n",
      "Train Epoch: 034 Batch: 00015/00094 | Loss: 695.7524 | CE: 0.2352 | KD: 1060.0780\n",
      "Train Epoch: 034 Batch: 00016/00094 | Loss: 695.7743 | CE: 0.2467 | KD: 1060.0940\n",
      "Train Epoch: 034 Batch: 00017/00094 | Loss: 695.7083 | CE: 0.2611 | KD: 1059.9714\n",
      "Train Epoch: 034 Batch: 00018/00094 | Loss: 695.6839 | CE: 0.2276 | KD: 1059.9854\n",
      "Train Epoch: 034 Batch: 00019/00094 | Loss: 695.9125 | CE: 0.3807 | KD: 1060.1003\n",
      "Train Epoch: 034 Batch: 00020/00094 | Loss: 695.6497 | CE: 0.2000 | KD: 1059.9752\n",
      "Train Epoch: 034 Batch: 00021/00094 | Loss: 695.7533 | CE: 0.2399 | KD: 1060.0723\n",
      "Train Epoch: 034 Batch: 00022/00094 | Loss: 695.7579 | CE: 0.2718 | KD: 1060.0306\n",
      "Train Epoch: 034 Batch: 00023/00094 | Loss: 695.9257 | CE: 0.4212 | KD: 1060.0587\n",
      "Train Epoch: 034 Batch: 00024/00094 | Loss: 695.7008 | CE: 0.2311 | KD: 1060.0056\n",
      "Train Epoch: 034 Batch: 00025/00094 | Loss: 695.6794 | CE: 0.2472 | KD: 1059.9486\n",
      "Train Epoch: 034 Batch: 00026/00094 | Loss: 695.6611 | CE: 0.2160 | KD: 1059.9683\n",
      "Train Epoch: 034 Batch: 00027/00094 | Loss: 695.7343 | CE: 0.2644 | KD: 1060.0061\n",
      "Train Epoch: 034 Batch: 00028/00094 | Loss: 695.7903 | CE: 0.2825 | KD: 1060.0638\n",
      "Train Epoch: 034 Batch: 00029/00094 | Loss: 695.8801 | CE: 0.3101 | KD: 1060.1587\n",
      "Train Epoch: 034 Batch: 00030/00094 | Loss: 695.6628 | CE: 0.2160 | KD: 1059.9708\n",
      "Train Epoch: 034 Batch: 00031/00094 | Loss: 695.6771 | CE: 0.2210 | KD: 1059.9850\n",
      "Train Epoch: 034 Batch: 00032/00094 | Loss: 695.6967 | CE: 0.2194 | KD: 1060.0173\n",
      "Train Epoch: 034 Batch: 00033/00094 | Loss: 695.7679 | CE: 0.2699 | KD: 1060.0488\n",
      "Train Epoch: 034 Batch: 00034/00094 | Loss: 695.7534 | CE: 0.2927 | KD: 1059.9921\n",
      "Train Epoch: 034 Batch: 00035/00094 | Loss: 695.7468 | CE: 0.2679 | KD: 1060.0197\n",
      "Train Epoch: 034 Batch: 00036/00094 | Loss: 695.7679 | CE: 0.2796 | KD: 1060.0341\n",
      "Train Epoch: 034 Batch: 00037/00094 | Loss: 695.7479 | CE: 0.2482 | KD: 1060.0514\n",
      "Train Epoch: 034 Batch: 00038/00094 | Loss: 695.7905 | CE: 0.2934 | KD: 1060.0475\n",
      "Train Epoch: 034 Batch: 00039/00094 | Loss: 695.6898 | CE: 0.1908 | KD: 1060.0503\n",
      "Train Epoch: 034 Batch: 00040/00094 | Loss: 695.6816 | CE: 0.2162 | KD: 1059.9990\n",
      "Train Epoch: 034 Batch: 00041/00094 | Loss: 695.7637 | CE: 0.2761 | KD: 1060.0330\n",
      "Train Epoch: 034 Batch: 00042/00094 | Loss: 695.6923 | CE: 0.2143 | KD: 1060.0183\n",
      "Train Epoch: 034 Batch: 00043/00094 | Loss: 695.7004 | CE: 0.2235 | KD: 1060.0167\n",
      "Train Epoch: 034 Batch: 00044/00094 | Loss: 695.6758 | CE: 0.1861 | KD: 1060.0363\n",
      "Train Epoch: 034 Batch: 00045/00094 | Loss: 695.7313 | CE: 0.2220 | KD: 1060.0660\n",
      "Train Epoch: 034 Batch: 00046/00094 | Loss: 695.7269 | CE: 0.2286 | KD: 1060.0492\n",
      "Train Epoch: 034 Batch: 00047/00094 | Loss: 695.7549 | CE: 0.2344 | KD: 1060.0830\n",
      "Train Epoch: 034 Batch: 00048/00094 | Loss: 695.6989 | CE: 0.2611 | KD: 1059.9570\n",
      "Train Epoch: 034 Batch: 00049/00094 | Loss: 695.8255 | CE: 0.3095 | KD: 1060.0762\n",
      "Train Epoch: 034 Batch: 00050/00094 | Loss: 695.7245 | CE: 0.2386 | KD: 1060.0304\n",
      "Train Epoch: 034 Batch: 00051/00094 | Loss: 695.7568 | CE: 0.3025 | KD: 1059.9823\n",
      "Train Epoch: 034 Batch: 00052/00094 | Loss: 695.7713 | CE: 0.2233 | KD: 1060.1250\n",
      "Train Epoch: 034 Batch: 00053/00094 | Loss: 695.7573 | CE: 0.3058 | KD: 1059.9779\n",
      "Train Epoch: 034 Batch: 00054/00094 | Loss: 695.7010 | CE: 0.2464 | KD: 1059.9828\n",
      "Train Epoch: 034 Batch: 00055/00094 | Loss: 695.7559 | CE: 0.2743 | KD: 1060.0237\n",
      "Train Epoch: 034 Batch: 00056/00094 | Loss: 695.7648 | CE: 0.2538 | KD: 1060.0686\n",
      "Train Epoch: 034 Batch: 00057/00094 | Loss: 695.6920 | CE: 0.2356 | KD: 1059.9854\n",
      "Train Epoch: 034 Batch: 00058/00094 | Loss: 695.8087 | CE: 0.3215 | KD: 1060.0323\n",
      "Train Epoch: 034 Batch: 00059/00094 | Loss: 695.7764 | CE: 0.2627 | KD: 1060.0729\n",
      "Train Epoch: 034 Batch: 00060/00094 | Loss: 695.8351 | CE: 0.3117 | KD: 1060.0875\n",
      "Train Epoch: 034 Batch: 00061/00094 | Loss: 695.8821 | CE: 0.3763 | KD: 1060.0608\n",
      "Train Epoch: 034 Batch: 00062/00094 | Loss: 695.6998 | CE: 0.1990 | KD: 1060.0531\n",
      "Train Epoch: 034 Batch: 00063/00094 | Loss: 695.7181 | CE: 0.2637 | KD: 1059.9824\n",
      "Train Epoch: 034 Batch: 00064/00094 | Loss: 695.9142 | CE: 0.3475 | KD: 1060.1536\n",
      "Train Epoch: 034 Batch: 00065/00094 | Loss: 695.6151 | CE: 0.2019 | KD: 1059.9196\n",
      "Train Epoch: 034 Batch: 00066/00094 | Loss: 695.7567 | CE: 0.2575 | KD: 1060.0505\n",
      "Train Epoch: 034 Batch: 00067/00094 | Loss: 695.6830 | CE: 0.2185 | KD: 1059.9978\n",
      "Train Epoch: 034 Batch: 00068/00094 | Loss: 695.7635 | CE: 0.2430 | KD: 1060.0830\n",
      "Train Epoch: 034 Batch: 00069/00094 | Loss: 695.6186 | CE: 0.2061 | KD: 1059.9185\n",
      "Train Epoch: 034 Batch: 00070/00094 | Loss: 695.7104 | CE: 0.2346 | KD: 1060.0149\n",
      "Train Epoch: 034 Batch: 00071/00094 | Loss: 695.8531 | CE: 0.3012 | KD: 1060.1311\n",
      "Train Epoch: 034 Batch: 00072/00094 | Loss: 695.8965 | CE: 0.3189 | KD: 1060.1702\n",
      "Train Epoch: 034 Batch: 00073/00094 | Loss: 695.7621 | CE: 0.2411 | KD: 1060.0837\n",
      "Train Epoch: 034 Batch: 00074/00094 | Loss: 695.6797 | CE: 0.2339 | KD: 1059.9692\n",
      "Train Epoch: 034 Batch: 00075/00094 | Loss: 695.7684 | CE: 0.2322 | KD: 1060.1071\n",
      "Train Epoch: 034 Batch: 00076/00094 | Loss: 695.7584 | CE: 0.2745 | KD: 1060.0273\n",
      "Train Epoch: 034 Batch: 00077/00094 | Loss: 695.6467 | CE: 0.2172 | KD: 1059.9443\n",
      "Train Epoch: 034 Batch: 00078/00094 | Loss: 695.8015 | CE: 0.2739 | KD: 1060.0940\n",
      "Train Epoch: 034 Batch: 00079/00094 | Loss: 695.6905 | CE: 0.2583 | KD: 1059.9485\n",
      "Train Epoch: 034 Batch: 00080/00094 | Loss: 695.7033 | CE: 0.2656 | KD: 1059.9569\n",
      "Train Epoch: 034 Batch: 00081/00094 | Loss: 695.8012 | CE: 0.2871 | KD: 1060.0735\n",
      "Train Epoch: 034 Batch: 00082/00094 | Loss: 695.7285 | CE: 0.2237 | KD: 1060.0591\n",
      "Train Epoch: 034 Batch: 00083/00094 | Loss: 695.7798 | CE: 0.3005 | KD: 1060.0204\n",
      "Train Epoch: 034 Batch: 00084/00094 | Loss: 695.6478 | CE: 0.1928 | KD: 1059.9833\n",
      "Train Epoch: 034 Batch: 00085/00094 | Loss: 695.7654 | CE: 0.2753 | KD: 1060.0369\n",
      "Train Epoch: 034 Batch: 00086/00094 | Loss: 695.9183 | CE: 0.3753 | KD: 1060.1176\n",
      "Train Epoch: 034 Batch: 00087/00094 | Loss: 695.6405 | CE: 0.1829 | KD: 1059.9872\n",
      "Train Epoch: 034 Batch: 00088/00094 | Loss: 695.7648 | CE: 0.2376 | KD: 1060.0934\n",
      "Train Epoch: 034 Batch: 00089/00094 | Loss: 695.7339 | CE: 0.2794 | KD: 1059.9825\n",
      "Train Epoch: 034 Batch: 00090/00094 | Loss: 695.7698 | CE: 0.3049 | KD: 1059.9983\n",
      "Train Epoch: 034 Batch: 00091/00094 | Loss: 695.6151 | CE: 0.1934 | KD: 1059.9326\n",
      "Train Epoch: 034 Batch: 00092/00094 | Loss: 695.7388 | CE: 0.2711 | KD: 1060.0024\n",
      "Train Epoch: 034 Batch: 00093/00094 | Loss: 695.6863 | CE: 0.2083 | KD: 1060.0184\n",
      "Train Epoch: 034 Batch: 00094/00094 | Loss: 695.6638 | CE: 0.2272 | KD: 1059.9552\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2793 | acc:90.0500\n",
      "[VAL Acc] Target: 90.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1695 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9833 | acc:48.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0698 | acc:47.3282\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7065 | acc:60.8542\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8041 | acc:57.5786\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 57.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5158 | acc:74.9216\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9253 | acc:55.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4812 | acc:74.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.70%\n",
      "[VAL Acc] Avg 62.15%\n",
      "Train Epoch: 035 Batch: 00001/00094 | Loss: 626.2232 | CE: 0.2943 | KD: 1060.0161\n",
      "Train Epoch: 035 Batch: 00002/00094 | Loss: 626.2811 | CE: 0.2812 | KD: 1060.1365\n",
      "Train Epoch: 035 Batch: 00003/00094 | Loss: 626.1685 | CE: 0.2305 | KD: 1060.0315\n",
      "Train Epoch: 035 Batch: 00004/00094 | Loss: 626.1785 | CE: 0.2472 | KD: 1060.0201\n",
      "Train Epoch: 035 Batch: 00005/00094 | Loss: 626.1409 | CE: 0.2247 | KD: 1059.9946\n",
      "Train Epoch: 035 Batch: 00006/00094 | Loss: 626.1118 | CE: 0.1927 | KD: 1059.9995\n",
      "Train Epoch: 035 Batch: 00007/00094 | Loss: 626.2964 | CE: 0.3630 | KD: 1060.0238\n",
      "Train Epoch: 035 Batch: 00008/00094 | Loss: 626.2545 | CE: 0.2828 | KD: 1060.0886\n",
      "Train Epoch: 035 Batch: 00009/00094 | Loss: 626.1530 | CE: 0.2393 | KD: 1059.9902\n",
      "Train Epoch: 035 Batch: 00010/00094 | Loss: 626.1588 | CE: 0.2334 | KD: 1060.0103\n",
      "Train Epoch: 035 Batch: 00011/00094 | Loss: 626.1383 | CE: 0.2014 | KD: 1060.0297\n",
      "Train Epoch: 035 Batch: 00012/00094 | Loss: 626.1636 | CE: 0.2271 | KD: 1060.0289\n",
      "Train Epoch: 035 Batch: 00013/00094 | Loss: 626.2969 | CE: 0.3486 | KD: 1060.0490\n",
      "Train Epoch: 035 Batch: 00014/00094 | Loss: 626.2338 | CE: 0.2771 | KD: 1060.0631\n",
      "Train Epoch: 035 Batch: 00015/00094 | Loss: 626.2031 | CE: 0.2541 | KD: 1060.0502\n",
      "Train Epoch: 035 Batch: 00016/00094 | Loss: 626.2509 | CE: 0.3055 | KD: 1060.0441\n",
      "Train Epoch: 035 Batch: 00017/00094 | Loss: 626.3159 | CE: 0.3392 | KD: 1060.0970\n",
      "Train Epoch: 035 Batch: 00018/00094 | Loss: 626.1177 | CE: 0.2143 | KD: 1059.9729\n",
      "Train Epoch: 035 Batch: 00019/00094 | Loss: 626.1713 | CE: 0.2377 | KD: 1060.0240\n",
      "Train Epoch: 035 Batch: 00020/00094 | Loss: 626.1837 | CE: 0.2414 | KD: 1060.0387\n",
      "Train Epoch: 035 Batch: 00021/00094 | Loss: 626.1309 | CE: 0.2204 | KD: 1059.9849\n",
      "Train Epoch: 035 Batch: 00022/00094 | Loss: 626.1740 | CE: 0.2734 | KD: 1059.9680\n",
      "Train Epoch: 035 Batch: 00023/00094 | Loss: 626.2271 | CE: 0.3054 | KD: 1060.0039\n",
      "Train Epoch: 035 Batch: 00024/00094 | Loss: 626.2000 | CE: 0.2409 | KD: 1060.0673\n",
      "Train Epoch: 035 Batch: 00025/00094 | Loss: 626.1611 | CE: 0.2325 | KD: 1060.0156\n",
      "Train Epoch: 035 Batch: 00026/00094 | Loss: 626.1263 | CE: 0.2194 | KD: 1059.9789\n",
      "Train Epoch: 035 Batch: 00027/00094 | Loss: 626.2333 | CE: 0.2704 | KD: 1060.0737\n",
      "Train Epoch: 035 Batch: 00028/00094 | Loss: 626.2206 | CE: 0.2416 | KD: 1060.1010\n",
      "Train Epoch: 035 Batch: 00029/00094 | Loss: 626.2285 | CE: 0.2487 | KD: 1060.1023\n",
      "Train Epoch: 035 Batch: 00030/00094 | Loss: 626.1336 | CE: 0.1929 | KD: 1060.0360\n",
      "Train Epoch: 035 Batch: 00031/00094 | Loss: 626.1548 | CE: 0.2250 | KD: 1060.0177\n",
      "Train Epoch: 035 Batch: 00032/00094 | Loss: 626.2269 | CE: 0.2747 | KD: 1060.0555\n",
      "Train Epoch: 035 Batch: 00033/00094 | Loss: 626.0948 | CE: 0.1836 | KD: 1059.9862\n",
      "Train Epoch: 035 Batch: 00034/00094 | Loss: 626.2259 | CE: 0.2553 | KD: 1060.0867\n",
      "Train Epoch: 035 Batch: 00035/00094 | Loss: 626.1392 | CE: 0.2238 | KD: 1059.9933\n",
      "Train Epoch: 035 Batch: 00036/00094 | Loss: 626.3087 | CE: 0.3339 | KD: 1060.0938\n",
      "Train Epoch: 035 Batch: 00037/00094 | Loss: 626.0936 | CE: 0.2010 | KD: 1059.9547\n",
      "Train Epoch: 035 Batch: 00038/00094 | Loss: 626.2661 | CE: 0.2960 | KD: 1060.0859\n",
      "Train Epoch: 035 Batch: 00039/00094 | Loss: 626.2621 | CE: 0.3174 | KD: 1060.0427\n",
      "Train Epoch: 035 Batch: 00040/00094 | Loss: 626.2290 | CE: 0.2947 | KD: 1060.0253\n",
      "Train Epoch: 035 Batch: 00041/00094 | Loss: 626.3169 | CE: 0.3431 | KD: 1060.0922\n",
      "Train Epoch: 035 Batch: 00042/00094 | Loss: 626.1727 | CE: 0.2092 | KD: 1060.0748\n",
      "Train Epoch: 035 Batch: 00043/00094 | Loss: 626.1790 | CE: 0.2516 | KD: 1060.0137\n",
      "Train Epoch: 035 Batch: 00044/00094 | Loss: 626.1660 | CE: 0.2436 | KD: 1060.0051\n",
      "Train Epoch: 035 Batch: 00045/00094 | Loss: 626.2192 | CE: 0.2710 | KD: 1060.0488\n",
      "Train Epoch: 035 Batch: 00046/00094 | Loss: 626.2592 | CE: 0.3044 | KD: 1060.0599\n",
      "Train Epoch: 035 Batch: 00047/00094 | Loss: 626.2139 | CE: 0.2934 | KD: 1060.0020\n",
      "Train Epoch: 035 Batch: 00048/00094 | Loss: 626.2672 | CE: 0.3104 | KD: 1060.0632\n",
      "Train Epoch: 035 Batch: 00049/00094 | Loss: 626.1967 | CE: 0.2317 | KD: 1060.0771\n",
      "Train Epoch: 035 Batch: 00050/00094 | Loss: 626.1926 | CE: 0.2742 | KD: 1059.9984\n",
      "Train Epoch: 035 Batch: 00051/00094 | Loss: 626.1138 | CE: 0.1914 | KD: 1060.0050\n",
      "Train Epoch: 035 Batch: 00052/00094 | Loss: 626.1832 | CE: 0.2422 | KD: 1060.0366\n",
      "Train Epoch: 035 Batch: 00053/00094 | Loss: 626.2041 | CE: 0.2422 | KD: 1060.0720\n",
      "Train Epoch: 035 Batch: 00054/00094 | Loss: 626.1207 | CE: 0.2432 | KD: 1059.9292\n",
      "Train Epoch: 035 Batch: 00055/00094 | Loss: 626.1699 | CE: 0.2518 | KD: 1059.9978\n",
      "Train Epoch: 035 Batch: 00056/00094 | Loss: 626.2971 | CE: 0.2799 | KD: 1060.1656\n",
      "Train Epoch: 035 Batch: 00057/00094 | Loss: 626.1912 | CE: 0.2643 | KD: 1060.0126\n",
      "Train Epoch: 035 Batch: 00058/00094 | Loss: 626.2369 | CE: 0.2828 | KD: 1060.0587\n",
      "Train Epoch: 035 Batch: 00059/00094 | Loss: 626.1352 | CE: 0.2345 | KD: 1059.9684\n",
      "Train Epoch: 035 Batch: 00060/00094 | Loss: 626.4235 | CE: 0.4415 | KD: 1060.1060\n",
      "Train Epoch: 035 Batch: 00061/00094 | Loss: 626.3229 | CE: 0.3181 | KD: 1060.1447\n",
      "Train Epoch: 035 Batch: 00062/00094 | Loss: 626.1371 | CE: 0.2170 | KD: 1060.0013\n",
      "Train Epoch: 035 Batch: 00063/00094 | Loss: 626.2740 | CE: 0.2764 | KD: 1060.1324\n",
      "Train Epoch: 035 Batch: 00064/00094 | Loss: 626.1266 | CE: 0.2152 | KD: 1059.9865\n",
      "Train Epoch: 035 Batch: 00065/00094 | Loss: 626.2551 | CE: 0.2948 | KD: 1060.0692\n",
      "Train Epoch: 035 Batch: 00066/00094 | Loss: 626.1976 | CE: 0.2716 | KD: 1060.0114\n",
      "Train Epoch: 035 Batch: 00067/00094 | Loss: 626.1071 | CE: 0.1929 | KD: 1059.9912\n",
      "Train Epoch: 035 Batch: 00068/00094 | Loss: 626.1614 | CE: 0.2338 | KD: 1060.0140\n",
      "Train Epoch: 035 Batch: 00069/00094 | Loss: 626.1163 | CE: 0.2136 | KD: 1059.9718\n",
      "Train Epoch: 035 Batch: 00070/00094 | Loss: 626.1887 | CE: 0.2520 | KD: 1060.0293\n",
      "Train Epoch: 035 Batch: 00071/00094 | Loss: 626.1624 | CE: 0.2053 | KD: 1060.0637\n",
      "Train Epoch: 035 Batch: 00072/00094 | Loss: 626.1730 | CE: 0.2355 | KD: 1060.0306\n",
      "Train Epoch: 035 Batch: 00073/00094 | Loss: 626.3043 | CE: 0.3228 | KD: 1060.1052\n",
      "Train Epoch: 035 Batch: 00074/00094 | Loss: 626.2644 | CE: 0.2938 | KD: 1060.0867\n",
      "Train Epoch: 035 Batch: 00075/00094 | Loss: 626.2070 | CE: 0.2654 | KD: 1060.0377\n",
      "Train Epoch: 035 Batch: 00076/00094 | Loss: 626.1827 | CE: 0.2672 | KD: 1059.9934\n",
      "Train Epoch: 035 Batch: 00077/00094 | Loss: 626.1795 | CE: 0.2157 | KD: 1060.0752\n",
      "Train Epoch: 035 Batch: 00078/00094 | Loss: 626.3395 | CE: 0.3303 | KD: 1060.1521\n",
      "Train Epoch: 035 Batch: 00079/00094 | Loss: 626.2158 | CE: 0.2301 | KD: 1060.1123\n",
      "Train Epoch: 035 Batch: 00080/00094 | Loss: 626.1375 | CE: 0.2370 | KD: 1059.9679\n",
      "Train Epoch: 035 Batch: 00081/00094 | Loss: 626.1553 | CE: 0.2470 | KD: 1059.9812\n",
      "Train Epoch: 035 Batch: 00082/00094 | Loss: 626.1631 | CE: 0.2291 | KD: 1060.0247\n",
      "Train Epoch: 035 Batch: 00083/00094 | Loss: 626.0797 | CE: 0.1997 | KD: 1059.9331\n",
      "Train Epoch: 035 Batch: 00084/00094 | Loss: 626.1588 | CE: 0.2256 | KD: 1060.0233\n",
      "Train Epoch: 035 Batch: 00085/00094 | Loss: 626.1306 | CE: 0.2080 | KD: 1060.0054\n",
      "Train Epoch: 035 Batch: 00086/00094 | Loss: 626.1373 | CE: 0.1960 | KD: 1060.0370\n",
      "Train Epoch: 035 Batch: 00087/00094 | Loss: 626.1857 | CE: 0.2327 | KD: 1060.0569\n",
      "Train Epoch: 035 Batch: 00088/00094 | Loss: 626.2183 | CE: 0.2499 | KD: 1060.0831\n",
      "Train Epoch: 035 Batch: 00089/00094 | Loss: 626.2486 | CE: 0.2918 | KD: 1060.0634\n",
      "Train Epoch: 035 Batch: 00090/00094 | Loss: 626.2304 | CE: 0.3148 | KD: 1059.9935\n",
      "Train Epoch: 035 Batch: 00091/00094 | Loss: 626.3332 | CE: 0.2979 | KD: 1060.1964\n",
      "Train Epoch: 035 Batch: 00092/00094 | Loss: 626.1239 | CE: 0.2147 | KD: 1059.9828\n",
      "Train Epoch: 035 Batch: 00093/00094 | Loss: 626.1931 | CE: 0.2710 | KD: 1060.0046\n",
      "Train Epoch: 035 Batch: 00094/00094 | Loss: 626.2180 | CE: 0.2590 | KD: 1060.0671\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2663 | acc:91.6500\n",
      "[VAL Acc] Target: 91.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1103 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9788 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9938 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7329 | acc:58.9734\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.97%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8439 | acc:56.6543\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 56.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5225 | acc:74.0987\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9698 | acc:55.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4469 | acc:76.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.80%\n",
      "[VAL Acc] Avg 62.42%\n",
      "Train Epoch: 036 Batch: 00001/00094 | Loss: 626.0761 | CE: 0.1835 | KD: 1059.9546\n",
      "Train Epoch: 036 Batch: 00002/00094 | Loss: 626.1171 | CE: 0.2260 | KD: 1059.9520\n",
      "Train Epoch: 036 Batch: 00003/00094 | Loss: 626.2247 | CE: 0.2859 | KD: 1060.0328\n",
      "Train Epoch: 036 Batch: 00004/00094 | Loss: 626.2208 | CE: 0.2647 | KD: 1060.0623\n",
      "Train Epoch: 036 Batch: 00005/00094 | Loss: 626.1834 | CE: 0.2374 | KD: 1060.0452\n",
      "Train Epoch: 036 Batch: 00006/00094 | Loss: 626.1467 | CE: 0.2014 | KD: 1060.0441\n",
      "Train Epoch: 036 Batch: 00007/00094 | Loss: 626.1671 | CE: 0.2123 | KD: 1060.0598\n",
      "Train Epoch: 036 Batch: 00008/00094 | Loss: 626.2886 | CE: 0.3514 | KD: 1060.0302\n",
      "Train Epoch: 036 Batch: 00009/00094 | Loss: 626.1773 | CE: 0.2187 | KD: 1060.0665\n",
      "Train Epoch: 036 Batch: 00010/00094 | Loss: 626.2179 | CE: 0.2440 | KD: 1060.0924\n",
      "Train Epoch: 036 Batch: 00011/00094 | Loss: 626.3121 | CE: 0.3172 | KD: 1060.1281\n",
      "Train Epoch: 036 Batch: 00012/00094 | Loss: 626.2740 | CE: 0.2481 | KD: 1060.1803\n",
      "Train Epoch: 036 Batch: 00013/00094 | Loss: 626.2495 | CE: 0.2807 | KD: 1060.0839\n",
      "Train Epoch: 036 Batch: 00014/00094 | Loss: 626.2563 | CE: 0.2899 | KD: 1060.0796\n",
      "Train Epoch: 036 Batch: 00015/00094 | Loss: 626.1859 | CE: 0.2604 | KD: 1060.0104\n",
      "Train Epoch: 036 Batch: 00016/00094 | Loss: 626.1449 | CE: 0.2026 | KD: 1060.0388\n",
      "Train Epoch: 036 Batch: 00017/00094 | Loss: 626.1859 | CE: 0.2626 | KD: 1060.0065\n",
      "Train Epoch: 036 Batch: 00018/00094 | Loss: 626.2306 | CE: 0.2504 | KD: 1060.1029\n",
      "Train Epoch: 036 Batch: 00019/00094 | Loss: 626.1929 | CE: 0.2158 | KD: 1060.0977\n",
      "Train Epoch: 036 Batch: 00020/00094 | Loss: 626.2708 | CE: 0.3058 | KD: 1060.0773\n",
      "Train Epoch: 036 Batch: 00021/00094 | Loss: 626.2858 | CE: 0.3239 | KD: 1060.0721\n",
      "Train Epoch: 036 Batch: 00022/00094 | Loss: 626.1570 | CE: 0.2318 | KD: 1060.0099\n",
      "Train Epoch: 036 Batch: 00023/00094 | Loss: 626.2399 | CE: 0.3150 | KD: 1060.0094\n",
      "Train Epoch: 036 Batch: 00024/00094 | Loss: 626.1772 | CE: 0.2302 | KD: 1060.0469\n",
      "Train Epoch: 036 Batch: 00025/00094 | Loss: 626.2417 | CE: 0.2827 | KD: 1060.0670\n",
      "Train Epoch: 036 Batch: 00026/00094 | Loss: 626.1759 | CE: 0.2108 | KD: 1060.0774\n",
      "Train Epoch: 036 Batch: 00027/00094 | Loss: 626.1315 | CE: 0.1996 | KD: 1060.0212\n",
      "Train Epoch: 036 Batch: 00028/00094 | Loss: 626.1150 | CE: 0.2004 | KD: 1059.9918\n",
      "Train Epoch: 036 Batch: 00029/00094 | Loss: 626.2032 | CE: 0.2438 | KD: 1060.0677\n",
      "Train Epoch: 036 Batch: 00030/00094 | Loss: 626.0736 | CE: 0.1816 | KD: 1059.9536\n",
      "Train Epoch: 036 Batch: 00031/00094 | Loss: 626.2291 | CE: 0.3037 | KD: 1060.0103\n",
      "Train Epoch: 036 Batch: 00032/00094 | Loss: 626.0758 | CE: 0.1711 | KD: 1059.9752\n",
      "Train Epoch: 036 Batch: 00033/00094 | Loss: 626.2426 | CE: 0.2540 | KD: 1060.1172\n",
      "Train Epoch: 036 Batch: 00034/00094 | Loss: 626.3149 | CE: 0.3695 | KD: 1060.0442\n",
      "Train Epoch: 036 Batch: 00035/00094 | Loss: 626.2013 | CE: 0.2673 | KD: 1060.0248\n",
      "Train Epoch: 036 Batch: 00036/00094 | Loss: 626.1944 | CE: 0.2512 | KD: 1060.0403\n",
      "Train Epoch: 036 Batch: 00037/00094 | Loss: 626.2133 | CE: 0.2195 | KD: 1060.1259\n",
      "Train Epoch: 036 Batch: 00038/00094 | Loss: 626.1968 | CE: 0.2417 | KD: 1060.0605\n",
      "Train Epoch: 036 Batch: 00039/00094 | Loss: 626.2249 | CE: 0.2759 | KD: 1060.0500\n",
      "Train Epoch: 036 Batch: 00040/00094 | Loss: 626.1303 | CE: 0.2125 | KD: 1059.9974\n",
      "Train Epoch: 036 Batch: 00041/00094 | Loss: 626.1607 | CE: 0.2598 | KD: 1059.9688\n",
      "Train Epoch: 036 Batch: 00042/00094 | Loss: 626.1953 | CE: 0.2529 | KD: 1060.0388\n",
      "Train Epoch: 036 Batch: 00043/00094 | Loss: 626.1818 | CE: 0.2363 | KD: 1060.0443\n",
      "Train Epoch: 036 Batch: 00044/00094 | Loss: 626.1982 | CE: 0.2784 | KD: 1060.0006\n",
      "Train Epoch: 036 Batch: 00045/00094 | Loss: 626.2302 | CE: 0.2739 | KD: 1060.0625\n",
      "Train Epoch: 036 Batch: 00046/00094 | Loss: 626.1957 | CE: 0.2380 | KD: 1060.0649\n",
      "Train Epoch: 036 Batch: 00047/00094 | Loss: 626.0667 | CE: 0.1995 | KD: 1059.9116\n",
      "Train Epoch: 036 Batch: 00048/00094 | Loss: 626.1576 | CE: 0.2053 | KD: 1060.0558\n",
      "Train Epoch: 036 Batch: 00049/00094 | Loss: 626.2123 | CE: 0.2332 | KD: 1060.1012\n",
      "Train Epoch: 036 Batch: 00050/00094 | Loss: 626.1140 | CE: 0.1864 | KD: 1060.0139\n",
      "Train Epoch: 036 Batch: 00051/00094 | Loss: 626.1071 | CE: 0.1989 | KD: 1059.9810\n",
      "Train Epoch: 036 Batch: 00052/00094 | Loss: 626.1266 | CE: 0.2141 | KD: 1059.9884\n",
      "Train Epoch: 036 Batch: 00053/00094 | Loss: 626.1835 | CE: 0.2022 | KD: 1060.1049\n",
      "Train Epoch: 036 Batch: 00054/00094 | Loss: 626.1740 | CE: 0.1977 | KD: 1060.0964\n",
      "Train Epoch: 036 Batch: 00055/00094 | Loss: 626.2695 | CE: 0.3133 | KD: 1060.0624\n",
      "Train Epoch: 036 Batch: 00056/00094 | Loss: 626.1280 | CE: 0.2094 | KD: 1059.9988\n",
      "Train Epoch: 036 Batch: 00057/00094 | Loss: 626.1624 | CE: 0.2282 | KD: 1060.0250\n",
      "Train Epoch: 036 Batch: 00058/00094 | Loss: 626.1281 | CE: 0.1799 | KD: 1060.0488\n",
      "Train Epoch: 036 Batch: 00059/00094 | Loss: 626.2089 | CE: 0.2785 | KD: 1060.0187\n",
      "Train Epoch: 036 Batch: 00060/00094 | Loss: 626.3445 | CE: 0.3451 | KD: 1060.1356\n",
      "Train Epoch: 036 Batch: 00061/00094 | Loss: 626.1779 | CE: 0.2469 | KD: 1060.0197\n",
      "Train Epoch: 036 Batch: 00062/00094 | Loss: 626.2224 | CE: 0.2713 | KD: 1060.0537\n",
      "Train Epoch: 036 Batch: 00063/00094 | Loss: 626.2344 | CE: 0.2406 | KD: 1060.1261\n",
      "Train Epoch: 036 Batch: 00064/00094 | Loss: 626.2120 | CE: 0.2745 | KD: 1060.0306\n",
      "Train Epoch: 036 Batch: 00065/00094 | Loss: 626.1620 | CE: 0.2392 | KD: 1060.0057\n",
      "Train Epoch: 036 Batch: 00066/00094 | Loss: 626.1882 | CE: 0.2465 | KD: 1060.0378\n",
      "Train Epoch: 036 Batch: 00067/00094 | Loss: 626.2542 | CE: 0.2918 | KD: 1060.0729\n",
      "Train Epoch: 036 Batch: 00068/00094 | Loss: 626.2323 | CE: 0.2924 | KD: 1060.0348\n",
      "Train Epoch: 036 Batch: 00069/00094 | Loss: 626.1380 | CE: 0.2102 | KD: 1060.0143\n",
      "Train Epoch: 036 Batch: 00070/00094 | Loss: 626.1278 | CE: 0.2032 | KD: 1060.0088\n",
      "Train Epoch: 036 Batch: 00071/00094 | Loss: 626.1199 | CE: 0.2112 | KD: 1059.9818\n",
      "Train Epoch: 036 Batch: 00072/00094 | Loss: 626.2192 | CE: 0.2310 | KD: 1060.1165\n",
      "Train Epoch: 036 Batch: 00073/00094 | Loss: 626.1953 | CE: 0.2737 | KD: 1060.0038\n",
      "Train Epoch: 036 Batch: 00074/00094 | Loss: 626.1393 | CE: 0.2118 | KD: 1060.0137\n",
      "Train Epoch: 036 Batch: 00075/00094 | Loss: 626.2200 | CE: 0.2492 | KD: 1060.0870\n",
      "Train Epoch: 036 Batch: 00076/00094 | Loss: 626.1189 | CE: 0.2204 | KD: 1059.9647\n",
      "Train Epoch: 036 Batch: 00077/00094 | Loss: 626.1396 | CE: 0.2354 | KD: 1059.9742\n",
      "Train Epoch: 036 Batch: 00078/00094 | Loss: 626.2973 | CE: 0.3116 | KD: 1060.1123\n",
      "Train Epoch: 036 Batch: 00079/00094 | Loss: 626.2245 | CE: 0.2299 | KD: 1060.1274\n",
      "Train Epoch: 036 Batch: 00080/00094 | Loss: 626.3911 | CE: 0.4233 | KD: 1060.0819\n",
      "Train Epoch: 036 Batch: 00081/00094 | Loss: 626.1481 | CE: 0.2168 | KD: 1060.0201\n",
      "Train Epoch: 036 Batch: 00082/00094 | Loss: 626.1801 | CE: 0.2357 | KD: 1060.0424\n",
      "Train Epoch: 036 Batch: 00083/00094 | Loss: 626.1769 | CE: 0.2286 | KD: 1060.0490\n",
      "Train Epoch: 036 Batch: 00084/00094 | Loss: 626.2348 | CE: 0.2498 | KD: 1060.1111\n",
      "Train Epoch: 036 Batch: 00085/00094 | Loss: 626.2833 | CE: 0.3271 | KD: 1060.0623\n",
      "Train Epoch: 036 Batch: 00086/00094 | Loss: 626.3005 | CE: 0.2953 | KD: 1060.1454\n",
      "Train Epoch: 036 Batch: 00087/00094 | Loss: 626.1927 | CE: 0.2325 | KD: 1060.0691\n",
      "Train Epoch: 036 Batch: 00088/00094 | Loss: 626.1520 | CE: 0.2240 | KD: 1060.0148\n",
      "Train Epoch: 036 Batch: 00089/00094 | Loss: 626.2368 | CE: 0.2585 | KD: 1060.0999\n",
      "Train Epoch: 036 Batch: 00090/00094 | Loss: 626.3086 | CE: 0.3344 | KD: 1060.0928\n",
      "Train Epoch: 036 Batch: 00091/00094 | Loss: 626.1931 | CE: 0.2661 | KD: 1060.0129\n",
      "Train Epoch: 036 Batch: 00092/00094 | Loss: 626.1296 | CE: 0.2129 | KD: 1059.9955\n",
      "Train Epoch: 036 Batch: 00093/00094 | Loss: 626.2232 | CE: 0.2863 | KD: 1060.0297\n",
      "Train Epoch: 036 Batch: 00094/00094 | Loss: 626.1528 | CE: 0.2543 | KD: 1059.9647\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2817 | acc:90.4500\n",
      "[VAL Acc] Target: 90.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0892 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9494 | acc:48.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9896 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6718 | acc:61.8730\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9365 | acc:54.4362\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4961 | acc:75.9796\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.98%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9735 | acc:56.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4553 | acc:76.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.00%\n",
      "[VAL Acc] Avg 62.22%\n",
      "Train Epoch: 037 Batch: 00001/00094 | Loss: 626.1520 | CE: 0.2400 | KD: 1059.9874\n",
      "Train Epoch: 037 Batch: 00002/00094 | Loss: 626.2651 | CE: 0.3135 | KD: 1060.0546\n",
      "Train Epoch: 037 Batch: 00003/00094 | Loss: 626.2213 | CE: 0.2780 | KD: 1060.0405\n",
      "Train Epoch: 037 Batch: 00004/00094 | Loss: 626.2178 | CE: 0.2442 | KD: 1060.0919\n",
      "Train Epoch: 037 Batch: 00005/00094 | Loss: 626.2305 | CE: 0.2827 | KD: 1060.0482\n",
      "Train Epoch: 037 Batch: 00006/00094 | Loss: 626.1948 | CE: 0.2507 | KD: 1060.0419\n",
      "Train Epoch: 037 Batch: 00007/00094 | Loss: 626.0503 | CE: 0.1841 | KD: 1059.9099\n",
      "Train Epoch: 037 Batch: 00008/00094 | Loss: 626.0928 | CE: 0.1872 | KD: 1059.9767\n",
      "Train Epoch: 037 Batch: 00009/00094 | Loss: 626.2362 | CE: 0.2901 | KD: 1060.0454\n",
      "Train Epoch: 037 Batch: 00010/00094 | Loss: 626.1533 | CE: 0.2498 | KD: 1059.9730\n",
      "Train Epoch: 037 Batch: 00011/00094 | Loss: 626.2230 | CE: 0.2853 | KD: 1060.0310\n",
      "Train Epoch: 037 Batch: 00012/00094 | Loss: 626.2314 | CE: 0.2804 | KD: 1060.0536\n",
      "Train Epoch: 037 Batch: 00013/00094 | Loss: 626.0894 | CE: 0.1984 | KD: 1059.9520\n",
      "Train Epoch: 037 Batch: 00014/00094 | Loss: 626.2896 | CE: 0.3020 | KD: 1060.1155\n",
      "Train Epoch: 037 Batch: 00015/00094 | Loss: 626.1365 | CE: 0.2247 | KD: 1059.9871\n",
      "Train Epoch: 037 Batch: 00016/00094 | Loss: 626.1247 | CE: 0.1897 | KD: 1060.0265\n",
      "Train Epoch: 037 Batch: 00017/00094 | Loss: 626.2812 | CE: 0.2663 | KD: 1060.1617\n",
      "Train Epoch: 037 Batch: 00018/00094 | Loss: 626.2409 | CE: 0.2745 | KD: 1060.0796\n",
      "Train Epoch: 037 Batch: 00019/00094 | Loss: 626.0952 | CE: 0.1966 | KD: 1059.9647\n",
      "Train Epoch: 037 Batch: 00020/00094 | Loss: 626.2030 | CE: 0.2705 | KD: 1060.0222\n",
      "Train Epoch: 037 Batch: 00021/00094 | Loss: 626.1995 | CE: 0.2189 | KD: 1060.1036\n",
      "Train Epoch: 037 Batch: 00022/00094 | Loss: 626.3038 | CE: 0.3634 | KD: 1060.0356\n",
      "Train Epoch: 037 Batch: 00023/00094 | Loss: 626.1491 | CE: 0.2257 | KD: 1060.0068\n",
      "Train Epoch: 037 Batch: 00024/00094 | Loss: 626.2124 | CE: 0.2397 | KD: 1060.0903\n",
      "Train Epoch: 037 Batch: 00025/00094 | Loss: 626.2952 | CE: 0.3031 | KD: 1060.1230\n",
      "Train Epoch: 037 Batch: 00026/00094 | Loss: 626.1257 | CE: 0.2008 | KD: 1060.0094\n",
      "Train Epoch: 037 Batch: 00027/00094 | Loss: 626.1859 | CE: 0.2398 | KD: 1060.0452\n",
      "Train Epoch: 037 Batch: 00028/00094 | Loss: 626.2463 | CE: 0.3110 | KD: 1060.0269\n",
      "Train Epoch: 037 Batch: 00029/00094 | Loss: 626.2642 | CE: 0.3233 | KD: 1060.0364\n",
      "Train Epoch: 037 Batch: 00030/00094 | Loss: 626.0859 | CE: 0.1973 | KD: 1059.9479\n",
      "Train Epoch: 037 Batch: 00031/00094 | Loss: 626.1692 | CE: 0.2169 | KD: 1060.0558\n",
      "Train Epoch: 037 Batch: 00032/00094 | Loss: 626.2906 | CE: 0.3459 | KD: 1060.0431\n",
      "Train Epoch: 037 Batch: 00033/00094 | Loss: 626.2100 | CE: 0.2768 | KD: 1060.0234\n",
      "Train Epoch: 037 Batch: 00034/00094 | Loss: 626.2535 | CE: 0.2955 | KD: 1060.0654\n",
      "Train Epoch: 037 Batch: 00035/00094 | Loss: 626.2184 | CE: 0.2605 | KD: 1060.0654\n",
      "Train Epoch: 037 Batch: 00036/00094 | Loss: 626.1738 | CE: 0.2335 | KD: 1060.0354\n",
      "Train Epoch: 037 Batch: 00037/00094 | Loss: 626.2440 | CE: 0.2637 | KD: 1060.1030\n",
      "Train Epoch: 037 Batch: 00038/00094 | Loss: 626.2475 | CE: 0.2833 | KD: 1060.0759\n",
      "Train Epoch: 037 Batch: 00039/00094 | Loss: 626.2894 | CE: 0.3322 | KD: 1060.0640\n",
      "Train Epoch: 037 Batch: 00040/00094 | Loss: 626.1644 | CE: 0.2354 | KD: 1060.0164\n",
      "Train Epoch: 037 Batch: 00041/00094 | Loss: 626.1578 | CE: 0.2251 | KD: 1060.0226\n",
      "Train Epoch: 037 Batch: 00042/00094 | Loss: 626.1554 | CE: 0.1864 | KD: 1060.0841\n",
      "Train Epoch: 037 Batch: 00043/00094 | Loss: 626.1519 | CE: 0.2237 | KD: 1060.0149\n",
      "Train Epoch: 037 Batch: 00044/00094 | Loss: 626.2743 | CE: 0.2797 | KD: 1060.1273\n",
      "Train Epoch: 037 Batch: 00045/00094 | Loss: 626.1445 | CE: 0.2284 | KD: 1059.9946\n",
      "Train Epoch: 037 Batch: 00046/00094 | Loss: 626.2280 | CE: 0.2406 | KD: 1060.1154\n",
      "Train Epoch: 037 Batch: 00047/00094 | Loss: 626.1669 | CE: 0.2352 | KD: 1060.0210\n",
      "Train Epoch: 037 Batch: 00048/00094 | Loss: 626.2642 | CE: 0.3017 | KD: 1060.0730\n",
      "Train Epoch: 037 Batch: 00049/00094 | Loss: 626.2310 | CE: 0.2893 | KD: 1060.0378\n",
      "Train Epoch: 037 Batch: 00050/00094 | Loss: 626.2502 | CE: 0.2117 | KD: 1060.2018\n",
      "Train Epoch: 037 Batch: 00051/00094 | Loss: 626.2482 | CE: 0.2421 | KD: 1060.1467\n",
      "Train Epoch: 037 Batch: 00052/00094 | Loss: 626.2883 | CE: 0.3128 | KD: 1060.0951\n",
      "Train Epoch: 037 Batch: 00053/00094 | Loss: 626.2390 | CE: 0.3006 | KD: 1060.0321\n",
      "Train Epoch: 037 Batch: 00054/00094 | Loss: 626.1266 | CE: 0.2222 | KD: 1059.9746\n",
      "Train Epoch: 037 Batch: 00055/00094 | Loss: 626.1989 | CE: 0.2719 | KD: 1060.0129\n",
      "Train Epoch: 037 Batch: 00056/00094 | Loss: 626.2877 | CE: 0.3029 | KD: 1060.1108\n",
      "Train Epoch: 037 Batch: 00057/00094 | Loss: 626.1557 | CE: 0.1994 | KD: 1060.0624\n",
      "Train Epoch: 037 Batch: 00058/00094 | Loss: 626.2449 | CE: 0.2803 | KD: 1060.0767\n",
      "Train Epoch: 037 Batch: 00059/00094 | Loss: 626.1683 | CE: 0.2601 | KD: 1059.9811\n",
      "Train Epoch: 037 Batch: 00060/00094 | Loss: 626.1151 | CE: 0.2244 | KD: 1059.9515\n",
      "Train Epoch: 037 Batch: 00061/00094 | Loss: 626.2095 | CE: 0.2537 | KD: 1060.0616\n",
      "Train Epoch: 037 Batch: 00062/00094 | Loss: 626.2084 | CE: 0.2541 | KD: 1060.0592\n",
      "Train Epoch: 037 Batch: 00063/00094 | Loss: 626.1921 | CE: 0.2560 | KD: 1060.0282\n",
      "Train Epoch: 037 Batch: 00064/00094 | Loss: 626.1349 | CE: 0.2187 | KD: 1059.9946\n",
      "Train Epoch: 037 Batch: 00065/00094 | Loss: 626.1395 | CE: 0.2039 | KD: 1060.0275\n",
      "Train Epoch: 037 Batch: 00066/00094 | Loss: 626.1657 | CE: 0.2099 | KD: 1060.0616\n",
      "Train Epoch: 037 Batch: 00067/00094 | Loss: 626.2513 | CE: 0.2659 | KD: 1060.1119\n",
      "Train Epoch: 037 Batch: 00068/00094 | Loss: 626.2297 | CE: 0.2442 | KD: 1060.1121\n",
      "Train Epoch: 037 Batch: 00069/00094 | Loss: 626.1821 | CE: 0.2382 | KD: 1060.0415\n",
      "Train Epoch: 037 Batch: 00070/00094 | Loss: 626.1701 | CE: 0.2463 | KD: 1060.0074\n",
      "Train Epoch: 037 Batch: 00071/00094 | Loss: 626.1417 | CE: 0.2154 | KD: 1060.0118\n",
      "Train Epoch: 037 Batch: 00072/00094 | Loss: 626.1748 | CE: 0.2264 | KD: 1060.0492\n",
      "Train Epoch: 037 Batch: 00073/00094 | Loss: 626.2018 | CE: 0.2741 | KD: 1060.0140\n",
      "Train Epoch: 037 Batch: 00074/00094 | Loss: 626.3995 | CE: 0.3771 | KD: 1060.1743\n",
      "Train Epoch: 037 Batch: 00075/00094 | Loss: 626.2878 | CE: 0.2835 | KD: 1060.1438\n",
      "Train Epoch: 037 Batch: 00076/00094 | Loss: 626.1394 | CE: 0.1970 | KD: 1060.0389\n",
      "Train Epoch: 037 Batch: 00077/00094 | Loss: 626.2500 | CE: 0.2793 | KD: 1060.0869\n",
      "Train Epoch: 037 Batch: 00078/00094 | Loss: 626.1595 | CE: 0.2222 | KD: 1060.0305\n",
      "Train Epoch: 037 Batch: 00079/00094 | Loss: 626.4039 | CE: 0.3946 | KD: 1060.1522\n",
      "Train Epoch: 037 Batch: 00080/00094 | Loss: 626.1921 | CE: 0.2502 | KD: 1060.0382\n",
      "Train Epoch: 037 Batch: 00081/00094 | Loss: 626.2160 | CE: 0.2564 | KD: 1060.0682\n",
      "Train Epoch: 037 Batch: 00082/00094 | Loss: 626.2833 | CE: 0.3098 | KD: 1060.0918\n",
      "Train Epoch: 037 Batch: 00083/00094 | Loss: 626.1767 | CE: 0.2716 | KD: 1059.9757\n",
      "Train Epoch: 037 Batch: 00084/00094 | Loss: 626.2422 | CE: 0.2630 | KD: 1060.1014\n",
      "Train Epoch: 037 Batch: 00085/00094 | Loss: 626.1331 | CE: 0.2065 | KD: 1060.0122\n",
      "Train Epoch: 037 Batch: 00086/00094 | Loss: 626.2137 | CE: 0.2491 | KD: 1060.0765\n",
      "Train Epoch: 037 Batch: 00087/00094 | Loss: 626.1525 | CE: 0.2409 | KD: 1059.9867\n",
      "Train Epoch: 037 Batch: 00088/00094 | Loss: 626.0526 | CE: 0.1912 | KD: 1059.9017\n",
      "Train Epoch: 037 Batch: 00089/00094 | Loss: 626.2209 | CE: 0.2686 | KD: 1060.0557\n",
      "Train Epoch: 037 Batch: 00090/00094 | Loss: 626.1096 | CE: 0.1862 | KD: 1060.0067\n",
      "Train Epoch: 037 Batch: 00091/00094 | Loss: 626.2885 | CE: 0.3183 | KD: 1060.0859\n",
      "Train Epoch: 037 Batch: 00092/00094 | Loss: 626.1288 | CE: 0.2316 | KD: 1059.9625\n",
      "Train Epoch: 037 Batch: 00093/00094 | Loss: 626.1095 | CE: 0.2035 | KD: 1059.9773\n",
      "Train Epoch: 037 Batch: 00094/00094 | Loss: 626.2778 | CE: 0.2562 | KD: 1060.1730\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2698 | acc:90.9500\n",
      "[VAL Acc] Target: 90.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1120 | acc:50.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9916 | acc:47.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0188 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6613 | acc:62.5392\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9048 | acc:54.3438\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4896 | acc:76.8809\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0034 | acc:54.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4533 | acc:76.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.45%\n",
      "[VAL Acc] Avg 62.47%\n",
      "Train Epoch: 038 Batch: 00001/00094 | Loss: 626.2009 | CE: 0.2680 | KD: 1060.0229\n",
      "Train Epoch: 038 Batch: 00002/00094 | Loss: 626.0989 | CE: 0.1928 | KD: 1059.9774\n",
      "Train Epoch: 038 Batch: 00003/00094 | Loss: 626.2236 | CE: 0.2474 | KD: 1060.0962\n",
      "Train Epoch: 038 Batch: 00004/00094 | Loss: 626.1070 | CE: 0.1653 | KD: 1060.0377\n",
      "Train Epoch: 038 Batch: 00005/00094 | Loss: 626.1503 | CE: 0.2658 | KD: 1059.9409\n",
      "Train Epoch: 038 Batch: 00006/00094 | Loss: 626.1721 | CE: 0.2555 | KD: 1059.9952\n",
      "Train Epoch: 038 Batch: 00007/00094 | Loss: 626.1332 | CE: 0.1929 | KD: 1060.0354\n",
      "Train Epoch: 038 Batch: 00008/00094 | Loss: 626.2474 | CE: 0.2875 | KD: 1060.0686\n",
      "Train Epoch: 038 Batch: 00009/00094 | Loss: 626.2180 | CE: 0.2531 | KD: 1060.0770\n",
      "Train Epoch: 038 Batch: 00010/00094 | Loss: 626.1253 | CE: 0.1906 | KD: 1060.0259\n",
      "Train Epoch: 038 Batch: 00011/00094 | Loss: 626.1152 | CE: 0.2122 | KD: 1059.9722\n",
      "Train Epoch: 038 Batch: 00012/00094 | Loss: 626.2595 | CE: 0.2994 | KD: 1060.0688\n",
      "Train Epoch: 038 Batch: 00013/00094 | Loss: 626.1086 | CE: 0.1974 | KD: 1059.9861\n",
      "Train Epoch: 038 Batch: 00014/00094 | Loss: 626.1777 | CE: 0.2179 | KD: 1060.0686\n",
      "Train Epoch: 038 Batch: 00015/00094 | Loss: 626.2142 | CE: 0.2686 | KD: 1060.0444\n",
      "Train Epoch: 038 Batch: 00016/00094 | Loss: 626.1900 | CE: 0.2539 | KD: 1060.0283\n",
      "Train Epoch: 038 Batch: 00017/00094 | Loss: 626.1149 | CE: 0.2017 | KD: 1059.9895\n",
      "Train Epoch: 038 Batch: 00018/00094 | Loss: 626.1562 | CE: 0.2538 | KD: 1059.9713\n",
      "Train Epoch: 038 Batch: 00019/00094 | Loss: 626.1362 | CE: 0.2186 | KD: 1059.9969\n",
      "Train Epoch: 038 Batch: 00020/00094 | Loss: 626.1542 | CE: 0.1870 | KD: 1060.0811\n",
      "Train Epoch: 038 Batch: 00021/00094 | Loss: 626.1917 | CE: 0.2338 | KD: 1060.0652\n",
      "Train Epoch: 038 Batch: 00022/00094 | Loss: 626.2034 | CE: 0.2371 | KD: 1060.0795\n",
      "Train Epoch: 038 Batch: 00023/00094 | Loss: 626.1555 | CE: 0.2351 | KD: 1060.0016\n",
      "Train Epoch: 038 Batch: 00024/00094 | Loss: 626.1824 | CE: 0.2683 | KD: 1059.9911\n",
      "Train Epoch: 038 Batch: 00025/00094 | Loss: 626.1420 | CE: 0.2270 | KD: 1059.9927\n",
      "Train Epoch: 038 Batch: 00026/00094 | Loss: 626.2393 | CE: 0.2348 | KD: 1060.1440\n",
      "Train Epoch: 038 Batch: 00027/00094 | Loss: 626.1750 | CE: 0.2342 | KD: 1060.0363\n",
      "Train Epoch: 038 Batch: 00028/00094 | Loss: 626.1838 | CE: 0.2422 | KD: 1060.0377\n",
      "Train Epoch: 038 Batch: 00029/00094 | Loss: 626.1567 | CE: 0.2201 | KD: 1060.0293\n",
      "Train Epoch: 038 Batch: 00030/00094 | Loss: 626.1627 | CE: 0.2459 | KD: 1059.9957\n",
      "Train Epoch: 038 Batch: 00031/00094 | Loss: 626.2798 | CE: 0.2602 | KD: 1060.1697\n",
      "Train Epoch: 038 Batch: 00032/00094 | Loss: 626.1819 | CE: 0.2409 | KD: 1060.0367\n",
      "Train Epoch: 038 Batch: 00033/00094 | Loss: 626.2301 | CE: 0.2787 | KD: 1060.0542\n",
      "Train Epoch: 038 Batch: 00034/00094 | Loss: 626.2004 | CE: 0.2835 | KD: 1059.9960\n",
      "Train Epoch: 038 Batch: 00035/00094 | Loss: 626.1653 | CE: 0.2161 | KD: 1060.0504\n",
      "Train Epoch: 038 Batch: 00036/00094 | Loss: 626.2642 | CE: 0.3113 | KD: 1060.0568\n",
      "Train Epoch: 038 Batch: 00037/00094 | Loss: 626.1604 | CE: 0.2587 | KD: 1059.9701\n",
      "Train Epoch: 038 Batch: 00038/00094 | Loss: 626.1393 | CE: 0.2156 | KD: 1060.0072\n",
      "Train Epoch: 038 Batch: 00039/00094 | Loss: 626.1537 | CE: 0.1950 | KD: 1060.0667\n",
      "Train Epoch: 038 Batch: 00040/00094 | Loss: 626.1840 | CE: 0.2450 | KD: 1060.0332\n",
      "Train Epoch: 038 Batch: 00041/00094 | Loss: 626.2109 | CE: 0.2425 | KD: 1060.0829\n",
      "Train Epoch: 038 Batch: 00042/00094 | Loss: 626.1395 | CE: 0.2321 | KD: 1059.9796\n",
      "Train Epoch: 038 Batch: 00043/00094 | Loss: 626.2593 | CE: 0.2552 | KD: 1060.1434\n",
      "Train Epoch: 038 Batch: 00044/00094 | Loss: 626.2485 | CE: 0.3044 | KD: 1060.0419\n",
      "Train Epoch: 038 Batch: 00045/00094 | Loss: 626.2043 | CE: 0.2652 | KD: 1060.0334\n",
      "Train Epoch: 038 Batch: 00046/00094 | Loss: 626.1523 | CE: 0.1985 | KD: 1060.0583\n",
      "Train Epoch: 038 Batch: 00047/00094 | Loss: 626.2440 | CE: 0.2697 | KD: 1060.0929\n",
      "Train Epoch: 038 Batch: 00048/00094 | Loss: 626.1008 | CE: 0.1668 | KD: 1060.0248\n",
      "Train Epoch: 038 Batch: 00049/00094 | Loss: 626.1091 | CE: 0.2087 | KD: 1059.9679\n",
      "Train Epoch: 038 Batch: 00050/00094 | Loss: 626.1755 | CE: 0.2656 | KD: 1059.9840\n",
      "Train Epoch: 038 Batch: 00051/00094 | Loss: 626.2154 | CE: 0.2729 | KD: 1060.0391\n",
      "Train Epoch: 038 Batch: 00052/00094 | Loss: 626.0844 | CE: 0.1995 | KD: 1059.9415\n",
      "Train Epoch: 038 Batch: 00053/00094 | Loss: 626.1907 | CE: 0.2116 | KD: 1060.1012\n",
      "Train Epoch: 038 Batch: 00054/00094 | Loss: 626.1737 | CE: 0.2046 | KD: 1060.0844\n",
      "Train Epoch: 038 Batch: 00055/00094 | Loss: 626.1112 | CE: 0.1852 | KD: 1060.0112\n",
      "Train Epoch: 038 Batch: 00056/00094 | Loss: 626.3317 | CE: 0.3375 | KD: 1060.1266\n",
      "Train Epoch: 038 Batch: 00057/00094 | Loss: 626.3568 | CE: 0.3089 | KD: 1060.2177\n",
      "Train Epoch: 038 Batch: 00058/00094 | Loss: 626.2141 | CE: 0.2447 | KD: 1060.0846\n",
      "Train Epoch: 038 Batch: 00059/00094 | Loss: 626.2054 | CE: 0.2564 | KD: 1060.0502\n",
      "Train Epoch: 038 Batch: 00060/00094 | Loss: 626.2901 | CE: 0.3249 | KD: 1060.0778\n",
      "Train Epoch: 038 Batch: 00061/00094 | Loss: 626.1722 | CE: 0.2403 | KD: 1060.0212\n",
      "Train Epoch: 038 Batch: 00062/00094 | Loss: 626.1061 | CE: 0.1841 | KD: 1060.0045\n",
      "Train Epoch: 038 Batch: 00063/00094 | Loss: 626.2164 | CE: 0.2488 | KD: 1060.0817\n",
      "Train Epoch: 038 Batch: 00064/00094 | Loss: 626.1188 | CE: 0.2244 | KD: 1059.9576\n",
      "Train Epoch: 038 Batch: 00065/00094 | Loss: 626.2352 | CE: 0.2725 | KD: 1060.0732\n",
      "Train Epoch: 038 Batch: 00066/00094 | Loss: 626.2148 | CE: 0.2606 | KD: 1060.0590\n",
      "Train Epoch: 038 Batch: 00067/00094 | Loss: 626.2338 | CE: 0.2627 | KD: 1060.0875\n",
      "Train Epoch: 038 Batch: 00068/00094 | Loss: 626.1834 | CE: 0.2598 | KD: 1060.0072\n",
      "Train Epoch: 038 Batch: 00069/00094 | Loss: 626.2439 | CE: 0.2966 | KD: 1060.0472\n",
      "Train Epoch: 038 Batch: 00070/00094 | Loss: 626.2025 | CE: 0.2616 | KD: 1060.0364\n",
      "Train Epoch: 038 Batch: 00071/00094 | Loss: 626.3091 | CE: 0.3349 | KD: 1060.0928\n",
      "Train Epoch: 038 Batch: 00072/00094 | Loss: 626.2016 | CE: 0.2851 | KD: 1059.9951\n",
      "Train Epoch: 038 Batch: 00073/00094 | Loss: 626.1871 | CE: 0.2726 | KD: 1059.9918\n",
      "Train Epoch: 038 Batch: 00074/00094 | Loss: 626.2060 | CE: 0.2811 | KD: 1060.0093\n",
      "Train Epoch: 038 Batch: 00075/00094 | Loss: 626.2666 | CE: 0.2973 | KD: 1060.0846\n",
      "Train Epoch: 038 Batch: 00076/00094 | Loss: 626.2269 | CE: 0.2846 | KD: 1060.0388\n",
      "Train Epoch: 038 Batch: 00077/00094 | Loss: 626.2847 | CE: 0.2961 | KD: 1060.1172\n",
      "Train Epoch: 038 Batch: 00078/00094 | Loss: 626.1080 | CE: 0.1972 | KD: 1059.9856\n",
      "Train Epoch: 038 Batch: 00079/00094 | Loss: 626.1574 | CE: 0.2404 | KD: 1059.9960\n",
      "Train Epoch: 038 Batch: 00080/00094 | Loss: 626.2023 | CE: 0.2407 | KD: 1060.0715\n",
      "Train Epoch: 038 Batch: 00081/00094 | Loss: 626.2220 | CE: 0.2583 | KD: 1060.0751\n",
      "Train Epoch: 038 Batch: 00082/00094 | Loss: 626.2244 | CE: 0.2636 | KD: 1060.0701\n",
      "Train Epoch: 038 Batch: 00083/00094 | Loss: 626.2208 | CE: 0.2667 | KD: 1060.0588\n",
      "Train Epoch: 038 Batch: 00084/00094 | Loss: 626.0853 | CE: 0.1869 | KD: 1059.9644\n",
      "Train Epoch: 038 Batch: 00085/00094 | Loss: 626.3234 | CE: 0.2936 | KD: 1060.1870\n",
      "Train Epoch: 038 Batch: 00086/00094 | Loss: 626.2291 | CE: 0.2456 | KD: 1060.1086\n",
      "Train Epoch: 038 Batch: 00087/00094 | Loss: 626.2792 | CE: 0.3177 | KD: 1060.0714\n",
      "Train Epoch: 038 Batch: 00088/00094 | Loss: 626.1217 | CE: 0.1849 | KD: 1060.0295\n",
      "Train Epoch: 038 Batch: 00089/00094 | Loss: 626.1953 | CE: 0.2477 | KD: 1060.0479\n",
      "Train Epoch: 038 Batch: 00090/00094 | Loss: 626.3037 | CE: 0.2811 | KD: 1060.1748\n",
      "Train Epoch: 038 Batch: 00091/00094 | Loss: 626.2257 | CE: 0.2690 | KD: 1060.0632\n",
      "Train Epoch: 038 Batch: 00092/00094 | Loss: 626.2311 | CE: 0.2811 | KD: 1060.0519\n",
      "Train Epoch: 038 Batch: 00093/00094 | Loss: 626.1889 | CE: 0.2620 | KD: 1060.0127\n",
      "Train Epoch: 038 Batch: 00094/00094 | Loss: 626.1450 | CE: 0.2131 | KD: 1060.0211\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2614 | acc:91.3500\n",
      "[VAL Acc] Target: 91.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1677 | acc:50.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0222 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0662 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.5679 | acc:70.4545\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 70.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0797 | acc:52.2181\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4167 | acc:81.5439\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 81.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0643 | acc:54.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4890 | acc:73.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.25%\n",
      "[VAL Acc] Avg 63.45%\n",
      "VAL Acc improve from 62.61% to 63.45%\n",
      "Save best model\n",
      "Train Epoch: 039 Batch: 00001/00094 | Loss: 626.1927 | CE: 0.2341 | KD: 1060.0664\n",
      "Train Epoch: 039 Batch: 00002/00094 | Loss: 626.2360 | CE: 0.2761 | KD: 1060.0686\n",
      "Train Epoch: 039 Batch: 00003/00094 | Loss: 626.2056 | CE: 0.2542 | KD: 1060.0542\n",
      "Train Epoch: 039 Batch: 00004/00094 | Loss: 626.1528 | CE: 0.2043 | KD: 1060.0494\n",
      "Train Epoch: 039 Batch: 00005/00094 | Loss: 626.2534 | CE: 0.3041 | KD: 1060.0505\n",
      "Train Epoch: 039 Batch: 00006/00094 | Loss: 626.1204 | CE: 0.1760 | KD: 1060.0424\n",
      "Train Epoch: 039 Batch: 00007/00094 | Loss: 626.3101 | CE: 0.2702 | KD: 1060.2040\n",
      "Train Epoch: 039 Batch: 00008/00094 | Loss: 626.4209 | CE: 0.3988 | KD: 1060.1740\n",
      "Train Epoch: 039 Batch: 00009/00094 | Loss: 626.2388 | CE: 0.2595 | KD: 1060.1014\n",
      "Train Epoch: 039 Batch: 00010/00094 | Loss: 626.1618 | CE: 0.2171 | KD: 1060.0428\n",
      "Train Epoch: 039 Batch: 00011/00094 | Loss: 626.2053 | CE: 0.2579 | KD: 1060.0474\n",
      "Train Epoch: 039 Batch: 00012/00094 | Loss: 626.2698 | CE: 0.2639 | KD: 1060.1464\n",
      "Train Epoch: 039 Batch: 00013/00094 | Loss: 626.1828 | CE: 0.2817 | KD: 1059.9690\n",
      "Train Epoch: 039 Batch: 00014/00094 | Loss: 626.1434 | CE: 0.2107 | KD: 1060.0226\n",
      "Train Epoch: 039 Batch: 00015/00094 | Loss: 626.2665 | CE: 0.2759 | KD: 1060.1206\n",
      "Train Epoch: 039 Batch: 00016/00094 | Loss: 626.2455 | CE: 0.2744 | KD: 1060.0876\n",
      "Train Epoch: 039 Batch: 00017/00094 | Loss: 626.2095 | CE: 0.2870 | KD: 1060.0054\n",
      "Train Epoch: 039 Batch: 00018/00094 | Loss: 626.0959 | CE: 0.1914 | KD: 1059.9749\n",
      "Train Epoch: 039 Batch: 00019/00094 | Loss: 626.2394 | CE: 0.2673 | KD: 1060.0892\n",
      "Train Epoch: 039 Batch: 00020/00094 | Loss: 626.1336 | CE: 0.2100 | KD: 1060.0072\n",
      "Train Epoch: 039 Batch: 00021/00094 | Loss: 626.2117 | CE: 0.2349 | KD: 1060.0972\n",
      "Train Epoch: 039 Batch: 00022/00094 | Loss: 626.1313 | CE: 0.2471 | KD: 1059.9404\n",
      "Train Epoch: 039 Batch: 00023/00094 | Loss: 626.1143 | CE: 0.1795 | KD: 1060.0260\n",
      "Train Epoch: 039 Batch: 00024/00094 | Loss: 626.1013 | CE: 0.1805 | KD: 1060.0023\n",
      "Train Epoch: 039 Batch: 00025/00094 | Loss: 626.2189 | CE: 0.2678 | KD: 1060.0538\n",
      "Train Epoch: 039 Batch: 00026/00094 | Loss: 626.2436 | CE: 0.2947 | KD: 1060.0500\n",
      "Train Epoch: 039 Batch: 00027/00094 | Loss: 626.1422 | CE: 0.1959 | KD: 1060.0457\n",
      "Train Epoch: 039 Batch: 00028/00094 | Loss: 626.2415 | CE: 0.2527 | KD: 1060.1176\n",
      "Train Epoch: 039 Batch: 00029/00094 | Loss: 626.2160 | CE: 0.2699 | KD: 1060.0452\n",
      "Train Epoch: 039 Batch: 00030/00094 | Loss: 626.1820 | CE: 0.2596 | KD: 1060.0050\n",
      "Train Epoch: 039 Batch: 00031/00094 | Loss: 626.0491 | CE: 0.1873 | KD: 1059.9025\n",
      "Train Epoch: 039 Batch: 00032/00094 | Loss: 626.1321 | CE: 0.2159 | KD: 1059.9948\n",
      "Train Epoch: 039 Batch: 00033/00094 | Loss: 626.2396 | CE: 0.3019 | KD: 1060.0310\n",
      "Train Epoch: 039 Batch: 00034/00094 | Loss: 626.1468 | CE: 0.2070 | KD: 1060.0345\n",
      "Train Epoch: 039 Batch: 00035/00094 | Loss: 626.1044 | CE: 0.1744 | KD: 1060.0178\n",
      "Train Epoch: 039 Batch: 00036/00094 | Loss: 626.1447 | CE: 0.2267 | KD: 1059.9977\n",
      "Train Epoch: 039 Batch: 00037/00094 | Loss: 626.0593 | CE: 0.1369 | KD: 1060.0050\n",
      "Train Epoch: 039 Batch: 00038/00094 | Loss: 626.1170 | CE: 0.2328 | KD: 1059.9404\n",
      "Train Epoch: 039 Batch: 00039/00094 | Loss: 626.0476 | CE: 0.1648 | KD: 1059.9381\n",
      "Train Epoch: 039 Batch: 00040/00094 | Loss: 626.1702 | CE: 0.2230 | KD: 1060.0472\n",
      "Train Epoch: 039 Batch: 00041/00094 | Loss: 626.1472 | CE: 0.2276 | KD: 1060.0004\n",
      "Train Epoch: 039 Batch: 00042/00094 | Loss: 626.2026 | CE: 0.2332 | KD: 1060.0846\n",
      "Train Epoch: 039 Batch: 00043/00094 | Loss: 626.1274 | CE: 0.1736 | KD: 1060.0583\n",
      "Train Epoch: 039 Batch: 00044/00094 | Loss: 626.1530 | CE: 0.2260 | KD: 1060.0129\n",
      "Train Epoch: 039 Batch: 00045/00094 | Loss: 626.2663 | CE: 0.3150 | KD: 1060.0541\n",
      "Train Epoch: 039 Batch: 00046/00094 | Loss: 626.1523 | CE: 0.2539 | KD: 1059.9645\n",
      "Train Epoch: 039 Batch: 00047/00094 | Loss: 626.1614 | CE: 0.1956 | KD: 1060.0785\n",
      "Train Epoch: 039 Batch: 00048/00094 | Loss: 626.1721 | CE: 0.2765 | KD: 1059.9597\n",
      "Train Epoch: 039 Batch: 00049/00094 | Loss: 626.1592 | CE: 0.2235 | KD: 1060.0276\n",
      "Train Epoch: 039 Batch: 00050/00094 | Loss: 626.1716 | CE: 0.2234 | KD: 1060.0487\n",
      "Train Epoch: 039 Batch: 00051/00094 | Loss: 626.3931 | CE: 0.3660 | KD: 1060.1824\n",
      "Train Epoch: 039 Batch: 00052/00094 | Loss: 626.2149 | CE: 0.2421 | KD: 1060.0906\n",
      "Train Epoch: 039 Batch: 00053/00094 | Loss: 626.1619 | CE: 0.2445 | KD: 1059.9967\n",
      "Train Epoch: 039 Batch: 00054/00094 | Loss: 626.2733 | CE: 0.3074 | KD: 1060.0787\n",
      "Train Epoch: 039 Batch: 00055/00094 | Loss: 626.2029 | CE: 0.2391 | KD: 1060.0753\n",
      "Train Epoch: 039 Batch: 00056/00094 | Loss: 626.0546 | CE: 0.1583 | KD: 1059.9609\n",
      "Train Epoch: 039 Batch: 00057/00094 | Loss: 626.3118 | CE: 0.3550 | KD: 1060.0635\n",
      "Train Epoch: 039 Batch: 00058/00094 | Loss: 626.1755 | CE: 0.2244 | KD: 1060.0536\n",
      "Train Epoch: 039 Batch: 00059/00094 | Loss: 626.2045 | CE: 0.2689 | KD: 1060.0275\n",
      "Train Epoch: 039 Batch: 00060/00094 | Loss: 626.1736 | CE: 0.2363 | KD: 1060.0305\n",
      "Train Epoch: 039 Batch: 00061/00094 | Loss: 626.1713 | CE: 0.2282 | KD: 1060.0402\n",
      "Train Epoch: 039 Batch: 00062/00094 | Loss: 626.1740 | CE: 0.2323 | KD: 1060.0377\n",
      "Train Epoch: 039 Batch: 00063/00094 | Loss: 626.1398 | CE: 0.1957 | KD: 1060.0419\n",
      "Train Epoch: 039 Batch: 00064/00094 | Loss: 626.1846 | CE: 0.2640 | KD: 1060.0021\n",
      "Train Epoch: 039 Batch: 00065/00094 | Loss: 626.3157 | CE: 0.3537 | KD: 1060.0721\n",
      "Train Epoch: 039 Batch: 00066/00094 | Loss: 626.1747 | CE: 0.2323 | KD: 1060.0392\n",
      "Train Epoch: 039 Batch: 00067/00094 | Loss: 626.1926 | CE: 0.2450 | KD: 1060.0476\n",
      "Train Epoch: 039 Batch: 00068/00094 | Loss: 626.1579 | CE: 0.2178 | KD: 1060.0352\n",
      "Train Epoch: 039 Batch: 00069/00094 | Loss: 626.0881 | CE: 0.1681 | KD: 1060.0011\n",
      "Train Epoch: 039 Batch: 00070/00094 | Loss: 626.2682 | CE: 0.2429 | KD: 1060.1796\n",
      "Train Epoch: 039 Batch: 00071/00094 | Loss: 626.2027 | CE: 0.2191 | KD: 1060.1088\n",
      "Train Epoch: 039 Batch: 00072/00094 | Loss: 626.1707 | CE: 0.2470 | KD: 1060.0071\n",
      "Train Epoch: 039 Batch: 00073/00094 | Loss: 626.1011 | CE: 0.2137 | KD: 1059.9458\n",
      "Train Epoch: 039 Batch: 00074/00094 | Loss: 626.2495 | CE: 0.2451 | KD: 1060.1440\n",
      "Train Epoch: 039 Batch: 00075/00094 | Loss: 626.1692 | CE: 0.2524 | KD: 1059.9956\n",
      "Train Epoch: 039 Batch: 00076/00094 | Loss: 626.2407 | CE: 0.2740 | KD: 1060.0801\n",
      "Train Epoch: 039 Batch: 00077/00094 | Loss: 626.1562 | CE: 0.2071 | KD: 1060.0504\n",
      "Train Epoch: 039 Batch: 00078/00094 | Loss: 626.2161 | CE: 0.2582 | KD: 1060.0652\n",
      "Train Epoch: 039 Batch: 00079/00094 | Loss: 626.1987 | CE: 0.2094 | KD: 1060.1184\n",
      "Train Epoch: 039 Batch: 00080/00094 | Loss: 626.3215 | CE: 0.4001 | KD: 1060.0034\n",
      "Train Epoch: 039 Batch: 00081/00094 | Loss: 626.2084 | CE: 0.2539 | KD: 1060.0593\n",
      "Train Epoch: 039 Batch: 00082/00094 | Loss: 626.2026 | CE: 0.2654 | KD: 1060.0302\n",
      "Train Epoch: 039 Batch: 00083/00094 | Loss: 626.1683 | CE: 0.2435 | KD: 1060.0092\n",
      "Train Epoch: 039 Batch: 00084/00094 | Loss: 626.2540 | CE: 0.2818 | KD: 1060.0894\n",
      "Train Epoch: 039 Batch: 00085/00094 | Loss: 626.3195 | CE: 0.3270 | KD: 1060.1239\n",
      "Train Epoch: 039 Batch: 00086/00094 | Loss: 626.1308 | CE: 0.2183 | KD: 1059.9883\n",
      "Train Epoch: 039 Batch: 00087/00094 | Loss: 626.1879 | CE: 0.2455 | KD: 1060.0389\n",
      "Train Epoch: 039 Batch: 00088/00094 | Loss: 626.1920 | CE: 0.2369 | KD: 1060.0604\n",
      "Train Epoch: 039 Batch: 00089/00094 | Loss: 626.2463 | CE: 0.2663 | KD: 1060.1028\n",
      "Train Epoch: 039 Batch: 00090/00094 | Loss: 626.2839 | CE: 0.3153 | KD: 1060.0834\n",
      "Train Epoch: 039 Batch: 00091/00094 | Loss: 626.2891 | CE: 0.3375 | KD: 1060.0547\n",
      "Train Epoch: 039 Batch: 00092/00094 | Loss: 626.1191 | CE: 0.2073 | KD: 1059.9872\n",
      "Train Epoch: 039 Batch: 00093/00094 | Loss: 626.2565 | CE: 0.2785 | KD: 1060.0991\n",
      "Train Epoch: 039 Batch: 00094/00094 | Loss: 626.1686 | CE: 0.2244 | KD: 1060.0421\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2648 | acc:91.2500\n",
      "[VAL Acc] Target: 91.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1010 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9848 | acc:48.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0324 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6113 | acc:66.4577\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 66.46%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0943 | acc:52.6802\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4492 | acc:80.7210\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 80.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1727 | acc:52.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 52.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4506 | acc:77.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 77.15%\n",
      "[VAL Acc] Avg 63.14%\n",
      "Train Epoch: 040 Batch: 00001/00094 | Loss: 626.2471 | CE: 0.2655 | KD: 1060.1052\n",
      "Train Epoch: 040 Batch: 00002/00094 | Loss: 626.1351 | CE: 0.1885 | KD: 1060.0461\n",
      "Train Epoch: 040 Batch: 00003/00094 | Loss: 626.1466 | CE: 0.2364 | KD: 1059.9845\n",
      "Train Epoch: 040 Batch: 00004/00094 | Loss: 626.2238 | CE: 0.2685 | KD: 1060.0608\n",
      "Train Epoch: 040 Batch: 00005/00094 | Loss: 626.2751 | CE: 0.2778 | KD: 1060.1318\n",
      "Train Epoch: 040 Batch: 00006/00094 | Loss: 626.1647 | CE: 0.2317 | KD: 1060.0231\n",
      "Train Epoch: 040 Batch: 00007/00094 | Loss: 626.1693 | CE: 0.2299 | KD: 1060.0339\n",
      "Train Epoch: 040 Batch: 00008/00094 | Loss: 626.1846 | CE: 0.2310 | KD: 1060.0579\n",
      "Train Epoch: 040 Batch: 00009/00094 | Loss: 626.0974 | CE: 0.1878 | KD: 1059.9835\n",
      "Train Epoch: 040 Batch: 00010/00094 | Loss: 626.1343 | CE: 0.2337 | KD: 1059.9684\n",
      "Train Epoch: 040 Batch: 00011/00094 | Loss: 626.2415 | CE: 0.2969 | KD: 1060.0427\n",
      "Train Epoch: 040 Batch: 00012/00094 | Loss: 626.2261 | CE: 0.2627 | KD: 1060.0745\n",
      "Train Epoch: 040 Batch: 00013/00094 | Loss: 626.0892 | CE: 0.2098 | KD: 1059.9324\n",
      "Train Epoch: 040 Batch: 00014/00094 | Loss: 626.2329 | CE: 0.2161 | KD: 1060.1649\n",
      "Train Epoch: 040 Batch: 00015/00094 | Loss: 626.0889 | CE: 0.2001 | KD: 1059.9482\n",
      "Train Epoch: 040 Batch: 00016/00094 | Loss: 626.2213 | CE: 0.2483 | KD: 1060.0907\n",
      "Train Epoch: 040 Batch: 00017/00094 | Loss: 626.0959 | CE: 0.2108 | KD: 1059.9419\n",
      "Train Epoch: 040 Batch: 00018/00094 | Loss: 626.1924 | CE: 0.2747 | KD: 1059.9972\n",
      "Train Epoch: 040 Batch: 00019/00094 | Loss: 626.1581 | CE: 0.2493 | KD: 1059.9821\n",
      "Train Epoch: 040 Batch: 00020/00094 | Loss: 626.2452 | CE: 0.2966 | KD: 1060.0496\n",
      "Train Epoch: 040 Batch: 00021/00094 | Loss: 626.2197 | CE: 0.2717 | KD: 1060.0486\n",
      "Train Epoch: 040 Batch: 00022/00094 | Loss: 626.3536 | CE: 0.3400 | KD: 1060.1595\n",
      "Train Epoch: 040 Batch: 00023/00094 | Loss: 626.2238 | CE: 0.2361 | KD: 1060.1158\n",
      "Train Epoch: 040 Batch: 00024/00094 | Loss: 626.1541 | CE: 0.2212 | KD: 1060.0229\n",
      "Train Epoch: 040 Batch: 00025/00094 | Loss: 626.1176 | CE: 0.2129 | KD: 1059.9751\n",
      "Train Epoch: 040 Batch: 00026/00094 | Loss: 626.1223 | CE: 0.2145 | KD: 1059.9805\n",
      "Train Epoch: 040 Batch: 00027/00094 | Loss: 626.2078 | CE: 0.2118 | KD: 1060.1298\n",
      "Train Epoch: 040 Batch: 00028/00094 | Loss: 626.1479 | CE: 0.1552 | KD: 1060.1243\n",
      "Train Epoch: 040 Batch: 00029/00094 | Loss: 626.1520 | CE: 0.2430 | KD: 1059.9824\n",
      "Train Epoch: 040 Batch: 00030/00094 | Loss: 626.2284 | CE: 0.2966 | KD: 1060.0211\n",
      "Train Epoch: 040 Batch: 00031/00094 | Loss: 626.2382 | CE: 0.2850 | KD: 1060.0573\n",
      "Train Epoch: 040 Batch: 00032/00094 | Loss: 626.1505 | CE: 0.2417 | KD: 1059.9822\n",
      "Train Epoch: 040 Batch: 00033/00094 | Loss: 626.1495 | CE: 0.2427 | KD: 1059.9788\n",
      "Train Epoch: 040 Batch: 00034/00094 | Loss: 626.3489 | CE: 0.3270 | KD: 1060.1736\n",
      "Train Epoch: 040 Batch: 00035/00094 | Loss: 626.3058 | CE: 0.3421 | KD: 1060.0751\n",
      "Train Epoch: 040 Batch: 00036/00094 | Loss: 626.1345 | CE: 0.1838 | KD: 1060.0531\n",
      "Train Epoch: 040 Batch: 00037/00094 | Loss: 626.2035 | CE: 0.2265 | KD: 1060.0974\n",
      "Train Epoch: 040 Batch: 00038/00094 | Loss: 626.1450 | CE: 0.2141 | KD: 1060.0197\n",
      "Train Epoch: 040 Batch: 00039/00094 | Loss: 626.1089 | CE: 0.1967 | KD: 1059.9879\n",
      "Train Epoch: 040 Batch: 00040/00094 | Loss: 626.1207 | CE: 0.2156 | KD: 1059.9758\n",
      "Train Epoch: 040 Batch: 00041/00094 | Loss: 626.2272 | CE: 0.2183 | KD: 1060.1516\n",
      "Train Epoch: 040 Batch: 00042/00094 | Loss: 626.1600 | CE: 0.1998 | KD: 1060.0691\n",
      "Train Epoch: 040 Batch: 00043/00094 | Loss: 626.1876 | CE: 0.2490 | KD: 1060.0326\n",
      "Train Epoch: 040 Batch: 00044/00094 | Loss: 626.1285 | CE: 0.1973 | KD: 1060.0200\n",
      "Train Epoch: 040 Batch: 00045/00094 | Loss: 626.1221 | CE: 0.2268 | KD: 1059.9594\n",
      "Train Epoch: 040 Batch: 00046/00094 | Loss: 626.1760 | CE: 0.2241 | KD: 1060.0551\n",
      "Train Epoch: 040 Batch: 00047/00094 | Loss: 626.2227 | CE: 0.2672 | KD: 1060.0613\n",
      "Train Epoch: 040 Batch: 00048/00094 | Loss: 626.3760 | CE: 0.3862 | KD: 1060.1191\n",
      "Train Epoch: 040 Batch: 00049/00094 | Loss: 626.1396 | CE: 0.2206 | KD: 1059.9993\n",
      "Train Epoch: 040 Batch: 00050/00094 | Loss: 626.1009 | CE: 0.1800 | KD: 1060.0024\n",
      "Train Epoch: 040 Batch: 00051/00094 | Loss: 626.1649 | CE: 0.2503 | KD: 1059.9918\n",
      "Train Epoch: 040 Batch: 00052/00094 | Loss: 626.1937 | CE: 0.2546 | KD: 1060.0333\n",
      "Train Epoch: 040 Batch: 00053/00094 | Loss: 626.1276 | CE: 0.1952 | KD: 1060.0221\n",
      "Train Epoch: 040 Batch: 00054/00094 | Loss: 626.2753 | CE: 0.2681 | KD: 1060.1487\n",
      "Train Epoch: 040 Batch: 00055/00094 | Loss: 626.1659 | CE: 0.2118 | KD: 1060.0588\n",
      "Train Epoch: 040 Batch: 00056/00094 | Loss: 626.1341 | CE: 0.2148 | KD: 1059.9998\n",
      "Train Epoch: 040 Batch: 00057/00094 | Loss: 626.1674 | CE: 0.2361 | KD: 1060.0201\n",
      "Train Epoch: 040 Batch: 00058/00094 | Loss: 626.1321 | CE: 0.2066 | KD: 1060.0105\n",
      "Train Epoch: 040 Batch: 00059/00094 | Loss: 626.1259 | CE: 0.2093 | KD: 1059.9952\n",
      "Train Epoch: 040 Batch: 00060/00094 | Loss: 626.2806 | CE: 0.3131 | KD: 1060.0814\n",
      "Train Epoch: 040 Batch: 00061/00094 | Loss: 626.1429 | CE: 0.2016 | KD: 1060.0371\n",
      "Train Epoch: 040 Batch: 00062/00094 | Loss: 626.1703 | CE: 0.2233 | KD: 1060.0468\n",
      "Train Epoch: 040 Batch: 00063/00094 | Loss: 626.1849 | CE: 0.2363 | KD: 1060.0494\n",
      "Train Epoch: 040 Batch: 00064/00094 | Loss: 626.1570 | CE: 0.2152 | KD: 1060.0378\n",
      "Train Epoch: 040 Batch: 00065/00094 | Loss: 626.2797 | CE: 0.2848 | KD: 1060.1278\n",
      "Train Epoch: 040 Batch: 00066/00094 | Loss: 626.2587 | CE: 0.2865 | KD: 1060.0895\n",
      "Train Epoch: 040 Batch: 00067/00094 | Loss: 626.1920 | CE: 0.2523 | KD: 1060.0343\n",
      "Train Epoch: 040 Batch: 00068/00094 | Loss: 626.1794 | CE: 0.2249 | KD: 1060.0596\n",
      "Train Epoch: 040 Batch: 00069/00094 | Loss: 626.1960 | CE: 0.2482 | KD: 1060.0481\n",
      "Train Epoch: 040 Batch: 00070/00094 | Loss: 626.3111 | CE: 0.3317 | KD: 1060.1017\n",
      "Train Epoch: 040 Batch: 00071/00094 | Loss: 626.1567 | CE: 0.2290 | KD: 1060.0140\n",
      "Train Epoch: 040 Batch: 00072/00094 | Loss: 626.2123 | CE: 0.2825 | KD: 1060.0177\n",
      "Train Epoch: 040 Batch: 00073/00094 | Loss: 626.2720 | CE: 0.2793 | KD: 1060.1243\n",
      "Train Epoch: 040 Batch: 00074/00094 | Loss: 626.2102 | CE: 0.2372 | KD: 1060.0909\n",
      "Train Epoch: 040 Batch: 00075/00094 | Loss: 626.1981 | CE: 0.2314 | KD: 1060.0801\n",
      "Train Epoch: 040 Batch: 00076/00094 | Loss: 626.1426 | CE: 0.2212 | KD: 1060.0035\n",
      "Train Epoch: 040 Batch: 00077/00094 | Loss: 626.2216 | CE: 0.2553 | KD: 1060.0793\n",
      "Train Epoch: 040 Batch: 00078/00094 | Loss: 626.2102 | CE: 0.2891 | KD: 1060.0029\n",
      "Train Epoch: 040 Batch: 00079/00094 | Loss: 626.2600 | CE: 0.3304 | KD: 1060.0173\n",
      "Train Epoch: 040 Batch: 00080/00094 | Loss: 626.1774 | CE: 0.2747 | KD: 1059.9718\n",
      "Train Epoch: 040 Batch: 00081/00094 | Loss: 626.2448 | CE: 0.2620 | KD: 1060.1074\n",
      "Train Epoch: 040 Batch: 00082/00094 | Loss: 626.1796 | CE: 0.2106 | KD: 1060.0840\n",
      "Train Epoch: 040 Batch: 00083/00094 | Loss: 626.1543 | CE: 0.2189 | KD: 1060.0271\n",
      "Train Epoch: 040 Batch: 00084/00094 | Loss: 626.1426 | CE: 0.2028 | KD: 1060.0347\n",
      "Train Epoch: 040 Batch: 00085/00094 | Loss: 626.1096 | CE: 0.1822 | KD: 1060.0134\n",
      "Train Epoch: 040 Batch: 00086/00094 | Loss: 626.2786 | CE: 0.3500 | KD: 1060.0156\n",
      "Train Epoch: 040 Batch: 00087/00094 | Loss: 626.1942 | CE: 0.2640 | KD: 1060.0182\n",
      "Train Epoch: 040 Batch: 00088/00094 | Loss: 626.2156 | CE: 0.2773 | KD: 1060.0320\n",
      "Train Epoch: 040 Batch: 00089/00094 | Loss: 626.1537 | CE: 0.2097 | KD: 1060.0417\n",
      "Train Epoch: 040 Batch: 00090/00094 | Loss: 626.2723 | CE: 0.2440 | KD: 1060.1844\n",
      "Train Epoch: 040 Batch: 00091/00094 | Loss: 626.3052 | CE: 0.3276 | KD: 1060.0986\n",
      "Train Epoch: 040 Batch: 00092/00094 | Loss: 626.1144 | CE: 0.2171 | KD: 1059.9628\n",
      "Train Epoch: 040 Batch: 00093/00094 | Loss: 626.1536 | CE: 0.1924 | KD: 1060.0708\n",
      "Train Epoch: 040 Batch: 00094/00094 | Loss: 626.1893 | CE: 0.2308 | KD: 1060.0663\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2783 | acc:90.7500\n",
      "[VAL Acc] Target: 90.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0286 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9513 | acc:47.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9754 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6426 | acc:63.1661\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 63.17%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9534 | acc:54.8983\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4899 | acc:77.2335\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 77.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0581 | acc:53.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 53.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4412 | acc:76.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.95%\n",
      "[VAL Acc] Avg 62.57%\n",
      "Train Epoch: 041 Batch: 00001/00094 | Loss: 626.2541 | CE: 0.2556 | KD: 1060.1339\n",
      "Train Epoch: 041 Batch: 00002/00094 | Loss: 626.1853 | CE: 0.1826 | KD: 1060.1411\n",
      "Train Epoch: 041 Batch: 00003/00094 | Loss: 626.1573 | CE: 0.2292 | KD: 1060.0148\n",
      "Train Epoch: 041 Batch: 00004/00094 | Loss: 626.2014 | CE: 0.2158 | KD: 1060.1121\n",
      "Train Epoch: 041 Batch: 00005/00094 | Loss: 626.0898 | CE: 0.1564 | KD: 1060.0238\n",
      "Train Epoch: 041 Batch: 00006/00094 | Loss: 626.2074 | CE: 0.2542 | KD: 1060.0573\n",
      "Train Epoch: 041 Batch: 00007/00094 | Loss: 626.1725 | CE: 0.2224 | KD: 1060.0521\n",
      "Train Epoch: 041 Batch: 00008/00094 | Loss: 626.1508 | CE: 0.1895 | KD: 1060.0709\n",
      "Train Epoch: 041 Batch: 00009/00094 | Loss: 626.0970 | CE: 0.1824 | KD: 1059.9921\n",
      "Train Epoch: 041 Batch: 00010/00094 | Loss: 626.2668 | CE: 0.2624 | KD: 1060.1439\n",
      "Train Epoch: 041 Batch: 00011/00094 | Loss: 626.2155 | CE: 0.2348 | KD: 1060.1039\n",
      "Train Epoch: 041 Batch: 00012/00094 | Loss: 626.1118 | CE: 0.2051 | KD: 1059.9785\n",
      "Train Epoch: 041 Batch: 00013/00094 | Loss: 626.3235 | CE: 0.3122 | KD: 1060.1559\n",
      "Train Epoch: 041 Batch: 00014/00094 | Loss: 626.1784 | CE: 0.2767 | KD: 1059.9700\n",
      "Train Epoch: 041 Batch: 00015/00094 | Loss: 626.1801 | CE: 0.2386 | KD: 1060.0375\n",
      "Train Epoch: 041 Batch: 00016/00094 | Loss: 626.1542 | CE: 0.2173 | KD: 1060.0295\n",
      "Train Epoch: 041 Batch: 00017/00094 | Loss: 626.1054 | CE: 0.1842 | KD: 1060.0032\n",
      "Train Epoch: 041 Batch: 00018/00094 | Loss: 626.1121 | CE: 0.2115 | KD: 1059.9681\n",
      "Train Epoch: 041 Batch: 00019/00094 | Loss: 626.0656 | CE: 0.1869 | KD: 1059.9312\n",
      "Train Epoch: 041 Batch: 00020/00094 | Loss: 626.1838 | CE: 0.2251 | KD: 1060.0667\n",
      "Train Epoch: 041 Batch: 00021/00094 | Loss: 626.1584 | CE: 0.2232 | KD: 1060.0266\n",
      "Train Epoch: 041 Batch: 00022/00094 | Loss: 626.1805 | CE: 0.2509 | KD: 1060.0173\n",
      "Train Epoch: 041 Batch: 00023/00094 | Loss: 626.2001 | CE: 0.2448 | KD: 1060.0608\n",
      "Train Epoch: 041 Batch: 00024/00094 | Loss: 626.3167 | CE: 0.3319 | KD: 1060.1108\n",
      "Train Epoch: 041 Batch: 00025/00094 | Loss: 626.2415 | CE: 0.2772 | KD: 1060.0759\n",
      "Train Epoch: 041 Batch: 00026/00094 | Loss: 626.2206 | CE: 0.2499 | KD: 1060.0868\n",
      "Train Epoch: 041 Batch: 00027/00094 | Loss: 626.1442 | CE: 0.1955 | KD: 1060.0497\n",
      "Train Epoch: 041 Batch: 00028/00094 | Loss: 626.2215 | CE: 0.2641 | KD: 1060.0645\n",
      "Train Epoch: 041 Batch: 00029/00094 | Loss: 626.1832 | CE: 0.2148 | KD: 1060.0830\n",
      "Train Epoch: 041 Batch: 00030/00094 | Loss: 626.1874 | CE: 0.2433 | KD: 1060.0419\n",
      "Train Epoch: 041 Batch: 00031/00094 | Loss: 626.1509 | CE: 0.2018 | KD: 1060.0502\n",
      "Train Epoch: 041 Batch: 00032/00094 | Loss: 626.1545 | CE: 0.2033 | KD: 1060.0538\n",
      "Train Epoch: 041 Batch: 00033/00094 | Loss: 626.2134 | CE: 0.2077 | KD: 1060.1461\n",
      "Train Epoch: 041 Batch: 00034/00094 | Loss: 626.1592 | CE: 0.2652 | KD: 1059.9570\n",
      "Train Epoch: 041 Batch: 00035/00094 | Loss: 626.1412 | CE: 0.2143 | KD: 1060.0127\n",
      "Train Epoch: 041 Batch: 00036/00094 | Loss: 626.1865 | CE: 0.2626 | KD: 1060.0076\n",
      "Train Epoch: 041 Batch: 00037/00094 | Loss: 626.2412 | CE: 0.2912 | KD: 1060.0519\n",
      "Train Epoch: 041 Batch: 00038/00094 | Loss: 626.1636 | CE: 0.2430 | KD: 1060.0021\n",
      "Train Epoch: 041 Batch: 00039/00094 | Loss: 626.2294 | CE: 0.2526 | KD: 1060.0972\n",
      "Train Epoch: 041 Batch: 00040/00094 | Loss: 626.1960 | CE: 0.2596 | KD: 1060.0288\n",
      "Train Epoch: 041 Batch: 00041/00094 | Loss: 626.2014 | CE: 0.2087 | KD: 1060.1241\n",
      "Train Epoch: 041 Batch: 00042/00094 | Loss: 626.1066 | CE: 0.1933 | KD: 1059.9897\n",
      "Train Epoch: 041 Batch: 00043/00094 | Loss: 626.1415 | CE: 0.2096 | KD: 1060.0212\n",
      "Train Epoch: 041 Batch: 00044/00094 | Loss: 626.2108 | CE: 0.2789 | KD: 1060.0211\n",
      "Train Epoch: 041 Batch: 00045/00094 | Loss: 626.1116 | CE: 0.1686 | KD: 1060.0400\n",
      "Train Epoch: 041 Batch: 00046/00094 | Loss: 626.2885 | CE: 0.3147 | KD: 1060.0922\n",
      "Train Epoch: 041 Batch: 00047/00094 | Loss: 626.2364 | CE: 0.2672 | KD: 1060.0844\n",
      "Train Epoch: 041 Batch: 00048/00094 | Loss: 626.1595 | CE: 0.1950 | KD: 1060.0765\n",
      "Train Epoch: 041 Batch: 00049/00094 | Loss: 626.1838 | CE: 0.2301 | KD: 1060.0581\n",
      "Train Epoch: 041 Batch: 00050/00094 | Loss: 626.1545 | CE: 0.2249 | KD: 1060.0172\n",
      "Train Epoch: 041 Batch: 00051/00094 | Loss: 626.2311 | CE: 0.2680 | KD: 1060.0740\n",
      "Train Epoch: 041 Batch: 00052/00094 | Loss: 626.1698 | CE: 0.2346 | KD: 1060.0267\n",
      "Train Epoch: 041 Batch: 00053/00094 | Loss: 626.2195 | CE: 0.2588 | KD: 1060.0698\n",
      "Train Epoch: 041 Batch: 00054/00094 | Loss: 626.1879 | CE: 0.2408 | KD: 1060.0470\n",
      "Train Epoch: 041 Batch: 00055/00094 | Loss: 626.2313 | CE: 0.3065 | KD: 1060.0092\n",
      "Train Epoch: 041 Batch: 00056/00094 | Loss: 626.2220 | CE: 0.2612 | KD: 1060.0701\n",
      "Train Epoch: 041 Batch: 00057/00094 | Loss: 626.1291 | CE: 0.1848 | KD: 1060.0421\n",
      "Train Epoch: 041 Batch: 00058/00094 | Loss: 626.2609 | CE: 0.2682 | KD: 1060.1243\n",
      "Train Epoch: 041 Batch: 00059/00094 | Loss: 626.1880 | CE: 0.2572 | KD: 1060.0194\n",
      "Train Epoch: 041 Batch: 00060/00094 | Loss: 626.2468 | CE: 0.2646 | KD: 1060.1064\n",
      "Train Epoch: 041 Batch: 00061/00094 | Loss: 626.2369 | CE: 0.2705 | KD: 1060.0796\n",
      "Train Epoch: 041 Batch: 00062/00094 | Loss: 626.2029 | CE: 0.2557 | KD: 1060.0472\n",
      "Train Epoch: 041 Batch: 00063/00094 | Loss: 626.1876 | CE: 0.2508 | KD: 1060.0295\n",
      "Train Epoch: 041 Batch: 00064/00094 | Loss: 626.1387 | CE: 0.2418 | KD: 1059.9619\n",
      "Train Epoch: 041 Batch: 00065/00094 | Loss: 626.1548 | CE: 0.2583 | KD: 1059.9612\n",
      "Train Epoch: 041 Batch: 00066/00094 | Loss: 626.1223 | CE: 0.2225 | KD: 1059.9668\n",
      "Train Epoch: 041 Batch: 00067/00094 | Loss: 626.2012 | CE: 0.2521 | KD: 1060.0504\n",
      "Train Epoch: 041 Batch: 00068/00094 | Loss: 626.2405 | CE: 0.2727 | KD: 1060.0820\n",
      "Train Epoch: 041 Batch: 00069/00094 | Loss: 626.1769 | CE: 0.2658 | KD: 1059.9860\n",
      "Train Epoch: 041 Batch: 00070/00094 | Loss: 626.2330 | CE: 0.3021 | KD: 1060.0197\n",
      "Train Epoch: 041 Batch: 00071/00094 | Loss: 626.1491 | CE: 0.2189 | KD: 1060.0184\n",
      "Train Epoch: 041 Batch: 00072/00094 | Loss: 626.1509 | CE: 0.2571 | KD: 1059.9565\n",
      "Train Epoch: 041 Batch: 00073/00094 | Loss: 626.1895 | CE: 0.2292 | KD: 1060.0693\n",
      "Train Epoch: 041 Batch: 00074/00094 | Loss: 626.1841 | CE: 0.2585 | KD: 1060.0104\n",
      "Train Epoch: 041 Batch: 00075/00094 | Loss: 626.1349 | CE: 0.2417 | KD: 1059.9557\n",
      "Train Epoch: 041 Batch: 00076/00094 | Loss: 626.3183 | CE: 0.3253 | KD: 1060.1246\n",
      "Train Epoch: 041 Batch: 00077/00094 | Loss: 626.2395 | CE: 0.2155 | KD: 1060.1771\n",
      "Train Epoch: 041 Batch: 00078/00094 | Loss: 626.1377 | CE: 0.2378 | KD: 1059.9670\n",
      "Train Epoch: 041 Batch: 00079/00094 | Loss: 626.1812 | CE: 0.2376 | KD: 1060.0410\n",
      "Train Epoch: 041 Batch: 00080/00094 | Loss: 626.1540 | CE: 0.2371 | KD: 1059.9958\n",
      "Train Epoch: 041 Batch: 00081/00094 | Loss: 626.1449 | CE: 0.1917 | KD: 1060.0573\n",
      "Train Epoch: 041 Batch: 00082/00094 | Loss: 626.1530 | CE: 0.2005 | KD: 1060.0562\n",
      "Train Epoch: 041 Batch: 00083/00094 | Loss: 626.1871 | CE: 0.2199 | KD: 1060.0809\n",
      "Train Epoch: 041 Batch: 00084/00094 | Loss: 626.2611 | CE: 0.2965 | KD: 1060.0765\n",
      "Train Epoch: 041 Batch: 00085/00094 | Loss: 626.0923 | CE: 0.1999 | KD: 1059.9542\n",
      "Train Epoch: 041 Batch: 00086/00094 | Loss: 626.0921 | CE: 0.1852 | KD: 1059.9789\n",
      "Train Epoch: 041 Batch: 00087/00094 | Loss: 626.2401 | CE: 0.2921 | KD: 1060.0485\n",
      "Train Epoch: 041 Batch: 00088/00094 | Loss: 626.2234 | CE: 0.2579 | KD: 1060.0781\n",
      "Train Epoch: 041 Batch: 00089/00094 | Loss: 626.1213 | CE: 0.1906 | KD: 1060.0192\n",
      "Train Epoch: 041 Batch: 00090/00094 | Loss: 626.2094 | CE: 0.2368 | KD: 1060.0900\n",
      "Train Epoch: 041 Batch: 00091/00094 | Loss: 626.1924 | CE: 0.2869 | KD: 1059.9764\n",
      "Train Epoch: 041 Batch: 00092/00094 | Loss: 626.2292 | CE: 0.2404 | KD: 1060.1176\n",
      "Train Epoch: 041 Batch: 00093/00094 | Loss: 626.2003 | CE: 0.2205 | KD: 1060.1024\n",
      "Train Epoch: 041 Batch: 00094/00094 | Loss: 626.2531 | CE: 0.2672 | KD: 1060.1125\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2925 | acc:90.5000\n",
      "[VAL Acc] Target: 90.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:0.9488 | acc:48.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8709 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8681 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6407 | acc:64.0282\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 64.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0335 | acc:52.6802\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5201 | acc:75.1567\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0990 | acc:52.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 52.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.3974 | acc:82.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 82.80%\n",
      "[VAL Acc] Avg 62.90%\n",
      "Train Epoch: 042 Batch: 00001/00094 | Loss: 626.1431 | CE: 0.2617 | KD: 1059.9355\n",
      "Train Epoch: 042 Batch: 00002/00094 | Loss: 626.0992 | CE: 0.2266 | KD: 1059.9207\n",
      "Train Epoch: 042 Batch: 00003/00094 | Loss: 626.1443 | CE: 0.2266 | KD: 1059.9972\n",
      "Train Epoch: 042 Batch: 00004/00094 | Loss: 626.2474 | CE: 0.2771 | KD: 1060.0863\n",
      "Train Epoch: 042 Batch: 00005/00094 | Loss: 626.2277 | CE: 0.1920 | KD: 1060.1970\n",
      "Train Epoch: 042 Batch: 00006/00094 | Loss: 626.2071 | CE: 0.2544 | KD: 1060.0565\n",
      "Train Epoch: 042 Batch: 00007/00094 | Loss: 626.1039 | CE: 0.1885 | KD: 1059.9934\n",
      "Train Epoch: 042 Batch: 00008/00094 | Loss: 626.3227 | CE: 0.2886 | KD: 1060.1943\n",
      "Train Epoch: 042 Batch: 00009/00094 | Loss: 626.1684 | CE: 0.2126 | KD: 1060.0616\n",
      "Train Epoch: 042 Batch: 00010/00094 | Loss: 626.1662 | CE: 0.2435 | KD: 1060.0056\n",
      "Train Epoch: 042 Batch: 00011/00094 | Loss: 626.2236 | CE: 0.2361 | KD: 1060.1154\n",
      "Train Epoch: 042 Batch: 00012/00094 | Loss: 626.2133 | CE: 0.2459 | KD: 1060.0812\n",
      "Train Epoch: 042 Batch: 00013/00094 | Loss: 626.1950 | CE: 0.2422 | KD: 1060.0566\n",
      "Train Epoch: 042 Batch: 00014/00094 | Loss: 626.1227 | CE: 0.2192 | KD: 1059.9731\n",
      "Train Epoch: 042 Batch: 00015/00094 | Loss: 626.2117 | CE: 0.2608 | KD: 1060.0532\n",
      "Train Epoch: 042 Batch: 00016/00094 | Loss: 626.1959 | CE: 0.2338 | KD: 1060.0723\n",
      "Train Epoch: 042 Batch: 00017/00094 | Loss: 626.1230 | CE: 0.2293 | KD: 1059.9564\n",
      "Train Epoch: 042 Batch: 00018/00094 | Loss: 626.3292 | CE: 0.2391 | KD: 1060.2889\n",
      "Train Epoch: 042 Batch: 00019/00094 | Loss: 626.2245 | CE: 0.2649 | KD: 1060.0682\n",
      "Train Epoch: 042 Batch: 00020/00094 | Loss: 626.1844 | CE: 0.2314 | KD: 1060.0570\n",
      "Train Epoch: 042 Batch: 00021/00094 | Loss: 626.1912 | CE: 0.2634 | KD: 1060.0143\n",
      "Train Epoch: 042 Batch: 00022/00094 | Loss: 626.1638 | CE: 0.2467 | KD: 1059.9961\n",
      "Train Epoch: 042 Batch: 00023/00094 | Loss: 626.2456 | CE: 0.2909 | KD: 1060.0599\n",
      "Train Epoch: 042 Batch: 00024/00094 | Loss: 626.2376 | CE: 0.3147 | KD: 1060.0061\n",
      "Train Epoch: 042 Batch: 00025/00094 | Loss: 626.2014 | CE: 0.2543 | KD: 1060.0469\n",
      "Train Epoch: 042 Batch: 00026/00094 | Loss: 626.4758 | CE: 0.4074 | KD: 1060.2523\n",
      "Train Epoch: 042 Batch: 00027/00094 | Loss: 626.1721 | CE: 0.2449 | KD: 1060.0132\n",
      "Train Epoch: 042 Batch: 00028/00094 | Loss: 626.2834 | CE: 0.3374 | KD: 1060.0452\n",
      "Train Epoch: 042 Batch: 00029/00094 | Loss: 626.2525 | CE: 0.2575 | KD: 1060.1281\n",
      "Train Epoch: 042 Batch: 00030/00094 | Loss: 626.2042 | CE: 0.2897 | KD: 1059.9917\n",
      "Train Epoch: 042 Batch: 00031/00094 | Loss: 626.1772 | CE: 0.2663 | KD: 1059.9855\n",
      "Train Epoch: 042 Batch: 00032/00094 | Loss: 626.1548 | CE: 0.2352 | KD: 1060.0004\n",
      "Train Epoch: 042 Batch: 00033/00094 | Loss: 626.1409 | CE: 0.2196 | KD: 1060.0031\n",
      "Train Epoch: 042 Batch: 00034/00094 | Loss: 626.1868 | CE: 0.2454 | KD: 1060.0372\n",
      "Train Epoch: 042 Batch: 00035/00094 | Loss: 626.1156 | CE: 0.1703 | KD: 1060.0441\n",
      "Train Epoch: 042 Batch: 00036/00094 | Loss: 626.1757 | CE: 0.2253 | KD: 1060.0524\n",
      "Train Epoch: 042 Batch: 00037/00094 | Loss: 626.1243 | CE: 0.2135 | KD: 1059.9856\n",
      "Train Epoch: 042 Batch: 00038/00094 | Loss: 626.0898 | CE: 0.1985 | KD: 1059.9525\n",
      "Train Epoch: 042 Batch: 00039/00094 | Loss: 626.2830 | CE: 0.2633 | KD: 1060.1698\n",
      "Train Epoch: 042 Batch: 00040/00094 | Loss: 626.1450 | CE: 0.2129 | KD: 1060.0215\n",
      "Train Epoch: 042 Batch: 00041/00094 | Loss: 626.2877 | CE: 0.3190 | KD: 1060.0834\n",
      "Train Epoch: 042 Batch: 00042/00094 | Loss: 626.1373 | CE: 0.2312 | KD: 1059.9774\n",
      "Train Epoch: 042 Batch: 00043/00094 | Loss: 626.2083 | CE: 0.2771 | KD: 1060.0200\n",
      "Train Epoch: 042 Batch: 00044/00094 | Loss: 626.1049 | CE: 0.1941 | KD: 1059.9856\n",
      "Train Epoch: 042 Batch: 00045/00094 | Loss: 626.1259 | CE: 0.1745 | KD: 1060.0541\n",
      "Train Epoch: 042 Batch: 00046/00094 | Loss: 626.2565 | CE: 0.2559 | KD: 1060.1376\n",
      "Train Epoch: 042 Batch: 00047/00094 | Loss: 626.1267 | CE: 0.2102 | KD: 1059.9951\n",
      "Train Epoch: 042 Batch: 00048/00094 | Loss: 626.1479 | CE: 0.2536 | KD: 1059.9574\n",
      "Train Epoch: 042 Batch: 00049/00094 | Loss: 626.1596 | CE: 0.2355 | KD: 1060.0079\n",
      "Train Epoch: 042 Batch: 00050/00094 | Loss: 626.2305 | CE: 0.2469 | KD: 1060.1088\n",
      "Train Epoch: 042 Batch: 00051/00094 | Loss: 626.2607 | CE: 0.2890 | KD: 1060.0887\n",
      "Train Epoch: 042 Batch: 00052/00094 | Loss: 626.1345 | CE: 0.1998 | KD: 1060.0259\n",
      "Train Epoch: 042 Batch: 00053/00094 | Loss: 626.2006 | CE: 0.2323 | KD: 1060.0829\n",
      "Train Epoch: 042 Batch: 00054/00094 | Loss: 626.2087 | CE: 0.2619 | KD: 1060.0465\n",
      "Train Epoch: 042 Batch: 00055/00094 | Loss: 626.2762 | CE: 0.3263 | KD: 1060.0516\n",
      "Train Epoch: 042 Batch: 00056/00094 | Loss: 626.2567 | CE: 0.2430 | KD: 1060.1598\n",
      "Train Epoch: 042 Batch: 00057/00094 | Loss: 626.2712 | CE: 0.2676 | KD: 1060.1427\n",
      "Train Epoch: 042 Batch: 00058/00094 | Loss: 626.1503 | CE: 0.2168 | KD: 1060.0239\n",
      "Train Epoch: 042 Batch: 00059/00094 | Loss: 626.1748 | CE: 0.2423 | KD: 1060.0222\n",
      "Train Epoch: 042 Batch: 00060/00094 | Loss: 626.1843 | CE: 0.2479 | KD: 1060.0289\n",
      "Train Epoch: 042 Batch: 00061/00094 | Loss: 626.1795 | CE: 0.2570 | KD: 1060.0052\n",
      "Train Epoch: 042 Batch: 00062/00094 | Loss: 626.2936 | CE: 0.3228 | KD: 1060.0872\n",
      "Train Epoch: 042 Batch: 00063/00094 | Loss: 626.2728 | CE: 0.2885 | KD: 1060.1099\n",
      "Train Epoch: 042 Batch: 00064/00094 | Loss: 626.2109 | CE: 0.2443 | KD: 1060.0800\n",
      "Train Epoch: 042 Batch: 00065/00094 | Loss: 626.1965 | CE: 0.2438 | KD: 1060.0563\n",
      "Train Epoch: 042 Batch: 00066/00094 | Loss: 626.1741 | CE: 0.2493 | KD: 1060.0092\n",
      "Train Epoch: 042 Batch: 00067/00094 | Loss: 626.1401 | CE: 0.2157 | KD: 1060.0084\n",
      "Train Epoch: 042 Batch: 00068/00094 | Loss: 626.1635 | CE: 0.2214 | KD: 1060.0385\n",
      "Train Epoch: 042 Batch: 00069/00094 | Loss: 626.2582 | CE: 0.2351 | KD: 1060.1757\n",
      "Train Epoch: 042 Batch: 00070/00094 | Loss: 626.1726 | CE: 0.2205 | KD: 1060.0554\n",
      "Train Epoch: 042 Batch: 00071/00094 | Loss: 626.1364 | CE: 0.2154 | KD: 1060.0027\n",
      "Train Epoch: 042 Batch: 00072/00094 | Loss: 626.2046 | CE: 0.2683 | KD: 1060.0286\n",
      "Train Epoch: 042 Batch: 00073/00094 | Loss: 626.1607 | CE: 0.2459 | KD: 1059.9922\n",
      "Train Epoch: 042 Batch: 00074/00094 | Loss: 626.1866 | CE: 0.2134 | KD: 1060.0912\n",
      "Train Epoch: 042 Batch: 00075/00094 | Loss: 626.2074 | CE: 0.2514 | KD: 1060.0621\n",
      "Train Epoch: 042 Batch: 00076/00094 | Loss: 626.2193 | CE: 0.2396 | KD: 1060.1022\n",
      "Train Epoch: 042 Batch: 00077/00094 | Loss: 626.2575 | CE: 0.2871 | KD: 1060.0864\n",
      "Train Epoch: 042 Batch: 00078/00094 | Loss: 626.1586 | CE: 0.2203 | KD: 1060.0320\n",
      "Train Epoch: 042 Batch: 00079/00094 | Loss: 626.2109 | CE: 0.2255 | KD: 1060.1117\n",
      "Train Epoch: 042 Batch: 00080/00094 | Loss: 626.2424 | CE: 0.2236 | KD: 1060.1682\n",
      "Train Epoch: 042 Batch: 00081/00094 | Loss: 626.1523 | CE: 0.2310 | KD: 1060.0033\n",
      "Train Epoch: 042 Batch: 00082/00094 | Loss: 626.1426 | CE: 0.2445 | KD: 1059.9639\n",
      "Train Epoch: 042 Batch: 00083/00094 | Loss: 626.1442 | CE: 0.1870 | KD: 1060.0641\n",
      "Train Epoch: 042 Batch: 00084/00094 | Loss: 626.1471 | CE: 0.1918 | KD: 1060.0609\n",
      "Train Epoch: 042 Batch: 00085/00094 | Loss: 626.2204 | CE: 0.2972 | KD: 1060.0063\n",
      "Train Epoch: 042 Batch: 00086/00094 | Loss: 626.2365 | CE: 0.2554 | KD: 1060.1045\n",
      "Train Epoch: 042 Batch: 00087/00094 | Loss: 626.1747 | CE: 0.2450 | KD: 1060.0175\n",
      "Train Epoch: 042 Batch: 00088/00094 | Loss: 626.3378 | CE: 0.3020 | KD: 1060.1973\n",
      "Train Epoch: 042 Batch: 00089/00094 | Loss: 626.1679 | CE: 0.2160 | KD: 1060.0551\n",
      "Train Epoch: 042 Batch: 00090/00094 | Loss: 626.1043 | CE: 0.1964 | KD: 1059.9806\n",
      "Train Epoch: 042 Batch: 00091/00094 | Loss: 626.2137 | CE: 0.2446 | KD: 1060.0842\n",
      "Train Epoch: 042 Batch: 00092/00094 | Loss: 626.1873 | CE: 0.2579 | KD: 1060.0170\n",
      "Train Epoch: 042 Batch: 00093/00094 | Loss: 626.1347 | CE: 0.1868 | KD: 1060.0482\n",
      "Train Epoch: 042 Batch: 00094/00094 | Loss: 626.2133 | CE: 0.2720 | KD: 1060.0371\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2504 | acc:91.6000\n",
      "[VAL Acc] Target: 91.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1482 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0080 | acc:49.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0434 | acc:51.7176\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 51.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7013 | acc:61.2853\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.29%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0295 | acc:52.9575\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5049 | acc:76.8025\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0599 | acc:54.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4941 | acc:73.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.25%\n",
      "[VAL Acc] Avg 62.40%\n",
      "Train Epoch: 043 Batch: 00001/00094 | Loss: 563.5325 | CE: 0.2158 | KD: 1059.9800\n",
      "Train Epoch: 043 Batch: 00002/00094 | Loss: 563.5883 | CE: 0.2069 | KD: 1060.1014\n",
      "Train Epoch: 043 Batch: 00003/00094 | Loss: 563.5165 | CE: 0.1379 | KD: 1060.0963\n",
      "Train Epoch: 043 Batch: 00004/00094 | Loss: 563.5821 | CE: 0.2272 | KD: 1060.0516\n",
      "Train Epoch: 043 Batch: 00005/00094 | Loss: 563.5212 | CE: 0.1822 | KD: 1060.0217\n",
      "Train Epoch: 043 Batch: 00006/00094 | Loss: 563.6422 | CE: 0.2606 | KD: 1060.1019\n",
      "Train Epoch: 043 Batch: 00007/00094 | Loss: 563.5427 | CE: 0.2024 | KD: 1060.0242\n",
      "Train Epoch: 043 Batch: 00008/00094 | Loss: 563.5754 | CE: 0.2364 | KD: 1060.0217\n",
      "Train Epoch: 043 Batch: 00009/00094 | Loss: 563.5927 | CE: 0.2470 | KD: 1060.0344\n",
      "Train Epoch: 043 Batch: 00010/00094 | Loss: 563.5713 | CE: 0.2227 | KD: 1060.0398\n",
      "Train Epoch: 043 Batch: 00011/00094 | Loss: 563.5922 | CE: 0.2320 | KD: 1060.0618\n",
      "Train Epoch: 043 Batch: 00012/00094 | Loss: 563.5490 | CE: 0.2203 | KD: 1060.0023\n",
      "Train Epoch: 043 Batch: 00013/00094 | Loss: 563.5671 | CE: 0.1972 | KD: 1060.0798\n",
      "Train Epoch: 043 Batch: 00014/00094 | Loss: 563.5638 | CE: 0.2174 | KD: 1060.0358\n",
      "Train Epoch: 043 Batch: 00015/00094 | Loss: 563.4981 | CE: 0.1644 | KD: 1060.0118\n",
      "Train Epoch: 043 Batch: 00016/00094 | Loss: 563.6655 | CE: 0.2425 | KD: 1060.1797\n",
      "Train Epoch: 043 Batch: 00017/00094 | Loss: 563.6583 | CE: 0.3373 | KD: 1059.9880\n",
      "Train Epoch: 043 Batch: 00018/00094 | Loss: 563.6201 | CE: 0.2619 | KD: 1060.0580\n",
      "Train Epoch: 043 Batch: 00019/00094 | Loss: 563.5823 | CE: 0.2400 | KD: 1060.0281\n",
      "Train Epoch: 043 Batch: 00020/00094 | Loss: 563.6808 | CE: 0.2819 | KD: 1060.1346\n",
      "Train Epoch: 043 Batch: 00021/00094 | Loss: 563.6974 | CE: 0.2928 | KD: 1060.1453\n",
      "Train Epoch: 043 Batch: 00022/00094 | Loss: 563.6364 | CE: 0.2732 | KD: 1060.0673\n",
      "Train Epoch: 043 Batch: 00023/00094 | Loss: 563.5228 | CE: 0.2098 | KD: 1059.9728\n",
      "Train Epoch: 043 Batch: 00024/00094 | Loss: 563.6057 | CE: 0.2303 | KD: 1060.0902\n",
      "Train Epoch: 043 Batch: 00025/00094 | Loss: 563.6189 | CE: 0.2339 | KD: 1060.1084\n",
      "Train Epoch: 043 Batch: 00026/00094 | Loss: 563.5914 | CE: 0.2426 | KD: 1060.0403\n",
      "Train Epoch: 043 Batch: 00027/00094 | Loss: 563.5479 | CE: 0.1879 | KD: 1060.0613\n",
      "Train Epoch: 043 Batch: 00028/00094 | Loss: 563.5977 | CE: 0.2592 | KD: 1060.0209\n",
      "Train Epoch: 043 Batch: 00029/00094 | Loss: 563.5723 | CE: 0.2323 | KD: 1060.0238\n",
      "Train Epoch: 043 Batch: 00030/00094 | Loss: 563.5081 | CE: 0.1726 | KD: 1060.0150\n",
      "Train Epoch: 043 Batch: 00031/00094 | Loss: 563.6390 | CE: 0.2525 | KD: 1060.1111\n",
      "Train Epoch: 043 Batch: 00032/00094 | Loss: 563.5826 | CE: 0.2004 | KD: 1060.1029\n",
      "Train Epoch: 043 Batch: 00033/00094 | Loss: 563.5966 | CE: 0.2164 | KD: 1060.0994\n",
      "Train Epoch: 043 Batch: 00034/00094 | Loss: 563.5955 | CE: 0.2494 | KD: 1060.0350\n",
      "Train Epoch: 043 Batch: 00035/00094 | Loss: 563.6311 | CE: 0.2867 | KD: 1060.0320\n",
      "Train Epoch: 043 Batch: 00036/00094 | Loss: 563.6644 | CE: 0.2932 | KD: 1060.0824\n",
      "Train Epoch: 043 Batch: 00037/00094 | Loss: 563.5907 | CE: 0.2227 | KD: 1060.0762\n",
      "Train Epoch: 043 Batch: 00038/00094 | Loss: 563.5513 | CE: 0.2139 | KD: 1060.0188\n",
      "Train Epoch: 043 Batch: 00039/00094 | Loss: 563.5417 | CE: 0.1870 | KD: 1060.0515\n",
      "Train Epoch: 043 Batch: 00040/00094 | Loss: 563.6508 | CE: 0.2725 | KD: 1060.0956\n",
      "Train Epoch: 043 Batch: 00041/00094 | Loss: 563.6279 | CE: 0.2384 | KD: 1060.1166\n",
      "Train Epoch: 043 Batch: 00042/00094 | Loss: 563.5238 | CE: 0.1965 | KD: 1059.9998\n",
      "Train Epoch: 043 Batch: 00043/00094 | Loss: 563.5512 | CE: 0.2462 | KD: 1059.9578\n",
      "Train Epoch: 043 Batch: 00044/00094 | Loss: 563.7192 | CE: 0.2981 | KD: 1060.1764\n",
      "Train Epoch: 043 Batch: 00045/00094 | Loss: 563.7390 | CE: 0.3264 | KD: 1060.1603\n",
      "Train Epoch: 043 Batch: 00046/00094 | Loss: 563.6583 | CE: 0.2852 | KD: 1060.0858\n",
      "Train Epoch: 043 Batch: 00047/00094 | Loss: 563.6233 | CE: 0.2794 | KD: 1060.0309\n",
      "Train Epoch: 043 Batch: 00048/00094 | Loss: 563.5262 | CE: 0.1995 | KD: 1059.9987\n",
      "Train Epoch: 043 Batch: 00049/00094 | Loss: 563.5071 | CE: 0.1723 | KD: 1060.0139\n",
      "Train Epoch: 043 Batch: 00050/00094 | Loss: 563.6038 | CE: 0.1796 | KD: 1060.1821\n",
      "Train Epoch: 043 Batch: 00051/00094 | Loss: 563.5578 | CE: 0.2210 | KD: 1060.0176\n",
      "Train Epoch: 043 Batch: 00052/00094 | Loss: 563.7357 | CE: 0.3460 | KD: 1060.1173\n",
      "Train Epoch: 043 Batch: 00053/00094 | Loss: 563.6689 | CE: 0.2895 | KD: 1060.0979\n",
      "Train Epoch: 043 Batch: 00054/00094 | Loss: 563.5887 | CE: 0.2080 | KD: 1060.1003\n",
      "Train Epoch: 043 Batch: 00055/00094 | Loss: 563.5847 | CE: 0.2447 | KD: 1060.0236\n",
      "Train Epoch: 043 Batch: 00056/00094 | Loss: 563.6602 | CE: 0.2870 | KD: 1060.0859\n",
      "Train Epoch: 043 Batch: 00057/00094 | Loss: 563.5275 | CE: 0.1689 | KD: 1060.0587\n",
      "Train Epoch: 043 Batch: 00058/00094 | Loss: 563.5638 | CE: 0.1972 | KD: 1060.0737\n",
      "Train Epoch: 043 Batch: 00059/00094 | Loss: 563.5549 | CE: 0.1888 | KD: 1060.0729\n",
      "Train Epoch: 043 Batch: 00060/00094 | Loss: 563.7001 | CE: 0.3070 | KD: 1060.1237\n",
      "Train Epoch: 043 Batch: 00061/00094 | Loss: 563.5435 | CE: 0.1994 | KD: 1060.0312\n",
      "Train Epoch: 043 Batch: 00062/00094 | Loss: 563.6727 | CE: 0.2727 | KD: 1060.1365\n",
      "Train Epoch: 043 Batch: 00063/00094 | Loss: 563.5812 | CE: 0.2241 | KD: 1060.0559\n",
      "Train Epoch: 043 Batch: 00064/00094 | Loss: 563.5349 | CE: 0.1983 | KD: 1060.0173\n",
      "Train Epoch: 043 Batch: 00065/00094 | Loss: 563.6519 | CE: 0.2981 | KD: 1060.0494\n",
      "Train Epoch: 043 Batch: 00066/00094 | Loss: 563.5157 | CE: 0.1765 | KD: 1060.0222\n",
      "Train Epoch: 043 Batch: 00067/00094 | Loss: 563.5988 | CE: 0.2141 | KD: 1060.1077\n",
      "Train Epoch: 043 Batch: 00068/00094 | Loss: 563.6174 | CE: 0.2753 | KD: 1060.0276\n",
      "Train Epoch: 043 Batch: 00069/00094 | Loss: 563.6524 | CE: 0.3288 | KD: 1059.9928\n",
      "Train Epoch: 043 Batch: 00070/00094 | Loss: 563.6603 | CE: 0.3246 | KD: 1060.0155\n",
      "Train Epoch: 043 Batch: 00071/00094 | Loss: 563.5685 | CE: 0.2125 | KD: 1060.0538\n",
      "Train Epoch: 043 Batch: 00072/00094 | Loss: 563.5693 | CE: 0.2266 | KD: 1060.0288\n",
      "Train Epoch: 043 Batch: 00073/00094 | Loss: 563.5328 | CE: 0.2011 | KD: 1060.0078\n",
      "Train Epoch: 043 Batch: 00074/00094 | Loss: 563.6634 | CE: 0.3042 | KD: 1060.0598\n",
      "Train Epoch: 043 Batch: 00075/00094 | Loss: 563.6428 | CE: 0.2503 | KD: 1060.1224\n",
      "Train Epoch: 043 Batch: 00076/00094 | Loss: 563.5601 | CE: 0.2371 | KD: 1059.9918\n",
      "Train Epoch: 043 Batch: 00077/00094 | Loss: 563.5895 | CE: 0.2465 | KD: 1060.0294\n",
      "Train Epoch: 043 Batch: 00078/00094 | Loss: 563.5898 | CE: 0.1976 | KD: 1060.1219\n",
      "Train Epoch: 043 Batch: 00079/00094 | Loss: 563.5583 | CE: 0.2185 | KD: 1060.0233\n",
      "Train Epoch: 043 Batch: 00080/00094 | Loss: 563.5742 | CE: 0.2051 | KD: 1060.0784\n",
      "Train Epoch: 043 Batch: 00081/00094 | Loss: 563.5645 | CE: 0.2139 | KD: 1060.0436\n",
      "Train Epoch: 043 Batch: 00082/00094 | Loss: 563.5565 | CE: 0.2053 | KD: 1060.0446\n",
      "Train Epoch: 043 Batch: 00083/00094 | Loss: 563.6154 | CE: 0.2423 | KD: 1060.0858\n",
      "Train Epoch: 043 Batch: 00084/00094 | Loss: 563.6107 | CE: 0.2532 | KD: 1060.0565\n",
      "Train Epoch: 043 Batch: 00085/00094 | Loss: 563.5569 | CE: 0.1998 | KD: 1060.0558\n",
      "Train Epoch: 043 Batch: 00086/00094 | Loss: 563.5124 | CE: 0.2063 | KD: 1059.9598\n",
      "Train Epoch: 043 Batch: 00087/00094 | Loss: 563.5613 | CE: 0.2199 | KD: 1060.0262\n",
      "Train Epoch: 043 Batch: 00088/00094 | Loss: 563.4938 | CE: 0.1778 | KD: 1059.9785\n",
      "Train Epoch: 043 Batch: 00089/00094 | Loss: 563.5227 | CE: 0.1968 | KD: 1059.9972\n",
      "Train Epoch: 043 Batch: 00090/00094 | Loss: 563.5630 | CE: 0.1856 | KD: 1060.0940\n",
      "Train Epoch: 043 Batch: 00091/00094 | Loss: 563.5206 | CE: 0.1635 | KD: 1060.0558\n",
      "Train Epoch: 043 Batch: 00092/00094 | Loss: 563.5931 | CE: 0.2210 | KD: 1060.0841\n",
      "Train Epoch: 043 Batch: 00093/00094 | Loss: 563.5262 | CE: 0.1725 | KD: 1060.0494\n",
      "Train Epoch: 043 Batch: 00094/00094 | Loss: 563.6772 | CE: 0.3017 | KD: 1060.0903\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2658 | acc:91.5000\n",
      "[VAL Acc] Target: 91.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.0793 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9723 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0286 | acc:47.3282\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7282 | acc:59.2868\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.29%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8916 | acc:55.7301\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.73%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5323 | acc:72.9232\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9670 | acc:55.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4551 | acc:76.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.25%\n",
      "[VAL Acc] Avg 61.88%\n",
      "Train Epoch: 044 Batch: 00001/00094 | Loss: 563.6219 | CE: 0.2468 | KD: 1060.0896\n",
      "Train Epoch: 044 Batch: 00002/00094 | Loss: 563.5556 | CE: 0.2097 | KD: 1060.0348\n",
      "Train Epoch: 044 Batch: 00003/00094 | Loss: 563.5636 | CE: 0.2009 | KD: 1060.0663\n",
      "Train Epoch: 044 Batch: 00004/00094 | Loss: 563.5038 | CE: 0.1965 | KD: 1059.9623\n",
      "Train Epoch: 044 Batch: 00005/00094 | Loss: 563.5989 | CE: 0.2480 | KD: 1060.0443\n",
      "Train Epoch: 044 Batch: 00006/00094 | Loss: 563.6567 | CE: 0.2478 | KD: 1060.1532\n",
      "Train Epoch: 044 Batch: 00007/00094 | Loss: 563.5863 | CE: 0.2288 | KD: 1060.0566\n",
      "Train Epoch: 044 Batch: 00008/00094 | Loss: 563.5480 | CE: 0.2205 | KD: 1060.0001\n",
      "Train Epoch: 044 Batch: 00009/00094 | Loss: 563.6038 | CE: 0.2594 | KD: 1060.0320\n",
      "Train Epoch: 044 Batch: 00010/00094 | Loss: 563.5118 | CE: 0.1989 | KD: 1059.9727\n",
      "Train Epoch: 044 Batch: 00011/00094 | Loss: 563.6520 | CE: 0.2597 | KD: 1060.1221\n",
      "Train Epoch: 044 Batch: 00012/00094 | Loss: 563.5980 | CE: 0.2355 | KD: 1060.0659\n",
      "Train Epoch: 044 Batch: 00013/00094 | Loss: 563.6837 | CE: 0.2814 | KD: 1060.1407\n",
      "Train Epoch: 044 Batch: 00014/00094 | Loss: 563.4649 | CE: 0.1914 | KD: 1059.8987\n",
      "Train Epoch: 044 Batch: 00015/00094 | Loss: 563.5227 | CE: 0.2096 | KD: 1059.9730\n",
      "Train Epoch: 044 Batch: 00016/00094 | Loss: 563.5591 | CE: 0.2152 | KD: 1060.0310\n",
      "Train Epoch: 044 Batch: 00017/00094 | Loss: 563.4519 | CE: 0.1631 | KD: 1059.9272\n",
      "Train Epoch: 044 Batch: 00018/00094 | Loss: 563.5963 | CE: 0.2434 | KD: 1060.0480\n",
      "Train Epoch: 044 Batch: 00019/00094 | Loss: 563.6456 | CE: 0.2719 | KD: 1060.0869\n",
      "Train Epoch: 044 Batch: 00020/00094 | Loss: 563.5986 | CE: 0.2330 | KD: 1060.0718\n",
      "Train Epoch: 044 Batch: 00021/00094 | Loss: 563.5298 | CE: 0.2230 | KD: 1059.9612\n",
      "Train Epoch: 044 Batch: 00022/00094 | Loss: 563.5793 | CE: 0.2372 | KD: 1060.0276\n",
      "Train Epoch: 044 Batch: 00023/00094 | Loss: 563.5086 | CE: 0.1889 | KD: 1059.9854\n",
      "Train Epoch: 044 Batch: 00024/00094 | Loss: 563.6466 | CE: 0.2993 | KD: 1060.0374\n",
      "Train Epoch: 044 Batch: 00025/00094 | Loss: 563.5570 | CE: 0.2227 | KD: 1060.0129\n",
      "Train Epoch: 044 Batch: 00026/00094 | Loss: 563.8211 | CE: 0.4295 | KD: 1060.1207\n",
      "Train Epoch: 044 Batch: 00027/00094 | Loss: 563.6003 | CE: 0.2700 | KD: 1060.0054\n",
      "Train Epoch: 044 Batch: 00028/00094 | Loss: 563.5484 | CE: 0.2009 | KD: 1060.0377\n",
      "Train Epoch: 044 Batch: 00029/00094 | Loss: 563.6015 | CE: 0.2560 | KD: 1060.0341\n",
      "Train Epoch: 044 Batch: 00030/00094 | Loss: 563.6595 | CE: 0.2733 | KD: 1060.1107\n",
      "Train Epoch: 044 Batch: 00031/00094 | Loss: 563.5981 | CE: 0.2422 | KD: 1060.0535\n",
      "Train Epoch: 044 Batch: 00032/00094 | Loss: 563.6608 | CE: 0.2918 | KD: 1060.0784\n",
      "Train Epoch: 044 Batch: 00033/00094 | Loss: 563.6174 | CE: 0.2242 | KD: 1060.1238\n",
      "Train Epoch: 044 Batch: 00034/00094 | Loss: 563.6304 | CE: 0.2796 | KD: 1060.0439\n",
      "Train Epoch: 044 Batch: 00035/00094 | Loss: 563.6258 | CE: 0.2873 | KD: 1060.0209\n",
      "Train Epoch: 044 Batch: 00036/00094 | Loss: 563.6260 | CE: 0.2361 | KD: 1060.1176\n",
      "Train Epoch: 044 Batch: 00037/00094 | Loss: 563.5423 | CE: 0.2043 | KD: 1060.0199\n",
      "Train Epoch: 044 Batch: 00038/00094 | Loss: 563.6808 | CE: 0.3091 | KD: 1060.0833\n",
      "Train Epoch: 044 Batch: 00039/00094 | Loss: 563.5978 | CE: 0.2368 | KD: 1060.0632\n",
      "Train Epoch: 044 Batch: 00040/00094 | Loss: 563.6140 | CE: 0.2426 | KD: 1060.0826\n",
      "Train Epoch: 044 Batch: 00041/00094 | Loss: 563.5428 | CE: 0.2143 | KD: 1060.0020\n",
      "Train Epoch: 044 Batch: 00042/00094 | Loss: 563.5286 | CE: 0.2204 | KD: 1059.9640\n",
      "Train Epoch: 044 Batch: 00043/00094 | Loss: 563.6771 | CE: 0.2915 | KD: 1060.1094\n",
      "Train Epoch: 044 Batch: 00044/00094 | Loss: 563.4944 | CE: 0.1718 | KD: 1059.9910\n",
      "Train Epoch: 044 Batch: 00045/00094 | Loss: 563.5120 | CE: 0.1917 | KD: 1059.9865\n",
      "Train Epoch: 044 Batch: 00046/00094 | Loss: 563.5367 | CE: 0.1709 | KD: 1060.0720\n",
      "Train Epoch: 044 Batch: 00047/00094 | Loss: 563.6033 | CE: 0.2166 | KD: 1060.1115\n",
      "Train Epoch: 044 Batch: 00048/00094 | Loss: 563.5421 | CE: 0.1850 | KD: 1060.0559\n",
      "Train Epoch: 044 Batch: 00049/00094 | Loss: 563.5245 | CE: 0.2017 | KD: 1059.9915\n",
      "Train Epoch: 044 Batch: 00050/00094 | Loss: 563.4998 | CE: 0.1598 | KD: 1060.0237\n",
      "Train Epoch: 044 Batch: 00051/00094 | Loss: 563.5382 | CE: 0.1968 | KD: 1060.0262\n",
      "Train Epoch: 044 Batch: 00052/00094 | Loss: 563.5917 | CE: 0.2315 | KD: 1060.0615\n",
      "Train Epoch: 044 Batch: 00053/00094 | Loss: 563.5812 | CE: 0.1957 | KD: 1060.1091\n",
      "Train Epoch: 044 Batch: 00054/00094 | Loss: 563.5251 | CE: 0.1651 | KD: 1060.0614\n",
      "Train Epoch: 044 Batch: 00055/00094 | Loss: 563.6066 | CE: 0.2337 | KD: 1060.0856\n",
      "Train Epoch: 044 Batch: 00056/00094 | Loss: 563.5306 | CE: 0.2014 | KD: 1060.0034\n",
      "Train Epoch: 044 Batch: 00057/00094 | Loss: 563.6716 | CE: 0.2671 | KD: 1060.1450\n",
      "Train Epoch: 044 Batch: 00058/00094 | Loss: 563.6572 | CE: 0.2513 | KD: 1060.1476\n",
      "Train Epoch: 044 Batch: 00059/00094 | Loss: 563.7014 | CE: 0.2588 | KD: 1060.2168\n",
      "Train Epoch: 044 Batch: 00060/00094 | Loss: 563.5053 | CE: 0.1628 | KD: 1060.0284\n",
      "Train Epoch: 044 Batch: 00061/00094 | Loss: 563.6226 | CE: 0.2383 | KD: 1060.1069\n",
      "Train Epoch: 044 Batch: 00062/00094 | Loss: 563.5600 | CE: 0.2217 | KD: 1060.0205\n",
      "Train Epoch: 044 Batch: 00063/00094 | Loss: 563.6247 | CE: 0.2769 | KD: 1060.0383\n",
      "Train Epoch: 044 Batch: 00064/00094 | Loss: 563.6093 | CE: 0.2384 | KD: 1060.0817\n",
      "Train Epoch: 044 Batch: 00065/00094 | Loss: 563.5359 | CE: 0.1979 | KD: 1060.0200\n",
      "Train Epoch: 044 Batch: 00066/00094 | Loss: 563.5939 | CE: 0.2138 | KD: 1060.0992\n",
      "Train Epoch: 044 Batch: 00067/00094 | Loss: 563.6182 | CE: 0.2688 | KD: 1060.0413\n",
      "Train Epoch: 044 Batch: 00068/00094 | Loss: 563.6548 | CE: 0.2750 | KD: 1060.0986\n",
      "Train Epoch: 044 Batch: 00069/00094 | Loss: 563.5934 | CE: 0.2626 | KD: 1060.0063\n",
      "Train Epoch: 044 Batch: 00070/00094 | Loss: 563.5848 | CE: 0.2220 | KD: 1060.0665\n",
      "Train Epoch: 044 Batch: 00071/00094 | Loss: 563.5757 | CE: 0.2098 | KD: 1060.0724\n",
      "Train Epoch: 044 Batch: 00072/00094 | Loss: 563.5793 | CE: 0.2224 | KD: 1060.0555\n",
      "Train Epoch: 044 Batch: 00073/00094 | Loss: 563.5262 | CE: 0.1791 | KD: 1060.0370\n",
      "Train Epoch: 044 Batch: 00074/00094 | Loss: 563.5203 | CE: 0.1794 | KD: 1060.0253\n",
      "Train Epoch: 044 Batch: 00075/00094 | Loss: 563.6041 | CE: 0.1800 | KD: 1060.1819\n",
      "Train Epoch: 044 Batch: 00076/00094 | Loss: 563.6665 | CE: 0.2980 | KD: 1060.0773\n",
      "Train Epoch: 044 Batch: 00077/00094 | Loss: 563.5900 | CE: 0.2247 | KD: 1060.0713\n",
      "Train Epoch: 044 Batch: 00078/00094 | Loss: 563.5256 | CE: 0.2097 | KD: 1059.9783\n",
      "Train Epoch: 044 Batch: 00079/00094 | Loss: 563.5231 | CE: 0.1828 | KD: 1060.0243\n",
      "Train Epoch: 044 Batch: 00080/00094 | Loss: 563.5394 | CE: 0.1764 | KD: 1060.0670\n",
      "Train Epoch: 044 Batch: 00081/00094 | Loss: 563.5806 | CE: 0.2318 | KD: 1060.0402\n",
      "Train Epoch: 044 Batch: 00082/00094 | Loss: 563.5560 | CE: 0.2088 | KD: 1060.0370\n",
      "Train Epoch: 044 Batch: 00083/00094 | Loss: 563.5786 | CE: 0.2396 | KD: 1060.0217\n",
      "Train Epoch: 044 Batch: 00084/00094 | Loss: 563.7087 | CE: 0.3329 | KD: 1060.0909\n",
      "Train Epoch: 044 Batch: 00085/00094 | Loss: 563.6062 | CE: 0.2803 | KD: 1059.9971\n",
      "Train Epoch: 044 Batch: 00086/00094 | Loss: 563.5253 | CE: 0.2004 | KD: 1059.9954\n",
      "Train Epoch: 044 Batch: 00087/00094 | Loss: 563.6504 | CE: 0.2717 | KD: 1060.0966\n",
      "Train Epoch: 044 Batch: 00088/00094 | Loss: 563.5928 | CE: 0.2294 | KD: 1060.0677\n",
      "Train Epoch: 044 Batch: 00089/00094 | Loss: 563.6323 | CE: 0.2525 | KD: 1060.0986\n",
      "Train Epoch: 044 Batch: 00090/00094 | Loss: 563.5775 | CE: 0.2170 | KD: 1060.0621\n",
      "Train Epoch: 044 Batch: 00091/00094 | Loss: 563.7062 | CE: 0.2968 | KD: 1060.1543\n",
      "Train Epoch: 044 Batch: 00092/00094 | Loss: 563.5303 | CE: 0.1854 | KD: 1060.0328\n",
      "Train Epoch: 044 Batch: 00093/00094 | Loss: 563.6655 | CE: 0.3061 | KD: 1060.0601\n",
      "Train Epoch: 044 Batch: 00094/00094 | Loss: 563.5491 | CE: 0.2015 | KD: 1060.0380\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2562 | acc:91.1000\n",
      "[VAL Acc] Target: 91.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3374 | acc:50.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1515 | acc:48.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2125 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7624 | acc:60.7367\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8660 | acc:57.5786\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 57.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5460 | acc:74.3730\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9903 | acc:55.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5665 | acc:70.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 70.00%\n",
      "[VAL Acc] Avg 61.94%\n",
      "Train Epoch: 045 Batch: 00001/00094 | Loss: 563.6154 | CE: 0.2548 | KD: 1060.0625\n",
      "Train Epoch: 045 Batch: 00002/00094 | Loss: 563.6063 | CE: 0.2372 | KD: 1060.0784\n",
      "Train Epoch: 045 Batch: 00003/00094 | Loss: 563.5776 | CE: 0.2317 | KD: 1060.0347\n",
      "Train Epoch: 045 Batch: 00004/00094 | Loss: 563.6299 | CE: 0.2591 | KD: 1060.0814\n",
      "Train Epoch: 045 Batch: 00005/00094 | Loss: 563.5631 | CE: 0.2091 | KD: 1060.0499\n",
      "Train Epoch: 045 Batch: 00006/00094 | Loss: 563.5259 | CE: 0.2199 | KD: 1059.9596\n",
      "Train Epoch: 045 Batch: 00007/00094 | Loss: 563.5428 | CE: 0.1862 | KD: 1060.0551\n",
      "Train Epoch: 045 Batch: 00008/00094 | Loss: 563.5655 | CE: 0.2246 | KD: 1060.0253\n",
      "Train Epoch: 045 Batch: 00009/00094 | Loss: 563.5611 | CE: 0.1896 | KD: 1060.0829\n",
      "Train Epoch: 045 Batch: 00010/00094 | Loss: 563.5240 | CE: 0.1897 | KD: 1060.0128\n",
      "Train Epoch: 045 Batch: 00011/00094 | Loss: 563.6417 | CE: 0.2878 | KD: 1060.0499\n",
      "Train Epoch: 045 Batch: 00012/00094 | Loss: 563.5521 | CE: 0.2348 | KD: 1059.9810\n",
      "Train Epoch: 045 Batch: 00013/00094 | Loss: 563.6138 | CE: 0.2567 | KD: 1060.0559\n",
      "Train Epoch: 045 Batch: 00014/00094 | Loss: 563.6421 | CE: 0.2373 | KD: 1060.1454\n",
      "Train Epoch: 045 Batch: 00015/00094 | Loss: 563.6042 | CE: 0.2625 | KD: 1060.0269\n",
      "Train Epoch: 045 Batch: 00016/00094 | Loss: 563.5604 | CE: 0.2078 | KD: 1060.0472\n",
      "Train Epoch: 045 Batch: 00017/00094 | Loss: 563.5936 | CE: 0.2570 | KD: 1060.0172\n",
      "Train Epoch: 045 Batch: 00018/00094 | Loss: 563.6270 | CE: 0.2309 | KD: 1060.1290\n",
      "Train Epoch: 045 Batch: 00019/00094 | Loss: 563.7220 | CE: 0.3265 | KD: 1060.1281\n",
      "Train Epoch: 045 Batch: 00020/00094 | Loss: 563.5820 | CE: 0.2224 | KD: 1060.0604\n",
      "Train Epoch: 045 Batch: 00021/00094 | Loss: 563.6230 | CE: 0.2414 | KD: 1060.1019\n",
      "Train Epoch: 045 Batch: 00022/00094 | Loss: 563.6147 | CE: 0.2728 | KD: 1060.0271\n",
      "Train Epoch: 045 Batch: 00023/00094 | Loss: 563.5715 | CE: 0.1747 | KD: 1060.1305\n",
      "Train Epoch: 045 Batch: 00024/00094 | Loss: 563.5245 | CE: 0.1928 | KD: 1060.0081\n",
      "Train Epoch: 045 Batch: 00025/00094 | Loss: 563.5360 | CE: 0.2093 | KD: 1059.9985\n",
      "Train Epoch: 045 Batch: 00026/00094 | Loss: 563.6416 | CE: 0.2399 | KD: 1060.1398\n",
      "Train Epoch: 045 Batch: 00027/00094 | Loss: 563.5733 | CE: 0.2211 | KD: 1060.0466\n",
      "Train Epoch: 045 Batch: 00028/00094 | Loss: 563.4896 | CE: 0.2044 | KD: 1059.9205\n",
      "Train Epoch: 045 Batch: 00029/00094 | Loss: 563.5940 | CE: 0.2507 | KD: 1060.0299\n",
      "Train Epoch: 045 Batch: 00030/00094 | Loss: 563.5576 | CE: 0.2070 | KD: 1060.0437\n",
      "Train Epoch: 045 Batch: 00031/00094 | Loss: 563.6077 | CE: 0.2360 | KD: 1060.0833\n",
      "Train Epoch: 045 Batch: 00032/00094 | Loss: 563.6294 | CE: 0.2486 | KD: 1060.1006\n",
      "Train Epoch: 045 Batch: 00033/00094 | Loss: 563.6185 | CE: 0.2683 | KD: 1060.0428\n",
      "Train Epoch: 045 Batch: 00034/00094 | Loss: 563.5172 | CE: 0.2023 | KD: 1059.9764\n",
      "Train Epoch: 045 Batch: 00035/00094 | Loss: 563.5220 | CE: 0.1799 | KD: 1060.0277\n",
      "Train Epoch: 045 Batch: 00036/00094 | Loss: 563.4937 | CE: 0.1590 | KD: 1060.0135\n",
      "Train Epoch: 045 Batch: 00037/00094 | Loss: 563.6138 | CE: 0.2276 | KD: 1060.1106\n",
      "Train Epoch: 045 Batch: 00038/00094 | Loss: 563.5837 | CE: 0.2588 | KD: 1059.9951\n",
      "Train Epoch: 045 Batch: 00039/00094 | Loss: 563.4960 | CE: 0.1609 | KD: 1060.0144\n",
      "Train Epoch: 045 Batch: 00040/00094 | Loss: 563.4921 | CE: 0.1588 | KD: 1060.0111\n",
      "Train Epoch: 045 Batch: 00041/00094 | Loss: 563.6143 | CE: 0.2510 | KD: 1060.0676\n",
      "Train Epoch: 045 Batch: 00042/00094 | Loss: 563.6063 | CE: 0.2535 | KD: 1060.0479\n",
      "Train Epoch: 045 Batch: 00043/00094 | Loss: 563.5338 | CE: 0.2123 | KD: 1059.9888\n",
      "Train Epoch: 045 Batch: 00044/00094 | Loss: 563.6155 | CE: 0.2539 | KD: 1060.0645\n",
      "Train Epoch: 045 Batch: 00045/00094 | Loss: 563.6714 | CE: 0.2975 | KD: 1060.0875\n",
      "Train Epoch: 045 Batch: 00046/00094 | Loss: 563.6453 | CE: 0.3008 | KD: 1060.0321\n",
      "Train Epoch: 045 Batch: 00047/00094 | Loss: 563.5886 | CE: 0.2566 | KD: 1060.0087\n",
      "Train Epoch: 045 Batch: 00048/00094 | Loss: 563.6573 | CE: 0.2746 | KD: 1060.1039\n",
      "Train Epoch: 045 Batch: 00049/00094 | Loss: 563.5261 | CE: 0.2013 | KD: 1059.9950\n",
      "Train Epoch: 045 Batch: 00050/00094 | Loss: 563.5911 | CE: 0.2558 | KD: 1060.0149\n",
      "Train Epoch: 045 Batch: 00051/00094 | Loss: 563.5223 | CE: 0.2165 | KD: 1059.9594\n",
      "Train Epoch: 045 Batch: 00052/00094 | Loss: 563.5738 | CE: 0.2292 | KD: 1060.0323\n",
      "Train Epoch: 045 Batch: 00053/00094 | Loss: 563.5112 | CE: 0.1747 | KD: 1060.0171\n",
      "Train Epoch: 045 Batch: 00054/00094 | Loss: 563.5903 | CE: 0.2485 | KD: 1060.0271\n",
      "Train Epoch: 045 Batch: 00055/00094 | Loss: 563.5170 | CE: 0.1920 | KD: 1059.9955\n",
      "Train Epoch: 045 Batch: 00056/00094 | Loss: 563.5473 | CE: 0.1895 | KD: 1060.0571\n",
      "Train Epoch: 045 Batch: 00057/00094 | Loss: 563.5994 | CE: 0.2189 | KD: 1060.0999\n",
      "Train Epoch: 045 Batch: 00058/00094 | Loss: 563.5996 | CE: 0.2539 | KD: 1060.0344\n",
      "Train Epoch: 045 Batch: 00059/00094 | Loss: 563.5464 | CE: 0.2233 | KD: 1059.9919\n",
      "Train Epoch: 045 Batch: 00060/00094 | Loss: 563.5229 | CE: 0.1708 | KD: 1060.0464\n",
      "Train Epoch: 045 Batch: 00061/00094 | Loss: 563.5140 | CE: 0.1744 | KD: 1060.0228\n",
      "Train Epoch: 045 Batch: 00062/00094 | Loss: 563.5884 | CE: 0.2294 | KD: 1060.0593\n",
      "Train Epoch: 045 Batch: 00063/00094 | Loss: 563.6107 | CE: 0.2531 | KD: 1060.0568\n",
      "Train Epoch: 045 Batch: 00064/00094 | Loss: 563.5166 | CE: 0.1772 | KD: 1060.0226\n",
      "Train Epoch: 045 Batch: 00065/00094 | Loss: 563.6375 | CE: 0.2202 | KD: 1060.1689\n",
      "Train Epoch: 045 Batch: 00066/00094 | Loss: 563.5548 | CE: 0.2076 | KD: 1060.0372\n",
      "Train Epoch: 045 Batch: 00067/00094 | Loss: 563.5457 | CE: 0.1879 | KD: 1060.0571\n",
      "Train Epoch: 045 Batch: 00068/00094 | Loss: 563.5207 | CE: 0.1787 | KD: 1060.0273\n",
      "Train Epoch: 045 Batch: 00069/00094 | Loss: 563.5873 | CE: 0.2101 | KD: 1060.0938\n",
      "Train Epoch: 045 Batch: 00070/00094 | Loss: 563.5447 | CE: 0.1763 | KD: 1060.0770\n",
      "Train Epoch: 045 Batch: 00071/00094 | Loss: 563.5241 | CE: 0.1841 | KD: 1060.0237\n",
      "Train Epoch: 045 Batch: 00072/00094 | Loss: 563.6307 | CE: 0.2480 | KD: 1060.1040\n",
      "Train Epoch: 045 Batch: 00073/00094 | Loss: 563.5925 | CE: 0.2423 | KD: 1060.0426\n",
      "Train Epoch: 045 Batch: 00074/00094 | Loss: 563.4825 | CE: 0.1809 | KD: 1059.9513\n",
      "Train Epoch: 045 Batch: 00075/00094 | Loss: 563.5477 | CE: 0.2604 | KD: 1059.9244\n",
      "Train Epoch: 045 Batch: 00076/00094 | Loss: 563.6828 | CE: 0.2791 | KD: 1060.1434\n",
      "Train Epoch: 045 Batch: 00077/00094 | Loss: 563.6122 | CE: 0.2620 | KD: 1060.0428\n",
      "Train Epoch: 045 Batch: 00078/00094 | Loss: 563.7099 | CE: 0.3080 | KD: 1060.1401\n",
      "Train Epoch: 045 Batch: 00079/00094 | Loss: 563.7000 | CE: 0.3053 | KD: 1060.1265\n",
      "Train Epoch: 045 Batch: 00080/00094 | Loss: 563.5079 | CE: 0.1785 | KD: 1060.0038\n",
      "Train Epoch: 045 Batch: 00081/00094 | Loss: 563.7084 | CE: 0.3136 | KD: 1060.1270\n",
      "Train Epoch: 045 Batch: 00082/00094 | Loss: 563.6727 | CE: 0.2976 | KD: 1060.0897\n",
      "Train Epoch: 045 Batch: 00083/00094 | Loss: 563.5948 | CE: 0.2094 | KD: 1060.1091\n",
      "Train Epoch: 045 Batch: 00084/00094 | Loss: 563.5997 | CE: 0.2679 | KD: 1060.0083\n",
      "Train Epoch: 045 Batch: 00085/00094 | Loss: 563.5917 | CE: 0.2501 | KD: 1060.0266\n",
      "Train Epoch: 045 Batch: 00086/00094 | Loss: 563.5755 | CE: 0.2286 | KD: 1060.0366\n",
      "Train Epoch: 045 Batch: 00087/00094 | Loss: 563.5863 | CE: 0.2365 | KD: 1060.0421\n",
      "Train Epoch: 045 Batch: 00088/00094 | Loss: 563.5895 | CE: 0.2199 | KD: 1060.0795\n",
      "Train Epoch: 045 Batch: 00089/00094 | Loss: 563.6323 | CE: 0.2741 | KD: 1060.0579\n",
      "Train Epoch: 045 Batch: 00090/00094 | Loss: 563.5953 | CE: 0.2256 | KD: 1060.0796\n",
      "Train Epoch: 045 Batch: 00091/00094 | Loss: 563.7365 | CE: 0.3710 | KD: 1060.0717\n",
      "Train Epoch: 045 Batch: 00092/00094 | Loss: 563.6564 | CE: 0.2742 | KD: 1060.1029\n",
      "Train Epoch: 045 Batch: 00093/00094 | Loss: 563.6233 | CE: 0.2685 | KD: 1060.0515\n",
      "Train Epoch: 045 Batch: 00094/00094 | Loss: 563.7123 | CE: 0.3146 | KD: 1060.1322\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2471 | acc:91.9000\n",
      "[VAL Acc] Target: 91.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2123 | acc:50.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0283 | acc:50.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1081 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7785 | acc:59.0517\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8842 | acc:54.6211\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5577 | acc:72.8056\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9736 | acc:55.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4801 | acc:75.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.30%\n",
      "[VAL Acc] Avg 62.23%\n",
      "Train Epoch: 046 Batch: 00001/00094 | Loss: 563.5536 | CE: 0.1812 | KD: 1060.0848\n",
      "Train Epoch: 046 Batch: 00002/00094 | Loss: 563.6138 | CE: 0.2330 | KD: 1060.1003\n",
      "Train Epoch: 046 Batch: 00003/00094 | Loss: 563.5321 | CE: 0.2264 | KD: 1059.9591\n",
      "Train Epoch: 046 Batch: 00004/00094 | Loss: 563.5453 | CE: 0.1909 | KD: 1060.0508\n",
      "Train Epoch: 046 Batch: 00005/00094 | Loss: 563.5258 | CE: 0.2092 | KD: 1059.9796\n",
      "Train Epoch: 046 Batch: 00006/00094 | Loss: 563.5225 | CE: 0.1814 | KD: 1060.0258\n",
      "Train Epoch: 046 Batch: 00007/00094 | Loss: 563.5760 | CE: 0.2185 | KD: 1060.0565\n",
      "Train Epoch: 046 Batch: 00008/00094 | Loss: 563.5467 | CE: 0.2059 | KD: 1060.0250\n",
      "Train Epoch: 046 Batch: 00009/00094 | Loss: 563.5540 | CE: 0.1945 | KD: 1060.0602\n",
      "Train Epoch: 046 Batch: 00010/00094 | Loss: 563.5723 | CE: 0.2059 | KD: 1060.0735\n",
      "Train Epoch: 046 Batch: 00011/00094 | Loss: 563.5273 | CE: 0.2059 | KD: 1059.9885\n",
      "Train Epoch: 046 Batch: 00012/00094 | Loss: 563.6110 | CE: 0.2608 | KD: 1060.0428\n",
      "Train Epoch: 046 Batch: 00013/00094 | Loss: 563.5067 | CE: 0.1974 | KD: 1059.9658\n",
      "Train Epoch: 046 Batch: 00014/00094 | Loss: 563.6894 | CE: 0.2752 | KD: 1060.1632\n",
      "Train Epoch: 046 Batch: 00015/00094 | Loss: 563.5841 | CE: 0.2180 | KD: 1060.0729\n",
      "Train Epoch: 046 Batch: 00016/00094 | Loss: 563.4789 | CE: 0.1749 | KD: 1059.9561\n",
      "Train Epoch: 046 Batch: 00017/00094 | Loss: 563.5494 | CE: 0.1801 | KD: 1060.0786\n",
      "Train Epoch: 046 Batch: 00018/00094 | Loss: 563.5851 | CE: 0.2250 | KD: 1060.0615\n",
      "Train Epoch: 046 Batch: 00019/00094 | Loss: 563.5475 | CE: 0.1840 | KD: 1060.0679\n",
      "Train Epoch: 046 Batch: 00020/00094 | Loss: 563.6425 | CE: 0.2534 | KD: 1060.1160\n",
      "Train Epoch: 046 Batch: 00021/00094 | Loss: 563.5444 | CE: 0.1513 | KD: 1060.1237\n",
      "Train Epoch: 046 Batch: 00022/00094 | Loss: 563.6290 | CE: 0.2586 | KD: 1060.0809\n",
      "Train Epoch: 046 Batch: 00023/00094 | Loss: 563.7001 | CE: 0.2807 | KD: 1060.1730\n",
      "Train Epoch: 046 Batch: 00024/00094 | Loss: 563.5190 | CE: 0.1875 | KD: 1060.0078\n",
      "Train Epoch: 046 Batch: 00025/00094 | Loss: 563.6053 | CE: 0.2476 | KD: 1060.0570\n",
      "Train Epoch: 046 Batch: 00026/00094 | Loss: 563.5912 | CE: 0.2561 | KD: 1060.0145\n",
      "Train Epoch: 046 Batch: 00027/00094 | Loss: 563.6907 | CE: 0.3088 | KD: 1060.1025\n",
      "Train Epoch: 046 Batch: 00028/00094 | Loss: 563.6166 | CE: 0.2457 | KD: 1060.0817\n",
      "Train Epoch: 046 Batch: 00029/00094 | Loss: 563.5366 | CE: 0.2172 | KD: 1059.9849\n",
      "Train Epoch: 046 Batch: 00030/00094 | Loss: 563.5869 | CE: 0.2316 | KD: 1060.0524\n",
      "Train Epoch: 046 Batch: 00031/00094 | Loss: 563.6851 | CE: 0.3330 | KD: 1060.0464\n",
      "Train Epoch: 046 Batch: 00032/00094 | Loss: 563.4868 | CE: 0.1686 | KD: 1059.9827\n",
      "Train Epoch: 046 Batch: 00033/00094 | Loss: 563.6346 | CE: 0.3011 | KD: 1060.0115\n",
      "Train Epoch: 046 Batch: 00034/00094 | Loss: 563.6786 | CE: 0.3110 | KD: 1060.0757\n",
      "Train Epoch: 046 Batch: 00035/00094 | Loss: 563.6166 | CE: 0.2303 | KD: 1060.1107\n",
      "Train Epoch: 046 Batch: 00036/00094 | Loss: 563.6321 | CE: 0.2941 | KD: 1060.0199\n",
      "Train Epoch: 046 Batch: 00037/00094 | Loss: 563.7036 | CE: 0.3425 | KD: 1060.0635\n",
      "Train Epoch: 046 Batch: 00038/00094 | Loss: 563.4877 | CE: 0.1853 | KD: 1059.9528\n",
      "Train Epoch: 046 Batch: 00039/00094 | Loss: 563.5629 | CE: 0.2292 | KD: 1060.0118\n",
      "Train Epoch: 046 Batch: 00040/00094 | Loss: 563.5694 | CE: 0.2414 | KD: 1060.0010\n",
      "Train Epoch: 046 Batch: 00041/00094 | Loss: 563.5468 | CE: 0.2121 | KD: 1060.0135\n",
      "Train Epoch: 046 Batch: 00042/00094 | Loss: 563.6212 | CE: 0.2491 | KD: 1060.0840\n",
      "Train Epoch: 046 Batch: 00043/00094 | Loss: 563.5428 | CE: 0.2287 | KD: 1059.9750\n",
      "Train Epoch: 046 Batch: 00044/00094 | Loss: 563.5443 | CE: 0.1876 | KD: 1060.0551\n",
      "Train Epoch: 046 Batch: 00045/00094 | Loss: 563.6113 | CE: 0.2625 | KD: 1060.0402\n",
      "Train Epoch: 046 Batch: 00046/00094 | Loss: 563.5264 | CE: 0.2164 | KD: 1059.9672\n",
      "Train Epoch: 046 Batch: 00047/00094 | Loss: 563.5555 | CE: 0.1943 | KD: 1060.0637\n",
      "Train Epoch: 046 Batch: 00048/00094 | Loss: 563.5063 | CE: 0.1926 | KD: 1059.9742\n",
      "Train Epoch: 046 Batch: 00049/00094 | Loss: 563.5367 | CE: 0.1973 | KD: 1060.0225\n",
      "Train Epoch: 046 Batch: 00050/00094 | Loss: 563.5806 | CE: 0.2039 | KD: 1060.0929\n",
      "Train Epoch: 046 Batch: 00051/00094 | Loss: 563.5218 | CE: 0.1845 | KD: 1060.0186\n",
      "Train Epoch: 046 Batch: 00052/00094 | Loss: 563.6143 | CE: 0.2470 | KD: 1060.0750\n",
      "Train Epoch: 046 Batch: 00053/00094 | Loss: 563.5390 | CE: 0.1882 | KD: 1060.0441\n",
      "Train Epoch: 046 Batch: 00054/00094 | Loss: 563.4534 | CE: 0.1637 | KD: 1059.9290\n",
      "Train Epoch: 046 Batch: 00055/00094 | Loss: 563.5663 | CE: 0.2042 | KD: 1060.0653\n",
      "Train Epoch: 046 Batch: 00056/00094 | Loss: 563.6582 | CE: 0.2690 | KD: 1060.1162\n",
      "Train Epoch: 046 Batch: 00057/00094 | Loss: 563.5952 | CE: 0.2587 | KD: 1060.0171\n",
      "Train Epoch: 046 Batch: 00058/00094 | Loss: 563.5795 | CE: 0.2338 | KD: 1060.0344\n",
      "Train Epoch: 046 Batch: 00059/00094 | Loss: 563.5686 | CE: 0.2452 | KD: 1059.9924\n",
      "Train Epoch: 046 Batch: 00060/00094 | Loss: 563.5795 | CE: 0.2308 | KD: 1060.0402\n",
      "Train Epoch: 046 Batch: 00061/00094 | Loss: 563.5974 | CE: 0.2499 | KD: 1060.0378\n",
      "Train Epoch: 046 Batch: 00062/00094 | Loss: 563.5385 | CE: 0.1903 | KD: 1060.0391\n",
      "Train Epoch: 046 Batch: 00063/00094 | Loss: 563.5104 | CE: 0.1951 | KD: 1059.9772\n",
      "Train Epoch: 046 Batch: 00064/00094 | Loss: 563.5386 | CE: 0.2015 | KD: 1060.0183\n",
      "Train Epoch: 046 Batch: 00065/00094 | Loss: 563.5662 | CE: 0.2148 | KD: 1060.0450\n",
      "Train Epoch: 046 Batch: 00066/00094 | Loss: 563.5681 | CE: 0.2098 | KD: 1060.0580\n",
      "Train Epoch: 046 Batch: 00067/00094 | Loss: 563.6254 | CE: 0.2251 | KD: 1060.1372\n",
      "Train Epoch: 046 Batch: 00068/00094 | Loss: 563.6035 | CE: 0.2129 | KD: 1060.1189\n",
      "Train Epoch: 046 Batch: 00069/00094 | Loss: 563.6352 | CE: 0.2167 | KD: 1060.1713\n",
      "Train Epoch: 046 Batch: 00070/00094 | Loss: 563.5794 | CE: 0.2342 | KD: 1060.0334\n",
      "Train Epoch: 046 Batch: 00071/00094 | Loss: 563.4894 | CE: 0.1706 | KD: 1059.9838\n",
      "Train Epoch: 046 Batch: 00072/00094 | Loss: 563.5349 | CE: 0.2184 | KD: 1059.9794\n",
      "Train Epoch: 046 Batch: 00073/00094 | Loss: 563.5765 | CE: 0.1969 | KD: 1060.0983\n",
      "Train Epoch: 046 Batch: 00074/00094 | Loss: 563.6039 | CE: 0.2303 | KD: 1060.0868\n",
      "Train Epoch: 046 Batch: 00075/00094 | Loss: 563.6992 | CE: 0.3271 | KD: 1060.0841\n",
      "Train Epoch: 046 Batch: 00076/00094 | Loss: 563.5728 | CE: 0.2115 | KD: 1060.0637\n",
      "Train Epoch: 046 Batch: 00077/00094 | Loss: 563.6680 | CE: 0.2899 | KD: 1060.0953\n",
      "Train Epoch: 046 Batch: 00078/00094 | Loss: 563.5554 | CE: 0.1705 | KD: 1060.1082\n",
      "Train Epoch: 046 Batch: 00079/00094 | Loss: 563.6169 | CE: 0.2389 | KD: 1060.0951\n",
      "Train Epoch: 046 Batch: 00080/00094 | Loss: 563.6032 | CE: 0.2534 | KD: 1060.0421\n",
      "Train Epoch: 046 Batch: 00081/00094 | Loss: 563.5471 | CE: 0.1788 | KD: 1060.0769\n",
      "Train Epoch: 046 Batch: 00082/00094 | Loss: 563.6425 | CE: 0.2756 | KD: 1060.0743\n",
      "Train Epoch: 046 Batch: 00083/00094 | Loss: 563.5714 | CE: 0.2301 | KD: 1060.0260\n",
      "Train Epoch: 046 Batch: 00084/00094 | Loss: 563.6181 | CE: 0.2633 | KD: 1060.0515\n",
      "Train Epoch: 046 Batch: 00085/00094 | Loss: 563.6601 | CE: 0.3129 | KD: 1060.0371\n",
      "Train Epoch: 046 Batch: 00086/00094 | Loss: 563.4766 | CE: 0.1536 | KD: 1059.9916\n",
      "Train Epoch: 046 Batch: 00087/00094 | Loss: 563.5995 | CE: 0.2445 | KD: 1060.0520\n",
      "Train Epoch: 046 Batch: 00088/00094 | Loss: 563.5166 | CE: 0.1984 | KD: 1059.9825\n",
      "Train Epoch: 046 Batch: 00089/00094 | Loss: 563.6221 | CE: 0.2642 | KD: 1060.0574\n",
      "Train Epoch: 046 Batch: 00090/00094 | Loss: 563.6399 | CE: 0.2679 | KD: 1060.0839\n",
      "Train Epoch: 046 Batch: 00091/00094 | Loss: 563.6095 | CE: 0.2491 | KD: 1060.0620\n",
      "Train Epoch: 046 Batch: 00092/00094 | Loss: 563.6333 | CE: 0.2364 | KD: 1060.1306\n",
      "Train Epoch: 046 Batch: 00093/00094 | Loss: 563.6570 | CE: 0.2489 | KD: 1060.1516\n",
      "Train Epoch: 046 Batch: 00094/00094 | Loss: 563.5792 | CE: 0.2069 | KD: 1060.0846\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2461 | acc:91.5000\n",
      "[VAL Acc] Target: 91.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2188 | acc:50.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0654 | acc:48.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1091 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7714 | acc:58.1897\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8957 | acc:56.6543\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 56.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5654 | acc:72.4138\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9927 | acc:55.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4991 | acc:73.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.85%\n",
      "[VAL Acc] Avg 61.88%\n",
      "Train Epoch: 047 Batch: 00001/00094 | Loss: 507.2827 | CE: 0.2400 | KD: 1060.1002\n",
      "Train Epoch: 047 Batch: 00002/00094 | Loss: 507.2316 | CE: 0.2360 | KD: 1060.0018\n",
      "Train Epoch: 047 Batch: 00003/00094 | Loss: 507.2138 | CE: 0.2252 | KD: 1059.9873\n",
      "Train Epoch: 047 Batch: 00004/00094 | Loss: 507.2466 | CE: 0.2393 | KD: 1060.0264\n",
      "Train Epoch: 047 Batch: 00005/00094 | Loss: 507.1654 | CE: 0.1760 | KD: 1059.9889\n",
      "Train Epoch: 047 Batch: 00006/00094 | Loss: 507.3002 | CE: 0.2467 | KD: 1060.1228\n",
      "Train Epoch: 047 Batch: 00007/00094 | Loss: 507.2318 | CE: 0.2292 | KD: 1060.0165\n",
      "Train Epoch: 047 Batch: 00008/00094 | Loss: 507.2104 | CE: 0.2085 | KD: 1060.0149\n",
      "Train Epoch: 047 Batch: 00009/00094 | Loss: 507.2169 | CE: 0.2085 | KD: 1060.0286\n",
      "Train Epoch: 047 Batch: 00010/00094 | Loss: 507.2051 | CE: 0.2105 | KD: 1059.9998\n",
      "Train Epoch: 047 Batch: 00011/00094 | Loss: 507.2563 | CE: 0.2434 | KD: 1060.0380\n",
      "Train Epoch: 047 Batch: 00012/00094 | Loss: 507.2861 | CE: 0.2559 | KD: 1060.0742\n",
      "Train Epoch: 047 Batch: 00013/00094 | Loss: 507.2377 | CE: 0.2084 | KD: 1060.0723\n",
      "Train Epoch: 047 Batch: 00014/00094 | Loss: 507.1687 | CE: 0.1846 | KD: 1059.9778\n",
      "Train Epoch: 047 Batch: 00015/00094 | Loss: 507.3420 | CE: 0.2682 | KD: 1060.1654\n",
      "Train Epoch: 047 Batch: 00016/00094 | Loss: 507.2142 | CE: 0.2206 | KD: 1059.9977\n",
      "Train Epoch: 047 Batch: 00017/00094 | Loss: 507.1992 | CE: 0.2068 | KD: 1059.9952\n",
      "Train Epoch: 047 Batch: 00018/00094 | Loss: 507.3121 | CE: 0.2569 | KD: 1060.1265\n",
      "Train Epoch: 047 Batch: 00019/00094 | Loss: 507.2332 | CE: 0.2072 | KD: 1060.0653\n",
      "Train Epoch: 047 Batch: 00020/00094 | Loss: 507.2719 | CE: 0.2414 | KD: 1060.0748\n",
      "Train Epoch: 047 Batch: 00021/00094 | Loss: 507.3489 | CE: 0.2774 | KD: 1060.1605\n",
      "Train Epoch: 047 Batch: 00022/00094 | Loss: 507.1594 | CE: 0.1845 | KD: 1059.9584\n",
      "Train Epoch: 047 Batch: 00023/00094 | Loss: 507.3309 | CE: 0.3209 | KD: 1060.0319\n",
      "Train Epoch: 047 Batch: 00024/00094 | Loss: 507.1894 | CE: 0.1925 | KD: 1060.0044\n",
      "Train Epoch: 047 Batch: 00025/00094 | Loss: 507.3017 | CE: 0.2958 | KD: 1060.0234\n",
      "Train Epoch: 047 Batch: 00026/00094 | Loss: 507.1848 | CE: 0.1682 | KD: 1060.0458\n",
      "Train Epoch: 047 Batch: 00027/00094 | Loss: 507.1886 | CE: 0.1900 | KD: 1060.0082\n",
      "Train Epoch: 047 Batch: 00028/00094 | Loss: 507.2968 | CE: 0.2545 | KD: 1060.0995\n",
      "Train Epoch: 047 Batch: 00029/00094 | Loss: 507.2453 | CE: 0.1976 | KD: 1060.1108\n",
      "Train Epoch: 047 Batch: 00030/00094 | Loss: 507.2323 | CE: 0.2080 | KD: 1060.0618\n",
      "Train Epoch: 047 Batch: 00031/00094 | Loss: 507.3192 | CE: 0.2557 | KD: 1060.1437\n",
      "Train Epoch: 047 Batch: 00032/00094 | Loss: 507.2163 | CE: 0.1755 | KD: 1060.0966\n",
      "Train Epoch: 047 Batch: 00033/00094 | Loss: 507.1891 | CE: 0.1999 | KD: 1059.9884\n",
      "Train Epoch: 047 Batch: 00034/00094 | Loss: 507.2407 | CE: 0.2337 | KD: 1060.0255\n",
      "Train Epoch: 047 Batch: 00035/00094 | Loss: 507.2223 | CE: 0.1665 | KD: 1060.1278\n",
      "Train Epoch: 047 Batch: 00036/00094 | Loss: 507.2469 | CE: 0.2334 | KD: 1060.0392\n",
      "Train Epoch: 047 Batch: 00037/00094 | Loss: 507.2343 | CE: 0.2075 | KD: 1060.0671\n",
      "Train Epoch: 047 Batch: 00038/00094 | Loss: 507.2259 | CE: 0.1942 | KD: 1060.0771\n",
      "Train Epoch: 047 Batch: 00039/00094 | Loss: 507.2159 | CE: 0.1927 | KD: 1060.0594\n",
      "Train Epoch: 047 Batch: 00040/00094 | Loss: 507.2877 | CE: 0.2566 | KD: 1060.0760\n",
      "Train Epoch: 047 Batch: 00041/00094 | Loss: 507.3481 | CE: 0.2864 | KD: 1060.1399\n",
      "Train Epoch: 047 Batch: 00042/00094 | Loss: 507.2827 | CE: 0.2456 | KD: 1060.0885\n",
      "Train Epoch: 047 Batch: 00043/00094 | Loss: 507.3194 | CE: 0.2524 | KD: 1060.1511\n",
      "Train Epoch: 047 Batch: 00044/00094 | Loss: 507.2119 | CE: 0.1969 | KD: 1060.0424\n",
      "Train Epoch: 047 Batch: 00045/00094 | Loss: 507.1809 | CE: 0.2020 | KD: 1059.9669\n",
      "Train Epoch: 047 Batch: 00046/00094 | Loss: 507.2243 | CE: 0.2130 | KD: 1060.0347\n",
      "Train Epoch: 047 Batch: 00047/00094 | Loss: 507.2186 | CE: 0.1760 | KD: 1060.1002\n",
      "Train Epoch: 047 Batch: 00048/00094 | Loss: 507.2550 | CE: 0.2333 | KD: 1060.0564\n",
      "Train Epoch: 047 Batch: 00049/00094 | Loss: 507.2943 | CE: 0.2690 | KD: 1060.0641\n",
      "Train Epoch: 047 Batch: 00050/00094 | Loss: 507.3225 | CE: 0.2550 | KD: 1060.1521\n",
      "Train Epoch: 047 Batch: 00051/00094 | Loss: 507.2254 | CE: 0.2175 | KD: 1060.0276\n",
      "Train Epoch: 047 Batch: 00052/00094 | Loss: 507.2360 | CE: 0.2145 | KD: 1060.0559\n",
      "Train Epoch: 047 Batch: 00053/00094 | Loss: 507.2535 | CE: 0.2377 | KD: 1060.0439\n",
      "Train Epoch: 047 Batch: 00054/00094 | Loss: 507.2375 | CE: 0.2252 | KD: 1060.0366\n",
      "Train Epoch: 047 Batch: 00055/00094 | Loss: 507.1865 | CE: 0.2058 | KD: 1059.9706\n",
      "Train Epoch: 047 Batch: 00056/00094 | Loss: 507.2279 | CE: 0.1865 | KD: 1060.0978\n",
      "Train Epoch: 047 Batch: 00057/00094 | Loss: 507.2325 | CE: 0.1919 | KD: 1060.0957\n",
      "Train Epoch: 047 Batch: 00058/00094 | Loss: 507.1863 | CE: 0.1988 | KD: 1059.9850\n",
      "Train Epoch: 047 Batch: 00059/00094 | Loss: 507.1876 | CE: 0.1782 | KD: 1060.0306\n",
      "Train Epoch: 047 Batch: 00060/00094 | Loss: 507.2075 | CE: 0.1995 | KD: 1060.0277\n",
      "Train Epoch: 047 Batch: 00061/00094 | Loss: 507.1579 | CE: 0.1900 | KD: 1059.9438\n",
      "Train Epoch: 047 Batch: 00062/00094 | Loss: 507.2289 | CE: 0.2285 | KD: 1060.0117\n",
      "Train Epoch: 047 Batch: 00063/00094 | Loss: 507.2652 | CE: 0.2215 | KD: 1060.1024\n",
      "Train Epoch: 047 Batch: 00064/00094 | Loss: 507.2825 | CE: 0.2486 | KD: 1060.0818\n",
      "Train Epoch: 047 Batch: 00065/00094 | Loss: 507.3378 | CE: 0.2912 | KD: 1060.1085\n",
      "Train Epoch: 047 Batch: 00066/00094 | Loss: 507.3015 | CE: 0.2558 | KD: 1060.1064\n",
      "Train Epoch: 047 Batch: 00067/00094 | Loss: 507.3687 | CE: 0.2834 | KD: 1060.1892\n",
      "Train Epoch: 047 Batch: 00068/00094 | Loss: 507.2792 | CE: 0.2379 | KD: 1060.0974\n",
      "Train Epoch: 047 Batch: 00069/00094 | Loss: 507.2243 | CE: 0.2165 | KD: 1060.0273\n",
      "Train Epoch: 047 Batch: 00070/00094 | Loss: 507.1880 | CE: 0.1933 | KD: 1059.9999\n",
      "Train Epoch: 047 Batch: 00071/00094 | Loss: 507.2386 | CE: 0.2042 | KD: 1060.0829\n",
      "Train Epoch: 047 Batch: 00072/00094 | Loss: 507.3231 | CE: 0.2717 | KD: 1060.1185\n",
      "Train Epoch: 047 Batch: 00073/00094 | Loss: 507.2301 | CE: 0.2224 | KD: 1060.0271\n",
      "Train Epoch: 047 Batch: 00074/00094 | Loss: 507.2129 | CE: 0.2052 | KD: 1060.0272\n",
      "Train Epoch: 047 Batch: 00075/00094 | Loss: 507.2990 | CE: 0.2618 | KD: 1060.0890\n",
      "Train Epoch: 047 Batch: 00076/00094 | Loss: 507.2816 | CE: 0.2331 | KD: 1060.1124\n",
      "Train Epoch: 047 Batch: 00077/00094 | Loss: 507.3017 | CE: 0.2576 | KD: 1060.1031\n",
      "Train Epoch: 047 Batch: 00078/00094 | Loss: 507.3273 | CE: 0.2850 | KD: 1060.0995\n",
      "Train Epoch: 047 Batch: 00079/00094 | Loss: 507.1538 | CE: 0.1368 | KD: 1060.0466\n",
      "Train Epoch: 047 Batch: 00080/00094 | Loss: 507.2858 | CE: 0.2077 | KD: 1060.1744\n",
      "Train Epoch: 047 Batch: 00081/00094 | Loss: 507.2720 | CE: 0.2195 | KD: 1060.1208\n",
      "Train Epoch: 047 Batch: 00082/00094 | Loss: 507.1897 | CE: 0.1770 | KD: 1060.0377\n",
      "Train Epoch: 047 Batch: 00083/00094 | Loss: 507.2034 | CE: 0.1913 | KD: 1060.0364\n",
      "Train Epoch: 047 Batch: 00084/00094 | Loss: 507.2715 | CE: 0.2226 | KD: 1060.1133\n",
      "Train Epoch: 047 Batch: 00085/00094 | Loss: 507.1651 | CE: 0.1818 | KD: 1059.9760\n",
      "Train Epoch: 047 Batch: 00086/00094 | Loss: 507.2795 | CE: 0.2582 | KD: 1060.0557\n",
      "Train Epoch: 047 Batch: 00087/00094 | Loss: 507.1570 | CE: 0.2067 | KD: 1059.9070\n",
      "Train Epoch: 047 Batch: 00088/00094 | Loss: 507.1965 | CE: 0.1921 | KD: 1060.0203\n",
      "Train Epoch: 047 Batch: 00089/00094 | Loss: 507.2252 | CE: 0.2046 | KD: 1060.0541\n",
      "Train Epoch: 047 Batch: 00090/00094 | Loss: 507.1961 | CE: 0.2193 | KD: 1059.9625\n",
      "Train Epoch: 047 Batch: 00091/00094 | Loss: 507.2649 | CE: 0.2472 | KD: 1060.0481\n",
      "Train Epoch: 047 Batch: 00092/00094 | Loss: 507.2209 | CE: 0.2149 | KD: 1060.0237\n",
      "Train Epoch: 047 Batch: 00093/00094 | Loss: 507.2449 | CE: 0.2097 | KD: 1060.0846\n",
      "Train Epoch: 047 Batch: 00094/00094 | Loss: 507.2847 | CE: 0.1887 | KD: 1060.2118\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2461 | acc:91.5000\n",
      "[VAL Acc] Target: 91.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2551 | acc:50.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0899 | acc:50.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1377 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7081 | acc:61.7163\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8885 | acc:56.1922\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 56.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5075 | acc:75.2743\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0005 | acc:55.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4881 | acc:75.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.30%\n",
      "[VAL Acc] Avg 62.79%\n",
      "Train Epoch: 048 Batch: 00001/00094 | Loss: 507.2608 | CE: 0.2227 | KD: 1060.0906\n",
      "Train Epoch: 048 Batch: 00002/00094 | Loss: 507.2648 | CE: 0.2371 | KD: 1060.0691\n",
      "Train Epoch: 048 Batch: 00003/00094 | Loss: 507.2499 | CE: 0.2244 | KD: 1060.0643\n",
      "Train Epoch: 048 Batch: 00004/00094 | Loss: 507.2796 | CE: 0.2589 | KD: 1060.0543\n",
      "Train Epoch: 048 Batch: 00005/00094 | Loss: 507.2005 | CE: 0.2067 | KD: 1059.9982\n",
      "Train Epoch: 048 Batch: 00006/00094 | Loss: 507.2902 | CE: 0.2485 | KD: 1060.0981\n",
      "Train Epoch: 048 Batch: 00007/00094 | Loss: 507.2946 | CE: 0.2399 | KD: 1060.1252\n",
      "Train Epoch: 048 Batch: 00008/00094 | Loss: 507.2898 | CE: 0.2878 | KD: 1060.0153\n",
      "Train Epoch: 048 Batch: 00009/00094 | Loss: 507.2001 | CE: 0.1822 | KD: 1060.0483\n",
      "Train Epoch: 048 Batch: 00010/00094 | Loss: 507.1752 | CE: 0.1601 | KD: 1060.0426\n",
      "Train Epoch: 048 Batch: 00011/00094 | Loss: 507.3170 | CE: 0.2779 | KD: 1060.0929\n",
      "Train Epoch: 048 Batch: 00012/00094 | Loss: 507.2730 | CE: 0.1851 | KD: 1060.1947\n",
      "Train Epoch: 048 Batch: 00013/00094 | Loss: 507.3630 | CE: 0.3392 | KD: 1060.0608\n",
      "Train Epoch: 048 Batch: 00014/00094 | Loss: 507.1849 | CE: 0.1854 | KD: 1060.0101\n",
      "Train Epoch: 048 Batch: 00015/00094 | Loss: 507.1267 | CE: 0.1701 | KD: 1059.9203\n",
      "Train Epoch: 048 Batch: 00016/00094 | Loss: 507.3502 | CE: 0.3009 | KD: 1060.1140\n",
      "Train Epoch: 048 Batch: 00017/00094 | Loss: 507.2262 | CE: 0.2164 | KD: 1060.0314\n",
      "Train Epoch: 048 Batch: 00018/00094 | Loss: 507.2942 | CE: 0.2534 | KD: 1060.0964\n",
      "Train Epoch: 048 Batch: 00019/00094 | Loss: 507.2550 | CE: 0.2044 | KD: 1060.1169\n",
      "Train Epoch: 048 Batch: 00020/00094 | Loss: 507.2127 | CE: 0.2200 | KD: 1059.9957\n",
      "Train Epoch: 048 Batch: 00021/00094 | Loss: 507.2036 | CE: 0.2232 | KD: 1059.9701\n",
      "Train Epoch: 048 Batch: 00022/00094 | Loss: 507.2909 | CE: 0.2414 | KD: 1060.1145\n",
      "Train Epoch: 048 Batch: 00023/00094 | Loss: 507.1547 | CE: 0.1597 | KD: 1060.0006\n",
      "Train Epoch: 048 Batch: 00024/00094 | Loss: 507.3755 | CE: 0.3289 | KD: 1060.1085\n",
      "Train Epoch: 048 Batch: 00025/00094 | Loss: 507.2032 | CE: 0.2123 | KD: 1059.9919\n",
      "Train Epoch: 048 Batch: 00026/00094 | Loss: 507.1722 | CE: 0.1771 | KD: 1060.0009\n",
      "Train Epoch: 048 Batch: 00027/00094 | Loss: 507.2446 | CE: 0.2468 | KD: 1060.0065\n",
      "Train Epoch: 048 Batch: 00028/00094 | Loss: 507.2081 | CE: 0.1798 | KD: 1060.0703\n",
      "Train Epoch: 048 Batch: 00029/00094 | Loss: 507.2784 | CE: 0.2363 | KD: 1060.0991\n",
      "Train Epoch: 048 Batch: 00030/00094 | Loss: 507.2843 | CE: 0.2557 | KD: 1060.0708\n",
      "Train Epoch: 048 Batch: 00031/00094 | Loss: 507.2407 | CE: 0.2095 | KD: 1060.0762\n",
      "Train Epoch: 048 Batch: 00032/00094 | Loss: 507.2421 | CE: 0.2412 | KD: 1060.0128\n",
      "Train Epoch: 048 Batch: 00033/00094 | Loss: 507.2500 | CE: 0.2375 | KD: 1060.0374\n",
      "Train Epoch: 048 Batch: 00034/00094 | Loss: 507.1742 | CE: 0.1614 | KD: 1060.0377\n",
      "Train Epoch: 048 Batch: 00035/00094 | Loss: 507.2728 | CE: 0.2312 | KD: 1060.0980\n",
      "Train Epoch: 048 Batch: 00036/00094 | Loss: 507.2593 | CE: 0.2200 | KD: 1060.0931\n",
      "Train Epoch: 048 Batch: 00037/00094 | Loss: 507.2293 | CE: 0.2048 | KD: 1060.0624\n",
      "Train Epoch: 048 Batch: 00038/00094 | Loss: 507.2095 | CE: 0.2264 | KD: 1059.9757\n",
      "Train Epoch: 048 Batch: 00039/00094 | Loss: 507.2069 | CE: 0.1820 | KD: 1060.0631\n",
      "Train Epoch: 048 Batch: 00040/00094 | Loss: 507.2062 | CE: 0.2024 | KD: 1060.0189\n",
      "Train Epoch: 048 Batch: 00041/00094 | Loss: 507.2402 | CE: 0.2052 | KD: 1060.0842\n",
      "Train Epoch: 048 Batch: 00042/00094 | Loss: 507.1948 | CE: 0.1782 | KD: 1060.0455\n",
      "Train Epoch: 048 Batch: 00043/00094 | Loss: 507.2142 | CE: 0.2037 | KD: 1060.0331\n",
      "Train Epoch: 048 Batch: 00044/00094 | Loss: 507.3215 | CE: 0.2788 | KD: 1060.1003\n",
      "Train Epoch: 048 Batch: 00045/00094 | Loss: 507.2267 | CE: 0.2225 | KD: 1060.0199\n",
      "Train Epoch: 048 Batch: 00046/00094 | Loss: 507.1589 | CE: 0.1706 | KD: 1059.9865\n",
      "Train Epoch: 048 Batch: 00047/00094 | Loss: 507.2197 | CE: 0.2047 | KD: 1060.0425\n",
      "Train Epoch: 048 Batch: 00048/00094 | Loss: 507.2737 | CE: 0.2346 | KD: 1060.0927\n",
      "Train Epoch: 048 Batch: 00049/00094 | Loss: 507.2559 | CE: 0.2318 | KD: 1060.0614\n",
      "Train Epoch: 048 Batch: 00050/00094 | Loss: 507.1801 | CE: 0.1802 | KD: 1060.0109\n",
      "Train Epoch: 048 Batch: 00051/00094 | Loss: 507.2821 | CE: 0.2701 | KD: 1060.0361\n",
      "Train Epoch: 048 Batch: 00052/00094 | Loss: 507.2915 | CE: 0.2527 | KD: 1060.0922\n",
      "Train Epoch: 048 Batch: 00053/00094 | Loss: 507.3086 | CE: 0.2793 | KD: 1060.0723\n",
      "Train Epoch: 048 Batch: 00054/00094 | Loss: 507.2654 | CE: 0.2300 | KD: 1060.0852\n",
      "Train Epoch: 048 Batch: 00055/00094 | Loss: 507.2455 | CE: 0.2167 | KD: 1060.0712\n",
      "Train Epoch: 048 Batch: 00056/00094 | Loss: 507.2478 | CE: 0.1970 | KD: 1060.1174\n",
      "Train Epoch: 048 Batch: 00057/00094 | Loss: 507.1910 | CE: 0.1923 | KD: 1060.0083\n",
      "Train Epoch: 048 Batch: 00058/00094 | Loss: 507.1878 | CE: 0.1941 | KD: 1059.9978\n",
      "Train Epoch: 048 Batch: 00059/00094 | Loss: 507.1867 | CE: 0.1755 | KD: 1060.0344\n",
      "Train Epoch: 048 Batch: 00060/00094 | Loss: 507.2858 | CE: 0.2447 | KD: 1060.0970\n",
      "Train Epoch: 048 Batch: 00061/00094 | Loss: 507.2414 | CE: 0.1928 | KD: 1060.1127\n",
      "Train Epoch: 048 Batch: 00062/00094 | Loss: 507.2101 | CE: 0.1967 | KD: 1060.0391\n",
      "Train Epoch: 048 Batch: 00063/00094 | Loss: 507.3258 | CE: 0.2487 | KD: 1060.1722\n",
      "Train Epoch: 048 Batch: 00064/00094 | Loss: 507.2537 | CE: 0.2268 | KD: 1060.0673\n",
      "Train Epoch: 048 Batch: 00065/00094 | Loss: 507.2047 | CE: 0.2028 | KD: 1060.0149\n",
      "Train Epoch: 048 Batch: 00066/00094 | Loss: 507.2436 | CE: 0.2314 | KD: 1060.0364\n",
      "Train Epoch: 048 Batch: 00067/00094 | Loss: 507.3641 | CE: 0.3136 | KD: 1060.1165\n",
      "Train Epoch: 048 Batch: 00068/00094 | Loss: 507.2138 | CE: 0.1971 | KD: 1060.0459\n",
      "Train Epoch: 048 Batch: 00069/00094 | Loss: 507.2645 | CE: 0.2136 | KD: 1060.1174\n",
      "Train Epoch: 048 Batch: 00070/00094 | Loss: 507.3220 | CE: 0.2746 | KD: 1060.1102\n",
      "Train Epoch: 048 Batch: 00071/00094 | Loss: 507.1700 | CE: 0.1773 | KD: 1059.9958\n",
      "Train Epoch: 048 Batch: 00072/00094 | Loss: 507.3119 | CE: 0.2652 | KD: 1060.1085\n",
      "Train Epoch: 048 Batch: 00073/00094 | Loss: 507.3636 | CE: 0.2896 | KD: 1060.1658\n",
      "Train Epoch: 048 Batch: 00074/00094 | Loss: 507.2263 | CE: 0.2058 | KD: 1060.0538\n",
      "Train Epoch: 048 Batch: 00075/00094 | Loss: 507.2777 | CE: 0.2343 | KD: 1060.1018\n",
      "Train Epoch: 048 Batch: 00076/00094 | Loss: 507.1940 | CE: 0.1859 | KD: 1060.0280\n",
      "Train Epoch: 048 Batch: 00077/00094 | Loss: 507.1964 | CE: 0.1823 | KD: 1060.0404\n",
      "Train Epoch: 048 Batch: 00078/00094 | Loss: 507.1837 | CE: 0.1866 | KD: 1060.0049\n",
      "Train Epoch: 048 Batch: 00079/00094 | Loss: 507.2203 | CE: 0.2123 | KD: 1060.0278\n",
      "Train Epoch: 048 Batch: 00080/00094 | Loss: 507.1558 | CE: 0.1770 | KD: 1059.9667\n",
      "Train Epoch: 048 Batch: 00081/00094 | Loss: 507.1802 | CE: 0.1672 | KD: 1060.0382\n",
      "Train Epoch: 048 Batch: 00082/00094 | Loss: 507.2423 | CE: 0.2452 | KD: 1060.0049\n",
      "Train Epoch: 048 Batch: 00083/00094 | Loss: 507.3370 | CE: 0.2752 | KD: 1060.1404\n",
      "Train Epoch: 048 Batch: 00084/00094 | Loss: 507.1949 | CE: 0.1756 | KD: 1060.0513\n",
      "Train Epoch: 048 Batch: 00085/00094 | Loss: 507.2795 | CE: 0.2204 | KD: 1060.1346\n",
      "Train Epoch: 048 Batch: 00086/00094 | Loss: 507.3692 | CE: 0.3154 | KD: 1060.1235\n",
      "Train Epoch: 048 Batch: 00087/00094 | Loss: 507.2180 | CE: 0.1877 | KD: 1060.0743\n",
      "Train Epoch: 048 Batch: 00088/00094 | Loss: 507.1286 | CE: 0.1378 | KD: 1059.9918\n",
      "Train Epoch: 048 Batch: 00089/00094 | Loss: 507.2563 | CE: 0.2366 | KD: 1060.0522\n",
      "Train Epoch: 048 Batch: 00090/00094 | Loss: 507.2835 | CE: 0.2353 | KD: 1060.1117\n",
      "Train Epoch: 048 Batch: 00091/00094 | Loss: 507.1997 | CE: 0.1793 | KD: 1060.0537\n",
      "Train Epoch: 048 Batch: 00092/00094 | Loss: 507.1619 | CE: 0.1778 | KD: 1059.9777\n",
      "Train Epoch: 048 Batch: 00093/00094 | Loss: 507.1980 | CE: 0.1688 | KD: 1060.0721\n",
      "Train Epoch: 048 Batch: 00094/00094 | Loss: 507.3239 | CE: 0.2510 | KD: 1060.1633\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2458 | acc:91.6000\n",
      "[VAL Acc] Target: 91.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2146 | acc:50.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0716 | acc:47.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0934 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7046 | acc:61.9122\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.91%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9853 | acc:54.4362\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5041 | acc:76.4890\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0541 | acc:54.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4705 | acc:75.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.10%\n",
      "[VAL Acc] Avg 62.20%\n",
      "Train Epoch: 049 Batch: 00001/00094 | Loss: 507.2169 | CE: 0.1936 | KD: 1060.0597\n",
      "Train Epoch: 049 Batch: 00002/00094 | Loss: 507.1768 | CE: 0.1548 | KD: 1060.0573\n",
      "Train Epoch: 049 Batch: 00003/00094 | Loss: 507.3092 | CE: 0.2586 | KD: 1060.1168\n",
      "Train Epoch: 049 Batch: 00004/00094 | Loss: 507.2197 | CE: 0.1865 | KD: 1060.0804\n",
      "Train Epoch: 049 Batch: 00005/00094 | Loss: 507.1345 | CE: 0.1432 | KD: 1059.9929\n",
      "Train Epoch: 049 Batch: 00006/00094 | Loss: 507.3463 | CE: 0.2826 | KD: 1060.1443\n",
      "Train Epoch: 049 Batch: 00007/00094 | Loss: 507.5199 | CE: 0.4376 | KD: 1060.1831\n",
      "Train Epoch: 049 Batch: 00008/00094 | Loss: 507.2025 | CE: 0.1992 | KD: 1060.0178\n",
      "Train Epoch: 049 Batch: 00009/00094 | Loss: 507.2027 | CE: 0.1938 | KD: 1060.0295\n",
      "Train Epoch: 049 Batch: 00010/00094 | Loss: 507.2496 | CE: 0.2191 | KD: 1060.0748\n",
      "Train Epoch: 049 Batch: 00011/00094 | Loss: 507.2494 | CE: 0.2186 | KD: 1060.0753\n",
      "Train Epoch: 049 Batch: 00012/00094 | Loss: 507.2793 | CE: 0.2424 | KD: 1060.0883\n",
      "Train Epoch: 049 Batch: 00013/00094 | Loss: 507.2391 | CE: 0.2420 | KD: 1060.0050\n",
      "Train Epoch: 049 Batch: 00014/00094 | Loss: 507.2057 | CE: 0.2260 | KD: 1059.9688\n",
      "Train Epoch: 049 Batch: 00015/00094 | Loss: 507.1865 | CE: 0.1955 | KD: 1059.9921\n",
      "Train Epoch: 049 Batch: 00016/00094 | Loss: 507.1805 | CE: 0.1820 | KD: 1060.0079\n",
      "Train Epoch: 049 Batch: 00017/00094 | Loss: 507.3432 | CE: 0.2802 | KD: 1060.1428\n",
      "Train Epoch: 049 Batch: 00018/00094 | Loss: 507.2137 | CE: 0.2084 | KD: 1060.0221\n",
      "Train Epoch: 049 Batch: 00019/00094 | Loss: 507.2192 | CE: 0.1732 | KD: 1060.1072\n",
      "Train Epoch: 049 Batch: 00020/00094 | Loss: 507.2401 | CE: 0.2513 | KD: 1059.9877\n",
      "Train Epoch: 049 Batch: 00021/00094 | Loss: 507.2807 | CE: 0.2376 | KD: 1060.1012\n",
      "Train Epoch: 049 Batch: 00022/00094 | Loss: 507.1741 | CE: 0.1705 | KD: 1060.0187\n",
      "Train Epoch: 049 Batch: 00023/00094 | Loss: 507.1930 | CE: 0.1949 | KD: 1060.0070\n",
      "Train Epoch: 049 Batch: 00024/00094 | Loss: 507.2606 | CE: 0.2498 | KD: 1060.0337\n",
      "Train Epoch: 049 Batch: 00025/00094 | Loss: 507.2148 | CE: 0.1712 | KD: 1060.1021\n",
      "Train Epoch: 049 Batch: 00026/00094 | Loss: 507.2169 | CE: 0.1829 | KD: 1060.0819\n",
      "Train Epoch: 049 Batch: 00027/00094 | Loss: 507.2527 | CE: 0.2364 | KD: 1060.0449\n",
      "Train Epoch: 049 Batch: 00028/00094 | Loss: 507.2721 | CE: 0.2216 | KD: 1060.1167\n",
      "Train Epoch: 049 Batch: 00029/00094 | Loss: 507.2393 | CE: 0.2119 | KD: 1060.0682\n",
      "Train Epoch: 049 Batch: 00030/00094 | Loss: 507.2652 | CE: 0.2715 | KD: 1059.9979\n",
      "Train Epoch: 049 Batch: 00031/00094 | Loss: 507.1449 | CE: 0.1693 | KD: 1059.9601\n",
      "Train Epoch: 049 Batch: 00032/00094 | Loss: 507.1596 | CE: 0.1909 | KD: 1059.9456\n",
      "Train Epoch: 049 Batch: 00033/00094 | Loss: 507.1914 | CE: 0.2117 | KD: 1059.9686\n",
      "Train Epoch: 049 Batch: 00034/00094 | Loss: 507.1888 | CE: 0.1902 | KD: 1060.0081\n",
      "Train Epoch: 049 Batch: 00035/00094 | Loss: 507.2598 | CE: 0.2386 | KD: 1060.0553\n",
      "Train Epoch: 049 Batch: 00036/00094 | Loss: 507.2921 | CE: 0.2359 | KD: 1060.1285\n",
      "Train Epoch: 049 Batch: 00037/00094 | Loss: 507.1964 | CE: 0.2021 | KD: 1059.9991\n",
      "Train Epoch: 049 Batch: 00038/00094 | Loss: 507.2141 | CE: 0.2023 | KD: 1060.0358\n",
      "Train Epoch: 049 Batch: 00039/00094 | Loss: 507.2731 | CE: 0.2326 | KD: 1060.0956\n",
      "Train Epoch: 049 Batch: 00040/00094 | Loss: 507.2796 | CE: 0.2409 | KD: 1060.0918\n",
      "Train Epoch: 049 Batch: 00041/00094 | Loss: 507.2121 | CE: 0.2156 | KD: 1060.0039\n",
      "Train Epoch: 049 Batch: 00042/00094 | Loss: 507.2753 | CE: 0.2210 | KD: 1060.1244\n",
      "Train Epoch: 049 Batch: 00043/00094 | Loss: 507.5647 | CE: 0.4395 | KD: 1060.2729\n",
      "Train Epoch: 049 Batch: 00044/00094 | Loss: 507.2083 | CE: 0.2332 | KD: 1059.9589\n",
      "Train Epoch: 049 Batch: 00045/00094 | Loss: 507.2062 | CE: 0.1962 | KD: 1060.0320\n",
      "Train Epoch: 049 Batch: 00046/00094 | Loss: 507.1808 | CE: 0.1870 | KD: 1059.9980\n",
      "Train Epoch: 049 Batch: 00047/00094 | Loss: 507.2358 | CE: 0.2096 | KD: 1060.0659\n",
      "Train Epoch: 049 Batch: 00048/00094 | Loss: 507.2388 | CE: 0.2360 | KD: 1060.0168\n",
      "Train Epoch: 049 Batch: 00049/00094 | Loss: 507.3772 | CE: 0.2857 | KD: 1060.2024\n",
      "Train Epoch: 049 Batch: 00050/00094 | Loss: 507.1845 | CE: 0.1756 | KD: 1060.0297\n",
      "Train Epoch: 049 Batch: 00051/00094 | Loss: 507.1970 | CE: 0.2099 | KD: 1059.9841\n",
      "Train Epoch: 049 Batch: 00052/00094 | Loss: 507.3723 | CE: 0.3323 | KD: 1060.0945\n",
      "Train Epoch: 049 Batch: 00053/00094 | Loss: 507.2308 | CE: 0.2115 | KD: 1060.0513\n",
      "Train Epoch: 049 Batch: 00054/00094 | Loss: 507.2228 | CE: 0.2080 | KD: 1060.0420\n",
      "Train Epoch: 049 Batch: 00055/00094 | Loss: 507.2826 | CE: 0.2634 | KD: 1060.0511\n",
      "Train Epoch: 049 Batch: 00056/00094 | Loss: 507.2252 | CE: 0.2098 | KD: 1060.0433\n",
      "Train Epoch: 049 Batch: 00057/00094 | Loss: 507.2498 | CE: 0.2591 | KD: 1059.9916\n",
      "Train Epoch: 049 Batch: 00058/00094 | Loss: 507.2229 | CE: 0.2112 | KD: 1060.0355\n",
      "Train Epoch: 049 Batch: 00059/00094 | Loss: 507.1862 | CE: 0.1586 | KD: 1060.0688\n",
      "Train Epoch: 049 Batch: 00060/00094 | Loss: 507.2518 | CE: 0.2619 | KD: 1059.9900\n",
      "Train Epoch: 049 Batch: 00061/00094 | Loss: 507.2169 | CE: 0.2269 | KD: 1059.9900\n",
      "Train Epoch: 049 Batch: 00062/00094 | Loss: 507.2000 | CE: 0.1915 | KD: 1060.0288\n",
      "Train Epoch: 049 Batch: 00063/00094 | Loss: 507.1825 | CE: 0.1502 | KD: 1060.0785\n",
      "Train Epoch: 049 Batch: 00064/00094 | Loss: 507.2376 | CE: 0.2083 | KD: 1060.0724\n",
      "Train Epoch: 049 Batch: 00065/00094 | Loss: 507.3561 | CE: 0.2623 | KD: 1060.2070\n",
      "Train Epoch: 049 Batch: 00066/00094 | Loss: 507.3022 | CE: 0.2833 | KD: 1060.0504\n",
      "Train Epoch: 049 Batch: 00067/00094 | Loss: 507.2169 | CE: 0.2112 | KD: 1060.0229\n",
      "Train Epoch: 049 Batch: 00068/00094 | Loss: 507.2140 | CE: 0.2076 | KD: 1060.0244\n",
      "Train Epoch: 049 Batch: 00069/00094 | Loss: 507.1632 | CE: 0.1432 | KD: 1060.0529\n",
      "Train Epoch: 049 Batch: 00070/00094 | Loss: 507.3170 | CE: 0.2794 | KD: 1060.0897\n",
      "Train Epoch: 049 Batch: 00071/00094 | Loss: 507.2517 | CE: 0.2177 | KD: 1060.0820\n",
      "Train Epoch: 049 Batch: 00072/00094 | Loss: 507.2828 | CE: 0.2525 | KD: 1060.0743\n",
      "Train Epoch: 049 Batch: 00073/00094 | Loss: 507.2584 | CE: 0.2425 | KD: 1060.0443\n",
      "Train Epoch: 049 Batch: 00074/00094 | Loss: 507.2215 | CE: 0.1947 | KD: 1060.0671\n",
      "Train Epoch: 049 Batch: 00075/00094 | Loss: 507.3058 | CE: 0.2812 | KD: 1060.0626\n",
      "Train Epoch: 049 Batch: 00076/00094 | Loss: 507.3663 | CE: 0.3037 | KD: 1060.1420\n",
      "Train Epoch: 049 Batch: 00077/00094 | Loss: 507.2492 | CE: 0.2156 | KD: 1060.0813\n",
      "Train Epoch: 049 Batch: 00078/00094 | Loss: 507.2595 | CE: 0.2415 | KD: 1060.0487\n",
      "Train Epoch: 049 Batch: 00079/00094 | Loss: 507.1450 | CE: 0.1371 | KD: 1060.0276\n",
      "Train Epoch: 049 Batch: 00080/00094 | Loss: 507.2715 | CE: 0.2561 | KD: 1060.0432\n",
      "Train Epoch: 049 Batch: 00081/00094 | Loss: 507.2100 | CE: 0.2178 | KD: 1059.9949\n",
      "Train Epoch: 049 Batch: 00082/00094 | Loss: 507.1995 | CE: 0.1738 | KD: 1060.0648\n",
      "Train Epoch: 049 Batch: 00083/00094 | Loss: 507.2616 | CE: 0.2243 | KD: 1060.0890\n",
      "Train Epoch: 049 Batch: 00084/00094 | Loss: 507.2022 | CE: 0.1727 | KD: 1060.0729\n",
      "Train Epoch: 049 Batch: 00085/00094 | Loss: 507.1652 | CE: 0.1682 | KD: 1060.0049\n",
      "Train Epoch: 049 Batch: 00086/00094 | Loss: 507.2510 | CE: 0.1958 | KD: 1060.1265\n",
      "Train Epoch: 049 Batch: 00087/00094 | Loss: 507.2491 | CE: 0.2125 | KD: 1060.0875\n",
      "Train Epoch: 049 Batch: 00088/00094 | Loss: 507.3066 | CE: 0.2565 | KD: 1060.1157\n",
      "Train Epoch: 049 Batch: 00089/00094 | Loss: 507.2344 | CE: 0.2083 | KD: 1060.0657\n",
      "Train Epoch: 049 Batch: 00090/00094 | Loss: 507.2005 | CE: 0.1841 | KD: 1060.0452\n",
      "Train Epoch: 049 Batch: 00091/00094 | Loss: 507.1866 | CE: 0.1943 | KD: 1059.9949\n",
      "Train Epoch: 049 Batch: 00092/00094 | Loss: 507.1836 | CE: 0.1896 | KD: 1059.9984\n",
      "Train Epoch: 049 Batch: 00093/00094 | Loss: 507.3255 | CE: 0.2631 | KD: 1060.1416\n",
      "Train Epoch: 049 Batch: 00094/00094 | Loss: 507.2114 | CE: 0.1969 | KD: 1060.0413\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2360 | acc:92.6000\n",
      "[VAL Acc] Target: 92.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2092 | acc:50.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0388 | acc:51.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0465 | acc:50.3817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6986 | acc:62.0690\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9683 | acc:55.2680\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5103 | acc:74.9608\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0148 | acc:55.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4799 | acc:75.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.20%\n",
      "[VAL Acc] Avg 63.11%\n",
      "Train Epoch: 050 Batch: 00001/00094 | Loss: 507.2864 | CE: 0.2597 | KD: 1060.0669\n",
      "Train Epoch: 050 Batch: 00002/00094 | Loss: 507.1877 | CE: 0.1875 | KD: 1060.0115\n",
      "Train Epoch: 050 Batch: 00003/00094 | Loss: 507.2754 | CE: 0.2322 | KD: 1060.1014\n",
      "Train Epoch: 050 Batch: 00004/00094 | Loss: 507.3709 | CE: 0.2676 | KD: 1060.2271\n",
      "Train Epoch: 050 Batch: 00005/00094 | Loss: 507.2447 | CE: 0.2437 | KD: 1060.0132\n",
      "Train Epoch: 050 Batch: 00006/00094 | Loss: 507.3858 | CE: 0.3222 | KD: 1060.1440\n",
      "Train Epoch: 050 Batch: 00007/00094 | Loss: 507.3630 | CE: 0.2786 | KD: 1060.1876\n",
      "Train Epoch: 050 Batch: 00008/00094 | Loss: 507.1685 | CE: 0.1620 | KD: 1060.0247\n",
      "Train Epoch: 050 Batch: 00009/00094 | Loss: 507.3438 | CE: 0.3286 | KD: 1060.0430\n",
      "Train Epoch: 050 Batch: 00010/00094 | Loss: 507.2322 | CE: 0.2130 | KD: 1060.0510\n",
      "Train Epoch: 050 Batch: 00011/00094 | Loss: 507.2886 | CE: 0.2492 | KD: 1060.0936\n",
      "Train Epoch: 050 Batch: 00012/00094 | Loss: 507.1782 | CE: 0.1771 | KD: 1060.0133\n",
      "Train Epoch: 050 Batch: 00013/00094 | Loss: 507.2760 | CE: 0.2271 | KD: 1060.1134\n",
      "Train Epoch: 050 Batch: 00014/00094 | Loss: 507.1996 | CE: 0.2138 | KD: 1059.9813\n",
      "Train Epoch: 050 Batch: 00015/00094 | Loss: 507.3146 | CE: 0.2956 | KD: 1060.0508\n",
      "Train Epoch: 050 Batch: 00016/00094 | Loss: 507.1504 | CE: 0.1879 | KD: 1059.9326\n",
      "Train Epoch: 050 Batch: 00017/00094 | Loss: 507.2156 | CE: 0.1881 | KD: 1060.0686\n",
      "Train Epoch: 050 Batch: 00018/00094 | Loss: 507.2139 | CE: 0.2145 | KD: 1060.0098\n",
      "Train Epoch: 050 Batch: 00019/00094 | Loss: 507.3427 | CE: 0.3022 | KD: 1060.0957\n",
      "Train Epoch: 050 Batch: 00020/00094 | Loss: 507.2305 | CE: 0.2492 | KD: 1059.9719\n",
      "Train Epoch: 050 Batch: 00021/00094 | Loss: 507.2263 | CE: 0.2246 | KD: 1060.0145\n",
      "Train Epoch: 050 Batch: 00022/00094 | Loss: 507.2393 | CE: 0.1911 | KD: 1060.1117\n",
      "Train Epoch: 050 Batch: 00023/00094 | Loss: 507.2265 | CE: 0.1725 | KD: 1060.1239\n",
      "Train Epoch: 050 Batch: 00024/00094 | Loss: 507.1839 | CE: 0.2023 | KD: 1059.9725\n",
      "Train Epoch: 050 Batch: 00025/00094 | Loss: 507.1633 | CE: 0.1773 | KD: 1059.9818\n",
      "Train Epoch: 050 Batch: 00026/00094 | Loss: 507.2441 | CE: 0.2062 | KD: 1060.0903\n",
      "Train Epoch: 050 Batch: 00027/00094 | Loss: 507.1677 | CE: 0.1644 | KD: 1060.0178\n",
      "Train Epoch: 050 Batch: 00028/00094 | Loss: 507.2440 | CE: 0.1949 | KD: 1060.1139\n",
      "Train Epoch: 050 Batch: 00029/00094 | Loss: 507.2748 | CE: 0.2337 | KD: 1060.0969\n",
      "Train Epoch: 050 Batch: 00030/00094 | Loss: 507.2833 | CE: 0.2541 | KD: 1060.0721\n",
      "Train Epoch: 050 Batch: 00031/00094 | Loss: 507.2084 | CE: 0.2143 | KD: 1059.9985\n",
      "Train Epoch: 050 Batch: 00032/00094 | Loss: 507.1215 | CE: 0.1427 | KD: 1059.9667\n",
      "Train Epoch: 050 Batch: 00033/00094 | Loss: 507.2210 | CE: 0.2322 | KD: 1059.9877\n",
      "Train Epoch: 050 Batch: 00034/00094 | Loss: 507.3136 | CE: 0.2752 | KD: 1060.0912\n",
      "Train Epoch: 050 Batch: 00035/00094 | Loss: 507.1905 | CE: 0.2086 | KD: 1059.9731\n",
      "Train Epoch: 050 Batch: 00036/00094 | Loss: 507.2244 | CE: 0.1679 | KD: 1060.1292\n",
      "Train Epoch: 050 Batch: 00037/00094 | Loss: 507.2338 | CE: 0.2129 | KD: 1060.0547\n",
      "Train Epoch: 050 Batch: 00038/00094 | Loss: 507.2557 | CE: 0.1954 | KD: 1060.1370\n",
      "Train Epoch: 050 Batch: 00039/00094 | Loss: 507.2991 | CE: 0.2401 | KD: 1060.1343\n",
      "Train Epoch: 050 Batch: 00040/00094 | Loss: 507.2559 | CE: 0.2344 | KD: 1060.0559\n",
      "Train Epoch: 050 Batch: 00041/00094 | Loss: 507.3173 | CE: 0.2731 | KD: 1060.1035\n",
      "Train Epoch: 050 Batch: 00042/00094 | Loss: 507.2077 | CE: 0.2166 | KD: 1059.9926\n",
      "Train Epoch: 050 Batch: 00043/00094 | Loss: 507.2069 | CE: 0.1978 | KD: 1060.0302\n",
      "Train Epoch: 050 Batch: 00044/00094 | Loss: 507.2671 | CE: 0.2337 | KD: 1060.0809\n",
      "Train Epoch: 050 Batch: 00045/00094 | Loss: 507.3205 | CE: 0.2907 | KD: 1060.0734\n",
      "Train Epoch: 050 Batch: 00046/00094 | Loss: 507.2433 | CE: 0.2043 | KD: 1060.0927\n",
      "Train Epoch: 050 Batch: 00047/00094 | Loss: 507.1857 | CE: 0.1783 | KD: 1060.0266\n",
      "Train Epoch: 050 Batch: 00048/00094 | Loss: 507.1952 | CE: 0.1600 | KD: 1060.0846\n",
      "Train Epoch: 050 Batch: 00049/00094 | Loss: 507.2728 | CE: 0.2420 | KD: 1060.0753\n",
      "Train Epoch: 050 Batch: 00050/00094 | Loss: 507.2046 | CE: 0.1870 | KD: 1060.0479\n",
      "Train Epoch: 050 Batch: 00051/00094 | Loss: 507.2694 | CE: 0.2168 | KD: 1060.1211\n",
      "Train Epoch: 050 Batch: 00052/00094 | Loss: 507.2474 | CE: 0.2538 | KD: 1059.9976\n",
      "Train Epoch: 050 Batch: 00053/00094 | Loss: 507.1879 | CE: 0.1850 | KD: 1060.0171\n",
      "Train Epoch: 050 Batch: 00054/00094 | Loss: 507.2014 | CE: 0.2008 | KD: 1060.0123\n",
      "Train Epoch: 050 Batch: 00055/00094 | Loss: 507.2720 | CE: 0.2316 | KD: 1060.0955\n",
      "Train Epoch: 050 Batch: 00056/00094 | Loss: 507.1684 | CE: 0.2058 | KD: 1059.9327\n",
      "Train Epoch: 050 Batch: 00057/00094 | Loss: 507.2307 | CE: 0.2056 | KD: 1060.0635\n",
      "Train Epoch: 050 Batch: 00058/00094 | Loss: 507.1464 | CE: 0.1416 | KD: 1060.0210\n",
      "Train Epoch: 050 Batch: 00059/00094 | Loss: 507.1874 | CE: 0.1854 | KD: 1060.0154\n",
      "Train Epoch: 050 Batch: 00060/00094 | Loss: 507.1838 | CE: 0.1760 | KD: 1060.0275\n",
      "Train Epoch: 050 Batch: 00061/00094 | Loss: 507.2737 | CE: 0.2566 | KD: 1060.0468\n",
      "Train Epoch: 050 Batch: 00062/00094 | Loss: 507.3466 | CE: 0.2841 | KD: 1060.1418\n",
      "Train Epoch: 050 Batch: 00063/00094 | Loss: 507.2167 | CE: 0.2091 | KD: 1060.0269\n",
      "Train Epoch: 050 Batch: 00064/00094 | Loss: 507.1181 | CE: 0.1520 | KD: 1059.9401\n",
      "Train Epoch: 050 Batch: 00065/00094 | Loss: 507.2512 | CE: 0.2380 | KD: 1060.0386\n",
      "Train Epoch: 050 Batch: 00066/00094 | Loss: 507.2826 | CE: 0.2644 | KD: 1060.0490\n",
      "Train Epoch: 050 Batch: 00067/00094 | Loss: 507.2778 | CE: 0.2413 | KD: 1060.0874\n",
      "Train Epoch: 050 Batch: 00068/00094 | Loss: 507.2175 | CE: 0.1975 | KD: 1060.0529\n",
      "Train Epoch: 050 Batch: 00069/00094 | Loss: 507.2127 | CE: 0.2160 | KD: 1060.0043\n",
      "Train Epoch: 050 Batch: 00070/00094 | Loss: 507.2618 | CE: 0.2242 | KD: 1060.0896\n",
      "Train Epoch: 050 Batch: 00071/00094 | Loss: 507.3036 | CE: 0.2536 | KD: 1060.1156\n",
      "Train Epoch: 050 Batch: 00072/00094 | Loss: 507.1637 | CE: 0.1851 | KD: 1059.9662\n",
      "Train Epoch: 050 Batch: 00073/00094 | Loss: 507.2431 | CE: 0.2109 | KD: 1060.0784\n",
      "Train Epoch: 050 Batch: 00074/00094 | Loss: 507.2053 | CE: 0.2116 | KD: 1059.9979\n",
      "Train Epoch: 050 Batch: 00075/00094 | Loss: 507.3636 | CE: 0.2847 | KD: 1060.1760\n",
      "Train Epoch: 050 Batch: 00076/00094 | Loss: 507.2325 | CE: 0.2072 | KD: 1060.0638\n",
      "Train Epoch: 050 Batch: 00077/00094 | Loss: 507.3061 | CE: 0.2433 | KD: 1060.1425\n",
      "Train Epoch: 050 Batch: 00078/00094 | Loss: 507.2119 | CE: 0.1834 | KD: 1060.0706\n",
      "Train Epoch: 050 Batch: 00079/00094 | Loss: 507.2268 | CE: 0.2203 | KD: 1060.0245\n",
      "Train Epoch: 050 Batch: 00080/00094 | Loss: 507.2753 | CE: 0.2228 | KD: 1060.1207\n",
      "Train Epoch: 050 Batch: 00081/00094 | Loss: 507.1756 | CE: 0.1850 | KD: 1059.9915\n",
      "Train Epoch: 050 Batch: 00082/00094 | Loss: 507.2327 | CE: 0.2479 | KD: 1059.9792\n",
      "Train Epoch: 050 Batch: 00083/00094 | Loss: 507.2247 | CE: 0.1916 | KD: 1060.0802\n",
      "Train Epoch: 050 Batch: 00084/00094 | Loss: 507.4134 | CE: 0.3431 | KD: 1060.1578\n",
      "Train Epoch: 050 Batch: 00085/00094 | Loss: 507.2188 | CE: 0.2138 | KD: 1060.0216\n",
      "Train Epoch: 050 Batch: 00086/00094 | Loss: 507.3181 | CE: 0.2682 | KD: 1060.1155\n",
      "Train Epoch: 050 Batch: 00087/00094 | Loss: 507.3033 | CE: 0.2677 | KD: 1060.0854\n",
      "Train Epoch: 050 Batch: 00088/00094 | Loss: 507.1731 | CE: 0.1517 | KD: 1060.0558\n",
      "Train Epoch: 050 Batch: 00089/00094 | Loss: 507.2084 | CE: 0.1999 | KD: 1060.0289\n",
      "Train Epoch: 050 Batch: 00090/00094 | Loss: 507.2840 | CE: 0.2503 | KD: 1060.0814\n",
      "Train Epoch: 050 Batch: 00091/00094 | Loss: 507.2699 | CE: 0.2295 | KD: 1060.0955\n",
      "Train Epoch: 050 Batch: 00092/00094 | Loss: 507.2687 | CE: 0.2495 | KD: 1060.0513\n",
      "Train Epoch: 050 Batch: 00093/00094 | Loss: 507.1950 | CE: 0.2080 | KD: 1059.9839\n",
      "Train Epoch: 050 Batch: 00094/00094 | Loss: 507.3026 | CE: 0.2762 | KD: 1060.0663\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2357 | acc:92.2500\n",
      "[VAL Acc] Target: 92.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1971 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0450 | acc:47.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0885 | acc:50.3817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7112 | acc:61.0893\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0523 | acc:54.2514\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5167 | acc:75.4310\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0709 | acc:54.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4661 | acc:75.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.65%\n",
      "[VAL Acc] Avg 62.23%\n",
      "Train Epoch: 051 Batch: 00001/00094 | Loss: 456.5894 | CE: 0.2846 | KD: 1060.0222\n",
      "Train Epoch: 051 Batch: 00002/00094 | Loss: 456.5414 | CE: 0.2115 | KD: 1060.0804\n",
      "Train Epoch: 051 Batch: 00003/00094 | Loss: 456.5078 | CE: 0.1899 | KD: 1060.0527\n",
      "Train Epoch: 051 Batch: 00004/00094 | Loss: 456.4862 | CE: 0.1477 | KD: 1060.1005\n",
      "Train Epoch: 051 Batch: 00005/00094 | Loss: 456.5066 | CE: 0.1844 | KD: 1060.0624\n",
      "Train Epoch: 051 Batch: 00006/00094 | Loss: 456.5979 | CE: 0.2480 | KD: 1060.1270\n",
      "Train Epoch: 051 Batch: 00007/00094 | Loss: 456.5686 | CE: 0.2501 | KD: 1060.0542\n",
      "Train Epoch: 051 Batch: 00008/00094 | Loss: 456.5992 | CE: 0.2449 | KD: 1060.1373\n",
      "Train Epoch: 051 Batch: 00009/00094 | Loss: 456.5281 | CE: 0.2198 | KD: 1060.0303\n",
      "Train Epoch: 051 Batch: 00010/00094 | Loss: 456.5815 | CE: 0.2531 | KD: 1060.0768\n",
      "Train Epoch: 051 Batch: 00011/00094 | Loss: 456.4540 | CE: 0.1742 | KD: 1059.9641\n",
      "Train Epoch: 051 Batch: 00012/00094 | Loss: 456.5095 | CE: 0.1540 | KD: 1060.1399\n",
      "Train Epoch: 051 Batch: 00013/00094 | Loss: 456.4434 | CE: 0.1574 | KD: 1059.9784\n",
      "Train Epoch: 051 Batch: 00014/00094 | Loss: 456.5657 | CE: 0.2466 | KD: 1060.0554\n",
      "Train Epoch: 051 Batch: 00015/00094 | Loss: 456.5675 | CE: 0.2187 | KD: 1060.1245\n",
      "Train Epoch: 051 Batch: 00016/00094 | Loss: 456.5847 | CE: 0.2261 | KD: 1060.1472\n",
      "Train Epoch: 051 Batch: 00017/00094 | Loss: 456.4710 | CE: 0.1632 | KD: 1060.0292\n",
      "Train Epoch: 051 Batch: 00018/00094 | Loss: 456.4761 | CE: 0.1488 | KD: 1060.0745\n",
      "Train Epoch: 051 Batch: 00019/00094 | Loss: 456.4414 | CE: 0.1590 | KD: 1059.9703\n",
      "Train Epoch: 051 Batch: 00020/00094 | Loss: 456.4543 | CE: 0.1718 | KD: 1059.9702\n",
      "Train Epoch: 051 Batch: 00021/00094 | Loss: 456.5220 | CE: 0.2301 | KD: 1059.9921\n",
      "Train Epoch: 051 Batch: 00022/00094 | Loss: 456.5381 | CE: 0.2160 | KD: 1060.0625\n",
      "Train Epoch: 051 Batch: 00023/00094 | Loss: 456.6459 | CE: 0.3319 | KD: 1060.0437\n",
      "Train Epoch: 051 Batch: 00024/00094 | Loss: 456.5163 | CE: 0.1820 | KD: 1060.0907\n",
      "Train Epoch: 051 Batch: 00025/00094 | Loss: 456.4969 | CE: 0.2168 | KD: 1059.9648\n",
      "Train Epoch: 051 Batch: 00026/00094 | Loss: 456.5550 | CE: 0.2469 | KD: 1060.0300\n",
      "Train Epoch: 051 Batch: 00027/00094 | Loss: 456.5880 | CE: 0.2251 | KD: 1060.1573\n",
      "Train Epoch: 051 Batch: 00028/00094 | Loss: 456.5672 | CE: 0.2296 | KD: 1060.0983\n",
      "Train Epoch: 051 Batch: 00029/00094 | Loss: 456.5767 | CE: 0.2266 | KD: 1060.1273\n",
      "Train Epoch: 051 Batch: 00030/00094 | Loss: 456.4861 | CE: 0.2068 | KD: 1059.9630\n",
      "Train Epoch: 051 Batch: 00031/00094 | Loss: 456.6271 | CE: 0.2899 | KD: 1060.0974\n",
      "Train Epoch: 051 Batch: 00032/00094 | Loss: 456.5251 | CE: 0.2146 | KD: 1060.0355\n",
      "Train Epoch: 051 Batch: 00033/00094 | Loss: 456.6487 | CE: 0.3053 | KD: 1060.1117\n",
      "Train Epoch: 051 Batch: 00034/00094 | Loss: 456.6670 | CE: 0.3266 | KD: 1060.1047\n",
      "Train Epoch: 051 Batch: 00035/00094 | Loss: 456.5079 | CE: 0.1979 | KD: 1060.0342\n",
      "Train Epoch: 051 Batch: 00036/00094 | Loss: 456.4956 | CE: 0.1731 | KD: 1060.0634\n",
      "Train Epoch: 051 Batch: 00037/00094 | Loss: 456.5720 | CE: 0.2455 | KD: 1060.0725\n",
      "Train Epoch: 051 Batch: 00038/00094 | Loss: 456.5405 | CE: 0.2331 | KD: 1060.0282\n",
      "Train Epoch: 051 Batch: 00039/00094 | Loss: 456.4520 | CE: 0.1492 | KD: 1060.0177\n",
      "Train Epoch: 051 Batch: 00040/00094 | Loss: 456.5585 | CE: 0.2422 | KD: 1060.0488\n",
      "Train Epoch: 051 Batch: 00041/00094 | Loss: 456.5963 | CE: 0.2675 | KD: 1060.0780\n",
      "Train Epoch: 051 Batch: 00042/00094 | Loss: 456.5228 | CE: 0.1981 | KD: 1060.0685\n",
      "Train Epoch: 051 Batch: 00043/00094 | Loss: 456.5847 | CE: 0.2389 | KD: 1060.1173\n",
      "Train Epoch: 051 Batch: 00044/00094 | Loss: 456.5338 | CE: 0.1940 | KD: 1060.1038\n",
      "Train Epoch: 051 Batch: 00045/00094 | Loss: 456.5493 | CE: 0.2371 | KD: 1060.0392\n",
      "Train Epoch: 051 Batch: 00046/00094 | Loss: 456.6794 | CE: 0.3251 | KD: 1060.1372\n",
      "Train Epoch: 051 Batch: 00047/00094 | Loss: 456.5238 | CE: 0.2110 | KD: 1060.0408\n",
      "Train Epoch: 051 Batch: 00048/00094 | Loss: 456.5330 | CE: 0.2434 | KD: 1059.9871\n",
      "Train Epoch: 051 Batch: 00049/00094 | Loss: 456.4922 | CE: 0.1760 | KD: 1060.0486\n",
      "Train Epoch: 051 Batch: 00050/00094 | Loss: 456.6100 | CE: 0.2585 | KD: 1060.1306\n",
      "Train Epoch: 051 Batch: 00051/00094 | Loss: 456.5612 | CE: 0.2399 | KD: 1060.0604\n",
      "Train Epoch: 051 Batch: 00052/00094 | Loss: 456.4974 | CE: 0.1884 | KD: 1060.0320\n",
      "Train Epoch: 051 Batch: 00053/00094 | Loss: 456.5991 | CE: 0.2654 | KD: 1060.0895\n",
      "Train Epoch: 051 Batch: 00054/00094 | Loss: 456.5509 | CE: 0.2527 | KD: 1060.0070\n",
      "Train Epoch: 051 Batch: 00055/00094 | Loss: 456.5275 | CE: 0.2009 | KD: 1060.0729\n",
      "Train Epoch: 051 Batch: 00056/00094 | Loss: 456.4932 | CE: 0.1869 | KD: 1060.0256\n",
      "Train Epoch: 051 Batch: 00057/00094 | Loss: 456.4648 | CE: 0.1858 | KD: 1059.9623\n",
      "Train Epoch: 051 Batch: 00058/00094 | Loss: 456.6194 | CE: 0.2543 | KD: 1060.1622\n",
      "Train Epoch: 051 Batch: 00059/00094 | Loss: 456.5299 | CE: 0.2261 | KD: 1060.0199\n",
      "Train Epoch: 051 Batch: 00060/00094 | Loss: 456.4973 | CE: 0.2035 | KD: 1059.9967\n",
      "Train Epoch: 051 Batch: 00061/00094 | Loss: 456.4900 | CE: 0.1908 | KD: 1060.0090\n",
      "Train Epoch: 051 Batch: 00062/00094 | Loss: 456.6112 | CE: 0.2759 | KD: 1060.0931\n",
      "Train Epoch: 051 Batch: 00063/00094 | Loss: 456.5757 | CE: 0.2744 | KD: 1060.0142\n",
      "Train Epoch: 051 Batch: 00064/00094 | Loss: 456.5806 | CE: 0.2436 | KD: 1060.0972\n",
      "Train Epoch: 051 Batch: 00065/00094 | Loss: 456.5016 | CE: 0.2044 | KD: 1060.0045\n",
      "Train Epoch: 051 Batch: 00066/00094 | Loss: 456.5735 | CE: 0.2557 | KD: 1060.0524\n",
      "Train Epoch: 051 Batch: 00067/00094 | Loss: 456.5434 | CE: 0.2103 | KD: 1060.0879\n",
      "Train Epoch: 051 Batch: 00068/00094 | Loss: 456.5103 | CE: 0.2057 | KD: 1060.0216\n",
      "Train Epoch: 051 Batch: 00069/00094 | Loss: 456.5934 | CE: 0.2248 | KD: 1060.1703\n",
      "Train Epoch: 051 Batch: 00070/00094 | Loss: 456.5116 | CE: 0.2138 | KD: 1060.0057\n",
      "Train Epoch: 051 Batch: 00071/00094 | Loss: 456.5259 | CE: 0.1806 | KD: 1060.1162\n",
      "Train Epoch: 051 Batch: 00072/00094 | Loss: 456.6045 | CE: 0.2384 | KD: 1060.1644\n",
      "Train Epoch: 051 Batch: 00073/00094 | Loss: 456.4731 | CE: 0.1627 | KD: 1060.0353\n",
      "Train Epoch: 051 Batch: 00074/00094 | Loss: 456.5594 | CE: 0.2507 | KD: 1060.0311\n",
      "Train Epoch: 051 Batch: 00075/00094 | Loss: 456.5436 | CE: 0.2378 | KD: 1060.0245\n",
      "Train Epoch: 051 Batch: 00076/00094 | Loss: 456.4449 | CE: 0.1734 | KD: 1059.9447\n",
      "Train Epoch: 051 Batch: 00077/00094 | Loss: 456.4817 | CE: 0.1646 | KD: 1060.0508\n",
      "Train Epoch: 051 Batch: 00078/00094 | Loss: 456.4816 | CE: 0.1853 | KD: 1060.0023\n",
      "Train Epoch: 051 Batch: 00079/00094 | Loss: 456.5893 | CE: 0.2707 | KD: 1060.0543\n",
      "Train Epoch: 051 Batch: 00080/00094 | Loss: 456.4522 | CE: 0.1598 | KD: 1059.9933\n",
      "Train Epoch: 051 Batch: 00081/00094 | Loss: 456.5108 | CE: 0.2042 | KD: 1060.0262\n",
      "Train Epoch: 051 Batch: 00082/00094 | Loss: 456.5142 | CE: 0.1913 | KD: 1060.0642\n",
      "Train Epoch: 051 Batch: 00083/00094 | Loss: 456.5011 | CE: 0.1838 | KD: 1060.0514\n",
      "Train Epoch: 051 Batch: 00084/00094 | Loss: 456.5125 | CE: 0.1961 | KD: 1060.0492\n",
      "Train Epoch: 051 Batch: 00085/00094 | Loss: 456.5387 | CE: 0.2259 | KD: 1060.0409\n",
      "Train Epoch: 051 Batch: 00086/00094 | Loss: 456.5065 | CE: 0.2042 | KD: 1060.0162\n",
      "Train Epoch: 051 Batch: 00087/00094 | Loss: 456.6150 | CE: 0.2713 | KD: 1060.1127\n",
      "Train Epoch: 051 Batch: 00088/00094 | Loss: 456.4816 | CE: 0.1851 | KD: 1060.0029\n",
      "Train Epoch: 051 Batch: 00089/00094 | Loss: 456.5995 | CE: 0.2322 | KD: 1060.1674\n",
      "Train Epoch: 051 Batch: 00090/00094 | Loss: 456.5968 | CE: 0.2744 | KD: 1060.0630\n",
      "Train Epoch: 051 Batch: 00091/00094 | Loss: 456.5383 | CE: 0.2394 | KD: 1060.0085\n",
      "Train Epoch: 051 Batch: 00092/00094 | Loss: 456.5190 | CE: 0.2004 | KD: 1060.0543\n",
      "Train Epoch: 051 Batch: 00093/00094 | Loss: 456.5434 | CE: 0.1935 | KD: 1060.1270\n",
      "Train Epoch: 051 Batch: 00094/00094 | Loss: 456.4961 | CE: 0.1764 | KD: 1060.0568\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2288 | acc:93.4000\n",
      "[VAL Acc] Target: 93.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1996 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0795 | acc:47.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1184 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7520 | acc:59.4044\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9909 | acc:54.3438\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5303 | acc:74.9216\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0257 | acc:55.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5033 | acc:73.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.45%\n",
      "[VAL Acc] Avg 61.93%\n",
      "Train Epoch: 052 Batch: 00001/00094 | Loss: 456.5192 | CE: 0.1882 | KD: 1060.0829\n",
      "Train Epoch: 052 Batch: 00002/00094 | Loss: 456.5368 | CE: 0.2034 | KD: 1060.0887\n",
      "Train Epoch: 052 Batch: 00003/00094 | Loss: 456.5972 | CE: 0.2391 | KD: 1060.1461\n",
      "Train Epoch: 052 Batch: 00004/00094 | Loss: 456.5358 | CE: 0.2470 | KD: 1059.9851\n",
      "Train Epoch: 052 Batch: 00005/00094 | Loss: 456.4971 | CE: 0.2199 | KD: 1059.9579\n",
      "Train Epoch: 052 Batch: 00006/00094 | Loss: 456.5772 | CE: 0.2275 | KD: 1060.1266\n",
      "Train Epoch: 052 Batch: 00007/00094 | Loss: 456.4750 | CE: 0.1723 | KD: 1060.0172\n",
      "Train Epoch: 052 Batch: 00008/00094 | Loss: 456.5023 | CE: 0.2034 | KD: 1060.0087\n",
      "Train Epoch: 052 Batch: 00009/00094 | Loss: 456.5868 | CE: 0.2496 | KD: 1060.0974\n",
      "Train Epoch: 052 Batch: 00010/00094 | Loss: 456.5406 | CE: 0.2176 | KD: 1060.0645\n",
      "Train Epoch: 052 Batch: 00011/00094 | Loss: 456.6441 | CE: 0.3219 | KD: 1060.0626\n",
      "Train Epoch: 052 Batch: 00012/00094 | Loss: 456.4890 | CE: 0.1911 | KD: 1060.0062\n",
      "Train Epoch: 052 Batch: 00013/00094 | Loss: 456.5822 | CE: 0.2487 | KD: 1060.0889\n",
      "Train Epoch: 052 Batch: 00014/00094 | Loss: 456.6898 | CE: 0.2911 | KD: 1060.2404\n",
      "Train Epoch: 052 Batch: 00015/00094 | Loss: 456.5631 | CE: 0.2589 | KD: 1060.0210\n",
      "Train Epoch: 052 Batch: 00016/00094 | Loss: 456.6130 | CE: 0.2792 | KD: 1060.0895\n",
      "Train Epoch: 052 Batch: 00017/00094 | Loss: 456.6600 | CE: 0.3308 | KD: 1060.0789\n",
      "Train Epoch: 052 Batch: 00018/00094 | Loss: 456.5233 | CE: 0.1971 | KD: 1060.0717\n",
      "Train Epoch: 052 Batch: 00019/00094 | Loss: 456.5542 | CE: 0.2198 | KD: 1060.0908\n",
      "Train Epoch: 052 Batch: 00020/00094 | Loss: 456.6176 | CE: 0.2631 | KD: 1060.1375\n",
      "Train Epoch: 052 Batch: 00021/00094 | Loss: 456.5325 | CE: 0.2081 | KD: 1060.0679\n",
      "Train Epoch: 052 Batch: 00022/00094 | Loss: 456.5051 | CE: 0.1936 | KD: 1060.0380\n",
      "Train Epoch: 052 Batch: 00023/00094 | Loss: 456.6385 | CE: 0.2903 | KD: 1060.1229\n",
      "Train Epoch: 052 Batch: 00024/00094 | Loss: 456.5319 | CE: 0.2158 | KD: 1060.0485\n",
      "Train Epoch: 052 Batch: 00025/00094 | Loss: 456.5319 | CE: 0.2262 | KD: 1060.0244\n",
      "Train Epoch: 052 Batch: 00026/00094 | Loss: 456.5421 | CE: 0.2418 | KD: 1060.0117\n",
      "Train Epoch: 052 Batch: 00027/00094 | Loss: 456.5298 | CE: 0.2176 | KD: 1060.0394\n",
      "Train Epoch: 052 Batch: 00028/00094 | Loss: 456.5668 | CE: 0.2494 | KD: 1060.0516\n",
      "Train Epoch: 052 Batch: 00029/00094 | Loss: 456.5218 | CE: 0.1666 | KD: 1060.1394\n",
      "Train Epoch: 052 Batch: 00030/00094 | Loss: 456.5164 | CE: 0.2360 | KD: 1059.9653\n",
      "Train Epoch: 052 Batch: 00031/00094 | Loss: 456.4789 | CE: 0.1779 | KD: 1060.0135\n",
      "Train Epoch: 052 Batch: 00032/00094 | Loss: 456.4939 | CE: 0.1846 | KD: 1060.0326\n",
      "Train Epoch: 052 Batch: 00033/00094 | Loss: 456.5461 | CE: 0.2153 | KD: 1060.0824\n",
      "Train Epoch: 052 Batch: 00034/00094 | Loss: 456.5675 | CE: 0.2523 | KD: 1060.0461\n",
      "Train Epoch: 052 Batch: 00035/00094 | Loss: 456.5995 | CE: 0.2744 | KD: 1060.0696\n",
      "Train Epoch: 052 Batch: 00036/00094 | Loss: 456.5190 | CE: 0.1935 | KD: 1060.0703\n",
      "Train Epoch: 052 Batch: 00037/00094 | Loss: 456.5034 | CE: 0.1907 | KD: 1060.0405\n",
      "Train Epoch: 052 Batch: 00038/00094 | Loss: 456.5384 | CE: 0.2198 | KD: 1060.0542\n",
      "Train Epoch: 052 Batch: 00039/00094 | Loss: 456.5260 | CE: 0.1912 | KD: 1060.0919\n",
      "Train Epoch: 052 Batch: 00040/00094 | Loss: 456.5038 | CE: 0.1876 | KD: 1060.0488\n",
      "Train Epoch: 052 Batch: 00041/00094 | Loss: 456.6249 | CE: 0.2797 | KD: 1060.1161\n",
      "Train Epoch: 052 Batch: 00042/00094 | Loss: 456.5976 | CE: 0.2754 | KD: 1060.0625\n",
      "Train Epoch: 052 Batch: 00043/00094 | Loss: 456.5108 | CE: 0.1900 | KD: 1060.0592\n",
      "Train Epoch: 052 Batch: 00044/00094 | Loss: 456.5131 | CE: 0.1692 | KD: 1060.1129\n",
      "Train Epoch: 052 Batch: 00045/00094 | Loss: 456.4935 | CE: 0.2059 | KD: 1059.9823\n",
      "Train Epoch: 052 Batch: 00046/00094 | Loss: 456.4932 | CE: 0.1849 | KD: 1060.0303\n",
      "Train Epoch: 052 Batch: 00047/00094 | Loss: 456.5073 | CE: 0.1923 | KD: 1060.0458\n",
      "Train Epoch: 052 Batch: 00048/00094 | Loss: 456.5094 | CE: 0.2106 | KD: 1060.0082\n",
      "Train Epoch: 052 Batch: 00049/00094 | Loss: 456.4910 | CE: 0.1952 | KD: 1060.0013\n",
      "Train Epoch: 052 Batch: 00050/00094 | Loss: 456.4347 | CE: 0.1314 | KD: 1060.0186\n",
      "Train Epoch: 052 Batch: 00051/00094 | Loss: 456.5164 | CE: 0.2113 | KD: 1060.0228\n",
      "Train Epoch: 052 Batch: 00052/00094 | Loss: 456.5621 | CE: 0.2422 | KD: 1060.0574\n",
      "Train Epoch: 052 Batch: 00053/00094 | Loss: 456.6230 | CE: 0.2880 | KD: 1060.0923\n",
      "Train Epoch: 052 Batch: 00054/00094 | Loss: 456.5111 | CE: 0.2012 | KD: 1060.0341\n",
      "Train Epoch: 052 Batch: 00055/00094 | Loss: 456.6151 | CE: 0.2639 | KD: 1060.1300\n",
      "Train Epoch: 052 Batch: 00056/00094 | Loss: 456.5910 | CE: 0.2631 | KD: 1060.0758\n",
      "Train Epoch: 052 Batch: 00057/00094 | Loss: 456.4880 | CE: 0.2194 | KD: 1059.9381\n",
      "Train Epoch: 052 Batch: 00058/00094 | Loss: 456.7305 | CE: 0.3878 | KD: 1060.1102\n",
      "Train Epoch: 052 Batch: 00059/00094 | Loss: 456.4935 | CE: 0.1892 | KD: 1060.0209\n",
      "Train Epoch: 052 Batch: 00060/00094 | Loss: 456.5305 | CE: 0.2160 | KD: 1060.0446\n",
      "Train Epoch: 052 Batch: 00061/00094 | Loss: 456.5903 | CE: 0.2810 | KD: 1060.0327\n",
      "Train Epoch: 052 Batch: 00062/00094 | Loss: 456.5956 | CE: 0.2510 | KD: 1060.1145\n",
      "Train Epoch: 052 Batch: 00063/00094 | Loss: 456.5285 | CE: 0.1605 | KD: 1060.1691\n",
      "Train Epoch: 052 Batch: 00064/00094 | Loss: 456.5283 | CE: 0.2203 | KD: 1060.0295\n",
      "Train Epoch: 052 Batch: 00065/00094 | Loss: 456.4778 | CE: 0.1823 | KD: 1060.0006\n",
      "Train Epoch: 052 Batch: 00066/00094 | Loss: 456.4678 | CE: 0.1840 | KD: 1059.9733\n",
      "Train Epoch: 052 Batch: 00067/00094 | Loss: 456.5807 | CE: 0.2505 | KD: 1060.0813\n",
      "Train Epoch: 052 Batch: 00068/00094 | Loss: 456.6149 | CE: 0.2803 | KD: 1060.0914\n",
      "Train Epoch: 052 Batch: 00069/00094 | Loss: 456.5092 | CE: 0.2020 | KD: 1060.0280\n",
      "Train Epoch: 052 Batch: 00070/00094 | Loss: 456.4371 | CE: 0.1434 | KD: 1059.9965\n",
      "Train Epoch: 052 Batch: 00071/00094 | Loss: 456.4878 | CE: 0.1702 | KD: 1060.0519\n",
      "Train Epoch: 052 Batch: 00072/00094 | Loss: 456.5913 | CE: 0.2843 | KD: 1060.0272\n",
      "Train Epoch: 052 Batch: 00073/00094 | Loss: 456.5874 | CE: 0.2475 | KD: 1060.1038\n",
      "Train Epoch: 052 Batch: 00074/00094 | Loss: 456.6042 | CE: 0.2518 | KD: 1060.1327\n",
      "Train Epoch: 052 Batch: 00075/00094 | Loss: 456.5079 | CE: 0.1956 | KD: 1060.0396\n",
      "Train Epoch: 052 Batch: 00076/00094 | Loss: 456.5244 | CE: 0.2200 | KD: 1060.0211\n",
      "Train Epoch: 052 Batch: 00077/00094 | Loss: 456.4913 | CE: 0.1777 | KD: 1060.0426\n",
      "Train Epoch: 052 Batch: 00078/00094 | Loss: 456.5821 | CE: 0.2346 | KD: 1060.1212\n",
      "Train Epoch: 052 Batch: 00079/00094 | Loss: 456.5343 | CE: 0.2376 | KD: 1060.0033\n",
      "Train Epoch: 052 Batch: 00080/00094 | Loss: 456.6585 | CE: 0.2714 | KD: 1060.2135\n",
      "Train Epoch: 052 Batch: 00081/00094 | Loss: 456.5104 | CE: 0.2006 | KD: 1060.0339\n",
      "Train Epoch: 052 Batch: 00082/00094 | Loss: 456.5207 | CE: 0.1868 | KD: 1060.0897\n",
      "Train Epoch: 052 Batch: 00083/00094 | Loss: 456.4793 | CE: 0.1954 | KD: 1059.9738\n",
      "Train Epoch: 052 Batch: 00084/00094 | Loss: 456.5300 | CE: 0.2385 | KD: 1059.9915\n",
      "Train Epoch: 052 Batch: 00085/00094 | Loss: 456.5126 | CE: 0.2020 | KD: 1060.0358\n",
      "Train Epoch: 052 Batch: 00086/00094 | Loss: 456.5535 | CE: 0.2093 | KD: 1060.1136\n",
      "Train Epoch: 052 Batch: 00087/00094 | Loss: 456.5818 | CE: 0.2542 | KD: 1060.0751\n",
      "Train Epoch: 052 Batch: 00088/00094 | Loss: 456.5921 | CE: 0.2469 | KD: 1060.1161\n",
      "Train Epoch: 052 Batch: 00089/00094 | Loss: 456.5634 | CE: 0.2288 | KD: 1060.0913\n",
      "Train Epoch: 052 Batch: 00090/00094 | Loss: 456.5943 | CE: 0.2438 | KD: 1060.1285\n",
      "Train Epoch: 052 Batch: 00091/00094 | Loss: 456.5386 | CE: 0.1908 | KD: 1060.1222\n",
      "Train Epoch: 052 Batch: 00092/00094 | Loss: 456.5033 | CE: 0.1810 | KD: 1060.0629\n",
      "Train Epoch: 052 Batch: 00093/00094 | Loss: 456.6145 | CE: 0.2884 | KD: 1060.0715\n",
      "Train Epoch: 052 Batch: 00094/00094 | Loss: 456.5884 | CE: 0.2613 | KD: 1060.0741\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2503 | acc:91.9000\n",
      "[VAL Acc] Target: 91.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1973 | acc:50.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0501 | acc:46.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 46.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0944 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7133 | acc:61.9122\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.91%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9859 | acc:54.1590\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5175 | acc:74.8824\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0247 | acc:55.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4724 | acc:75.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 75.05%\n",
      "[VAL Acc] Avg 62.12%\n",
      "Train Epoch: 053 Batch: 00001/00094 | Loss: 456.5315 | CE: 0.2230 | KD: 1060.0309\n",
      "Train Epoch: 053 Batch: 00002/00094 | Loss: 456.4580 | CE: 0.1699 | KD: 1059.9834\n",
      "Train Epoch: 053 Batch: 00003/00094 | Loss: 456.5736 | CE: 0.2640 | KD: 1060.0334\n",
      "Train Epoch: 053 Batch: 00004/00094 | Loss: 456.6371 | CE: 0.2520 | KD: 1060.2087\n",
      "Train Epoch: 053 Batch: 00005/00094 | Loss: 456.5386 | CE: 0.2553 | KD: 1059.9724\n",
      "Train Epoch: 053 Batch: 00006/00094 | Loss: 456.5853 | CE: 0.2205 | KD: 1060.1617\n",
      "Train Epoch: 053 Batch: 00007/00094 | Loss: 456.5330 | CE: 0.1950 | KD: 1060.0992\n",
      "Train Epoch: 053 Batch: 00008/00094 | Loss: 456.5765 | CE: 0.2461 | KD: 1060.0815\n",
      "Train Epoch: 053 Batch: 00009/00094 | Loss: 456.5285 | CE: 0.2116 | KD: 1060.0504\n",
      "Train Epoch: 053 Batch: 00010/00094 | Loss: 456.4982 | CE: 0.1942 | KD: 1060.0204\n",
      "Train Epoch: 053 Batch: 00011/00094 | Loss: 456.5995 | CE: 0.2448 | KD: 1060.1379\n",
      "Train Epoch: 053 Batch: 00012/00094 | Loss: 456.5929 | CE: 0.2466 | KD: 1060.1187\n",
      "Train Epoch: 053 Batch: 00013/00094 | Loss: 456.5119 | CE: 0.2051 | KD: 1060.0269\n",
      "Train Epoch: 053 Batch: 00014/00094 | Loss: 456.5485 | CE: 0.2268 | KD: 1060.0614\n",
      "Train Epoch: 053 Batch: 00015/00094 | Loss: 456.5960 | CE: 0.2620 | KD: 1060.0900\n",
      "Train Epoch: 053 Batch: 00016/00094 | Loss: 456.5316 | CE: 0.1987 | KD: 1060.0874\n",
      "Train Epoch: 053 Batch: 00017/00094 | Loss: 456.5396 | CE: 0.2035 | KD: 1060.0950\n",
      "Train Epoch: 053 Batch: 00018/00094 | Loss: 456.4493 | CE: 0.1565 | KD: 1059.9943\n",
      "Train Epoch: 053 Batch: 00019/00094 | Loss: 456.5003 | CE: 0.1858 | KD: 1060.0449\n",
      "Train Epoch: 053 Batch: 00020/00094 | Loss: 456.5169 | CE: 0.1818 | KD: 1060.0928\n",
      "Train Epoch: 053 Batch: 00021/00094 | Loss: 456.4985 | CE: 0.1828 | KD: 1060.0475\n",
      "Train Epoch: 053 Batch: 00022/00094 | Loss: 456.5720 | CE: 0.2571 | KD: 1060.0455\n",
      "Train Epoch: 053 Batch: 00023/00094 | Loss: 456.5698 | CE: 0.2660 | KD: 1060.0198\n",
      "Train Epoch: 053 Batch: 00024/00094 | Loss: 456.4846 | CE: 0.2085 | KD: 1059.9556\n",
      "Train Epoch: 053 Batch: 00025/00094 | Loss: 456.5092 | CE: 0.1892 | KD: 1060.0574\n",
      "Train Epoch: 053 Batch: 00026/00094 | Loss: 456.5211 | CE: 0.1842 | KD: 1060.0969\n",
      "Train Epoch: 053 Batch: 00027/00094 | Loss: 456.5725 | CE: 0.2291 | KD: 1060.1119\n",
      "Train Epoch: 053 Batch: 00028/00094 | Loss: 456.5113 | CE: 0.1948 | KD: 1060.0493\n",
      "Train Epoch: 053 Batch: 00029/00094 | Loss: 456.4859 | CE: 0.1899 | KD: 1060.0017\n",
      "Train Epoch: 053 Batch: 00030/00094 | Loss: 456.5701 | CE: 0.2385 | KD: 1060.0844\n",
      "Train Epoch: 053 Batch: 00031/00094 | Loss: 456.5763 | CE: 0.2566 | KD: 1060.0569\n",
      "Train Epoch: 053 Batch: 00032/00094 | Loss: 456.5618 | CE: 0.2146 | KD: 1060.1206\n",
      "Train Epoch: 053 Batch: 00033/00094 | Loss: 456.5770 | CE: 0.2732 | KD: 1060.0199\n",
      "Train Epoch: 053 Batch: 00034/00094 | Loss: 456.5979 | CE: 0.2700 | KD: 1060.0759\n",
      "Train Epoch: 053 Batch: 00035/00094 | Loss: 456.6357 | CE: 0.2828 | KD: 1060.1338\n",
      "Train Epoch: 053 Batch: 00036/00094 | Loss: 456.5156 | CE: 0.1969 | KD: 1060.0544\n",
      "Train Epoch: 053 Batch: 00037/00094 | Loss: 456.5220 | CE: 0.2101 | KD: 1060.0388\n",
      "Train Epoch: 053 Batch: 00038/00094 | Loss: 456.5401 | CE: 0.2014 | KD: 1060.1010\n",
      "Train Epoch: 053 Batch: 00039/00094 | Loss: 456.5336 | CE: 0.2231 | KD: 1060.0354\n",
      "Train Epoch: 053 Batch: 00040/00094 | Loss: 456.4436 | CE: 0.1615 | KD: 1059.9695\n",
      "Train Epoch: 053 Batch: 00041/00094 | Loss: 456.5579 | CE: 0.2280 | KD: 1060.0804\n",
      "Train Epoch: 053 Batch: 00042/00094 | Loss: 456.5453 | CE: 0.2532 | KD: 1059.9928\n",
      "Train Epoch: 053 Batch: 00043/00094 | Loss: 456.4697 | CE: 0.1809 | KD: 1059.9850\n",
      "Train Epoch: 053 Batch: 00044/00094 | Loss: 456.5005 | CE: 0.2008 | KD: 1060.0105\n",
      "Train Epoch: 053 Batch: 00045/00094 | Loss: 456.5931 | CE: 0.2604 | KD: 1060.0872\n",
      "Train Epoch: 053 Batch: 00046/00094 | Loss: 456.5564 | CE: 0.2227 | KD: 1060.0891\n",
      "Train Epoch: 053 Batch: 00047/00094 | Loss: 456.4699 | CE: 0.1472 | KD: 1060.0638\n",
      "Train Epoch: 053 Batch: 00048/00094 | Loss: 456.4934 | CE: 0.1854 | KD: 1060.0295\n",
      "Train Epoch: 053 Batch: 00049/00094 | Loss: 456.5383 | CE: 0.2202 | KD: 1060.0532\n",
      "Train Epoch: 053 Batch: 00050/00094 | Loss: 456.6105 | CE: 0.2810 | KD: 1060.0796\n",
      "Train Epoch: 053 Batch: 00051/00094 | Loss: 456.5332 | CE: 0.1950 | KD: 1060.0999\n",
      "Train Epoch: 053 Batch: 00052/00094 | Loss: 456.4893 | CE: 0.1917 | KD: 1060.0055\n",
      "Train Epoch: 053 Batch: 00053/00094 | Loss: 456.5504 | CE: 0.2167 | KD: 1060.0894\n",
      "Train Epoch: 053 Batch: 00054/00094 | Loss: 456.5738 | CE: 0.2284 | KD: 1060.1165\n",
      "Train Epoch: 053 Batch: 00055/00094 | Loss: 456.4928 | CE: 0.1965 | KD: 1060.0024\n",
      "Train Epoch: 053 Batch: 00056/00094 | Loss: 456.4948 | CE: 0.1792 | KD: 1060.0474\n",
      "Train Epoch: 053 Batch: 00057/00094 | Loss: 456.5942 | CE: 0.2839 | KD: 1060.0348\n",
      "Train Epoch: 053 Batch: 00058/00094 | Loss: 456.5197 | CE: 0.2197 | KD: 1060.0110\n",
      "Train Epoch: 053 Batch: 00059/00094 | Loss: 456.5805 | CE: 0.2614 | KD: 1060.0553\n",
      "Train Epoch: 053 Batch: 00060/00094 | Loss: 456.5648 | CE: 0.2302 | KD: 1060.0914\n",
      "Train Epoch: 053 Batch: 00061/00094 | Loss: 456.5517 | CE: 0.1975 | KD: 1060.1368\n",
      "Train Epoch: 053 Batch: 00062/00094 | Loss: 456.5002 | CE: 0.1924 | KD: 1060.0291\n",
      "Train Epoch: 053 Batch: 00063/00094 | Loss: 456.5067 | CE: 0.1960 | KD: 1060.0358\n",
      "Train Epoch: 053 Batch: 00064/00094 | Loss: 456.5865 | CE: 0.2503 | KD: 1060.0952\n",
      "Train Epoch: 053 Batch: 00065/00094 | Loss: 456.4785 | CE: 0.1817 | KD: 1060.0037\n",
      "Train Epoch: 053 Batch: 00066/00094 | Loss: 456.5472 | CE: 0.2055 | KD: 1060.1079\n",
      "Train Epoch: 053 Batch: 00067/00094 | Loss: 456.5142 | CE: 0.1952 | KD: 1060.0553\n",
      "Train Epoch: 053 Batch: 00068/00094 | Loss: 456.4633 | CE: 0.1749 | KD: 1059.9840\n",
      "Train Epoch: 053 Batch: 00069/00094 | Loss: 456.5969 | CE: 0.2971 | KD: 1060.0106\n",
      "Train Epoch: 053 Batch: 00070/00094 | Loss: 456.5312 | CE: 0.2241 | KD: 1060.0275\n",
      "Train Epoch: 053 Batch: 00071/00094 | Loss: 456.4896 | CE: 0.1963 | KD: 1059.9954\n",
      "Train Epoch: 053 Batch: 00072/00094 | Loss: 456.5160 | CE: 0.1858 | KD: 1060.0812\n",
      "Train Epoch: 053 Batch: 00073/00094 | Loss: 456.5756 | CE: 0.2492 | KD: 1060.0723\n",
      "Train Epoch: 053 Batch: 00074/00094 | Loss: 456.5095 | CE: 0.1723 | KD: 1060.0974\n",
      "Train Epoch: 053 Batch: 00075/00094 | Loss: 456.6359 | CE: 0.2377 | KD: 1060.2393\n",
      "Train Epoch: 053 Batch: 00076/00094 | Loss: 456.5443 | CE: 0.2179 | KD: 1060.0725\n",
      "Train Epoch: 053 Batch: 00077/00094 | Loss: 456.5048 | CE: 0.1864 | KD: 1060.0537\n",
      "Train Epoch: 053 Batch: 00078/00094 | Loss: 456.5609 | CE: 0.2159 | KD: 1060.1156\n",
      "Train Epoch: 053 Batch: 00079/00094 | Loss: 456.5421 | CE: 0.2379 | KD: 1060.0210\n",
      "Train Epoch: 053 Batch: 00080/00094 | Loss: 456.5222 | CE: 0.1824 | KD: 1060.1035\n",
      "Train Epoch: 053 Batch: 00081/00094 | Loss: 456.5766 | CE: 0.2358 | KD: 1060.1060\n",
      "Train Epoch: 053 Batch: 00082/00094 | Loss: 456.5417 | CE: 0.2081 | KD: 1060.0891\n",
      "Train Epoch: 053 Batch: 00083/00094 | Loss: 456.5989 | CE: 0.2325 | KD: 1060.1653\n",
      "Train Epoch: 053 Batch: 00084/00094 | Loss: 456.5025 | CE: 0.1924 | KD: 1060.0344\n",
      "Train Epoch: 053 Batch: 00085/00094 | Loss: 456.5717 | CE: 0.2582 | KD: 1060.0425\n",
      "Train Epoch: 053 Batch: 00086/00094 | Loss: 456.5638 | CE: 0.2102 | KD: 1060.1355\n",
      "Train Epoch: 053 Batch: 00087/00094 | Loss: 456.5215 | CE: 0.1970 | KD: 1060.0680\n",
      "Train Epoch: 053 Batch: 00088/00094 | Loss: 456.4803 | CE: 0.1785 | KD: 1060.0153\n",
      "Train Epoch: 053 Batch: 00089/00094 | Loss: 456.4266 | CE: 0.1552 | KD: 1059.9447\n",
      "Train Epoch: 053 Batch: 00090/00094 | Loss: 456.5615 | CE: 0.2298 | KD: 1060.0846\n",
      "Train Epoch: 053 Batch: 00091/00094 | Loss: 456.4832 | CE: 0.2034 | KD: 1059.9642\n",
      "Train Epoch: 053 Batch: 00092/00094 | Loss: 456.5171 | CE: 0.1857 | KD: 1060.0840\n",
      "Train Epoch: 053 Batch: 00093/00094 | Loss: 456.4929 | CE: 0.1767 | KD: 1060.0486\n",
      "Train Epoch: 053 Batch: 00094/00094 | Loss: 456.5518 | CE: 0.2345 | KD: 1060.0511\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2519 | acc:90.9000\n",
      "[VAL Acc] Target: 90.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2243 | acc:50.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0853 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0800 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7184 | acc:60.5799\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 60.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9857 | acc:54.8059\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5242 | acc:74.4122\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0310 | acc:54.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5062 | acc:73.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.10%\n",
      "[VAL Acc] Avg 61.90%\n",
      "Train Epoch: 054 Batch: 00001/00094 | Loss: 456.4509 | CE: 0.1336 | KD: 1060.0511\n",
      "Train Epoch: 054 Batch: 00002/00094 | Loss: 456.5799 | CE: 0.2661 | KD: 1060.0430\n",
      "Train Epoch: 054 Batch: 00003/00094 | Loss: 456.4180 | CE: 0.1456 | KD: 1059.9469\n",
      "Train Epoch: 054 Batch: 00004/00094 | Loss: 456.6232 | CE: 0.2761 | KD: 1060.1206\n",
      "Train Epoch: 054 Batch: 00005/00094 | Loss: 456.5252 | CE: 0.1895 | KD: 1060.0940\n",
      "Train Epoch: 054 Batch: 00006/00094 | Loss: 456.5195 | CE: 0.2121 | KD: 1060.0282\n",
      "Train Epoch: 054 Batch: 00007/00094 | Loss: 456.5094 | CE: 0.1801 | KD: 1060.0791\n",
      "Train Epoch: 054 Batch: 00008/00094 | Loss: 456.5382 | CE: 0.2086 | KD: 1060.0798\n",
      "Train Epoch: 054 Batch: 00009/00094 | Loss: 456.5190 | CE: 0.1743 | KD: 1060.1149\n",
      "Train Epoch: 054 Batch: 00010/00094 | Loss: 456.6155 | CE: 0.2482 | KD: 1060.1675\n",
      "Train Epoch: 054 Batch: 00011/00094 | Loss: 456.4866 | CE: 0.1560 | KD: 1060.0822\n",
      "Train Epoch: 054 Batch: 00012/00094 | Loss: 456.5074 | CE: 0.2107 | KD: 1060.0034\n",
      "Train Epoch: 054 Batch: 00013/00094 | Loss: 456.5216 | CE: 0.1783 | KD: 1060.1116\n",
      "Train Epoch: 054 Batch: 00014/00094 | Loss: 456.4406 | CE: 0.1582 | KD: 1059.9701\n",
      "Train Epoch: 054 Batch: 00015/00094 | Loss: 456.6123 | CE: 0.2401 | KD: 1060.1788\n",
      "Train Epoch: 054 Batch: 00016/00094 | Loss: 456.4756 | CE: 0.1460 | KD: 1060.0798\n",
      "Train Epoch: 054 Batch: 00017/00094 | Loss: 456.5697 | CE: 0.2355 | KD: 1060.0906\n",
      "Train Epoch: 054 Batch: 00018/00094 | Loss: 456.4850 | CE: 0.1820 | KD: 1060.0181\n",
      "Train Epoch: 054 Batch: 00019/00094 | Loss: 456.6144 | CE: 0.2406 | KD: 1060.1825\n",
      "Train Epoch: 054 Batch: 00020/00094 | Loss: 456.4686 | CE: 0.1779 | KD: 1059.9894\n",
      "Train Epoch: 054 Batch: 00021/00094 | Loss: 456.4863 | CE: 0.1812 | KD: 1060.0229\n",
      "Train Epoch: 054 Batch: 00022/00094 | Loss: 456.5189 | CE: 0.2154 | KD: 1060.0192\n",
      "Train Epoch: 054 Batch: 00023/00094 | Loss: 456.5547 | CE: 0.2274 | KD: 1060.0742\n",
      "Train Epoch: 054 Batch: 00024/00094 | Loss: 456.5328 | CE: 0.2046 | KD: 1060.0767\n",
      "Train Epoch: 054 Batch: 00025/00094 | Loss: 456.5645 | CE: 0.1848 | KD: 1060.1963\n",
      "Train Epoch: 054 Batch: 00026/00094 | Loss: 456.4806 | CE: 0.1792 | KD: 1060.0142\n",
      "Train Epoch: 054 Batch: 00027/00094 | Loss: 456.6391 | CE: 0.2627 | KD: 1060.1884\n",
      "Train Epoch: 054 Batch: 00028/00094 | Loss: 456.4932 | CE: 0.1838 | KD: 1060.0331\n",
      "Train Epoch: 054 Batch: 00029/00094 | Loss: 456.5406 | CE: 0.2025 | KD: 1060.0995\n",
      "Train Epoch: 054 Batch: 00030/00094 | Loss: 456.4714 | CE: 0.1579 | KD: 1060.0424\n",
      "Train Epoch: 054 Batch: 00031/00094 | Loss: 456.5651 | CE: 0.2497 | KD: 1060.0468\n",
      "Train Epoch: 054 Batch: 00032/00094 | Loss: 456.6006 | CE: 0.2393 | KD: 1060.1534\n",
      "Train Epoch: 054 Batch: 00033/00094 | Loss: 456.5128 | CE: 0.2102 | KD: 1060.0170\n",
      "Train Epoch: 054 Batch: 00034/00094 | Loss: 456.5122 | CE: 0.1805 | KD: 1060.0848\n",
      "Train Epoch: 054 Batch: 00035/00094 | Loss: 456.5700 | CE: 0.2180 | KD: 1060.1318\n",
      "Train Epoch: 054 Batch: 00036/00094 | Loss: 456.4941 | CE: 0.2076 | KD: 1059.9795\n",
      "Train Epoch: 054 Batch: 00037/00094 | Loss: 456.5487 | CE: 0.1905 | KD: 1060.1464\n",
      "Train Epoch: 054 Batch: 00038/00094 | Loss: 456.5653 | CE: 0.2452 | KD: 1060.0576\n",
      "Train Epoch: 054 Batch: 00039/00094 | Loss: 456.5760 | CE: 0.2298 | KD: 1060.1183\n",
      "Train Epoch: 054 Batch: 00040/00094 | Loss: 456.4885 | CE: 0.1999 | KD: 1059.9845\n",
      "Train Epoch: 054 Batch: 00041/00094 | Loss: 456.5927 | CE: 0.2702 | KD: 1060.0631\n",
      "Train Epoch: 054 Batch: 00042/00094 | Loss: 456.4892 | CE: 0.2189 | KD: 1059.9420\n",
      "Train Epoch: 054 Batch: 00043/00094 | Loss: 456.5395 | CE: 0.2386 | KD: 1060.0132\n",
      "Train Epoch: 054 Batch: 00044/00094 | Loss: 456.5775 | CE: 0.2546 | KD: 1060.0643\n",
      "Train Epoch: 054 Batch: 00045/00094 | Loss: 456.5357 | CE: 0.2199 | KD: 1060.0477\n",
      "Train Epoch: 054 Batch: 00046/00094 | Loss: 456.5570 | CE: 0.2146 | KD: 1060.1096\n",
      "Train Epoch: 054 Batch: 00047/00094 | Loss: 456.4600 | CE: 0.1656 | KD: 1059.9980\n",
      "Train Epoch: 054 Batch: 00048/00094 | Loss: 456.5907 | CE: 0.2598 | KD: 1060.0828\n",
      "Train Epoch: 054 Batch: 00049/00094 | Loss: 456.5754 | CE: 0.2368 | KD: 1060.1007\n",
      "Train Epoch: 054 Batch: 00050/00094 | Loss: 456.4875 | CE: 0.1978 | KD: 1059.9871\n",
      "Train Epoch: 054 Batch: 00051/00094 | Loss: 456.4812 | CE: 0.1676 | KD: 1060.0427\n",
      "Train Epoch: 054 Batch: 00052/00094 | Loss: 456.4667 | CE: 0.1811 | KD: 1059.9777\n",
      "Train Epoch: 054 Batch: 00053/00094 | Loss: 456.6034 | CE: 0.2725 | KD: 1060.0828\n",
      "Train Epoch: 054 Batch: 00054/00094 | Loss: 456.5337 | CE: 0.2251 | KD: 1060.0308\n",
      "Train Epoch: 054 Batch: 00055/00094 | Loss: 456.6479 | CE: 0.3391 | KD: 1060.0314\n",
      "Train Epoch: 054 Batch: 00056/00094 | Loss: 456.5998 | CE: 0.2723 | KD: 1060.0750\n",
      "Train Epoch: 054 Batch: 00057/00094 | Loss: 456.5678 | CE: 0.2406 | KD: 1060.0743\n",
      "Train Epoch: 054 Batch: 00058/00094 | Loss: 456.5558 | CE: 0.2152 | KD: 1060.1053\n",
      "Train Epoch: 054 Batch: 00059/00094 | Loss: 456.6203 | CE: 0.2705 | KD: 1060.1267\n",
      "Train Epoch: 054 Batch: 00060/00094 | Loss: 456.5890 | CE: 0.2314 | KD: 1060.1448\n",
      "Train Epoch: 054 Batch: 00061/00094 | Loss: 456.5460 | CE: 0.2234 | KD: 1060.0635\n",
      "Train Epoch: 054 Batch: 00062/00094 | Loss: 456.5685 | CE: 0.2317 | KD: 1060.0966\n",
      "Train Epoch: 054 Batch: 00063/00094 | Loss: 456.6008 | CE: 0.2551 | KD: 1060.1171\n",
      "Train Epoch: 054 Batch: 00064/00094 | Loss: 456.6873 | CE: 0.3111 | KD: 1060.1882\n",
      "Train Epoch: 054 Batch: 00065/00094 | Loss: 456.5194 | CE: 0.1818 | KD: 1060.0985\n",
      "Train Epoch: 054 Batch: 00066/00094 | Loss: 456.5321 | CE: 0.2280 | KD: 1060.0205\n",
      "Train Epoch: 054 Batch: 00067/00094 | Loss: 456.5121 | CE: 0.1702 | KD: 1060.1084\n",
      "Train Epoch: 054 Batch: 00068/00094 | Loss: 456.4767 | CE: 0.1725 | KD: 1060.0206\n",
      "Train Epoch: 054 Batch: 00069/00094 | Loss: 456.5495 | CE: 0.2429 | KD: 1060.0265\n",
      "Train Epoch: 054 Batch: 00070/00094 | Loss: 456.4206 | CE: 0.1378 | KD: 1059.9711\n",
      "Train Epoch: 054 Batch: 00071/00094 | Loss: 456.5274 | CE: 0.1974 | KD: 1060.0807\n",
      "Train Epoch: 054 Batch: 00072/00094 | Loss: 456.5151 | CE: 0.1762 | KD: 1060.1014\n",
      "Train Epoch: 054 Batch: 00073/00094 | Loss: 456.4828 | CE: 0.1748 | KD: 1060.0298\n",
      "Train Epoch: 054 Batch: 00074/00094 | Loss: 456.5543 | CE: 0.2149 | KD: 1060.1025\n",
      "Train Epoch: 054 Batch: 00075/00094 | Loss: 456.4893 | CE: 0.1736 | KD: 1060.0474\n",
      "Train Epoch: 054 Batch: 00076/00094 | Loss: 456.5170 | CE: 0.1785 | KD: 1060.1003\n",
      "Train Epoch: 054 Batch: 00077/00094 | Loss: 456.5006 | CE: 0.2076 | KD: 1059.9949\n",
      "Train Epoch: 054 Batch: 00078/00094 | Loss: 456.6212 | CE: 0.3255 | KD: 1060.0011\n",
      "Train Epoch: 054 Batch: 00079/00094 | Loss: 456.5605 | CE: 0.2436 | KD: 1060.0502\n",
      "Train Epoch: 054 Batch: 00080/00094 | Loss: 456.4951 | CE: 0.1957 | KD: 1060.0095\n",
      "Train Epoch: 054 Batch: 00081/00094 | Loss: 456.5260 | CE: 0.2384 | KD: 1059.9821\n",
      "Train Epoch: 054 Batch: 00082/00094 | Loss: 456.6465 | CE: 0.2882 | KD: 1060.1467\n",
      "Train Epoch: 054 Batch: 00083/00094 | Loss: 456.5588 | CE: 0.2467 | KD: 1060.0391\n",
      "Train Epoch: 054 Batch: 00084/00094 | Loss: 456.4964 | CE: 0.1719 | KD: 1060.0680\n",
      "Train Epoch: 054 Batch: 00085/00094 | Loss: 456.5546 | CE: 0.2146 | KD: 1060.1040\n",
      "Train Epoch: 054 Batch: 00086/00094 | Loss: 456.5679 | CE: 0.2800 | KD: 1059.9828\n",
      "Train Epoch: 054 Batch: 00087/00094 | Loss: 456.4738 | CE: 0.1746 | KD: 1060.0093\n",
      "Train Epoch: 054 Batch: 00088/00094 | Loss: 456.5185 | CE: 0.2054 | KD: 1060.0413\n",
      "Train Epoch: 054 Batch: 00089/00094 | Loss: 456.5307 | CE: 0.1964 | KD: 1060.0908\n",
      "Train Epoch: 054 Batch: 00090/00094 | Loss: 456.5466 | CE: 0.1889 | KD: 1060.1451\n",
      "Train Epoch: 054 Batch: 00091/00094 | Loss: 456.6043 | CE: 0.2729 | KD: 1060.0840\n",
      "Train Epoch: 054 Batch: 00092/00094 | Loss: 456.5597 | CE: 0.2220 | KD: 1060.0985\n",
      "Train Epoch: 054 Batch: 00093/00094 | Loss: 456.7355 | CE: 0.4076 | KD: 1060.0757\n",
      "Train Epoch: 054 Batch: 00094/00094 | Loss: 456.5019 | CE: 0.1732 | KD: 1060.0776\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2420 | acc:91.6500\n",
      "[VAL Acc] Target: 91.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1582 | acc:51.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0423 | acc:47.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0822 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6894 | acc:62.8527\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0395 | acc:53.3272\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5038 | acc:75.8229\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0803 | acc:54.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4832 | acc:74.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.55%\n",
      "[VAL Acc] Avg 62.31%\n",
      "Train Epoch: 055 Batch: 00001/00094 | Loss: 410.8241 | CE: 0.1655 | KD: 1059.9817\n",
      "Train Epoch: 055 Batch: 00002/00094 | Loss: 410.8540 | CE: 0.1720 | KD: 1060.0422\n",
      "Train Epoch: 055 Batch: 00003/00094 | Loss: 410.8753 | CE: 0.1989 | KD: 1060.0275\n",
      "Train Epoch: 055 Batch: 00004/00094 | Loss: 410.9296 | CE: 0.2509 | KD: 1060.0337\n",
      "Train Epoch: 055 Batch: 00005/00094 | Loss: 410.9087 | CE: 0.2053 | KD: 1060.0970\n",
      "Train Epoch: 055 Batch: 00006/00094 | Loss: 410.9855 | CE: 0.2916 | KD: 1060.0726\n",
      "Train Epoch: 055 Batch: 00007/00094 | Loss: 410.9728 | CE: 0.2351 | KD: 1060.1859\n",
      "Train Epoch: 055 Batch: 00008/00094 | Loss: 410.8725 | CE: 0.2104 | KD: 1059.9907\n",
      "Train Epoch: 055 Batch: 00009/00094 | Loss: 410.9328 | CE: 0.2481 | KD: 1060.0490\n",
      "Train Epoch: 055 Batch: 00010/00094 | Loss: 410.9126 | CE: 0.2381 | KD: 1060.0227\n",
      "Train Epoch: 055 Batch: 00011/00094 | Loss: 410.8728 | CE: 0.1848 | KD: 1060.0575\n",
      "Train Epoch: 055 Batch: 00012/00094 | Loss: 410.9286 | CE: 0.2274 | KD: 1060.0914\n",
      "Train Epoch: 055 Batch: 00013/00094 | Loss: 410.8864 | CE: 0.2021 | KD: 1060.0480\n",
      "Train Epoch: 055 Batch: 00014/00094 | Loss: 410.9173 | CE: 0.2065 | KD: 1060.1163\n",
      "Train Epoch: 055 Batch: 00015/00094 | Loss: 410.8496 | CE: 0.1792 | KD: 1060.0122\n",
      "Train Epoch: 055 Batch: 00016/00094 | Loss: 410.9341 | CE: 0.2341 | KD: 1060.0883\n",
      "Train Epoch: 055 Batch: 00017/00094 | Loss: 410.8505 | CE: 0.1730 | KD: 1060.0304\n",
      "Train Epoch: 055 Batch: 00018/00094 | Loss: 410.8892 | CE: 0.1993 | KD: 1060.0625\n",
      "Train Epoch: 055 Batch: 00019/00094 | Loss: 410.8884 | CE: 0.2017 | KD: 1060.0541\n",
      "Train Epoch: 055 Batch: 00020/00094 | Loss: 410.8746 | CE: 0.1919 | KD: 1060.0437\n",
      "Train Epoch: 055 Batch: 00021/00094 | Loss: 410.8809 | CE: 0.1900 | KD: 1060.0652\n",
      "Train Epoch: 055 Batch: 00022/00094 | Loss: 410.9101 | CE: 0.2371 | KD: 1060.0189\n",
      "Train Epoch: 055 Batch: 00023/00094 | Loss: 410.9169 | CE: 0.2039 | KD: 1060.1221\n",
      "Train Epoch: 055 Batch: 00024/00094 | Loss: 410.9809 | CE: 0.2671 | KD: 1060.1243\n",
      "Train Epoch: 055 Batch: 00025/00094 | Loss: 410.8834 | CE: 0.1992 | KD: 1060.0476\n",
      "Train Epoch: 055 Batch: 00026/00094 | Loss: 410.8029 | CE: 0.1540 | KD: 1059.9567\n",
      "Train Epoch: 055 Batch: 00027/00094 | Loss: 410.9538 | CE: 0.2476 | KD: 1060.1047\n",
      "Train Epoch: 055 Batch: 00028/00094 | Loss: 410.9870 | CE: 0.2646 | KD: 1060.1464\n",
      "Train Epoch: 055 Batch: 00029/00094 | Loss: 410.9521 | CE: 0.2306 | KD: 1060.1439\n",
      "Train Epoch: 055 Batch: 00030/00094 | Loss: 410.9826 | CE: 0.2887 | KD: 1060.0729\n",
      "Train Epoch: 055 Batch: 00031/00094 | Loss: 410.8405 | CE: 0.1879 | KD: 1059.9661\n",
      "Train Epoch: 055 Batch: 00032/00094 | Loss: 410.8900 | CE: 0.2127 | KD: 1060.0298\n",
      "Train Epoch: 055 Batch: 00033/00094 | Loss: 410.8799 | CE: 0.2036 | KD: 1060.0275\n",
      "Train Epoch: 055 Batch: 00034/00094 | Loss: 410.8423 | CE: 0.1831 | KD: 1059.9834\n",
      "Train Epoch: 055 Batch: 00035/00094 | Loss: 411.0185 | CE: 0.2557 | KD: 1060.2506\n",
      "Train Epoch: 055 Batch: 00036/00094 | Loss: 410.9108 | CE: 0.2285 | KD: 1060.0430\n",
      "Train Epoch: 055 Batch: 00037/00094 | Loss: 410.8720 | CE: 0.2059 | KD: 1060.0011\n",
      "Train Epoch: 055 Batch: 00038/00094 | Loss: 410.9235 | CE: 0.1960 | KD: 1060.1595\n",
      "Train Epoch: 055 Batch: 00039/00094 | Loss: 410.8443 | CE: 0.1583 | KD: 1060.0524\n",
      "Train Epoch: 055 Batch: 00040/00094 | Loss: 410.9152 | CE: 0.2196 | KD: 1060.0771\n",
      "Train Epoch: 055 Batch: 00041/00094 | Loss: 410.9601 | CE: 0.2530 | KD: 1060.1068\n",
      "Train Epoch: 055 Batch: 00042/00094 | Loss: 410.8410 | CE: 0.1796 | KD: 1059.9890\n",
      "Train Epoch: 055 Batch: 00043/00094 | Loss: 410.9186 | CE: 0.2327 | KD: 1060.0521\n",
      "Train Epoch: 055 Batch: 00044/00094 | Loss: 410.9481 | CE: 0.2614 | KD: 1060.0541\n",
      "Train Epoch: 055 Batch: 00045/00094 | Loss: 410.8842 | CE: 0.1998 | KD: 1060.0483\n",
      "Train Epoch: 055 Batch: 00046/00094 | Loss: 410.9522 | CE: 0.2603 | KD: 1060.0676\n",
      "Train Epoch: 055 Batch: 00047/00094 | Loss: 410.9324 | CE: 0.2303 | KD: 1060.0939\n",
      "Train Epoch: 055 Batch: 00048/00094 | Loss: 410.8768 | CE: 0.2060 | KD: 1060.0132\n",
      "Train Epoch: 055 Batch: 00049/00094 | Loss: 410.9820 | CE: 0.2124 | KD: 1060.2683\n",
      "Train Epoch: 055 Batch: 00050/00094 | Loss: 410.9225 | CE: 0.2246 | KD: 1060.0830\n",
      "Train Epoch: 055 Batch: 00051/00094 | Loss: 410.8620 | CE: 0.1848 | KD: 1060.0297\n",
      "Train Epoch: 055 Batch: 00052/00094 | Loss: 410.8971 | CE: 0.2232 | KD: 1060.0211\n",
      "Train Epoch: 055 Batch: 00053/00094 | Loss: 410.9683 | CE: 0.2746 | KD: 1060.0723\n",
      "Train Epoch: 055 Batch: 00054/00094 | Loss: 410.9122 | CE: 0.2518 | KD: 1059.9863\n",
      "Train Epoch: 055 Batch: 00055/00094 | Loss: 410.8813 | CE: 0.1802 | KD: 1060.0914\n",
      "Train Epoch: 055 Batch: 00056/00094 | Loss: 410.8793 | CE: 0.1875 | KD: 1060.0674\n",
      "Train Epoch: 055 Batch: 00057/00094 | Loss: 410.9131 | CE: 0.1792 | KD: 1060.1760\n",
      "Train Epoch: 055 Batch: 00058/00094 | Loss: 410.8774 | CE: 0.1906 | KD: 1060.0546\n",
      "Train Epoch: 055 Batch: 00059/00094 | Loss: 410.9364 | CE: 0.2487 | KD: 1060.0568\n",
      "Train Epoch: 055 Batch: 00060/00094 | Loss: 410.9117 | CE: 0.2288 | KD: 1060.0443\n",
      "Train Epoch: 055 Batch: 00061/00094 | Loss: 410.9070 | CE: 0.2094 | KD: 1060.0824\n",
      "Train Epoch: 055 Batch: 00062/00094 | Loss: 410.9075 | CE: 0.2351 | KD: 1060.0175\n",
      "Train Epoch: 055 Batch: 00063/00094 | Loss: 410.9039 | CE: 0.2019 | KD: 1060.0936\n",
      "Train Epoch: 055 Batch: 00064/00094 | Loss: 410.9186 | CE: 0.2262 | KD: 1060.0690\n",
      "Train Epoch: 055 Batch: 00065/00094 | Loss: 410.8794 | CE: 0.1943 | KD: 1060.0500\n",
      "Train Epoch: 055 Batch: 00066/00094 | Loss: 410.8633 | CE: 0.2079 | KD: 1059.9734\n",
      "Train Epoch: 055 Batch: 00067/00094 | Loss: 410.8367 | CE: 0.1693 | KD: 1060.0043\n",
      "Train Epoch: 055 Batch: 00068/00094 | Loss: 410.9235 | CE: 0.2221 | KD: 1060.0920\n",
      "Train Epoch: 055 Batch: 00069/00094 | Loss: 410.8521 | CE: 0.1416 | KD: 1060.1155\n",
      "Train Epoch: 055 Batch: 00070/00094 | Loss: 410.9295 | CE: 0.2143 | KD: 1060.1278\n",
      "Train Epoch: 055 Batch: 00071/00094 | Loss: 410.8740 | CE: 0.1681 | KD: 1060.1038\n",
      "Train Epoch: 055 Batch: 00072/00094 | Loss: 410.8500 | CE: 0.1658 | KD: 1060.0477\n",
      "Train Epoch: 055 Batch: 00073/00094 | Loss: 410.7862 | CE: 0.1383 | KD: 1059.9540\n",
      "Train Epoch: 055 Batch: 00074/00094 | Loss: 410.9361 | CE: 0.2117 | KD: 1060.1516\n",
      "Train Epoch: 055 Batch: 00075/00094 | Loss: 410.9511 | CE: 0.2363 | KD: 1060.1267\n",
      "Train Epoch: 055 Batch: 00076/00094 | Loss: 410.9126 | CE: 0.2023 | KD: 1060.1151\n",
      "Train Epoch: 055 Batch: 00077/00094 | Loss: 410.9644 | CE: 0.2507 | KD: 1060.1240\n",
      "Train Epoch: 055 Batch: 00078/00094 | Loss: 410.8547 | CE: 0.1857 | KD: 1060.0085\n",
      "Train Epoch: 055 Batch: 00079/00094 | Loss: 410.9221 | CE: 0.2211 | KD: 1060.0912\n",
      "Train Epoch: 055 Batch: 00080/00094 | Loss: 410.9242 | CE: 0.2420 | KD: 1060.0424\n",
      "Train Epoch: 055 Batch: 00081/00094 | Loss: 410.9257 | CE: 0.2216 | KD: 1060.0990\n",
      "Train Epoch: 055 Batch: 00082/00094 | Loss: 411.0254 | CE: 0.2927 | KD: 1060.1727\n",
      "Train Epoch: 055 Batch: 00083/00094 | Loss: 410.8220 | CE: 0.1260 | KD: 1060.0781\n",
      "Train Epoch: 055 Batch: 00084/00094 | Loss: 410.8540 | CE: 0.1808 | KD: 1060.0194\n",
      "Train Epoch: 055 Batch: 00085/00094 | Loss: 410.9134 | CE: 0.2000 | KD: 1060.1232\n",
      "Train Epoch: 055 Batch: 00086/00094 | Loss: 410.8630 | CE: 0.1520 | KD: 1060.1169\n",
      "Train Epoch: 055 Batch: 00087/00094 | Loss: 410.8872 | CE: 0.2139 | KD: 1060.0195\n",
      "Train Epoch: 055 Batch: 00088/00094 | Loss: 410.9608 | CE: 0.2618 | KD: 1060.0859\n",
      "Train Epoch: 055 Batch: 00089/00094 | Loss: 410.8919 | CE: 0.2118 | KD: 1060.0371\n",
      "Train Epoch: 055 Batch: 00090/00094 | Loss: 410.9275 | CE: 0.1847 | KD: 1060.1991\n",
      "Train Epoch: 055 Batch: 00091/00094 | Loss: 410.9356 | CE: 0.2465 | KD: 1060.0605\n",
      "Train Epoch: 055 Batch: 00092/00094 | Loss: 410.9561 | CE: 0.2540 | KD: 1060.0940\n",
      "Train Epoch: 055 Batch: 00093/00094 | Loss: 410.9315 | CE: 0.2451 | KD: 1060.0532\n",
      "Train Epoch: 055 Batch: 00094/00094 | Loss: 410.8600 | CE: 0.1813 | KD: 1060.0336\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2393 | acc:91.5000\n",
      "[VAL Acc] Target: 91.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2483 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1093 | acc:47.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1583 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6826 | acc:64.3417\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 64.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0210 | acc:53.8817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4841 | acc:76.7633\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0487 | acc:55.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5103 | acc:73.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.15%\n",
      "[VAL Acc] Avg 62.42%\n",
      "Train Epoch: 056 Batch: 00001/00094 | Loss: 410.8858 | CE: 0.1684 | KD: 1060.1335\n",
      "Train Epoch: 056 Batch: 00002/00094 | Loss: 410.9174 | CE: 0.2080 | KD: 1060.1128\n",
      "Train Epoch: 056 Batch: 00003/00094 | Loss: 410.8912 | CE: 0.1906 | KD: 1060.0902\n",
      "Train Epoch: 056 Batch: 00004/00094 | Loss: 410.8173 | CE: 0.1779 | KD: 1059.9320\n",
      "Train Epoch: 056 Batch: 00005/00094 | Loss: 410.9035 | CE: 0.1878 | KD: 1060.1292\n",
      "Train Epoch: 056 Batch: 00006/00094 | Loss: 410.8897 | CE: 0.1983 | KD: 1060.0663\n",
      "Train Epoch: 056 Batch: 00007/00094 | Loss: 410.8445 | CE: 0.1670 | KD: 1060.0304\n",
      "Train Epoch: 056 Batch: 00008/00094 | Loss: 410.9930 | CE: 0.2486 | KD: 1060.2031\n",
      "Train Epoch: 056 Batch: 00009/00094 | Loss: 410.9360 | CE: 0.2355 | KD: 1060.0897\n",
      "Train Epoch: 056 Batch: 00010/00094 | Loss: 410.8257 | CE: 0.1550 | KD: 1060.0128\n",
      "Train Epoch: 056 Batch: 00011/00094 | Loss: 410.8824 | CE: 0.2018 | KD: 1060.0382\n",
      "Train Epoch: 056 Batch: 00012/00094 | Loss: 410.9339 | CE: 0.2444 | KD: 1060.0616\n",
      "Train Epoch: 056 Batch: 00013/00094 | Loss: 410.9167 | CE: 0.2224 | KD: 1060.0737\n",
      "Train Epoch: 056 Batch: 00014/00094 | Loss: 410.9189 | CE: 0.2230 | KD: 1060.0779\n",
      "Train Epoch: 056 Batch: 00015/00094 | Loss: 410.8765 | CE: 0.1873 | KD: 1060.0605\n",
      "Train Epoch: 056 Batch: 00016/00094 | Loss: 410.9819 | CE: 0.2415 | KD: 1060.1929\n",
      "Train Epoch: 056 Batch: 00017/00094 | Loss: 410.9256 | CE: 0.2248 | KD: 1060.0905\n",
      "Train Epoch: 056 Batch: 00018/00094 | Loss: 410.9994 | CE: 0.3019 | KD: 1060.0820\n",
      "Train Epoch: 056 Batch: 00019/00094 | Loss: 410.8230 | CE: 0.1716 | KD: 1059.9629\n",
      "Train Epoch: 056 Batch: 00020/00094 | Loss: 410.8405 | CE: 0.1748 | KD: 1059.9999\n",
      "Train Epoch: 056 Batch: 00021/00094 | Loss: 411.0237 | CE: 0.2359 | KD: 1060.3152\n",
      "Train Epoch: 056 Batch: 00022/00094 | Loss: 410.9633 | CE: 0.2967 | KD: 1060.0023\n",
      "Train Epoch: 056 Batch: 00023/00094 | Loss: 410.9573 | CE: 0.2467 | KD: 1060.1160\n",
      "Train Epoch: 056 Batch: 00024/00094 | Loss: 410.9455 | CE: 0.2461 | KD: 1060.0869\n",
      "Train Epoch: 056 Batch: 00025/00094 | Loss: 410.8202 | CE: 0.1316 | KD: 1060.0591\n",
      "Train Epoch: 056 Batch: 00026/00094 | Loss: 410.8331 | CE: 0.1574 | KD: 1060.0258\n",
      "Train Epoch: 056 Batch: 00027/00094 | Loss: 410.8529 | CE: 0.1771 | KD: 1060.0262\n",
      "Train Epoch: 056 Batch: 00028/00094 | Loss: 410.9533 | CE: 0.2451 | KD: 1060.1096\n",
      "Train Epoch: 056 Batch: 00029/00094 | Loss: 410.8884 | CE: 0.1969 | KD: 1060.0664\n",
      "Train Epoch: 056 Batch: 00030/00094 | Loss: 410.9509 | CE: 0.2497 | KD: 1060.0914\n",
      "Train Epoch: 056 Batch: 00031/00094 | Loss: 410.9079 | CE: 0.2336 | KD: 1060.0221\n",
      "Train Epoch: 056 Batch: 00032/00094 | Loss: 410.9895 | CE: 0.2858 | KD: 1060.0980\n",
      "Train Epoch: 056 Batch: 00033/00094 | Loss: 410.8188 | CE: 0.1579 | KD: 1059.9877\n",
      "Train Epoch: 056 Batch: 00034/00094 | Loss: 410.8772 | CE: 0.2173 | KD: 1059.9850\n",
      "Train Epoch: 056 Batch: 00035/00094 | Loss: 410.8697 | CE: 0.1791 | KD: 1060.0643\n",
      "Train Epoch: 056 Batch: 00036/00094 | Loss: 410.8714 | CE: 0.1718 | KD: 1060.0873\n",
      "Train Epoch: 056 Batch: 00037/00094 | Loss: 410.8939 | CE: 0.2054 | KD: 1060.0587\n",
      "Train Epoch: 056 Batch: 00038/00094 | Loss: 410.9562 | CE: 0.2674 | KD: 1060.0597\n",
      "Train Epoch: 056 Batch: 00039/00094 | Loss: 410.8548 | CE: 0.1597 | KD: 1060.0757\n",
      "Train Epoch: 056 Batch: 00040/00094 | Loss: 410.8435 | CE: 0.1709 | KD: 1060.0178\n",
      "Train Epoch: 056 Batch: 00041/00094 | Loss: 410.9352 | CE: 0.2246 | KD: 1060.1160\n",
      "Train Epoch: 056 Batch: 00042/00094 | Loss: 410.8161 | CE: 0.1602 | KD: 1059.9746\n",
      "Train Epoch: 056 Batch: 00043/00094 | Loss: 410.9100 | CE: 0.2238 | KD: 1060.0529\n",
      "Train Epoch: 056 Batch: 00044/00094 | Loss: 410.9268 | CE: 0.2470 | KD: 1060.0366\n",
      "Train Epoch: 056 Batch: 00045/00094 | Loss: 411.0012 | CE: 0.2499 | KD: 1060.2207\n",
      "Train Epoch: 056 Batch: 00046/00094 | Loss: 410.8552 | CE: 0.1743 | KD: 1060.0393\n",
      "Train Epoch: 056 Batch: 00047/00094 | Loss: 410.9391 | CE: 0.2531 | KD: 1060.0524\n",
      "Train Epoch: 056 Batch: 00048/00094 | Loss: 410.8275 | CE: 0.1692 | KD: 1059.9808\n",
      "Train Epoch: 056 Batch: 00049/00094 | Loss: 411.0044 | CE: 0.2827 | KD: 1060.1447\n",
      "Train Epoch: 056 Batch: 00050/00094 | Loss: 410.9249 | CE: 0.2001 | KD: 1060.1526\n",
      "Train Epoch: 056 Batch: 00051/00094 | Loss: 410.9063 | CE: 0.2150 | KD: 1060.0659\n",
      "Train Epoch: 056 Batch: 00052/00094 | Loss: 410.9253 | CE: 0.2026 | KD: 1060.1470\n",
      "Train Epoch: 056 Batch: 00053/00094 | Loss: 410.9856 | CE: 0.2523 | KD: 1060.1744\n",
      "Train Epoch: 056 Batch: 00054/00094 | Loss: 410.8935 | CE: 0.2066 | KD: 1060.0547\n",
      "Train Epoch: 056 Batch: 00055/00094 | Loss: 410.9250 | CE: 0.2310 | KD: 1060.0729\n",
      "Train Epoch: 056 Batch: 00056/00094 | Loss: 410.8629 | CE: 0.1882 | KD: 1060.0232\n",
      "Train Epoch: 056 Batch: 00057/00094 | Loss: 410.9676 | CE: 0.2749 | KD: 1060.0696\n",
      "Train Epoch: 056 Batch: 00058/00094 | Loss: 410.8682 | CE: 0.1901 | KD: 1060.0320\n",
      "Train Epoch: 056 Batch: 00059/00094 | Loss: 410.9013 | CE: 0.2216 | KD: 1060.0361\n",
      "Train Epoch: 056 Batch: 00060/00094 | Loss: 410.8738 | CE: 0.1981 | KD: 1060.0259\n",
      "Train Epoch: 056 Batch: 00061/00094 | Loss: 410.8786 | CE: 0.1730 | KD: 1060.1031\n",
      "Train Epoch: 056 Batch: 00062/00094 | Loss: 410.8871 | CE: 0.1955 | KD: 1060.0668\n",
      "Train Epoch: 056 Batch: 00063/00094 | Loss: 410.9112 | CE: 0.2097 | KD: 1060.0924\n",
      "Train Epoch: 056 Batch: 00064/00094 | Loss: 410.9614 | CE: 0.2471 | KD: 1060.1255\n",
      "Train Epoch: 056 Batch: 00065/00094 | Loss: 410.9314 | CE: 0.2257 | KD: 1060.1030\n",
      "Train Epoch: 056 Batch: 00066/00094 | Loss: 411.0141 | CE: 0.3204 | KD: 1060.0723\n",
      "Train Epoch: 056 Batch: 00067/00094 | Loss: 410.8920 | CE: 0.1945 | KD: 1060.0823\n",
      "Train Epoch: 056 Batch: 00068/00094 | Loss: 410.9543 | CE: 0.2511 | KD: 1060.0969\n",
      "Train Epoch: 056 Batch: 00069/00094 | Loss: 410.8713 | CE: 0.1942 | KD: 1060.0293\n",
      "Train Epoch: 056 Batch: 00070/00094 | Loss: 410.8826 | CE: 0.1913 | KD: 1060.0659\n",
      "Train Epoch: 056 Batch: 00071/00094 | Loss: 410.8870 | CE: 0.1676 | KD: 1060.1387\n",
      "Train Epoch: 056 Batch: 00072/00094 | Loss: 410.9910 | CE: 0.3065 | KD: 1060.0487\n",
      "Train Epoch: 056 Batch: 00073/00094 | Loss: 410.8806 | CE: 0.2053 | KD: 1060.0248\n",
      "Train Epoch: 056 Batch: 00074/00094 | Loss: 410.8500 | CE: 0.1620 | KD: 1060.0575\n",
      "Train Epoch: 056 Batch: 00075/00094 | Loss: 410.9299 | CE: 0.2423 | KD: 1060.0566\n",
      "Train Epoch: 056 Batch: 00076/00094 | Loss: 410.9680 | CE: 0.2581 | KD: 1060.1141\n",
      "Train Epoch: 056 Batch: 00077/00094 | Loss: 410.8439 | CE: 0.1661 | KD: 1060.0312\n",
      "Train Epoch: 056 Batch: 00078/00094 | Loss: 410.9844 | CE: 0.2746 | KD: 1060.1139\n",
      "Train Epoch: 056 Batch: 00079/00094 | Loss: 411.0515 | CE: 0.3312 | KD: 1060.1409\n",
      "Train Epoch: 056 Batch: 00080/00094 | Loss: 411.0255 | CE: 0.2923 | KD: 1060.1741\n",
      "Train Epoch: 056 Batch: 00081/00094 | Loss: 410.8814 | CE: 0.2170 | KD: 1059.9967\n",
      "Train Epoch: 056 Batch: 00082/00094 | Loss: 410.9222 | CE: 0.2133 | KD: 1060.1113\n",
      "Train Epoch: 056 Batch: 00083/00094 | Loss: 410.9149 | CE: 0.2431 | KD: 1060.0157\n",
      "Train Epoch: 056 Batch: 00084/00094 | Loss: 410.8944 | CE: 0.2226 | KD: 1060.0159\n",
      "Train Epoch: 056 Batch: 00085/00094 | Loss: 410.9485 | CE: 0.2326 | KD: 1060.1295\n",
      "Train Epoch: 056 Batch: 00086/00094 | Loss: 410.8650 | CE: 0.1728 | KD: 1060.0684\n",
      "Train Epoch: 056 Batch: 00087/00094 | Loss: 411.0344 | CE: 0.2958 | KD: 1060.1881\n",
      "Train Epoch: 056 Batch: 00088/00094 | Loss: 410.9463 | CE: 0.2453 | KD: 1060.0912\n",
      "Train Epoch: 056 Batch: 00089/00094 | Loss: 410.8836 | CE: 0.2119 | KD: 1060.0156\n",
      "Train Epoch: 056 Batch: 00090/00094 | Loss: 410.8368 | CE: 0.1792 | KD: 1059.9791\n",
      "Train Epoch: 056 Batch: 00091/00094 | Loss: 410.9279 | CE: 0.1971 | KD: 1060.1678\n",
      "Train Epoch: 056 Batch: 00092/00094 | Loss: 410.9171 | CE: 0.1849 | KD: 1060.1715\n",
      "Train Epoch: 056 Batch: 00093/00094 | Loss: 410.9270 | CE: 0.1728 | KD: 1060.2285\n",
      "Train Epoch: 056 Batch: 00094/00094 | Loss: 410.8737 | CE: 0.1850 | KD: 1060.0596\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2255 | acc:92.1000\n",
      "[VAL Acc] Target: 92.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2872 | acc:49.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1271 | acc:47.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1864 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7204 | acc:62.1473\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.1021 | acc:52.3105\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5061 | acc:74.9216\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0850 | acc:54.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5127 | acc:74.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.05%\n",
      "[VAL Acc] Avg 61.89%\n",
      "Train Epoch: 057 Batch: 00001/00094 | Loss: 410.9214 | CE: 0.1966 | KD: 1060.1525\n",
      "Train Epoch: 057 Batch: 00002/00094 | Loss: 410.9234 | CE: 0.2154 | KD: 1060.1091\n",
      "Train Epoch: 057 Batch: 00003/00094 | Loss: 411.0967 | CE: 0.3700 | KD: 1060.1575\n",
      "Train Epoch: 057 Batch: 00004/00094 | Loss: 410.9770 | CE: 0.2692 | KD: 1060.1084\n",
      "Train Epoch: 057 Batch: 00005/00094 | Loss: 410.9138 | CE: 0.2240 | KD: 1060.0623\n",
      "Train Epoch: 057 Batch: 00006/00094 | Loss: 410.8694 | CE: 0.1843 | KD: 1060.0500\n",
      "Train Epoch: 057 Batch: 00007/00094 | Loss: 411.0193 | CE: 0.2653 | KD: 1060.2280\n",
      "Train Epoch: 057 Batch: 00008/00094 | Loss: 410.8486 | CE: 0.1561 | KD: 1060.0691\n",
      "Train Epoch: 057 Batch: 00009/00094 | Loss: 410.8535 | CE: 0.1740 | KD: 1060.0355\n",
      "Train Epoch: 057 Batch: 00010/00094 | Loss: 410.8643 | CE: 0.1681 | KD: 1060.0787\n",
      "Train Epoch: 057 Batch: 00011/00094 | Loss: 410.9329 | CE: 0.2731 | KD: 1059.9849\n",
      "Train Epoch: 057 Batch: 00012/00094 | Loss: 410.8903 | CE: 0.2133 | KD: 1060.0293\n",
      "Train Epoch: 057 Batch: 00013/00094 | Loss: 410.8936 | CE: 0.1917 | KD: 1060.0934\n",
      "Train Epoch: 057 Batch: 00014/00094 | Loss: 410.9148 | CE: 0.2012 | KD: 1060.1238\n",
      "Train Epoch: 057 Batch: 00015/00094 | Loss: 410.8217 | CE: 0.1681 | KD: 1059.9688\n",
      "Train Epoch: 057 Batch: 00016/00094 | Loss: 410.8516 | CE: 0.1652 | KD: 1060.0535\n",
      "Train Epoch: 057 Batch: 00017/00094 | Loss: 410.9455 | CE: 0.2501 | KD: 1060.0767\n",
      "Train Epoch: 057 Batch: 00018/00094 | Loss: 410.9291 | CE: 0.2274 | KD: 1060.0929\n",
      "Train Epoch: 057 Batch: 00019/00094 | Loss: 410.8670 | CE: 0.1683 | KD: 1060.0851\n",
      "Train Epoch: 057 Batch: 00020/00094 | Loss: 410.9131 | CE: 0.2271 | KD: 1060.0522\n",
      "Train Epoch: 057 Batch: 00021/00094 | Loss: 410.9926 | CE: 0.2933 | KD: 1060.0867\n",
      "Train Epoch: 057 Batch: 00022/00094 | Loss: 410.8706 | CE: 0.1901 | KD: 1060.0382\n",
      "Train Epoch: 057 Batch: 00023/00094 | Loss: 410.8475 | CE: 0.1750 | KD: 1060.0177\n",
      "Train Epoch: 057 Batch: 00024/00094 | Loss: 410.8752 | CE: 0.1818 | KD: 1060.0714\n",
      "Train Epoch: 057 Batch: 00025/00094 | Loss: 410.8596 | CE: 0.1663 | KD: 1060.0712\n",
      "Train Epoch: 057 Batch: 00026/00094 | Loss: 410.8789 | CE: 0.2346 | KD: 1059.9447\n",
      "Train Epoch: 057 Batch: 00027/00094 | Loss: 410.9053 | CE: 0.2021 | KD: 1060.0967\n",
      "Train Epoch: 057 Batch: 00028/00094 | Loss: 410.9946 | CE: 0.2553 | KD: 1060.1899\n",
      "Train Epoch: 057 Batch: 00029/00094 | Loss: 410.7999 | CE: 0.1564 | KD: 1059.9425\n",
      "Train Epoch: 057 Batch: 00030/00094 | Loss: 410.8618 | CE: 0.1816 | KD: 1060.0375\n",
      "Train Epoch: 057 Batch: 00031/00094 | Loss: 410.9451 | CE: 0.2346 | KD: 1060.1156\n",
      "Train Epoch: 057 Batch: 00032/00094 | Loss: 410.8874 | CE: 0.1927 | KD: 1060.0748\n",
      "Train Epoch: 057 Batch: 00033/00094 | Loss: 410.8803 | CE: 0.1986 | KD: 1060.0414\n",
      "Train Epoch: 057 Batch: 00034/00094 | Loss: 410.8806 | CE: 0.2036 | KD: 1060.0291\n",
      "Train Epoch: 057 Batch: 00035/00094 | Loss: 410.9069 | CE: 0.2221 | KD: 1060.0493\n",
      "Train Epoch: 057 Batch: 00036/00094 | Loss: 410.9237 | CE: 0.2524 | KD: 1060.0144\n",
      "Train Epoch: 057 Batch: 00037/00094 | Loss: 410.8282 | CE: 0.1788 | KD: 1059.9579\n",
      "Train Epoch: 057 Batch: 00038/00094 | Loss: 410.8774 | CE: 0.1820 | KD: 1060.0767\n",
      "Train Epoch: 057 Batch: 00039/00094 | Loss: 410.9302 | CE: 0.2439 | KD: 1060.0531\n",
      "Train Epoch: 057 Batch: 00040/00094 | Loss: 410.9078 | CE: 0.2261 | KD: 1060.0413\n",
      "Train Epoch: 057 Batch: 00041/00094 | Loss: 410.9023 | CE: 0.2070 | KD: 1060.0765\n",
      "Train Epoch: 057 Batch: 00042/00094 | Loss: 410.9198 | CE: 0.2122 | KD: 1060.1080\n",
      "Train Epoch: 057 Batch: 00043/00094 | Loss: 410.8701 | CE: 0.1652 | KD: 1060.1013\n",
      "Train Epoch: 057 Batch: 00044/00094 | Loss: 410.8513 | CE: 0.1815 | KD: 1060.0104\n",
      "Train Epoch: 057 Batch: 00045/00094 | Loss: 410.9328 | CE: 0.2522 | KD: 1060.0385\n",
      "Train Epoch: 057 Batch: 00046/00094 | Loss: 410.9024 | CE: 0.1911 | KD: 1060.1176\n",
      "Train Epoch: 057 Batch: 00047/00094 | Loss: 410.8387 | CE: 0.1409 | KD: 1060.0829\n",
      "Train Epoch: 057 Batch: 00048/00094 | Loss: 410.9305 | CE: 0.2483 | KD: 1060.0426\n",
      "Train Epoch: 057 Batch: 00049/00094 | Loss: 410.9598 | CE: 0.2246 | KD: 1060.1794\n",
      "Train Epoch: 057 Batch: 00050/00094 | Loss: 410.9547 | CE: 0.2634 | KD: 1060.0659\n",
      "Train Epoch: 057 Batch: 00051/00094 | Loss: 410.9148 | CE: 0.2143 | KD: 1060.0898\n",
      "Train Epoch: 057 Batch: 00052/00094 | Loss: 410.8786 | CE: 0.1779 | KD: 1060.0905\n",
      "Train Epoch: 057 Batch: 00053/00094 | Loss: 410.8046 | CE: 0.1609 | KD: 1059.9432\n",
      "Train Epoch: 057 Batch: 00054/00094 | Loss: 410.8809 | CE: 0.1734 | KD: 1060.1079\n",
      "Train Epoch: 057 Batch: 00055/00094 | Loss: 410.9539 | CE: 0.2274 | KD: 1060.1569\n",
      "Train Epoch: 057 Batch: 00056/00094 | Loss: 410.9720 | CE: 0.2409 | KD: 1060.1686\n",
      "Train Epoch: 057 Batch: 00057/00094 | Loss: 410.9280 | CE: 0.2119 | KD: 1060.1301\n",
      "Train Epoch: 057 Batch: 00058/00094 | Loss: 411.0833 | CE: 0.3150 | KD: 1060.2649\n",
      "Train Epoch: 057 Batch: 00059/00094 | Loss: 410.8784 | CE: 0.1639 | KD: 1060.1261\n",
      "Train Epoch: 057 Batch: 00060/00094 | Loss: 410.9600 | CE: 0.2455 | KD: 1060.1260\n",
      "Train Epoch: 057 Batch: 00061/00094 | Loss: 410.9082 | CE: 0.2216 | KD: 1060.0537\n",
      "Train Epoch: 057 Batch: 00062/00094 | Loss: 410.9543 | CE: 0.2462 | KD: 1060.1093\n",
      "Train Epoch: 057 Batch: 00063/00094 | Loss: 410.9951 | CE: 0.2950 | KD: 1060.0889\n",
      "Train Epoch: 057 Batch: 00064/00094 | Loss: 410.9746 | CE: 0.2551 | KD: 1060.1388\n",
      "Train Epoch: 057 Batch: 00065/00094 | Loss: 410.8459 | CE: 0.1800 | KD: 1060.0006\n",
      "Train Epoch: 057 Batch: 00066/00094 | Loss: 410.8700 | CE: 0.1938 | KD: 1060.0269\n",
      "Train Epoch: 057 Batch: 00067/00094 | Loss: 410.8827 | CE: 0.1734 | KD: 1060.1125\n",
      "Train Epoch: 057 Batch: 00068/00094 | Loss: 410.8735 | CE: 0.1975 | KD: 1060.0266\n",
      "Train Epoch: 057 Batch: 00069/00094 | Loss: 410.8405 | CE: 0.1595 | KD: 1060.0397\n",
      "Train Epoch: 057 Batch: 00070/00094 | Loss: 410.9151 | CE: 0.1977 | KD: 1060.1334\n",
      "Train Epoch: 057 Batch: 00071/00094 | Loss: 410.9571 | CE: 0.2447 | KD: 1060.1207\n",
      "Train Epoch: 057 Batch: 00072/00094 | Loss: 410.9266 | CE: 0.2178 | KD: 1060.1112\n",
      "Train Epoch: 057 Batch: 00073/00094 | Loss: 410.8901 | CE: 0.1568 | KD: 1060.1746\n",
      "Train Epoch: 057 Batch: 00074/00094 | Loss: 410.8448 | CE: 0.1844 | KD: 1059.9865\n",
      "Train Epoch: 057 Batch: 00075/00094 | Loss: 410.9321 | CE: 0.2113 | KD: 1060.1421\n",
      "Train Epoch: 057 Batch: 00076/00094 | Loss: 410.9500 | CE: 0.2016 | KD: 1060.2134\n",
      "Train Epoch: 057 Batch: 00077/00094 | Loss: 410.9568 | CE: 0.2639 | KD: 1060.0702\n",
      "Train Epoch: 057 Batch: 00078/00094 | Loss: 410.9333 | CE: 0.2161 | KD: 1060.1328\n",
      "Train Epoch: 057 Batch: 00079/00094 | Loss: 410.9430 | CE: 0.2403 | KD: 1060.0953\n",
      "Train Epoch: 057 Batch: 00080/00094 | Loss: 410.8146 | CE: 0.1551 | KD: 1059.9840\n",
      "Train Epoch: 057 Batch: 00081/00094 | Loss: 410.8372 | CE: 0.1767 | KD: 1059.9865\n",
      "Train Epoch: 057 Batch: 00082/00094 | Loss: 410.9618 | CE: 0.2686 | KD: 1060.0709\n",
      "Train Epoch: 057 Batch: 00083/00094 | Loss: 410.9149 | CE: 0.2076 | KD: 1060.1074\n",
      "Train Epoch: 057 Batch: 00084/00094 | Loss: 410.8774 | CE: 0.2138 | KD: 1059.9948\n",
      "Train Epoch: 057 Batch: 00085/00094 | Loss: 410.8998 | CE: 0.1827 | KD: 1060.1328\n",
      "Train Epoch: 057 Batch: 00086/00094 | Loss: 410.8633 | CE: 0.1989 | KD: 1059.9968\n",
      "Train Epoch: 057 Batch: 00087/00094 | Loss: 410.9318 | CE: 0.2182 | KD: 1060.1235\n",
      "Train Epoch: 057 Batch: 00088/00094 | Loss: 410.9023 | CE: 0.2002 | KD: 1060.0938\n",
      "Train Epoch: 057 Batch: 00089/00094 | Loss: 410.8464 | CE: 0.1703 | KD: 1060.0269\n",
      "Train Epoch: 057 Batch: 00090/00094 | Loss: 410.8517 | CE: 0.1784 | KD: 1060.0198\n",
      "Train Epoch: 057 Batch: 00091/00094 | Loss: 410.8306 | CE: 0.1771 | KD: 1059.9686\n",
      "Train Epoch: 057 Batch: 00092/00094 | Loss: 410.9340 | CE: 0.2370 | KD: 1060.0808\n",
      "Train Epoch: 057 Batch: 00093/00094 | Loss: 410.9185 | CE: 0.2198 | KD: 1060.0852\n",
      "Train Epoch: 057 Batch: 00094/00094 | Loss: 410.8655 | CE: 0.1396 | KD: 1060.1555\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2276 | acc:93.1000\n",
      "[VAL Acc] Target: 93.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2406 | acc:50.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0946 | acc:48.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1181 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7270 | acc:61.5987\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0893 | acc:52.7726\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.77%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5128 | acc:76.3323\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0752 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5093 | acc:73.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.95%\n",
      "[VAL Acc] Avg 62.18%\n",
      "Train Epoch: 058 Batch: 00001/00094 | Loss: 410.8640 | CE: 0.1782 | KD: 1060.0518\n",
      "Train Epoch: 058 Batch: 00002/00094 | Loss: 410.9491 | CE: 0.1904 | KD: 1060.2400\n",
      "Train Epoch: 058 Batch: 00003/00094 | Loss: 410.9323 | CE: 0.2528 | KD: 1060.0355\n",
      "Train Epoch: 058 Batch: 00004/00094 | Loss: 410.8722 | CE: 0.1992 | KD: 1060.0188\n",
      "Train Epoch: 058 Batch: 00005/00094 | Loss: 410.9663 | CE: 0.2603 | KD: 1060.1040\n",
      "Train Epoch: 058 Batch: 00006/00094 | Loss: 410.9131 | CE: 0.2273 | KD: 1060.0519\n",
      "Train Epoch: 058 Batch: 00007/00094 | Loss: 410.8365 | CE: 0.1986 | KD: 1059.9282\n",
      "Train Epoch: 058 Batch: 00008/00094 | Loss: 410.9012 | CE: 0.2059 | KD: 1060.0765\n",
      "Train Epoch: 058 Batch: 00009/00094 | Loss: 410.9081 | CE: 0.2376 | KD: 1060.0123\n",
      "Train Epoch: 058 Batch: 00010/00094 | Loss: 410.9757 | CE: 0.2729 | KD: 1060.0959\n",
      "Train Epoch: 058 Batch: 00011/00094 | Loss: 410.8923 | CE: 0.1853 | KD: 1060.1064\n",
      "Train Epoch: 058 Batch: 00012/00094 | Loss: 410.8495 | CE: 0.1508 | KD: 1060.0850\n",
      "Train Epoch: 058 Batch: 00013/00094 | Loss: 410.9575 | CE: 0.2541 | KD: 1060.0970\n",
      "Train Epoch: 058 Batch: 00014/00094 | Loss: 410.9228 | CE: 0.2126 | KD: 1060.1150\n",
      "Train Epoch: 058 Batch: 00015/00094 | Loss: 410.8535 | CE: 0.1725 | KD: 1060.0394\n",
      "Train Epoch: 058 Batch: 00016/00094 | Loss: 410.8487 | CE: 0.1694 | KD: 1060.0349\n",
      "Train Epoch: 058 Batch: 00017/00094 | Loss: 410.8701 | CE: 0.1518 | KD: 1060.1357\n",
      "Train Epoch: 058 Batch: 00018/00094 | Loss: 410.9633 | CE: 0.2670 | KD: 1060.0791\n",
      "Train Epoch: 058 Batch: 00019/00094 | Loss: 410.9076 | CE: 0.2074 | KD: 1060.0890\n",
      "Train Epoch: 058 Batch: 00020/00094 | Loss: 410.9886 | CE: 0.2697 | KD: 1060.1372\n",
      "Train Epoch: 058 Batch: 00021/00094 | Loss: 410.9135 | CE: 0.1965 | KD: 1060.1324\n",
      "Train Epoch: 058 Batch: 00022/00094 | Loss: 410.8931 | CE: 0.1941 | KD: 1060.0859\n",
      "Train Epoch: 058 Batch: 00023/00094 | Loss: 410.9341 | CE: 0.2329 | KD: 1060.0916\n",
      "Train Epoch: 058 Batch: 00024/00094 | Loss: 410.9472 | CE: 0.2031 | KD: 1060.2024\n",
      "Train Epoch: 058 Batch: 00025/00094 | Loss: 411.0185 | CE: 0.2828 | KD: 1060.1807\n",
      "Train Epoch: 058 Batch: 00026/00094 | Loss: 410.9083 | CE: 0.2193 | KD: 1060.0602\n",
      "Train Epoch: 058 Batch: 00027/00094 | Loss: 410.8824 | CE: 0.1958 | KD: 1060.0538\n",
      "Train Epoch: 058 Batch: 00028/00094 | Loss: 410.8801 | CE: 0.1691 | KD: 1060.1171\n",
      "Train Epoch: 058 Batch: 00029/00094 | Loss: 410.8824 | CE: 0.1776 | KD: 1060.1010\n",
      "Train Epoch: 058 Batch: 00030/00094 | Loss: 410.9814 | CE: 0.2761 | KD: 1060.1024\n",
      "Train Epoch: 058 Batch: 00031/00094 | Loss: 410.9472 | CE: 0.2123 | KD: 1060.1788\n",
      "Train Epoch: 058 Batch: 00032/00094 | Loss: 410.9502 | CE: 0.2648 | KD: 1060.0509\n",
      "Train Epoch: 058 Batch: 00033/00094 | Loss: 410.8716 | CE: 0.1889 | KD: 1060.0437\n",
      "Train Epoch: 058 Batch: 00034/00094 | Loss: 410.8052 | CE: 0.1651 | KD: 1059.9338\n",
      "Train Epoch: 058 Batch: 00035/00094 | Loss: 410.8833 | CE: 0.1740 | KD: 1060.1124\n",
      "Train Epoch: 058 Batch: 00036/00094 | Loss: 410.9461 | CE: 0.2290 | KD: 1060.1327\n",
      "Train Epoch: 058 Batch: 00037/00094 | Loss: 411.0050 | CE: 0.2925 | KD: 1060.1208\n",
      "Train Epoch: 058 Batch: 00038/00094 | Loss: 410.9459 | CE: 0.2591 | KD: 1060.0544\n",
      "Train Epoch: 058 Batch: 00039/00094 | Loss: 410.9426 | CE: 0.2641 | KD: 1060.0332\n",
      "Train Epoch: 058 Batch: 00040/00094 | Loss: 410.8419 | CE: 0.1650 | KD: 1060.0291\n",
      "Train Epoch: 058 Batch: 00041/00094 | Loss: 410.8195 | CE: 0.1567 | KD: 1059.9924\n",
      "Train Epoch: 058 Batch: 00042/00094 | Loss: 410.8929 | CE: 0.2022 | KD: 1060.0647\n",
      "Train Epoch: 058 Batch: 00043/00094 | Loss: 410.8750 | CE: 0.1994 | KD: 1060.0255\n",
      "Train Epoch: 058 Batch: 00044/00094 | Loss: 410.8673 | CE: 0.1926 | KD: 1060.0233\n",
      "Train Epoch: 058 Batch: 00045/00094 | Loss: 410.9084 | CE: 0.2193 | KD: 1060.0605\n",
      "Train Epoch: 058 Batch: 00046/00094 | Loss: 411.0857 | CE: 0.3512 | KD: 1060.1775\n",
      "Train Epoch: 058 Batch: 00047/00094 | Loss: 410.9378 | CE: 0.2415 | KD: 1060.0790\n",
      "Train Epoch: 058 Batch: 00048/00094 | Loss: 410.8022 | CE: 0.1401 | KD: 1059.9907\n",
      "Train Epoch: 058 Batch: 00049/00094 | Loss: 410.8676 | CE: 0.1855 | KD: 1060.0425\n",
      "Train Epoch: 058 Batch: 00050/00094 | Loss: 410.8796 | CE: 0.1772 | KD: 1060.0947\n",
      "Train Epoch: 058 Batch: 00051/00094 | Loss: 410.8008 | CE: 0.1691 | KD: 1059.9122\n",
      "Train Epoch: 058 Batch: 00052/00094 | Loss: 410.9203 | CE: 0.2356 | KD: 1060.0491\n",
      "Train Epoch: 058 Batch: 00053/00094 | Loss: 410.8939 | CE: 0.1855 | KD: 1060.1104\n",
      "Train Epoch: 058 Batch: 00054/00094 | Loss: 410.8979 | CE: 0.1750 | KD: 1060.1476\n",
      "Train Epoch: 058 Batch: 00055/00094 | Loss: 410.9039 | CE: 0.1955 | KD: 1060.1102\n",
      "Train Epoch: 058 Batch: 00056/00094 | Loss: 410.8952 | CE: 0.1989 | KD: 1060.0790\n",
      "Train Epoch: 058 Batch: 00057/00094 | Loss: 410.8796 | CE: 0.2051 | KD: 1060.0227\n",
      "Train Epoch: 058 Batch: 00058/00094 | Loss: 410.9301 | CE: 0.2458 | KD: 1060.0482\n",
      "Train Epoch: 058 Batch: 00059/00094 | Loss: 410.8734 | CE: 0.2062 | KD: 1060.0039\n",
      "Train Epoch: 058 Batch: 00060/00094 | Loss: 410.8536 | CE: 0.1650 | KD: 1060.0591\n",
      "Train Epoch: 058 Batch: 00061/00094 | Loss: 411.0132 | CE: 0.3018 | KD: 1060.1179\n",
      "Train Epoch: 058 Batch: 00062/00094 | Loss: 410.9379 | CE: 0.2464 | KD: 1060.0668\n",
      "Train Epoch: 058 Batch: 00063/00094 | Loss: 410.9293 | CE: 0.2088 | KD: 1060.1415\n",
      "Train Epoch: 058 Batch: 00064/00094 | Loss: 410.9348 | CE: 0.1860 | KD: 1060.2145\n",
      "Train Epoch: 058 Batch: 00065/00094 | Loss: 410.9070 | CE: 0.2228 | KD: 1060.0476\n",
      "Train Epoch: 058 Batch: 00066/00094 | Loss: 410.8987 | CE: 0.1942 | KD: 1060.1002\n",
      "Train Epoch: 058 Batch: 00067/00094 | Loss: 410.9525 | CE: 0.2257 | KD: 1060.1577\n",
      "Train Epoch: 058 Batch: 00068/00094 | Loss: 410.9926 | CE: 0.2693 | KD: 1060.1487\n",
      "Train Epoch: 058 Batch: 00069/00094 | Loss: 410.9791 | CE: 0.2434 | KD: 1060.1807\n",
      "Train Epoch: 058 Batch: 00070/00094 | Loss: 410.9977 | CE: 0.2825 | KD: 1060.1278\n",
      "Train Epoch: 058 Batch: 00071/00094 | Loss: 410.9718 | CE: 0.2606 | KD: 1060.1174\n",
      "Train Epoch: 058 Batch: 00072/00094 | Loss: 410.9135 | CE: 0.2287 | KD: 1060.0492\n",
      "Train Epoch: 058 Batch: 00073/00094 | Loss: 410.8750 | CE: 0.2074 | KD: 1060.0050\n",
      "Train Epoch: 058 Batch: 00074/00094 | Loss: 410.8738 | CE: 0.1973 | KD: 1060.0277\n",
      "Train Epoch: 058 Batch: 00075/00094 | Loss: 411.0446 | CE: 0.2869 | KD: 1060.2375\n",
      "Train Epoch: 058 Batch: 00076/00094 | Loss: 410.9221 | CE: 0.2132 | KD: 1060.1115\n",
      "Train Epoch: 058 Batch: 00077/00094 | Loss: 410.9469 | CE: 0.2357 | KD: 1060.1173\n",
      "Train Epoch: 058 Batch: 00078/00094 | Loss: 410.8675 | CE: 0.1746 | KD: 1060.0702\n",
      "Train Epoch: 058 Batch: 00079/00094 | Loss: 410.9780 | CE: 0.2327 | KD: 1060.2054\n",
      "Train Epoch: 058 Batch: 00080/00094 | Loss: 410.8192 | CE: 0.1590 | KD: 1059.9860\n",
      "Train Epoch: 058 Batch: 00081/00094 | Loss: 410.9393 | CE: 0.2423 | KD: 1060.0808\n",
      "Train Epoch: 058 Batch: 00082/00094 | Loss: 410.8510 | CE: 0.1739 | KD: 1060.0297\n",
      "Train Epoch: 058 Batch: 00083/00094 | Loss: 410.8364 | CE: 0.1610 | KD: 1060.0250\n",
      "Train Epoch: 058 Batch: 00084/00094 | Loss: 410.9585 | CE: 0.2775 | KD: 1060.0394\n",
      "Train Epoch: 058 Batch: 00085/00094 | Loss: 410.8869 | CE: 0.1979 | KD: 1060.0602\n",
      "Train Epoch: 058 Batch: 00086/00094 | Loss: 410.9406 | CE: 0.2401 | KD: 1060.0898\n",
      "Train Epoch: 058 Batch: 00087/00094 | Loss: 410.9265 | CE: 0.2210 | KD: 1060.1027\n",
      "Train Epoch: 058 Batch: 00088/00094 | Loss: 410.9728 | CE: 0.2442 | KD: 1060.1624\n",
      "Train Epoch: 058 Batch: 00089/00094 | Loss: 410.9615 | CE: 0.2354 | KD: 1060.1558\n",
      "Train Epoch: 058 Batch: 00090/00094 | Loss: 410.8556 | CE: 0.1948 | KD: 1059.9873\n",
      "Train Epoch: 058 Batch: 00091/00094 | Loss: 410.8589 | CE: 0.1736 | KD: 1060.0507\n",
      "Train Epoch: 058 Batch: 00092/00094 | Loss: 410.9582 | CE: 0.2153 | KD: 1060.1993\n",
      "Train Epoch: 058 Batch: 00093/00094 | Loss: 410.8996 | CE: 0.2312 | KD: 1060.0070\n",
      "Train Epoch: 058 Batch: 00094/00094 | Loss: 410.8143 | CE: 0.1553 | KD: 1059.9827\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2295 | acc:91.5500\n",
      "[VAL Acc] Target: 91.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3904 | acc:49.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1665 | acc:48.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2502 | acc:50.3817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7351 | acc:62.2649\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.26%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0100 | acc:54.5287\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5192 | acc:75.4702\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0040 | acc:56.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5795 | acc:70.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 70.80%\n",
      "[VAL Acc] Avg 62.17%\n",
      "Train Epoch: 059 Batch: 00001/00094 | Loss: 369.8748 | CE: 0.1870 | KD: 1060.2544\n",
      "Train Epoch: 059 Batch: 00002/00094 | Loss: 369.8317 | CE: 0.1950 | KD: 1060.1078\n",
      "Train Epoch: 059 Batch: 00003/00094 | Loss: 369.9371 | CE: 0.3185 | KD: 1060.0558\n",
      "Train Epoch: 059 Batch: 00004/00094 | Loss: 369.7167 | CE: 0.1192 | KD: 1059.9951\n",
      "Train Epoch: 059 Batch: 00005/00094 | Loss: 369.8383 | CE: 0.2129 | KD: 1060.0752\n",
      "Train Epoch: 059 Batch: 00006/00094 | Loss: 369.7523 | CE: 0.1692 | KD: 1059.9542\n",
      "Train Epoch: 059 Batch: 00007/00094 | Loss: 369.8119 | CE: 0.1906 | KD: 1060.0636\n",
      "Train Epoch: 059 Batch: 00008/00094 | Loss: 369.9251 | CE: 0.2713 | KD: 1060.1569\n",
      "Train Epoch: 059 Batch: 00009/00094 | Loss: 369.8964 | CE: 0.2479 | KD: 1060.1416\n",
      "Train Epoch: 059 Batch: 00010/00094 | Loss: 369.9017 | CE: 0.2369 | KD: 1060.1882\n",
      "Train Epoch: 059 Batch: 00011/00094 | Loss: 369.7758 | CE: 0.1888 | KD: 1059.9653\n",
      "Train Epoch: 059 Batch: 00012/00094 | Loss: 369.7514 | CE: 0.1206 | KD: 1060.0906\n",
      "Train Epoch: 059 Batch: 00013/00094 | Loss: 369.9534 | CE: 0.2921 | KD: 1060.1782\n",
      "Train Epoch: 059 Batch: 00014/00094 | Loss: 369.9000 | CE: 0.2789 | KD: 1060.0630\n",
      "Train Epoch: 059 Batch: 00015/00094 | Loss: 369.8170 | CE: 0.1962 | KD: 1060.0620\n",
      "Train Epoch: 059 Batch: 00016/00094 | Loss: 369.7781 | CE: 0.1740 | KD: 1060.0142\n",
      "Train Epoch: 059 Batch: 00017/00094 | Loss: 369.8313 | CE: 0.1997 | KD: 1060.0930\n",
      "Train Epoch: 059 Batch: 00018/00094 | Loss: 369.8498 | CE: 0.2211 | KD: 1060.0847\n",
      "Train Epoch: 059 Batch: 00019/00094 | Loss: 369.8138 | CE: 0.1779 | KD: 1060.1053\n",
      "Train Epoch: 059 Batch: 00020/00094 | Loss: 369.8096 | CE: 0.1998 | KD: 1060.0305\n",
      "Train Epoch: 059 Batch: 00021/00094 | Loss: 369.8344 | CE: 0.2021 | KD: 1060.0950\n",
      "Train Epoch: 059 Batch: 00022/00094 | Loss: 369.8839 | CE: 0.2502 | KD: 1060.0990\n",
      "Train Epoch: 059 Batch: 00023/00094 | Loss: 369.8972 | CE: 0.2572 | KD: 1060.1173\n",
      "Train Epoch: 059 Batch: 00024/00094 | Loss: 369.8535 | CE: 0.2323 | KD: 1060.0632\n",
      "Train Epoch: 059 Batch: 00025/00094 | Loss: 369.7855 | CE: 0.1846 | KD: 1060.0050\n",
      "Train Epoch: 059 Batch: 00026/00094 | Loss: 369.8699 | CE: 0.2382 | KD: 1060.0933\n",
      "Train Epoch: 059 Batch: 00027/00094 | Loss: 369.8163 | CE: 0.1905 | KD: 1060.0764\n",
      "Train Epoch: 059 Batch: 00028/00094 | Loss: 369.8596 | CE: 0.2158 | KD: 1060.1282\n",
      "Train Epoch: 059 Batch: 00029/00094 | Loss: 369.7711 | CE: 0.1496 | KD: 1060.0641\n",
      "Train Epoch: 059 Batch: 00030/00094 | Loss: 369.8422 | CE: 0.1946 | KD: 1060.1388\n",
      "Train Epoch: 059 Batch: 00031/00094 | Loss: 369.8323 | CE: 0.1979 | KD: 1060.1012\n",
      "Train Epoch: 059 Batch: 00032/00094 | Loss: 369.9118 | CE: 0.2215 | KD: 1060.2614\n",
      "Train Epoch: 059 Batch: 00033/00094 | Loss: 369.8384 | CE: 0.1900 | KD: 1060.1411\n",
      "Train Epoch: 059 Batch: 00034/00094 | Loss: 369.8028 | CE: 0.2112 | KD: 1059.9784\n",
      "Train Epoch: 059 Batch: 00035/00094 | Loss: 369.7855 | CE: 0.1594 | KD: 1060.0774\n",
      "Train Epoch: 059 Batch: 00036/00094 | Loss: 369.8263 | CE: 0.2038 | KD: 1060.0668\n",
      "Train Epoch: 059 Batch: 00037/00094 | Loss: 369.9275 | CE: 0.2767 | KD: 1060.1481\n",
      "Train Epoch: 059 Batch: 00038/00094 | Loss: 369.8213 | CE: 0.2075 | KD: 1060.0420\n",
      "Train Epoch: 059 Batch: 00039/00094 | Loss: 369.8712 | CE: 0.2366 | KD: 1060.1019\n",
      "Train Epoch: 059 Batch: 00040/00094 | Loss: 369.7436 | CE: 0.1321 | KD: 1060.0353\n",
      "Train Epoch: 059 Batch: 00041/00094 | Loss: 369.7834 | CE: 0.1719 | KD: 1060.0354\n",
      "Train Epoch: 059 Batch: 00042/00094 | Loss: 369.7871 | CE: 0.1876 | KD: 1060.0011\n",
      "Train Epoch: 059 Batch: 00043/00094 | Loss: 369.8665 | CE: 0.2207 | KD: 1060.1340\n",
      "Train Epoch: 059 Batch: 00044/00094 | Loss: 369.8294 | CE: 0.2034 | KD: 1060.0770\n",
      "Train Epoch: 059 Batch: 00045/00094 | Loss: 369.8840 | CE: 0.2532 | KD: 1060.0907\n",
      "Train Epoch: 059 Batch: 00046/00094 | Loss: 369.8781 | CE: 0.2288 | KD: 1060.1438\n",
      "Train Epoch: 059 Batch: 00047/00094 | Loss: 369.8567 | CE: 0.2253 | KD: 1060.0925\n",
      "Train Epoch: 059 Batch: 00048/00094 | Loss: 369.8115 | CE: 0.1753 | KD: 1060.1063\n",
      "Train Epoch: 059 Batch: 00049/00094 | Loss: 369.7969 | CE: 0.1620 | KD: 1060.1023\n",
      "Train Epoch: 059 Batch: 00050/00094 | Loss: 369.8407 | CE: 0.1882 | KD: 1060.1532\n",
      "Train Epoch: 059 Batch: 00051/00094 | Loss: 369.8290 | CE: 0.2079 | KD: 1060.0629\n",
      "Train Epoch: 059 Batch: 00052/00094 | Loss: 369.8580 | CE: 0.2194 | KD: 1060.1132\n",
      "Train Epoch: 059 Batch: 00053/00094 | Loss: 369.8188 | CE: 0.1975 | KD: 1060.0636\n",
      "Train Epoch: 059 Batch: 00054/00094 | Loss: 369.7785 | CE: 0.1864 | KD: 1059.9797\n",
      "Train Epoch: 059 Batch: 00055/00094 | Loss: 369.8731 | CE: 0.1998 | KD: 1060.2126\n",
      "Train Epoch: 059 Batch: 00056/00094 | Loss: 369.8710 | CE: 0.2435 | KD: 1060.0812\n",
      "Train Epoch: 059 Batch: 00057/00094 | Loss: 369.8338 | CE: 0.2099 | KD: 1060.0708\n",
      "Train Epoch: 059 Batch: 00058/00094 | Loss: 369.8539 | CE: 0.2063 | KD: 1060.1388\n",
      "Train Epoch: 059 Batch: 00059/00094 | Loss: 369.7993 | CE: 0.1596 | KD: 1060.1162\n",
      "Train Epoch: 059 Batch: 00060/00094 | Loss: 369.8613 | CE: 0.2440 | KD: 1060.0521\n",
      "Train Epoch: 059 Batch: 00061/00094 | Loss: 369.7559 | CE: 0.1464 | KD: 1060.0298\n",
      "Train Epoch: 059 Batch: 00062/00094 | Loss: 369.8805 | CE: 0.2424 | KD: 1060.1117\n",
      "Train Epoch: 059 Batch: 00063/00094 | Loss: 369.8242 | CE: 0.1769 | KD: 1060.1384\n",
      "Train Epoch: 059 Batch: 00064/00094 | Loss: 369.9211 | CE: 0.2822 | KD: 1060.1143\n",
      "Train Epoch: 059 Batch: 00065/00094 | Loss: 369.8284 | CE: 0.1747 | KD: 1060.1565\n",
      "Train Epoch: 059 Batch: 00066/00094 | Loss: 369.8245 | CE: 0.1766 | KD: 1060.1398\n",
      "Train Epoch: 059 Batch: 00067/00094 | Loss: 369.9045 | CE: 0.2635 | KD: 1060.1199\n",
      "Train Epoch: 059 Batch: 00068/00094 | Loss: 369.8133 | CE: 0.2075 | KD: 1060.0192\n",
      "Train Epoch: 059 Batch: 00069/00094 | Loss: 369.9183 | CE: 0.2811 | KD: 1060.1093\n",
      "Train Epoch: 059 Batch: 00070/00094 | Loss: 369.8719 | CE: 0.2256 | KD: 1060.1353\n",
      "Train Epoch: 059 Batch: 00071/00094 | Loss: 369.9304 | CE: 0.2856 | KD: 1060.1311\n",
      "Train Epoch: 059 Batch: 00072/00094 | Loss: 369.7472 | CE: 0.1472 | KD: 1060.0024\n",
      "Train Epoch: 059 Batch: 00073/00094 | Loss: 369.8228 | CE: 0.2092 | KD: 1060.0414\n",
      "Train Epoch: 059 Batch: 00074/00094 | Loss: 369.7911 | CE: 0.1581 | KD: 1060.0972\n",
      "Train Epoch: 059 Batch: 00075/00094 | Loss: 369.7785 | CE: 0.1646 | KD: 1060.0422\n",
      "Train Epoch: 059 Batch: 00076/00094 | Loss: 369.7740 | CE: 0.1719 | KD: 1060.0083\n",
      "Train Epoch: 059 Batch: 00077/00094 | Loss: 369.7897 | CE: 0.1783 | KD: 1060.0350\n",
      "Train Epoch: 059 Batch: 00078/00094 | Loss: 369.8683 | CE: 0.2302 | KD: 1060.1117\n",
      "Train Epoch: 059 Batch: 00079/00094 | Loss: 369.7556 | CE: 0.1408 | KD: 1060.0449\n",
      "Train Epoch: 059 Batch: 00080/00094 | Loss: 369.7775 | CE: 0.1523 | KD: 1060.0747\n",
      "Train Epoch: 059 Batch: 00081/00094 | Loss: 369.8031 | CE: 0.1537 | KD: 1060.1440\n",
      "Train Epoch: 059 Batch: 00082/00094 | Loss: 370.0063 | CE: 0.3730 | KD: 1060.0980\n",
      "Train Epoch: 059 Batch: 00083/00094 | Loss: 369.8370 | CE: 0.2043 | KD: 1060.0962\n",
      "Train Epoch: 059 Batch: 00084/00094 | Loss: 369.8671 | CE: 0.2556 | KD: 1060.0353\n",
      "Train Epoch: 059 Batch: 00085/00094 | Loss: 369.8053 | CE: 0.1490 | KD: 1060.1641\n",
      "Train Epoch: 059 Batch: 00086/00094 | Loss: 369.8723 | CE: 0.2441 | KD: 1060.0833\n",
      "Train Epoch: 059 Batch: 00087/00094 | Loss: 369.8323 | CE: 0.2077 | KD: 1060.0729\n",
      "Train Epoch: 059 Batch: 00088/00094 | Loss: 369.8415 | CE: 0.2028 | KD: 1060.1136\n",
      "Train Epoch: 059 Batch: 00089/00094 | Loss: 369.8162 | CE: 0.2119 | KD: 1060.0146\n",
      "Train Epoch: 059 Batch: 00090/00094 | Loss: 369.7840 | CE: 0.1933 | KD: 1059.9756\n",
      "Train Epoch: 059 Batch: 00091/00094 | Loss: 369.9644 | CE: 0.3176 | KD: 1060.1368\n",
      "Train Epoch: 059 Batch: 00092/00094 | Loss: 369.9094 | CE: 0.2413 | KD: 1060.1976\n",
      "Train Epoch: 059 Batch: 00093/00094 | Loss: 369.8872 | CE: 0.2583 | KD: 1060.0854\n",
      "Train Epoch: 059 Batch: 00094/00094 | Loss: 369.7843 | CE: 0.1419 | KD: 1060.1240\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2272 | acc:92.1000\n",
      "[VAL Acc] Target: 92.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2208 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0740 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1641 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6021 | acc:69.3966\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 69.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0597 | acc:53.4196\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5022 | acc:75.9796\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.98%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0788 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4968 | acc:74.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 74.55%\n",
      "[VAL Acc] Avg 63.15%\n",
      "Train Epoch: 060 Batch: 00001/00094 | Loss: 369.8247 | CE: 0.2264 | KD: 1059.9976\n",
      "Train Epoch: 060 Batch: 00002/00094 | Loss: 369.7638 | CE: 0.1445 | KD: 1060.0575\n",
      "Train Epoch: 060 Batch: 00003/00094 | Loss: 369.7443 | CE: 0.1228 | KD: 1060.0641\n",
      "Train Epoch: 060 Batch: 00004/00094 | Loss: 369.8753 | CE: 0.2390 | KD: 1060.1063\n",
      "Train Epoch: 060 Batch: 00005/00094 | Loss: 369.7750 | CE: 0.1536 | KD: 1060.0637\n",
      "Train Epoch: 060 Batch: 00006/00094 | Loss: 369.8729 | CE: 0.2491 | KD: 1060.0706\n",
      "Train Epoch: 060 Batch: 00007/00094 | Loss: 369.7411 | CE: 0.1406 | KD: 1060.0039\n",
      "Train Epoch: 060 Batch: 00008/00094 | Loss: 369.7885 | CE: 0.1731 | KD: 1060.0464\n",
      "Train Epoch: 060 Batch: 00009/00094 | Loss: 369.8348 | CE: 0.1939 | KD: 1060.1199\n",
      "Train Epoch: 060 Batch: 00010/00094 | Loss: 369.9185 | CE: 0.2852 | KD: 1060.0979\n",
      "Train Epoch: 060 Batch: 00011/00094 | Loss: 369.9798 | CE: 0.2925 | KD: 1060.2527\n",
      "Train Epoch: 060 Batch: 00012/00094 | Loss: 369.8714 | CE: 0.2547 | KD: 1060.0503\n",
      "Train Epoch: 060 Batch: 00013/00094 | Loss: 369.8448 | CE: 0.1985 | KD: 1060.1353\n",
      "Train Epoch: 060 Batch: 00014/00094 | Loss: 369.8462 | CE: 0.1966 | KD: 1060.1447\n",
      "Train Epoch: 060 Batch: 00015/00094 | Loss: 369.8131 | CE: 0.1920 | KD: 1060.0629\n",
      "Train Epoch: 060 Batch: 00016/00094 | Loss: 369.8214 | CE: 0.1917 | KD: 1060.0876\n",
      "Train Epoch: 060 Batch: 00017/00094 | Loss: 369.9060 | CE: 0.2504 | KD: 1060.1617\n",
      "Train Epoch: 060 Batch: 00018/00094 | Loss: 369.8327 | CE: 0.1704 | KD: 1060.1810\n",
      "Train Epoch: 060 Batch: 00019/00094 | Loss: 369.7910 | CE: 0.1650 | KD: 1060.0769\n",
      "Train Epoch: 060 Batch: 00020/00094 | Loss: 369.8710 | CE: 0.2405 | KD: 1060.0900\n",
      "Train Epoch: 060 Batch: 00021/00094 | Loss: 369.9277 | CE: 0.3113 | KD: 1060.0496\n",
      "Train Epoch: 060 Batch: 00022/00094 | Loss: 369.8911 | CE: 0.2411 | KD: 1060.1458\n",
      "Train Epoch: 060 Batch: 00023/00094 | Loss: 369.8520 | CE: 0.2375 | KD: 1060.0441\n",
      "Train Epoch: 060 Batch: 00024/00094 | Loss: 369.8667 | CE: 0.2368 | KD: 1060.0881\n",
      "Train Epoch: 060 Batch: 00025/00094 | Loss: 369.8213 | CE: 0.1970 | KD: 1060.0720\n",
      "Train Epoch: 060 Batch: 00026/00094 | Loss: 369.8280 | CE: 0.1912 | KD: 1060.1080\n",
      "Train Epoch: 060 Batch: 00027/00094 | Loss: 369.8962 | CE: 0.2763 | KD: 1060.0596\n",
      "Train Epoch: 060 Batch: 00028/00094 | Loss: 369.8452 | CE: 0.2160 | KD: 1060.0863\n",
      "Train Epoch: 060 Batch: 00029/00094 | Loss: 369.8749 | CE: 0.2282 | KD: 1060.1362\n",
      "Train Epoch: 060 Batch: 00030/00094 | Loss: 369.7537 | CE: 0.1518 | KD: 1060.0081\n",
      "Train Epoch: 060 Batch: 00031/00094 | Loss: 369.8044 | CE: 0.1896 | KD: 1060.0448\n",
      "Train Epoch: 060 Batch: 00032/00094 | Loss: 369.8251 | CE: 0.2150 | KD: 1060.0315\n",
      "Train Epoch: 060 Batch: 00033/00094 | Loss: 369.8533 | CE: 0.2307 | KD: 1060.0671\n",
      "Train Epoch: 060 Batch: 00034/00094 | Loss: 369.8020 | CE: 0.1689 | KD: 1060.0975\n",
      "Train Epoch: 060 Batch: 00035/00094 | Loss: 369.9315 | CE: 0.2644 | KD: 1060.1951\n",
      "Train Epoch: 060 Batch: 00036/00094 | Loss: 369.9776 | CE: 0.2956 | KD: 1060.2374\n",
      "Train Epoch: 060 Batch: 00037/00094 | Loss: 369.8434 | CE: 0.2095 | KD: 1060.0999\n",
      "Train Epoch: 060 Batch: 00038/00094 | Loss: 369.8409 | CE: 0.2153 | KD: 1060.0758\n",
      "Train Epoch: 060 Batch: 00039/00094 | Loss: 369.8424 | CE: 0.2292 | KD: 1060.0404\n",
      "Train Epoch: 060 Batch: 00040/00094 | Loss: 369.8742 | CE: 0.2196 | KD: 1060.1592\n",
      "Train Epoch: 060 Batch: 00041/00094 | Loss: 369.9196 | CE: 0.2359 | KD: 1060.2424\n",
      "Train Epoch: 060 Batch: 00042/00094 | Loss: 369.7537 | CE: 0.1499 | KD: 1060.0132\n",
      "Train Epoch: 060 Batch: 00043/00094 | Loss: 369.8674 | CE: 0.2239 | KD: 1060.1271\n",
      "Train Epoch: 060 Batch: 00044/00094 | Loss: 369.9048 | CE: 0.2477 | KD: 1060.1664\n",
      "Train Epoch: 060 Batch: 00045/00094 | Loss: 369.8146 | CE: 0.2170 | KD: 1059.9957\n",
      "Train Epoch: 060 Batch: 00046/00094 | Loss: 369.7854 | CE: 0.1647 | KD: 1060.0620\n",
      "Train Epoch: 060 Batch: 00047/00094 | Loss: 369.9137 | CE: 0.2842 | KD: 1060.0870\n",
      "Train Epoch: 060 Batch: 00048/00094 | Loss: 369.8680 | CE: 0.2331 | KD: 1060.1023\n",
      "Train Epoch: 060 Batch: 00049/00094 | Loss: 369.7874 | CE: 0.1803 | KD: 1060.0227\n",
      "Train Epoch: 060 Batch: 00050/00094 | Loss: 369.8951 | CE: 0.2466 | KD: 1060.1416\n",
      "Train Epoch: 060 Batch: 00051/00094 | Loss: 369.8164 | CE: 0.1911 | KD: 1060.0752\n",
      "Train Epoch: 060 Batch: 00052/00094 | Loss: 369.8806 | CE: 0.2524 | KD: 1060.0834\n",
      "Train Epoch: 060 Batch: 00053/00094 | Loss: 369.9171 | CE: 0.2831 | KD: 1060.1000\n",
      "Train Epoch: 060 Batch: 00054/00094 | Loss: 369.8079 | CE: 0.2054 | KD: 1060.0098\n",
      "Train Epoch: 060 Batch: 00055/00094 | Loss: 369.8370 | CE: 0.2001 | KD: 1060.1084\n",
      "Train Epoch: 060 Batch: 00056/00094 | Loss: 369.8067 | CE: 0.1635 | KD: 1060.1262\n",
      "Train Epoch: 060 Batch: 00057/00094 | Loss: 369.8642 | CE: 0.2434 | KD: 1060.0623\n",
      "Train Epoch: 060 Batch: 00058/00094 | Loss: 369.8248 | CE: 0.2048 | KD: 1060.0597\n",
      "Train Epoch: 060 Batch: 00059/00094 | Loss: 369.7935 | CE: 0.1619 | KD: 1060.0931\n",
      "Train Epoch: 060 Batch: 00060/00094 | Loss: 369.8584 | CE: 0.2314 | KD: 1060.0800\n",
      "Train Epoch: 060 Batch: 00061/00094 | Loss: 369.7930 | CE: 0.1671 | KD: 1060.0767\n",
      "Train Epoch: 060 Batch: 00062/00094 | Loss: 369.8176 | CE: 0.1902 | KD: 1060.0811\n",
      "Train Epoch: 060 Batch: 00063/00094 | Loss: 369.9084 | CE: 0.2605 | KD: 1060.1398\n",
      "Train Epoch: 060 Batch: 00064/00094 | Loss: 369.8477 | CE: 0.2219 | KD: 1060.0764\n",
      "Train Epoch: 060 Batch: 00065/00094 | Loss: 369.8002 | CE: 0.1664 | KD: 1060.0995\n",
      "Train Epoch: 060 Batch: 00066/00094 | Loss: 369.8461 | CE: 0.2111 | KD: 1060.1029\n",
      "Train Epoch: 060 Batch: 00067/00094 | Loss: 369.8104 | CE: 0.1956 | KD: 1060.0447\n",
      "Train Epoch: 060 Batch: 00068/00094 | Loss: 369.7649 | CE: 0.1288 | KD: 1060.1061\n",
      "Train Epoch: 060 Batch: 00069/00094 | Loss: 369.7603 | CE: 0.1535 | KD: 1060.0221\n",
      "Train Epoch: 060 Batch: 00070/00094 | Loss: 370.0456 | CE: 0.3372 | KD: 1060.3132\n",
      "Train Epoch: 060 Batch: 00071/00094 | Loss: 369.8410 | CE: 0.1982 | KD: 1060.1252\n",
      "Train Epoch: 060 Batch: 00072/00094 | Loss: 369.8145 | CE: 0.1891 | KD: 1060.0751\n",
      "Train Epoch: 060 Batch: 00073/00094 | Loss: 369.8808 | CE: 0.2709 | KD: 1060.0309\n",
      "Train Epoch: 060 Batch: 00074/00094 | Loss: 369.8089 | CE: 0.1813 | KD: 1060.0814\n",
      "Train Epoch: 060 Batch: 00075/00094 | Loss: 369.8681 | CE: 0.2237 | KD: 1060.1300\n",
      "Train Epoch: 060 Batch: 00076/00094 | Loss: 369.8439 | CE: 0.2023 | KD: 1060.1218\n",
      "Train Epoch: 060 Batch: 00077/00094 | Loss: 369.8584 | CE: 0.2286 | KD: 1060.0879\n",
      "Train Epoch: 060 Batch: 00078/00094 | Loss: 369.8062 | CE: 0.1881 | KD: 1060.0542\n",
      "Train Epoch: 060 Batch: 00079/00094 | Loss: 369.7714 | CE: 0.1559 | KD: 1060.0470\n",
      "Train Epoch: 060 Batch: 00080/00094 | Loss: 369.8355 | CE: 0.2056 | KD: 1060.0881\n",
      "Train Epoch: 060 Batch: 00081/00094 | Loss: 369.7946 | CE: 0.1458 | KD: 1060.1422\n",
      "Train Epoch: 060 Batch: 00082/00094 | Loss: 369.7975 | CE: 0.1788 | KD: 1060.0559\n",
      "Train Epoch: 060 Batch: 00083/00094 | Loss: 369.8293 | CE: 0.1895 | KD: 1060.1167\n",
      "Train Epoch: 060 Batch: 00084/00094 | Loss: 369.8731 | CE: 0.2374 | KD: 1060.1050\n",
      "Train Epoch: 060 Batch: 00085/00094 | Loss: 369.8641 | CE: 0.2397 | KD: 1060.0725\n",
      "Train Epoch: 060 Batch: 00086/00094 | Loss: 369.7936 | CE: 0.1724 | KD: 1060.0631\n",
      "Train Epoch: 060 Batch: 00087/00094 | Loss: 369.8734 | CE: 0.2191 | KD: 1060.1583\n",
      "Train Epoch: 060 Batch: 00088/00094 | Loss: 369.8438 | CE: 0.2147 | KD: 1060.0861\n",
      "Train Epoch: 060 Batch: 00089/00094 | Loss: 369.7538 | CE: 0.1683 | KD: 1059.9607\n",
      "Train Epoch: 060 Batch: 00090/00094 | Loss: 369.8605 | CE: 0.2222 | KD: 1060.1123\n",
      "Train Epoch: 060 Batch: 00091/00094 | Loss: 369.8724 | CE: 0.2514 | KD: 1060.0629\n",
      "Train Epoch: 060 Batch: 00092/00094 | Loss: 369.8796 | CE: 0.2436 | KD: 1060.1056\n",
      "Train Epoch: 060 Batch: 00093/00094 | Loss: 369.7884 | CE: 0.1764 | KD: 1060.0369\n",
      "Train Epoch: 060 Batch: 00094/00094 | Loss: 370.0037 | CE: 0.3107 | KD: 1060.2690\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2494 | acc:91.5500\n",
      "[VAL Acc] Target: 91.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1082 | acc:50.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9575 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0417 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6334 | acc:67.0455\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 67.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9583 | acc:54.3438\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4945 | acc:76.8417\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9914 | acc:54.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.4643 | acc:76.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 76.30%\n",
      "[VAL Acc] Avg 63.28%\n",
      "Train Epoch: 061 Batch: 00001/00094 | Loss: 369.8664 | CE: 0.2228 | KD: 1060.1276\n",
      "Train Epoch: 061 Batch: 00002/00094 | Loss: 369.8098 | CE: 0.1946 | KD: 1060.0463\n",
      "Train Epoch: 061 Batch: 00003/00094 | Loss: 369.8155 | CE: 0.1874 | KD: 1060.0831\n",
      "Train Epoch: 061 Batch: 00004/00094 | Loss: 369.7810 | CE: 0.1724 | KD: 1060.0273\n",
      "Train Epoch: 061 Batch: 00005/00094 | Loss: 369.8627 | CE: 0.2075 | KD: 1060.1606\n",
      "Train Epoch: 061 Batch: 00006/00094 | Loss: 369.7303 | CE: 0.1366 | KD: 1059.9841\n",
      "Train Epoch: 061 Batch: 00007/00094 | Loss: 369.8310 | CE: 0.1970 | KD: 1060.0997\n",
      "Train Epoch: 061 Batch: 00008/00094 | Loss: 369.8475 | CE: 0.2062 | KD: 1060.1210\n",
      "Train Epoch: 061 Batch: 00009/00094 | Loss: 369.9148 | CE: 0.2852 | KD: 1060.0872\n",
      "Train Epoch: 061 Batch: 00010/00094 | Loss: 369.8451 | CE: 0.1917 | KD: 1060.1558\n",
      "Train Epoch: 061 Batch: 00011/00094 | Loss: 369.7875 | CE: 0.1679 | KD: 1060.0585\n",
      "Train Epoch: 061 Batch: 00012/00094 | Loss: 369.7991 | CE: 0.1882 | KD: 1060.0338\n",
      "Train Epoch: 061 Batch: 00013/00094 | Loss: 369.9237 | CE: 0.2441 | KD: 1060.2307\n",
      "Train Epoch: 061 Batch: 00014/00094 | Loss: 369.8993 | CE: 0.2691 | KD: 1060.0890\n",
      "Train Epoch: 061 Batch: 00015/00094 | Loss: 369.8200 | CE: 0.1880 | KD: 1060.0942\n",
      "Train Epoch: 061 Batch: 00016/00094 | Loss: 369.7816 | CE: 0.1412 | KD: 1060.1184\n",
      "Train Epoch: 061 Batch: 00017/00094 | Loss: 369.8258 | CE: 0.2042 | KD: 1060.0645\n",
      "Train Epoch: 061 Batch: 00018/00094 | Loss: 369.8184 | CE: 0.1836 | KD: 1060.1022\n",
      "Train Epoch: 061 Batch: 00019/00094 | Loss: 369.8282 | CE: 0.1818 | KD: 1060.1354\n",
      "Train Epoch: 061 Batch: 00020/00094 | Loss: 369.9088 | CE: 0.2522 | KD: 1060.1649\n",
      "Train Epoch: 061 Batch: 00021/00094 | Loss: 369.8918 | CE: 0.2608 | KD: 1060.0916\n",
      "Train Epoch: 061 Batch: 00022/00094 | Loss: 369.8951 | CE: 0.2267 | KD: 1060.1986\n",
      "Train Epoch: 061 Batch: 00023/00094 | Loss: 369.7996 | CE: 0.1704 | KD: 1060.0862\n",
      "Train Epoch: 061 Batch: 00024/00094 | Loss: 369.8337 | CE: 0.1662 | KD: 1060.1959\n",
      "Train Epoch: 061 Batch: 00025/00094 | Loss: 369.8835 | CE: 0.2382 | KD: 1060.1324\n",
      "Train Epoch: 061 Batch: 00026/00094 | Loss: 369.9330 | CE: 0.2900 | KD: 1060.1255\n",
      "Train Epoch: 061 Batch: 00027/00094 | Loss: 369.8151 | CE: 0.1630 | KD: 1060.1519\n",
      "Train Epoch: 061 Batch: 00028/00094 | Loss: 369.8773 | CE: 0.2256 | KD: 1060.1506\n",
      "Train Epoch: 061 Batch: 00029/00094 | Loss: 369.7708 | CE: 0.1730 | KD: 1059.9960\n",
      "Train Epoch: 061 Batch: 00030/00094 | Loss: 369.8508 | CE: 0.2247 | KD: 1060.0774\n",
      "Train Epoch: 061 Batch: 00031/00094 | Loss: 369.8123 | CE: 0.2102 | KD: 1060.0085\n",
      "Train Epoch: 061 Batch: 00032/00094 | Loss: 369.7849 | CE: 0.1929 | KD: 1059.9797\n",
      "Train Epoch: 061 Batch: 00033/00094 | Loss: 369.8925 | CE: 0.2530 | KD: 1060.1158\n",
      "Train Epoch: 061 Batch: 00034/00094 | Loss: 369.8542 | CE: 0.2245 | KD: 1060.0875\n",
      "Train Epoch: 061 Batch: 00035/00094 | Loss: 369.8197 | CE: 0.1928 | KD: 1060.0796\n",
      "Train Epoch: 061 Batch: 00036/00094 | Loss: 369.8947 | CE: 0.2377 | KD: 1060.1663\n",
      "Train Epoch: 061 Batch: 00037/00094 | Loss: 369.8639 | CE: 0.2357 | KD: 1060.0833\n",
      "Train Epoch: 061 Batch: 00038/00094 | Loss: 369.9482 | CE: 0.3184 | KD: 1060.0878\n",
      "Train Epoch: 061 Batch: 00039/00094 | Loss: 369.7784 | CE: 0.1529 | KD: 1060.0756\n",
      "Train Epoch: 061 Batch: 00040/00094 | Loss: 369.8780 | CE: 0.2535 | KD: 1060.0728\n",
      "Train Epoch: 061 Batch: 00041/00094 | Loss: 369.8071 | CE: 0.1660 | KD: 1060.1201\n",
      "Train Epoch: 061 Batch: 00042/00094 | Loss: 369.7380 | CE: 0.1309 | KD: 1060.0226\n",
      "Train Epoch: 061 Batch: 00043/00094 | Loss: 369.8780 | CE: 0.2362 | KD: 1060.1224\n",
      "Train Epoch: 061 Batch: 00044/00094 | Loss: 369.8394 | CE: 0.2318 | KD: 1060.0244\n",
      "Train Epoch: 061 Batch: 00045/00094 | Loss: 369.7931 | CE: 0.2003 | KD: 1059.9817\n",
      "Train Epoch: 061 Batch: 00046/00094 | Loss: 369.8466 | CE: 0.1990 | KD: 1060.1389\n",
      "Train Epoch: 061 Batch: 00047/00094 | Loss: 369.8846 | CE: 0.2390 | KD: 1060.1332\n",
      "Train Epoch: 061 Batch: 00048/00094 | Loss: 369.8062 | CE: 0.1746 | KD: 1060.0930\n",
      "Train Epoch: 061 Batch: 00049/00094 | Loss: 369.8466 | CE: 0.2213 | KD: 1060.0752\n",
      "Train Epoch: 061 Batch: 00050/00094 | Loss: 369.8828 | CE: 0.2148 | KD: 1060.1974\n",
      "Train Epoch: 061 Batch: 00051/00094 | Loss: 369.8275 | CE: 0.2107 | KD: 1060.0504\n",
      "Train Epoch: 061 Batch: 00052/00094 | Loss: 369.8185 | CE: 0.1966 | KD: 1060.0654\n",
      "Train Epoch: 061 Batch: 00053/00094 | Loss: 369.7978 | CE: 0.1801 | KD: 1060.0532\n",
      "Train Epoch: 061 Batch: 00054/00094 | Loss: 369.8341 | CE: 0.2165 | KD: 1060.0529\n",
      "Train Epoch: 061 Batch: 00055/00094 | Loss: 369.7672 | CE: 0.1471 | KD: 1060.0603\n",
      "Train Epoch: 061 Batch: 00056/00094 | Loss: 369.7654 | CE: 0.1864 | KD: 1059.9423\n",
      "Train Epoch: 061 Batch: 00057/00094 | Loss: 369.8467 | CE: 0.1992 | KD: 1060.1389\n",
      "Train Epoch: 061 Batch: 00058/00094 | Loss: 369.7820 | CE: 0.1364 | KD: 1060.1332\n",
      "Train Epoch: 061 Batch: 00059/00094 | Loss: 369.8503 | CE: 0.2248 | KD: 1060.0756\n",
      "Train Epoch: 061 Batch: 00060/00094 | Loss: 369.7594 | CE: 0.1549 | KD: 1060.0151\n",
      "Train Epoch: 061 Batch: 00061/00094 | Loss: 369.8186 | CE: 0.1804 | KD: 1060.1119\n",
      "Train Epoch: 061 Batch: 00062/00094 | Loss: 369.8022 | CE: 0.1806 | KD: 1060.0642\n",
      "Train Epoch: 061 Batch: 00063/00094 | Loss: 369.7950 | CE: 0.1550 | KD: 1060.1173\n",
      "Train Epoch: 061 Batch: 00064/00094 | Loss: 369.8303 | CE: 0.1821 | KD: 1060.1409\n",
      "Train Epoch: 061 Batch: 00065/00094 | Loss: 369.7792 | CE: 0.1469 | KD: 1060.0950\n",
      "Train Epoch: 061 Batch: 00066/00094 | Loss: 369.8848 | CE: 0.2045 | KD: 1060.2329\n",
      "Train Epoch: 061 Batch: 00067/00094 | Loss: 369.8450 | CE: 0.2158 | KD: 1060.0862\n",
      "Train Epoch: 061 Batch: 00068/00094 | Loss: 369.8168 | CE: 0.1963 | KD: 1060.0613\n",
      "Train Epoch: 061 Batch: 00069/00094 | Loss: 369.8053 | CE: 0.1587 | KD: 1060.1362\n",
      "Train Epoch: 061 Batch: 00070/00094 | Loss: 369.9367 | CE: 0.2903 | KD: 1060.1356\n",
      "Train Epoch: 061 Batch: 00071/00094 | Loss: 369.9306 | CE: 0.2825 | KD: 1060.1404\n",
      "Train Epoch: 061 Batch: 00072/00094 | Loss: 369.8510 | CE: 0.2100 | KD: 1060.1199\n",
      "Train Epoch: 061 Batch: 00073/00094 | Loss: 369.8219 | CE: 0.1647 | KD: 1060.1664\n",
      "Train Epoch: 061 Batch: 00074/00094 | Loss: 369.7723 | CE: 0.1676 | KD: 1060.0161\n",
      "Train Epoch: 061 Batch: 00075/00094 | Loss: 370.0156 | CE: 0.3479 | KD: 1060.1967\n",
      "Train Epoch: 061 Batch: 00076/00094 | Loss: 369.9072 | CE: 0.2363 | KD: 1060.2056\n",
      "Train Epoch: 061 Batch: 00077/00094 | Loss: 369.8487 | CE: 0.2230 | KD: 1060.0762\n",
      "Train Epoch: 061 Batch: 00078/00094 | Loss: 369.7774 | CE: 0.1685 | KD: 1060.0278\n",
      "Train Epoch: 061 Batch: 00079/00094 | Loss: 369.8738 | CE: 0.2353 | KD: 1060.1130\n",
      "Train Epoch: 061 Batch: 00080/00094 | Loss: 369.8021 | CE: 0.1891 | KD: 1060.0397\n",
      "Train Epoch: 061 Batch: 00081/00094 | Loss: 369.8414 | CE: 0.1994 | KD: 1060.1229\n",
      "Train Epoch: 061 Batch: 00082/00094 | Loss: 369.7570 | CE: 0.1534 | KD: 1060.0128\n",
      "Train Epoch: 061 Batch: 00083/00094 | Loss: 369.8827 | CE: 0.2435 | KD: 1060.1149\n",
      "Train Epoch: 061 Batch: 00084/00094 | Loss: 369.8910 | CE: 0.2423 | KD: 1060.1422\n",
      "Train Epoch: 061 Batch: 00085/00094 | Loss: 369.8637 | CE: 0.2305 | KD: 1060.0977\n",
      "Train Epoch: 061 Batch: 00086/00094 | Loss: 369.7505 | CE: 0.1599 | KD: 1059.9755\n",
      "Train Epoch: 061 Batch: 00087/00094 | Loss: 369.8432 | CE: 0.1879 | KD: 1060.1609\n",
      "Train Epoch: 061 Batch: 00088/00094 | Loss: 369.8880 | CE: 0.2481 | KD: 1060.1168\n",
      "Train Epoch: 061 Batch: 00089/00094 | Loss: 369.9288 | CE: 0.2704 | KD: 1060.1700\n",
      "Train Epoch: 061 Batch: 00090/00094 | Loss: 369.8550 | CE: 0.2341 | KD: 1060.0624\n",
      "Train Epoch: 061 Batch: 00091/00094 | Loss: 369.8716 | CE: 0.2226 | KD: 1060.1429\n",
      "Train Epoch: 061 Batch: 00092/00094 | Loss: 369.8708 | CE: 0.2484 | KD: 1060.0668\n",
      "Train Epoch: 061 Batch: 00093/00094 | Loss: 369.7824 | CE: 0.1742 | KD: 1060.0259\n",
      "Train Epoch: 061 Batch: 00094/00094 | Loss: 369.8789 | CE: 0.2194 | KD: 1060.1733\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2512 | acc:90.5000\n",
      "[VAL Acc] Target: 90.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3216 | acc:50.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1308 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2417 | acc:50.1908\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6873 | acc:66.6928\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 66.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8562 | acc:57.5786\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 57.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5415 | acc:74.0596\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9045 | acc:59.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5957 | acc:68.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 68.45%\n",
      "[VAL Acc] Avg 63.07%\n",
      "Train Epoch: 062 Batch: 00001/00094 | Loss: 369.7717 | CE: 0.1771 | KD: 1059.9869\n",
      "Train Epoch: 062 Batch: 00002/00094 | Loss: 369.8729 | CE: 0.1998 | KD: 1060.2122\n",
      "Train Epoch: 062 Batch: 00003/00094 | Loss: 369.8479 | CE: 0.2361 | KD: 1060.0364\n",
      "Train Epoch: 062 Batch: 00004/00094 | Loss: 369.9352 | CE: 0.2582 | KD: 1060.2233\n",
      "Train Epoch: 062 Batch: 00005/00094 | Loss: 369.7936 | CE: 0.1689 | KD: 1060.0732\n",
      "Train Epoch: 062 Batch: 00006/00094 | Loss: 369.8424 | CE: 0.1689 | KD: 1060.2135\n",
      "Train Epoch: 062 Batch: 00007/00094 | Loss: 369.8310 | CE: 0.1894 | KD: 1060.1217\n",
      "Train Epoch: 062 Batch: 00008/00094 | Loss: 369.9111 | CE: 0.2530 | KD: 1060.1691\n",
      "Train Epoch: 062 Batch: 00009/00094 | Loss: 369.8035 | CE: 0.1777 | KD: 1060.0763\n",
      "Train Epoch: 062 Batch: 00010/00094 | Loss: 369.9757 | CE: 0.3293 | KD: 1060.1356\n",
      "Train Epoch: 062 Batch: 00011/00094 | Loss: 369.8261 | CE: 0.1807 | KD: 1060.1326\n",
      "Train Epoch: 062 Batch: 00012/00094 | Loss: 369.8708 | CE: 0.1991 | KD: 1060.2080\n",
      "Train Epoch: 062 Batch: 00013/00094 | Loss: 369.7896 | CE: 0.1739 | KD: 1060.0474\n",
      "Train Epoch: 062 Batch: 00014/00094 | Loss: 369.9020 | CE: 0.2469 | KD: 1060.1606\n",
      "Train Epoch: 062 Batch: 00015/00094 | Loss: 369.7828 | CE: 0.1950 | KD: 1059.9675\n",
      "Train Epoch: 062 Batch: 00016/00094 | Loss: 369.7543 | CE: 0.1458 | KD: 1060.0267\n",
      "Train Epoch: 062 Batch: 00017/00094 | Loss: 369.8430 | CE: 0.1856 | KD: 1060.1671\n",
      "Train Epoch: 062 Batch: 00018/00094 | Loss: 369.8358 | CE: 0.2151 | KD: 1060.0619\n",
      "Train Epoch: 062 Batch: 00019/00094 | Loss: 369.9222 | CE: 0.2841 | KD: 1060.1117\n",
      "Train Epoch: 062 Batch: 00020/00094 | Loss: 369.8346 | CE: 0.2105 | KD: 1060.0715\n",
      "Train Epoch: 062 Batch: 00021/00094 | Loss: 369.9206 | CE: 0.2549 | KD: 1060.1908\n",
      "Train Epoch: 062 Batch: 00022/00094 | Loss: 369.8430 | CE: 0.2061 | KD: 1060.1083\n",
      "Train Epoch: 062 Batch: 00023/00094 | Loss: 369.8710 | CE: 0.2442 | KD: 1060.0796\n",
      "Train Epoch: 062 Batch: 00024/00094 | Loss: 369.8686 | CE: 0.2315 | KD: 1060.1088\n",
      "Train Epoch: 062 Batch: 00025/00094 | Loss: 369.8820 | CE: 0.2204 | KD: 1060.1791\n",
      "Train Epoch: 062 Batch: 00026/00094 | Loss: 369.8210 | CE: 0.2134 | KD: 1060.0244\n",
      "Train Epoch: 062 Batch: 00027/00094 | Loss: 369.8017 | CE: 0.1670 | KD: 1060.1021\n",
      "Train Epoch: 062 Batch: 00028/00094 | Loss: 369.8821 | CE: 0.2297 | KD: 1060.1528\n",
      "Train Epoch: 062 Batch: 00029/00094 | Loss: 369.8470 | CE: 0.2439 | KD: 1060.0114\n",
      "Train Epoch: 062 Batch: 00030/00094 | Loss: 369.8698 | CE: 0.2259 | KD: 1060.1283\n",
      "Train Epoch: 062 Batch: 00031/00094 | Loss: 369.8301 | CE: 0.2190 | KD: 1060.0343\n",
      "Train Epoch: 062 Batch: 00032/00094 | Loss: 369.7542 | CE: 0.1424 | KD: 1060.0363\n",
      "Train Epoch: 062 Batch: 00033/00094 | Loss: 369.8446 | CE: 0.2547 | KD: 1059.9736\n",
      "Train Epoch: 062 Batch: 00034/00094 | Loss: 369.9576 | CE: 0.2871 | KD: 1060.2047\n",
      "Train Epoch: 062 Batch: 00035/00094 | Loss: 369.8412 | CE: 0.1906 | KD: 1060.1477\n",
      "Train Epoch: 062 Batch: 00036/00094 | Loss: 369.8000 | CE: 0.1625 | KD: 1060.1099\n",
      "Train Epoch: 062 Batch: 00037/00094 | Loss: 369.8755 | CE: 0.2201 | KD: 1060.1611\n",
      "Train Epoch: 062 Batch: 00038/00094 | Loss: 369.8397 | CE: 0.2071 | KD: 1060.0959\n",
      "Train Epoch: 062 Batch: 00039/00094 | Loss: 369.9430 | CE: 0.2809 | KD: 1060.1807\n",
      "Train Epoch: 062 Batch: 00040/00094 | Loss: 369.8448 | CE: 0.2131 | KD: 1060.0935\n",
      "Train Epoch: 062 Batch: 00041/00094 | Loss: 369.8086 | CE: 0.1935 | KD: 1060.0458\n",
      "Train Epoch: 062 Batch: 00042/00094 | Loss: 369.8430 | CE: 0.2299 | KD: 1060.0398\n",
      "Train Epoch: 062 Batch: 00043/00094 | Loss: 369.8837 | CE: 0.2687 | KD: 1060.0457\n",
      "Train Epoch: 062 Batch: 00044/00094 | Loss: 369.7852 | CE: 0.1665 | KD: 1060.0563\n",
      "Train Epoch: 062 Batch: 00045/00094 | Loss: 369.8907 | CE: 0.2370 | KD: 1060.1564\n",
      "Train Epoch: 062 Batch: 00046/00094 | Loss: 369.7897 | CE: 0.1833 | KD: 1060.0209\n",
      "Train Epoch: 062 Batch: 00047/00094 | Loss: 369.9499 | CE: 0.3320 | KD: 1060.0537\n",
      "Train Epoch: 062 Batch: 00048/00094 | Loss: 369.8301 | CE: 0.2019 | KD: 1060.0834\n",
      "Train Epoch: 062 Batch: 00049/00094 | Loss: 369.7986 | CE: 0.1980 | KD: 1060.0040\n",
      "Train Epoch: 062 Batch: 00050/00094 | Loss: 369.8072 | CE: 0.1700 | KD: 1060.1093\n",
      "Train Epoch: 062 Batch: 00051/00094 | Loss: 369.8376 | CE: 0.1870 | KD: 1060.1477\n",
      "Train Epoch: 062 Batch: 00052/00094 | Loss: 369.8870 | CE: 0.2192 | KD: 1060.1969\n",
      "Train Epoch: 062 Batch: 00053/00094 | Loss: 369.8072 | CE: 0.1473 | KD: 1060.1742\n",
      "Train Epoch: 062 Batch: 00054/00094 | Loss: 369.8237 | CE: 0.1889 | KD: 1060.1022\n",
      "Train Epoch: 062 Batch: 00055/00094 | Loss: 369.8234 | CE: 0.2157 | KD: 1060.0244\n",
      "Train Epoch: 062 Batch: 00056/00094 | Loss: 369.9347 | CE: 0.2448 | KD: 1060.2603\n",
      "Train Epoch: 062 Batch: 00057/00094 | Loss: 369.7417 | CE: 0.1439 | KD: 1059.9963\n",
      "Train Epoch: 062 Batch: 00058/00094 | Loss: 369.8326 | CE: 0.1524 | KD: 1060.2324\n",
      "Train Epoch: 062 Batch: 00059/00094 | Loss: 369.8338 | CE: 0.2063 | KD: 1060.0812\n",
      "Train Epoch: 062 Batch: 00060/00094 | Loss: 369.8431 | CE: 0.2161 | KD: 1060.0800\n",
      "Train Epoch: 062 Batch: 00061/00094 | Loss: 369.7823 | CE: 0.1788 | KD: 1060.0127\n",
      "Train Epoch: 062 Batch: 00062/00094 | Loss: 369.9708 | CE: 0.2824 | KD: 1060.2560\n",
      "Train Epoch: 062 Batch: 00063/00094 | Loss: 369.8744 | CE: 0.2511 | KD: 1060.0695\n",
      "Train Epoch: 062 Batch: 00064/00094 | Loss: 369.8413 | CE: 0.1933 | KD: 1060.1403\n",
      "Train Epoch: 062 Batch: 00065/00094 | Loss: 369.8427 | CE: 0.2143 | KD: 1060.0837\n",
      "Train Epoch: 062 Batch: 00066/00094 | Loss: 369.8826 | CE: 0.2452 | KD: 1060.1096\n",
      "Train Epoch: 062 Batch: 00067/00094 | Loss: 369.8289 | CE: 0.2003 | KD: 1060.0844\n",
      "Train Epoch: 062 Batch: 00068/00094 | Loss: 369.8977 | CE: 0.2604 | KD: 1060.1094\n",
      "Train Epoch: 062 Batch: 00069/00094 | Loss: 369.9006 | CE: 0.2779 | KD: 1060.0676\n",
      "Train Epoch: 062 Batch: 00070/00094 | Loss: 369.7900 | CE: 0.1943 | KD: 1059.9902\n",
      "Train Epoch: 062 Batch: 00071/00094 | Loss: 369.7903 | CE: 0.1745 | KD: 1060.0477\n",
      "Train Epoch: 062 Batch: 00072/00094 | Loss: 369.8129 | CE: 0.1853 | KD: 1060.0817\n",
      "Train Epoch: 062 Batch: 00073/00094 | Loss: 369.7484 | CE: 0.1346 | KD: 1060.0421\n",
      "Train Epoch: 062 Batch: 00074/00094 | Loss: 369.8713 | CE: 0.2200 | KD: 1060.1495\n",
      "Train Epoch: 062 Batch: 00075/00094 | Loss: 369.8663 | CE: 0.2653 | KD: 1060.0052\n",
      "Train Epoch: 062 Batch: 00076/00094 | Loss: 369.8144 | CE: 0.1824 | KD: 1060.0944\n",
      "Train Epoch: 062 Batch: 00077/00094 | Loss: 369.8632 | CE: 0.2225 | KD: 1060.1193\n",
      "Train Epoch: 062 Batch: 00078/00094 | Loss: 369.8003 | CE: 0.1642 | KD: 1060.1060\n",
      "Train Epoch: 062 Batch: 00079/00094 | Loss: 369.8162 | CE: 0.1861 | KD: 1060.0889\n",
      "Train Epoch: 062 Batch: 00080/00094 | Loss: 369.7886 | CE: 0.1554 | KD: 1060.0979\n",
      "Train Epoch: 062 Batch: 00081/00094 | Loss: 369.7672 | CE: 0.1687 | KD: 1059.9982\n",
      "Train Epoch: 062 Batch: 00082/00094 | Loss: 369.7480 | CE: 0.1213 | KD: 1060.0790\n",
      "Train Epoch: 062 Batch: 00083/00094 | Loss: 369.7774 | CE: 0.1498 | KD: 1060.0817\n",
      "Train Epoch: 062 Batch: 00084/00094 | Loss: 369.9372 | CE: 0.2780 | KD: 1060.1722\n",
      "Train Epoch: 062 Batch: 00085/00094 | Loss: 369.7927 | CE: 0.1476 | KD: 1060.1317\n",
      "Train Epoch: 062 Batch: 00086/00094 | Loss: 369.8433 | CE: 0.2137 | KD: 1060.0874\n",
      "Train Epoch: 062 Batch: 00087/00094 | Loss: 369.7985 | CE: 0.1886 | KD: 1060.0310\n",
      "Train Epoch: 062 Batch: 00088/00094 | Loss: 369.8267 | CE: 0.1740 | KD: 1060.1534\n",
      "Train Epoch: 062 Batch: 00089/00094 | Loss: 369.8537 | CE: 0.1843 | KD: 1060.2013\n",
      "Train Epoch: 062 Batch: 00090/00094 | Loss: 369.8499 | CE: 0.2269 | KD: 1060.0685\n",
      "Train Epoch: 062 Batch: 00091/00094 | Loss: 369.7802 | CE: 0.1622 | KD: 1060.0540\n",
      "Train Epoch: 062 Batch: 00092/00094 | Loss: 369.8495 | CE: 0.2231 | KD: 1060.0781\n",
      "Train Epoch: 062 Batch: 00093/00094 | Loss: 369.9196 | CE: 0.2596 | KD: 1060.1744\n",
      "Train Epoch: 062 Batch: 00094/00094 | Loss: 369.8314 | CE: 0.1880 | KD: 1060.1271\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2357 | acc:92.1000\n",
      "[VAL Acc] Target: 92.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2991 | acc:50.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1226 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1929 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6559 | acc:66.6928\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 66.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9067 | acc:56.7468\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 56.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5008 | acc:76.4890\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9633 | acc:56.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5900 | acc:69.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 69.15%\n",
      "[VAL Acc] Avg 62.94%\n",
      "Train Epoch: 063 Batch: 00001/00094 | Loss: 332.8983 | CE: 0.2240 | KD: 1060.1118\n",
      "Train Epoch: 063 Batch: 00002/00094 | Loss: 332.8676 | CE: 0.2120 | KD: 1060.0522\n",
      "Train Epoch: 063 Batch: 00003/00094 | Loss: 332.8696 | CE: 0.1978 | KD: 1060.1040\n",
      "Train Epoch: 063 Batch: 00004/00094 | Loss: 332.8929 | CE: 0.2261 | KD: 1060.0878\n",
      "Train Epoch: 063 Batch: 00005/00094 | Loss: 332.9406 | CE: 0.2640 | KD: 1060.1189\n",
      "Train Epoch: 063 Batch: 00006/00094 | Loss: 332.8616 | CE: 0.1908 | KD: 1060.1006\n",
      "Train Epoch: 063 Batch: 00007/00094 | Loss: 332.8642 | CE: 0.2082 | KD: 1060.0536\n",
      "Train Epoch: 063 Batch: 00008/00094 | Loss: 332.8539 | CE: 0.1895 | KD: 1060.0803\n",
      "Train Epoch: 063 Batch: 00009/00094 | Loss: 332.8968 | CE: 0.2147 | KD: 1060.1366\n",
      "Train Epoch: 063 Batch: 00010/00094 | Loss: 332.8668 | CE: 0.1949 | KD: 1060.1042\n",
      "Train Epoch: 063 Batch: 00011/00094 | Loss: 332.8733 | CE: 0.2072 | KD: 1060.0856\n",
      "Train Epoch: 063 Batch: 00012/00094 | Loss: 332.9160 | CE: 0.2474 | KD: 1060.0934\n",
      "Train Epoch: 063 Batch: 00013/00094 | Loss: 332.8802 | CE: 0.2017 | KD: 1060.1252\n",
      "Train Epoch: 063 Batch: 00014/00094 | Loss: 332.8251 | CE: 0.1907 | KD: 1059.9846\n",
      "Train Epoch: 063 Batch: 00015/00094 | Loss: 332.8827 | CE: 0.2079 | KD: 1060.1134\n",
      "Train Epoch: 063 Batch: 00016/00094 | Loss: 332.8425 | CE: 0.1692 | KD: 1060.1086\n",
      "Train Epoch: 063 Batch: 00017/00094 | Loss: 332.8502 | CE: 0.1842 | KD: 1060.0851\n",
      "Train Epoch: 063 Batch: 00018/00094 | Loss: 332.8882 | CE: 0.1959 | KD: 1060.1691\n",
      "Train Epoch: 063 Batch: 00019/00094 | Loss: 332.8652 | CE: 0.2169 | KD: 1060.0289\n",
      "Train Epoch: 063 Batch: 00020/00094 | Loss: 332.8563 | CE: 0.2246 | KD: 1059.9761\n",
      "Train Epoch: 063 Batch: 00021/00094 | Loss: 332.8634 | CE: 0.2017 | KD: 1060.0718\n",
      "Train Epoch: 063 Batch: 00022/00094 | Loss: 332.8915 | CE: 0.1982 | KD: 1060.1726\n",
      "Train Epoch: 063 Batch: 00023/00094 | Loss: 332.7947 | CE: 0.1397 | KD: 1060.0502\n",
      "Train Epoch: 063 Batch: 00024/00094 | Loss: 332.8865 | CE: 0.2166 | KD: 1060.0978\n",
      "Train Epoch: 063 Batch: 00025/00094 | Loss: 332.8388 | CE: 0.1580 | KD: 1060.1324\n",
      "Train Epoch: 063 Batch: 00026/00094 | Loss: 332.8610 | CE: 0.1992 | KD: 1060.0720\n",
      "Train Epoch: 063 Batch: 00027/00094 | Loss: 332.8831 | CE: 0.1993 | KD: 1060.1418\n",
      "Train Epoch: 063 Batch: 00028/00094 | Loss: 332.8271 | CE: 0.1741 | KD: 1060.0438\n",
      "Train Epoch: 063 Batch: 00029/00094 | Loss: 333.0255 | CE: 0.3448 | KD: 1060.1320\n",
      "Train Epoch: 063 Batch: 00030/00094 | Loss: 332.8734 | CE: 0.2268 | KD: 1060.0236\n",
      "Train Epoch: 063 Batch: 00031/00094 | Loss: 332.8006 | CE: 0.1448 | KD: 1060.0529\n",
      "Train Epoch: 063 Batch: 00032/00094 | Loss: 332.8470 | CE: 0.1785 | KD: 1060.0934\n",
      "Train Epoch: 063 Batch: 00033/00094 | Loss: 332.9120 | CE: 0.2415 | KD: 1060.0999\n",
      "Train Epoch: 063 Batch: 00034/00094 | Loss: 332.7643 | CE: 0.1191 | KD: 1060.0193\n",
      "Train Epoch: 063 Batch: 00035/00094 | Loss: 332.9506 | CE: 0.2179 | KD: 1060.2977\n",
      "Train Epoch: 063 Batch: 00036/00094 | Loss: 333.0122 | CE: 0.2975 | KD: 1060.2406\n",
      "Train Epoch: 063 Batch: 00037/00094 | Loss: 332.8958 | CE: 0.2031 | KD: 1060.1702\n",
      "Train Epoch: 063 Batch: 00038/00094 | Loss: 332.8775 | CE: 0.2284 | KD: 1060.0316\n",
      "Train Epoch: 063 Batch: 00039/00094 | Loss: 332.8267 | CE: 0.1703 | KD: 1060.0548\n",
      "Train Epoch: 063 Batch: 00040/00094 | Loss: 332.8489 | CE: 0.1856 | KD: 1060.0767\n",
      "Train Epoch: 063 Batch: 00041/00094 | Loss: 332.8092 | CE: 0.1577 | KD: 1060.0393\n",
      "Train Epoch: 063 Batch: 00042/00094 | Loss: 332.9174 | CE: 0.2396 | KD: 1060.1232\n",
      "Train Epoch: 063 Batch: 00043/00094 | Loss: 332.8114 | CE: 0.1711 | KD: 1060.0035\n",
      "Train Epoch: 063 Batch: 00044/00094 | Loss: 332.8690 | CE: 0.1936 | KD: 1060.1154\n",
      "Train Epoch: 063 Batch: 00045/00094 | Loss: 332.8189 | CE: 0.1676 | KD: 1060.0387\n",
      "Train Epoch: 063 Batch: 00046/00094 | Loss: 332.9262 | CE: 0.2393 | KD: 1060.1521\n",
      "Train Epoch: 063 Batch: 00047/00094 | Loss: 332.8056 | CE: 0.1256 | KD: 1060.1298\n",
      "Train Epoch: 063 Batch: 00048/00094 | Loss: 332.9065 | CE: 0.2147 | KD: 1060.1676\n",
      "Train Epoch: 063 Batch: 00049/00094 | Loss: 332.8860 | CE: 0.1972 | KD: 1060.1582\n",
      "Train Epoch: 063 Batch: 00050/00094 | Loss: 332.8567 | CE: 0.2089 | KD: 1060.0273\n",
      "Train Epoch: 063 Batch: 00051/00094 | Loss: 332.8547 | CE: 0.1835 | KD: 1060.1019\n",
      "Train Epoch: 063 Batch: 00052/00094 | Loss: 332.8682 | CE: 0.1713 | KD: 1060.1840\n",
      "Train Epoch: 063 Batch: 00053/00094 | Loss: 332.8553 | CE: 0.1911 | KD: 1060.0797\n",
      "Train Epoch: 063 Batch: 00054/00094 | Loss: 332.8540 | CE: 0.1825 | KD: 1060.1030\n",
      "Train Epoch: 063 Batch: 00055/00094 | Loss: 332.8557 | CE: 0.1808 | KD: 1060.1139\n",
      "Train Epoch: 063 Batch: 00056/00094 | Loss: 332.8245 | CE: 0.1689 | KD: 1060.0522\n",
      "Train Epoch: 063 Batch: 00057/00094 | Loss: 332.9477 | CE: 0.2465 | KD: 1060.1975\n",
      "Train Epoch: 063 Batch: 00058/00094 | Loss: 332.8521 | CE: 0.1967 | KD: 1060.0515\n",
      "Train Epoch: 063 Batch: 00059/00094 | Loss: 332.9038 | CE: 0.2248 | KD: 1060.1266\n",
      "Train Epoch: 063 Batch: 00060/00094 | Loss: 333.0474 | CE: 0.3531 | KD: 1060.1755\n",
      "Train Epoch: 063 Batch: 00061/00094 | Loss: 332.8918 | CE: 0.2018 | KD: 1060.1615\n",
      "Train Epoch: 063 Batch: 00062/00094 | Loss: 332.8833 | CE: 0.2125 | KD: 1060.1005\n",
      "Train Epoch: 063 Batch: 00063/00094 | Loss: 332.9091 | CE: 0.2359 | KD: 1060.1085\n",
      "Train Epoch: 063 Batch: 00064/00094 | Loss: 332.8414 | CE: 0.1857 | KD: 1060.0526\n",
      "Train Epoch: 063 Batch: 00065/00094 | Loss: 332.8235 | CE: 0.1780 | KD: 1060.0201\n",
      "Train Epoch: 063 Batch: 00066/00094 | Loss: 332.8958 | CE: 0.2230 | KD: 1060.1069\n",
      "Train Epoch: 063 Batch: 00067/00094 | Loss: 332.9182 | CE: 0.2410 | KD: 1060.1212\n",
      "Train Epoch: 063 Batch: 00068/00094 | Loss: 332.7671 | CE: 0.1152 | KD: 1060.0402\n",
      "Train Epoch: 063 Batch: 00069/00094 | Loss: 332.8330 | CE: 0.1861 | KD: 1060.0244\n",
      "Train Epoch: 063 Batch: 00070/00094 | Loss: 332.8785 | CE: 0.1639 | KD: 1060.2402\n",
      "Train Epoch: 063 Batch: 00071/00094 | Loss: 332.8159 | CE: 0.1698 | KD: 1060.0219\n",
      "Train Epoch: 063 Batch: 00072/00094 | Loss: 332.8210 | CE: 0.1658 | KD: 1060.0509\n",
      "Train Epoch: 063 Batch: 00073/00094 | Loss: 332.8386 | CE: 0.1828 | KD: 1060.0527\n",
      "Train Epoch: 063 Batch: 00074/00094 | Loss: 332.9914 | CE: 0.2509 | KD: 1060.3230\n",
      "Train Epoch: 063 Batch: 00075/00094 | Loss: 332.9733 | CE: 0.2646 | KD: 1060.2213\n",
      "Train Epoch: 063 Batch: 00076/00094 | Loss: 332.9830 | CE: 0.2980 | KD: 1060.1458\n",
      "Train Epoch: 063 Batch: 00077/00094 | Loss: 332.8742 | CE: 0.2150 | KD: 1060.0637\n",
      "Train Epoch: 063 Batch: 00078/00094 | Loss: 332.8900 | CE: 0.1880 | KD: 1060.2001\n",
      "Train Epoch: 063 Batch: 00079/00094 | Loss: 332.8396 | CE: 0.1670 | KD: 1060.1066\n",
      "Train Epoch: 063 Batch: 00080/00094 | Loss: 332.8061 | CE: 0.1697 | KD: 1059.9907\n",
      "Train Epoch: 063 Batch: 00081/00094 | Loss: 332.9076 | CE: 0.2447 | KD: 1060.0754\n",
      "Train Epoch: 063 Batch: 00082/00094 | Loss: 332.8465 | CE: 0.1773 | KD: 1060.0955\n",
      "Train Epoch: 063 Batch: 00083/00094 | Loss: 332.8996 | CE: 0.2106 | KD: 1060.1587\n",
      "Train Epoch: 063 Batch: 00084/00094 | Loss: 332.8816 | CE: 0.2076 | KD: 1060.1108\n",
      "Train Epoch: 063 Batch: 00085/00094 | Loss: 332.8810 | CE: 0.2090 | KD: 1060.1044\n",
      "Train Epoch: 063 Batch: 00086/00094 | Loss: 332.9258 | CE: 0.2466 | KD: 1060.1272\n",
      "Train Epoch: 063 Batch: 00087/00094 | Loss: 332.7991 | CE: 0.1327 | KD: 1060.0867\n",
      "Train Epoch: 063 Batch: 00088/00094 | Loss: 332.8405 | CE: 0.1876 | KD: 1060.0437\n",
      "Train Epoch: 063 Batch: 00089/00094 | Loss: 332.8801 | CE: 0.1962 | KD: 1060.1423\n",
      "Train Epoch: 063 Batch: 00090/00094 | Loss: 332.8416 | CE: 0.1839 | KD: 1060.0590\n",
      "Train Epoch: 063 Batch: 00091/00094 | Loss: 332.9167 | CE: 0.2426 | KD: 1060.1110\n",
      "Train Epoch: 063 Batch: 00092/00094 | Loss: 332.8557 | CE: 0.1875 | KD: 1060.0924\n",
      "Train Epoch: 063 Batch: 00093/00094 | Loss: 332.9082 | CE: 0.2503 | KD: 1060.0594\n",
      "Train Epoch: 063 Batch: 00094/00094 | Loss: 332.8510 | CE: 0.1844 | KD: 1060.0872\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2328 | acc:91.7500\n",
      "[VAL Acc] Target: 91.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2421 | acc:49.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1096 | acc:49.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1428 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6842 | acc:64.6160\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 64.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.1887 | acc:52.2181\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5472 | acc:73.7069\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1270 | acc:54.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5224 | acc:73.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.20%\n",
      "[VAL Acc] Avg 62.04%\n",
      "Train Epoch: 064 Batch: 00001/00094 | Loss: 332.8638 | CE: 0.1866 | KD: 1060.1211\n",
      "Train Epoch: 064 Batch: 00002/00094 | Loss: 332.8990 | CE: 0.2146 | KD: 1060.1442\n",
      "Train Epoch: 064 Batch: 00003/00094 | Loss: 332.7675 | CE: 0.1220 | KD: 1060.0200\n",
      "Train Epoch: 064 Batch: 00004/00094 | Loss: 332.9126 | CE: 0.2263 | KD: 1060.1498\n",
      "Train Epoch: 064 Batch: 00005/00094 | Loss: 332.8685 | CE: 0.2044 | KD: 1060.0792\n",
      "Train Epoch: 064 Batch: 00006/00094 | Loss: 332.8567 | CE: 0.1959 | KD: 1060.0687\n",
      "Train Epoch: 064 Batch: 00007/00094 | Loss: 332.8062 | CE: 0.1580 | KD: 1060.0287\n",
      "Train Epoch: 064 Batch: 00008/00094 | Loss: 332.8312 | CE: 0.1660 | KD: 1060.0828\n",
      "Train Epoch: 064 Batch: 00009/00094 | Loss: 332.8719 | CE: 0.2086 | KD: 1060.0769\n",
      "Train Epoch: 064 Batch: 00010/00094 | Loss: 332.8473 | CE: 0.1929 | KD: 1060.0481\n",
      "Train Epoch: 064 Batch: 00011/00094 | Loss: 332.8964 | CE: 0.2093 | KD: 1060.1525\n",
      "Train Epoch: 064 Batch: 00012/00094 | Loss: 332.9380 | CE: 0.2680 | KD: 1060.0980\n",
      "Train Epoch: 064 Batch: 00013/00094 | Loss: 332.8736 | CE: 0.1882 | KD: 1060.1473\n",
      "Train Epoch: 064 Batch: 00014/00094 | Loss: 332.8800 | CE: 0.1977 | KD: 1060.1375\n",
      "Train Epoch: 064 Batch: 00015/00094 | Loss: 332.8351 | CE: 0.1774 | KD: 1060.0590\n",
      "Train Epoch: 064 Batch: 00016/00094 | Loss: 332.9750 | CE: 0.2860 | KD: 1060.1587\n",
      "Train Epoch: 064 Batch: 00017/00094 | Loss: 332.8309 | CE: 0.1700 | KD: 1060.0691\n",
      "Train Epoch: 064 Batch: 00018/00094 | Loss: 332.9019 | CE: 0.2287 | KD: 1060.1083\n",
      "Train Epoch: 064 Batch: 00019/00094 | Loss: 332.8249 | CE: 0.1407 | KD: 1060.1432\n",
      "Train Epoch: 064 Batch: 00020/00094 | Loss: 332.8600 | CE: 0.1682 | KD: 1060.1674\n",
      "Train Epoch: 064 Batch: 00021/00094 | Loss: 332.8271 | CE: 0.1648 | KD: 1060.0737\n",
      "Train Epoch: 064 Batch: 00022/00094 | Loss: 332.8176 | CE: 0.1572 | KD: 1060.0675\n",
      "Train Epoch: 064 Batch: 00023/00094 | Loss: 332.9473 | CE: 0.2580 | KD: 1060.1598\n",
      "Train Epoch: 064 Batch: 00024/00094 | Loss: 332.9322 | CE: 0.2674 | KD: 1060.0813\n",
      "Train Epoch: 064 Batch: 00025/00094 | Loss: 332.8572 | CE: 0.1858 | KD: 1060.1027\n",
      "Train Epoch: 064 Batch: 00026/00094 | Loss: 332.8249 | CE: 0.1697 | KD: 1060.0509\n",
      "Train Epoch: 064 Batch: 00027/00094 | Loss: 332.9050 | CE: 0.2468 | KD: 1060.0607\n",
      "Train Epoch: 064 Batch: 00028/00094 | Loss: 332.8716 | CE: 0.2003 | KD: 1060.1025\n",
      "Train Epoch: 064 Batch: 00029/00094 | Loss: 332.8202 | CE: 0.1705 | KD: 1060.0334\n",
      "Train Epoch: 064 Batch: 00030/00094 | Loss: 332.8845 | CE: 0.1955 | KD: 1060.1586\n",
      "Train Epoch: 064 Batch: 00031/00094 | Loss: 332.8826 | CE: 0.2018 | KD: 1060.1323\n",
      "Train Epoch: 064 Batch: 00032/00094 | Loss: 332.8567 | CE: 0.1975 | KD: 1060.0635\n",
      "Train Epoch: 064 Batch: 00033/00094 | Loss: 332.7996 | CE: 0.1376 | KD: 1060.0728\n",
      "Train Epoch: 064 Batch: 00034/00094 | Loss: 332.8813 | CE: 0.2185 | KD: 1060.0750\n",
      "Train Epoch: 064 Batch: 00035/00094 | Loss: 332.9306 | CE: 0.2441 | KD: 1060.1506\n",
      "Train Epoch: 064 Batch: 00036/00094 | Loss: 332.8168 | CE: 0.1509 | KD: 1060.0851\n",
      "Train Epoch: 064 Batch: 00037/00094 | Loss: 332.8471 | CE: 0.1369 | KD: 1060.2262\n",
      "Train Epoch: 064 Batch: 00038/00094 | Loss: 332.8195 | CE: 0.1497 | KD: 1060.0975\n",
      "Train Epoch: 064 Batch: 00039/00094 | Loss: 332.9638 | CE: 0.2470 | KD: 1060.2473\n",
      "Train Epoch: 064 Batch: 00040/00094 | Loss: 332.8899 | CE: 0.2115 | KD: 1060.1248\n",
      "Train Epoch: 064 Batch: 00041/00094 | Loss: 332.8520 | CE: 0.1785 | KD: 1060.1090\n",
      "Train Epoch: 064 Batch: 00042/00094 | Loss: 332.8817 | CE: 0.2030 | KD: 1060.1259\n",
      "Train Epoch: 064 Batch: 00043/00094 | Loss: 332.8867 | CE: 0.2224 | KD: 1060.0798\n",
      "Train Epoch: 064 Batch: 00044/00094 | Loss: 332.8242 | CE: 0.1671 | KD: 1060.0570\n",
      "Train Epoch: 064 Batch: 00045/00094 | Loss: 332.9723 | CE: 0.2850 | KD: 1060.1532\n",
      "Train Epoch: 064 Batch: 00046/00094 | Loss: 332.8064 | CE: 0.1454 | KD: 1060.0695\n",
      "Train Epoch: 064 Batch: 00047/00094 | Loss: 332.7557 | CE: 0.1083 | KD: 1060.0262\n",
      "Train Epoch: 064 Batch: 00048/00094 | Loss: 332.9053 | CE: 0.2157 | KD: 1060.1605\n",
      "Train Epoch: 064 Batch: 00049/00094 | Loss: 332.8552 | CE: 0.1906 | KD: 1060.0809\n",
      "Train Epoch: 064 Batch: 00050/00094 | Loss: 332.8549 | CE: 0.1654 | KD: 1060.1603\n",
      "Train Epoch: 064 Batch: 00051/00094 | Loss: 332.7653 | CE: 0.1472 | KD: 1059.9327\n",
      "Train Epoch: 064 Batch: 00052/00094 | Loss: 332.9348 | CE: 0.2361 | KD: 1060.1893\n",
      "Train Epoch: 064 Batch: 00053/00094 | Loss: 332.7949 | CE: 0.1363 | KD: 1060.0616\n",
      "Train Epoch: 064 Batch: 00054/00094 | Loss: 332.8327 | CE: 0.1605 | KD: 1060.1051\n",
      "Train Epoch: 064 Batch: 00055/00094 | Loss: 332.8636 | CE: 0.1835 | KD: 1060.1301\n",
      "Train Epoch: 064 Batch: 00056/00094 | Loss: 332.8021 | CE: 0.1499 | KD: 1060.0413\n",
      "Train Epoch: 064 Batch: 00057/00094 | Loss: 332.9505 | CE: 0.2677 | KD: 1060.1388\n",
      "Train Epoch: 064 Batch: 00058/00094 | Loss: 332.8984 | CE: 0.2313 | KD: 1060.0890\n",
      "Train Epoch: 064 Batch: 00059/00094 | Loss: 332.8871 | CE: 0.2213 | KD: 1060.0847\n",
      "Train Epoch: 064 Batch: 00060/00094 | Loss: 332.9165 | CE: 0.2455 | KD: 1060.1013\n",
      "Train Epoch: 064 Batch: 00061/00094 | Loss: 332.7811 | CE: 0.1442 | KD: 1059.9928\n",
      "Train Epoch: 064 Batch: 00062/00094 | Loss: 332.9715 | CE: 0.2927 | KD: 1060.1263\n",
      "Train Epoch: 064 Batch: 00063/00094 | Loss: 332.9825 | CE: 0.2413 | KD: 1060.3251\n",
      "Train Epoch: 064 Batch: 00064/00094 | Loss: 332.9396 | CE: 0.2570 | KD: 1060.1384\n",
      "Train Epoch: 064 Batch: 00065/00094 | Loss: 332.9813 | CE: 0.2717 | KD: 1060.2242\n",
      "Train Epoch: 064 Batch: 00066/00094 | Loss: 332.9039 | CE: 0.2473 | KD: 1060.0553\n",
      "Train Epoch: 064 Batch: 00067/00094 | Loss: 332.9081 | CE: 0.2076 | KD: 1060.1953\n",
      "Train Epoch: 064 Batch: 00068/00094 | Loss: 332.8415 | CE: 0.1813 | KD: 1060.0669\n",
      "Train Epoch: 064 Batch: 00069/00094 | Loss: 332.8861 | CE: 0.1863 | KD: 1060.1930\n",
      "Train Epoch: 064 Batch: 00070/00094 | Loss: 332.8581 | CE: 0.1716 | KD: 1060.1508\n",
      "Train Epoch: 064 Batch: 00071/00094 | Loss: 332.8468 | CE: 0.1622 | KD: 1060.1448\n",
      "Train Epoch: 064 Batch: 00072/00094 | Loss: 332.8735 | CE: 0.2113 | KD: 1060.0735\n",
      "Train Epoch: 064 Batch: 00073/00094 | Loss: 332.8073 | CE: 0.1398 | KD: 1060.0900\n",
      "Train Epoch: 064 Batch: 00074/00094 | Loss: 332.8394 | CE: 0.1933 | KD: 1060.0217\n",
      "Train Epoch: 064 Batch: 00075/00094 | Loss: 332.8540 | CE: 0.1774 | KD: 1060.1193\n",
      "Train Epoch: 064 Batch: 00076/00094 | Loss: 332.8700 | CE: 0.1809 | KD: 1060.1591\n",
      "Train Epoch: 064 Batch: 00077/00094 | Loss: 332.8806 | CE: 0.2238 | KD: 1060.0560\n",
      "Train Epoch: 064 Batch: 00078/00094 | Loss: 332.8808 | CE: 0.2406 | KD: 1060.0031\n",
      "Train Epoch: 064 Batch: 00079/00094 | Loss: 332.8661 | CE: 0.2043 | KD: 1060.0719\n",
      "Train Epoch: 064 Batch: 00080/00094 | Loss: 332.8312 | CE: 0.1737 | KD: 1060.0581\n",
      "Train Epoch: 064 Batch: 00081/00094 | Loss: 332.8317 | CE: 0.1645 | KD: 1060.0890\n",
      "Train Epoch: 064 Batch: 00082/00094 | Loss: 332.8723 | CE: 0.2183 | KD: 1060.0474\n",
      "Train Epoch: 064 Batch: 00083/00094 | Loss: 332.8169 | CE: 0.1544 | KD: 1060.0741\n",
      "Train Epoch: 064 Batch: 00084/00094 | Loss: 332.9060 | CE: 0.2513 | KD: 1060.0493\n",
      "Train Epoch: 064 Batch: 00085/00094 | Loss: 332.9147 | CE: 0.2331 | KD: 1060.1350\n",
      "Train Epoch: 064 Batch: 00086/00094 | Loss: 332.8625 | CE: 0.2084 | KD: 1060.0475\n",
      "Train Epoch: 064 Batch: 00087/00094 | Loss: 332.9305 | CE: 0.2447 | KD: 1060.1483\n",
      "Train Epoch: 064 Batch: 00088/00094 | Loss: 332.8252 | CE: 0.1781 | KD: 1060.0253\n",
      "Train Epoch: 064 Batch: 00089/00094 | Loss: 332.8497 | CE: 0.1769 | KD: 1060.1072\n",
      "Train Epoch: 064 Batch: 00090/00094 | Loss: 332.8262 | CE: 0.1545 | KD: 1060.1033\n",
      "Train Epoch: 064 Batch: 00091/00094 | Loss: 332.8345 | CE: 0.1762 | KD: 1060.0608\n",
      "Train Epoch: 064 Batch: 00092/00094 | Loss: 332.9722 | CE: 0.2738 | KD: 1060.1885\n",
      "Train Epoch: 064 Batch: 00093/00094 | Loss: 332.7769 | CE: 0.1365 | KD: 1060.0035\n",
      "Train Epoch: 064 Batch: 00094/00094 | Loss: 332.8418 | CE: 0.1892 | KD: 1060.0426\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2221 | acc:92.5500\n",
      "[VAL Acc] Target: 92.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2727 | acc:49.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1323 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2242 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7552 | acc:62.8135\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0184 | acc:54.2514\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5449 | acc:72.8448\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 72.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0048 | acc:56.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5782 | acc:69.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 69.45%\n",
      "[VAL Acc] Avg 61.80%\n",
      "Train Epoch: 065 Batch: 00001/00094 | Loss: 332.9013 | CE: 0.2060 | KD: 1060.1787\n",
      "Train Epoch: 065 Batch: 00002/00094 | Loss: 332.8943 | CE: 0.2030 | KD: 1060.1658\n",
      "Train Epoch: 065 Batch: 00003/00094 | Loss: 332.8238 | CE: 0.1539 | KD: 1060.0978\n",
      "Train Epoch: 065 Batch: 00004/00094 | Loss: 332.8278 | CE: 0.1249 | KD: 1060.2028\n",
      "Train Epoch: 065 Batch: 00005/00094 | Loss: 332.8623 | CE: 0.1749 | KD: 1060.1536\n",
      "Train Epoch: 065 Batch: 00006/00094 | Loss: 332.8315 | CE: 0.1518 | KD: 1060.1292\n",
      "Train Epoch: 065 Batch: 00007/00094 | Loss: 332.9497 | CE: 0.2486 | KD: 1060.1970\n",
      "Train Epoch: 065 Batch: 00008/00094 | Loss: 332.9418 | CE: 0.2302 | KD: 1060.2305\n",
      "Train Epoch: 065 Batch: 00009/00094 | Loss: 332.8332 | CE: 0.1738 | KD: 1060.0641\n",
      "Train Epoch: 065 Batch: 00010/00094 | Loss: 332.8943 | CE: 0.2161 | KD: 1060.1239\n",
      "Train Epoch: 065 Batch: 00011/00094 | Loss: 332.9347 | CE: 0.1969 | KD: 1060.3142\n",
      "Train Epoch: 065 Batch: 00012/00094 | Loss: 332.8811 | CE: 0.2041 | KD: 1060.1205\n",
      "Train Epoch: 065 Batch: 00013/00094 | Loss: 332.7775 | CE: 0.1421 | KD: 1059.9878\n",
      "Train Epoch: 065 Batch: 00014/00094 | Loss: 332.8260 | CE: 0.1606 | KD: 1060.0837\n",
      "Train Epoch: 065 Batch: 00015/00094 | Loss: 332.8180 | CE: 0.1437 | KD: 1060.1117\n",
      "Train Epoch: 065 Batch: 00016/00094 | Loss: 332.8675 | CE: 0.2019 | KD: 1060.0840\n",
      "Train Epoch: 065 Batch: 00017/00094 | Loss: 332.8714 | CE: 0.2058 | KD: 1060.0840\n",
      "Train Epoch: 065 Batch: 00018/00094 | Loss: 332.7881 | CE: 0.1390 | KD: 1060.0314\n",
      "Train Epoch: 065 Batch: 00019/00094 | Loss: 332.8707 | CE: 0.1950 | KD: 1060.1163\n",
      "Train Epoch: 065 Batch: 00020/00094 | Loss: 332.7611 | CE: 0.1131 | KD: 1060.0280\n",
      "Train Epoch: 065 Batch: 00021/00094 | Loss: 332.8409 | CE: 0.1865 | KD: 1060.0486\n",
      "Train Epoch: 065 Batch: 00022/00094 | Loss: 332.9480 | CE: 0.2577 | KD: 1060.1628\n",
      "Train Epoch: 065 Batch: 00023/00094 | Loss: 332.8722 | CE: 0.1717 | KD: 1060.1953\n",
      "Train Epoch: 065 Batch: 00024/00094 | Loss: 332.8324 | CE: 0.1500 | KD: 1060.1375\n",
      "Train Epoch: 065 Batch: 00025/00094 | Loss: 332.8217 | CE: 0.1800 | KD: 1060.0081\n",
      "Train Epoch: 065 Batch: 00026/00094 | Loss: 332.8248 | CE: 0.1617 | KD: 1060.0759\n",
      "Train Epoch: 065 Batch: 00027/00094 | Loss: 332.8842 | CE: 0.1800 | KD: 1060.2069\n",
      "Train Epoch: 065 Batch: 00028/00094 | Loss: 332.8891 | CE: 0.2050 | KD: 1060.1432\n",
      "Train Epoch: 065 Batch: 00029/00094 | Loss: 332.8777 | CE: 0.2023 | KD: 1060.1154\n",
      "Train Epoch: 065 Batch: 00030/00094 | Loss: 332.8351 | CE: 0.1855 | KD: 1060.0330\n",
      "Train Epoch: 065 Batch: 00031/00094 | Loss: 332.8777 | CE: 0.2125 | KD: 1060.0828\n",
      "Train Epoch: 065 Batch: 00032/00094 | Loss: 332.8148 | CE: 0.1673 | KD: 1060.0264\n",
      "Train Epoch: 065 Batch: 00033/00094 | Loss: 332.8386 | CE: 0.1719 | KD: 1060.0873\n",
      "Train Epoch: 065 Batch: 00034/00094 | Loss: 332.9098 | CE: 0.2038 | KD: 1060.2130\n",
      "Train Epoch: 065 Batch: 00035/00094 | Loss: 332.9133 | CE: 0.1924 | KD: 1060.2603\n",
      "Train Epoch: 065 Batch: 00036/00094 | Loss: 332.8652 | CE: 0.1810 | KD: 1060.1433\n",
      "Train Epoch: 065 Batch: 00037/00094 | Loss: 333.0870 | CE: 0.3859 | KD: 1060.1971\n",
      "Train Epoch: 065 Batch: 00038/00094 | Loss: 332.8371 | CE: 0.1651 | KD: 1060.1044\n",
      "Train Epoch: 065 Batch: 00039/00094 | Loss: 332.8589 | CE: 0.1884 | KD: 1060.0997\n",
      "Train Epoch: 065 Batch: 00040/00094 | Loss: 332.8952 | CE: 0.2122 | KD: 1060.1395\n",
      "Train Epoch: 065 Batch: 00041/00094 | Loss: 332.8557 | CE: 0.2011 | KD: 1060.0493\n",
      "Train Epoch: 065 Batch: 00042/00094 | Loss: 332.8763 | CE: 0.1928 | KD: 1060.1412\n",
      "Train Epoch: 065 Batch: 00043/00094 | Loss: 332.8525 | CE: 0.1801 | KD: 1060.1055\n",
      "Train Epoch: 065 Batch: 00044/00094 | Loss: 332.9153 | CE: 0.2087 | KD: 1060.2147\n",
      "Train Epoch: 065 Batch: 00045/00094 | Loss: 332.8217 | CE: 0.1744 | KD: 1060.0256\n",
      "Train Epoch: 065 Batch: 00046/00094 | Loss: 332.8854 | CE: 0.1927 | KD: 1060.1704\n",
      "Train Epoch: 065 Batch: 00047/00094 | Loss: 332.9153 | CE: 0.2253 | KD: 1060.1620\n",
      "Train Epoch: 065 Batch: 00048/00094 | Loss: 332.8711 | CE: 0.2104 | KD: 1060.0685\n",
      "Train Epoch: 065 Batch: 00049/00094 | Loss: 332.7818 | CE: 0.1382 | KD: 1060.0139\n",
      "Train Epoch: 065 Batch: 00050/00094 | Loss: 332.8826 | CE: 0.2090 | KD: 1060.1096\n",
      "Train Epoch: 065 Batch: 00051/00094 | Loss: 332.7965 | CE: 0.1483 | KD: 1060.0286\n",
      "Train Epoch: 065 Batch: 00052/00094 | Loss: 332.8216 | CE: 0.1398 | KD: 1060.1356\n",
      "Train Epoch: 065 Batch: 00053/00094 | Loss: 332.7569 | CE: 0.1053 | KD: 1060.0394\n",
      "Train Epoch: 065 Batch: 00054/00094 | Loss: 332.8793 | CE: 0.2051 | KD: 1060.1116\n",
      "Train Epoch: 065 Batch: 00055/00094 | Loss: 332.7932 | CE: 0.1494 | KD: 1060.0144\n",
      "Train Epoch: 065 Batch: 00056/00094 | Loss: 332.8788 | CE: 0.1860 | KD: 1060.1709\n",
      "Train Epoch: 065 Batch: 00057/00094 | Loss: 332.8166 | CE: 0.1350 | KD: 1060.1350\n",
      "Train Epoch: 065 Batch: 00058/00094 | Loss: 332.8581 | CE: 0.1884 | KD: 1060.0972\n",
      "Train Epoch: 065 Batch: 00059/00094 | Loss: 332.9382 | CE: 0.2530 | KD: 1060.1464\n",
      "Train Epoch: 065 Batch: 00060/00094 | Loss: 332.7863 | CE: 0.1541 | KD: 1059.9777\n",
      "Train Epoch: 065 Batch: 00061/00094 | Loss: 332.8969 | CE: 0.2417 | KD: 1060.0508\n",
      "Train Epoch: 065 Batch: 00062/00094 | Loss: 332.8701 | CE: 0.1837 | KD: 1060.1506\n",
      "Train Epoch: 065 Batch: 00063/00094 | Loss: 332.8112 | CE: 0.1476 | KD: 1060.0775\n",
      "Train Epoch: 065 Batch: 00064/00094 | Loss: 332.9090 | CE: 0.2229 | KD: 1060.1493\n",
      "Train Epoch: 065 Batch: 00065/00094 | Loss: 332.7749 | CE: 0.1295 | KD: 1060.0198\n",
      "Train Epoch: 065 Batch: 00066/00094 | Loss: 332.8776 | CE: 0.1950 | KD: 1060.1383\n",
      "Train Epoch: 065 Batch: 00067/00094 | Loss: 332.8220 | CE: 0.1286 | KD: 1060.1725\n",
      "Train Epoch: 065 Batch: 00068/00094 | Loss: 332.8330 | CE: 0.1687 | KD: 1060.0798\n",
      "Train Epoch: 065 Batch: 00069/00094 | Loss: 332.8308 | CE: 0.1717 | KD: 1060.0631\n",
      "Train Epoch: 065 Batch: 00070/00094 | Loss: 332.9052 | CE: 0.2377 | KD: 1060.0901\n",
      "Train Epoch: 065 Batch: 00071/00094 | Loss: 332.8968 | CE: 0.2255 | KD: 1060.1023\n",
      "Train Epoch: 065 Batch: 00072/00094 | Loss: 332.8579 | CE: 0.1662 | KD: 1060.1671\n",
      "Train Epoch: 065 Batch: 00073/00094 | Loss: 332.8455 | CE: 0.1544 | KD: 1060.1652\n",
      "Train Epoch: 065 Batch: 00074/00094 | Loss: 332.7797 | CE: 0.1222 | KD: 1060.0582\n",
      "Train Epoch: 065 Batch: 00075/00094 | Loss: 332.8601 | CE: 0.1770 | KD: 1060.1398\n",
      "Train Epoch: 065 Batch: 00076/00094 | Loss: 332.8777 | CE: 0.1993 | KD: 1060.1248\n",
      "Train Epoch: 065 Batch: 00077/00094 | Loss: 332.9637 | CE: 0.2589 | KD: 1060.2087\n",
      "Train Epoch: 065 Batch: 00078/00094 | Loss: 332.8854 | CE: 0.2005 | KD: 1060.1456\n",
      "Train Epoch: 065 Batch: 00079/00094 | Loss: 332.8114 | CE: 0.1243 | KD: 1060.1525\n",
      "Train Epoch: 065 Batch: 00080/00094 | Loss: 332.8479 | CE: 0.2025 | KD: 1060.0198\n",
      "Train Epoch: 065 Batch: 00081/00094 | Loss: 332.8771 | CE: 0.1784 | KD: 1060.1895\n",
      "Train Epoch: 065 Batch: 00082/00094 | Loss: 332.8927 | CE: 0.2457 | KD: 1060.0245\n",
      "Train Epoch: 065 Batch: 00083/00094 | Loss: 332.9684 | CE: 0.2898 | KD: 1060.1255\n",
      "Train Epoch: 065 Batch: 00084/00094 | Loss: 332.8980 | CE: 0.2108 | KD: 1060.1530\n",
      "Train Epoch: 065 Batch: 00085/00094 | Loss: 332.8644 | CE: 0.2167 | KD: 1060.0271\n",
      "Train Epoch: 065 Batch: 00086/00094 | Loss: 332.9200 | CE: 0.2309 | KD: 1060.1589\n",
      "Train Epoch: 065 Batch: 00087/00094 | Loss: 332.8902 | CE: 0.2229 | KD: 1060.0895\n",
      "Train Epoch: 065 Batch: 00088/00094 | Loss: 332.8778 | CE: 0.2311 | KD: 1060.0237\n",
      "Train Epoch: 065 Batch: 00089/00094 | Loss: 332.7979 | CE: 0.1531 | KD: 1060.0177\n",
      "Train Epoch: 065 Batch: 00090/00094 | Loss: 332.8652 | CE: 0.2053 | KD: 1060.0659\n",
      "Train Epoch: 065 Batch: 00091/00094 | Loss: 332.8598 | CE: 0.1991 | KD: 1060.0685\n",
      "Train Epoch: 065 Batch: 00092/00094 | Loss: 332.8548 | CE: 0.1898 | KD: 1060.0820\n",
      "Train Epoch: 065 Batch: 00093/00094 | Loss: 332.8333 | CE: 0.1660 | KD: 1060.0896\n",
      "Train Epoch: 065 Batch: 00094/00094 | Loss: 332.8321 | CE: 0.1750 | KD: 1060.0568\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2117 | acc:92.8000\n",
      "[VAL Acc] Target: 92.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3108 | acc:50.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1517 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2566 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7125 | acc:63.9890\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 63.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9324 | acc:55.8226\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5481 | acc:75.2743\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9430 | acc:57.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5964 | acc:68.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 68.35%\n",
      "[VAL Acc] Avg 62.56%\n",
      "Train Epoch: 066 Batch: 00001/00094 | Loss: 332.8359 | CE: 0.1569 | KD: 1060.1266\n",
      "Train Epoch: 066 Batch: 00002/00094 | Loss: 332.7871 | CE: 0.1372 | KD: 1060.0339\n",
      "Train Epoch: 066 Batch: 00003/00094 | Loss: 332.9058 | CE: 0.2400 | KD: 1060.0848\n",
      "Train Epoch: 066 Batch: 00004/00094 | Loss: 332.8805 | CE: 0.1934 | KD: 1060.1526\n",
      "Train Epoch: 066 Batch: 00005/00094 | Loss: 332.8369 | CE: 0.1740 | KD: 1060.0753\n",
      "Train Epoch: 066 Batch: 00006/00094 | Loss: 332.8297 | CE: 0.1487 | KD: 1060.1332\n",
      "Train Epoch: 066 Batch: 00007/00094 | Loss: 332.9193 | CE: 0.2259 | KD: 1060.1724\n",
      "Train Epoch: 066 Batch: 00008/00094 | Loss: 332.7994 | CE: 0.1488 | KD: 1060.0364\n",
      "Train Epoch: 066 Batch: 00009/00094 | Loss: 332.8493 | CE: 0.1792 | KD: 1060.0984\n",
      "Train Epoch: 066 Batch: 00010/00094 | Loss: 332.8401 | CE: 0.1617 | KD: 1060.1251\n",
      "Train Epoch: 066 Batch: 00011/00094 | Loss: 332.8196 | CE: 0.1468 | KD: 1060.1069\n",
      "Train Epoch: 066 Batch: 00012/00094 | Loss: 332.9078 | CE: 0.2498 | KD: 1060.0599\n",
      "Train Epoch: 066 Batch: 00013/00094 | Loss: 332.8354 | CE: 0.1686 | KD: 1060.0881\n",
      "Train Epoch: 066 Batch: 00014/00094 | Loss: 332.9787 | CE: 0.2769 | KD: 1060.1995\n",
      "Train Epoch: 066 Batch: 00015/00094 | Loss: 332.8180 | CE: 0.1686 | KD: 1060.0327\n",
      "Train Epoch: 066 Batch: 00016/00094 | Loss: 332.8491 | CE: 0.1617 | KD: 1060.1536\n",
      "Train Epoch: 066 Batch: 00017/00094 | Loss: 332.8848 | CE: 0.2210 | KD: 1060.0785\n",
      "Train Epoch: 066 Batch: 00018/00094 | Loss: 332.8328 | CE: 0.1921 | KD: 1060.0045\n",
      "Train Epoch: 066 Batch: 00019/00094 | Loss: 332.8387 | CE: 0.1450 | KD: 1060.1737\n",
      "Train Epoch: 066 Batch: 00020/00094 | Loss: 332.8952 | CE: 0.1794 | KD: 1060.2439\n",
      "Train Epoch: 066 Batch: 00021/00094 | Loss: 332.8410 | CE: 0.1755 | KD: 1060.0836\n",
      "Train Epoch: 066 Batch: 00022/00094 | Loss: 332.9116 | CE: 0.1787 | KD: 1060.2987\n",
      "Train Epoch: 066 Batch: 00023/00094 | Loss: 332.8151 | CE: 0.1547 | KD: 1060.0675\n",
      "Train Epoch: 066 Batch: 00024/00094 | Loss: 332.7755 | CE: 0.1202 | KD: 1060.0510\n",
      "Train Epoch: 066 Batch: 00025/00094 | Loss: 332.7686 | CE: 0.1181 | KD: 1060.0359\n",
      "Train Epoch: 066 Batch: 00026/00094 | Loss: 332.8046 | CE: 0.1347 | KD: 1060.0978\n",
      "Train Epoch: 066 Batch: 00027/00094 | Loss: 332.8871 | CE: 0.1822 | KD: 1060.2094\n",
      "Train Epoch: 066 Batch: 00028/00094 | Loss: 332.8779 | CE: 0.1688 | KD: 1060.2228\n",
      "Train Epoch: 066 Batch: 00029/00094 | Loss: 332.8691 | CE: 0.2028 | KD: 1060.0863\n",
      "Train Epoch: 066 Batch: 00030/00094 | Loss: 332.8642 | CE: 0.1734 | KD: 1060.1642\n",
      "Train Epoch: 066 Batch: 00031/00094 | Loss: 332.9080 | CE: 0.2176 | KD: 1060.1630\n",
      "Train Epoch: 066 Batch: 00032/00094 | Loss: 332.9406 | CE: 0.2671 | KD: 1060.1094\n",
      "Train Epoch: 066 Batch: 00033/00094 | Loss: 332.9300 | CE: 0.2552 | KD: 1060.1135\n",
      "Train Epoch: 066 Batch: 00034/00094 | Loss: 332.8270 | CE: 0.1638 | KD: 1060.0764\n",
      "Train Epoch: 066 Batch: 00035/00094 | Loss: 332.8179 | CE: 0.1407 | KD: 1060.1211\n",
      "Train Epoch: 066 Batch: 00036/00094 | Loss: 332.8249 | CE: 0.1819 | KD: 1060.0121\n",
      "Train Epoch: 066 Batch: 00037/00094 | Loss: 332.9076 | CE: 0.2082 | KD: 1060.1918\n",
      "Train Epoch: 066 Batch: 00038/00094 | Loss: 332.8139 | CE: 0.1554 | KD: 1060.0614\n",
      "Train Epoch: 066 Batch: 00039/00094 | Loss: 332.8851 | CE: 0.2114 | KD: 1060.1099\n",
      "Train Epoch: 066 Batch: 00040/00094 | Loss: 332.8029 | CE: 0.1407 | KD: 1060.0732\n",
      "Train Epoch: 066 Batch: 00041/00094 | Loss: 332.9217 | CE: 0.2451 | KD: 1060.1191\n",
      "Train Epoch: 066 Batch: 00042/00094 | Loss: 332.8836 | CE: 0.2248 | KD: 1060.0623\n",
      "Train Epoch: 066 Batch: 00043/00094 | Loss: 332.8242 | CE: 0.1567 | KD: 1060.0900\n",
      "Train Epoch: 066 Batch: 00044/00094 | Loss: 332.9223 | CE: 0.2202 | KD: 1060.2002\n",
      "Train Epoch: 066 Batch: 00045/00094 | Loss: 332.8551 | CE: 0.1908 | KD: 1060.0801\n",
      "Train Epoch: 066 Batch: 00046/00094 | Loss: 332.9050 | CE: 0.2179 | KD: 1060.1525\n",
      "Train Epoch: 066 Batch: 00047/00094 | Loss: 332.8823 | CE: 0.1953 | KD: 1060.1522\n",
      "Train Epoch: 066 Batch: 00048/00094 | Loss: 332.8289 | CE: 0.1512 | KD: 1060.1227\n",
      "Train Epoch: 066 Batch: 00049/00094 | Loss: 332.8844 | CE: 0.2010 | KD: 1060.1407\n",
      "Train Epoch: 066 Batch: 00050/00094 | Loss: 333.0055 | CE: 0.3191 | KD: 1060.1504\n",
      "Train Epoch: 066 Batch: 00051/00094 | Loss: 332.8466 | CE: 0.1658 | KD: 1060.1324\n",
      "Train Epoch: 066 Batch: 00052/00094 | Loss: 332.8468 | CE: 0.1914 | KD: 1060.0516\n",
      "Train Epoch: 066 Batch: 00053/00094 | Loss: 332.8662 | CE: 0.1983 | KD: 1060.0913\n",
      "Train Epoch: 066 Batch: 00054/00094 | Loss: 332.7795 | CE: 0.1240 | KD: 1060.0521\n",
      "Train Epoch: 066 Batch: 00055/00094 | Loss: 332.8560 | CE: 0.1844 | KD: 1060.1033\n",
      "Train Epoch: 066 Batch: 00056/00094 | Loss: 332.9567 | CE: 0.2634 | KD: 1060.1722\n",
      "Train Epoch: 066 Batch: 00057/00094 | Loss: 332.8203 | CE: 0.1681 | KD: 1060.0415\n",
      "Train Epoch: 066 Batch: 00058/00094 | Loss: 332.8073 | CE: 0.1476 | KD: 1060.0653\n",
      "Train Epoch: 066 Batch: 00059/00094 | Loss: 332.8221 | CE: 0.1742 | KD: 1060.0276\n",
      "Train Epoch: 066 Batch: 00060/00094 | Loss: 332.8992 | CE: 0.2357 | KD: 1060.0773\n",
      "Train Epoch: 066 Batch: 00061/00094 | Loss: 332.9110 | CE: 0.2398 | KD: 1060.1018\n",
      "Train Epoch: 066 Batch: 00062/00094 | Loss: 332.8852 | CE: 0.1961 | KD: 1060.1588\n",
      "Train Epoch: 066 Batch: 00063/00094 | Loss: 332.8506 | CE: 0.1754 | KD: 1060.1147\n",
      "Train Epoch: 066 Batch: 00064/00094 | Loss: 332.9178 | CE: 0.2428 | KD: 1060.1140\n",
      "Train Epoch: 066 Batch: 00065/00094 | Loss: 332.8315 | CE: 0.1440 | KD: 1060.1541\n",
      "Train Epoch: 066 Batch: 00066/00094 | Loss: 332.8887 | CE: 0.2227 | KD: 1060.0853\n",
      "Train Epoch: 066 Batch: 00067/00094 | Loss: 332.8302 | CE: 0.1550 | KD: 1060.1146\n",
      "Train Epoch: 066 Batch: 00068/00094 | Loss: 332.7851 | CE: 0.1304 | KD: 1060.0492\n",
      "Train Epoch: 066 Batch: 00069/00094 | Loss: 332.8949 | CE: 0.2089 | KD: 1060.1492\n",
      "Train Epoch: 066 Batch: 00070/00094 | Loss: 332.8950 | CE: 0.2014 | KD: 1060.1732\n",
      "Train Epoch: 066 Batch: 00071/00094 | Loss: 332.8878 | CE: 0.2100 | KD: 1060.1228\n",
      "Train Epoch: 066 Batch: 00072/00094 | Loss: 332.8814 | CE: 0.2171 | KD: 1060.0800\n",
      "Train Epoch: 066 Batch: 00073/00094 | Loss: 332.7747 | CE: 0.1176 | KD: 1060.0571\n",
      "Train Epoch: 066 Batch: 00074/00094 | Loss: 332.8415 | CE: 0.1883 | KD: 1060.0444\n",
      "Train Epoch: 066 Batch: 00075/00094 | Loss: 332.8079 | CE: 0.1680 | KD: 1060.0022\n",
      "Train Epoch: 066 Batch: 00076/00094 | Loss: 332.9874 | CE: 0.3192 | KD: 1060.0924\n",
      "Train Epoch: 066 Batch: 00077/00094 | Loss: 332.9188 | CE: 0.2011 | KD: 1060.2502\n",
      "Train Epoch: 066 Batch: 00078/00094 | Loss: 332.8734 | CE: 0.1823 | KD: 1060.1654\n",
      "Train Epoch: 066 Batch: 00079/00094 | Loss: 332.8604 | CE: 0.1804 | KD: 1060.1301\n",
      "Train Epoch: 066 Batch: 00080/00094 | Loss: 332.9118 | CE: 0.2233 | KD: 1060.1572\n",
      "Train Epoch: 066 Batch: 00081/00094 | Loss: 332.9442 | CE: 0.2609 | KD: 1060.1405\n",
      "Train Epoch: 066 Batch: 00082/00094 | Loss: 332.7781 | CE: 0.1298 | KD: 1060.0291\n",
      "Train Epoch: 066 Batch: 00083/00094 | Loss: 332.8030 | CE: 0.1648 | KD: 1059.9966\n",
      "Train Epoch: 066 Batch: 00084/00094 | Loss: 332.8615 | CE: 0.2020 | KD: 1060.0647\n",
      "Train Epoch: 066 Batch: 00085/00094 | Loss: 332.8514 | CE: 0.1771 | KD: 1060.1116\n",
      "Train Epoch: 066 Batch: 00086/00094 | Loss: 332.9284 | CE: 0.2387 | KD: 1060.1609\n",
      "Train Epoch: 066 Batch: 00087/00094 | Loss: 332.9288 | CE: 0.2526 | KD: 1060.1179\n",
      "Train Epoch: 066 Batch: 00088/00094 | Loss: 332.8513 | CE: 0.1840 | KD: 1060.0894\n",
      "Train Epoch: 066 Batch: 00089/00094 | Loss: 332.9106 | CE: 0.2153 | KD: 1060.1790\n",
      "Train Epoch: 066 Batch: 00090/00094 | Loss: 332.8167 | CE: 0.1661 | KD: 1060.0363\n",
      "Train Epoch: 066 Batch: 00091/00094 | Loss: 332.7618 | CE: 0.1227 | KD: 1059.9998\n",
      "Train Epoch: 066 Batch: 00092/00094 | Loss: 332.9650 | CE: 0.2699 | KD: 1060.1780\n",
      "Train Epoch: 066 Batch: 00093/00094 | Loss: 332.9656 | CE: 0.2687 | KD: 1060.1837\n",
      "Train Epoch: 066 Batch: 00094/00094 | Loss: 332.9298 | CE: 0.2238 | KD: 1060.2129\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2088 | acc:92.5000\n",
      "[VAL Acc] Target: 92.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3369 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2155 | acc:46.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 46.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2545 | acc:50.3817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6730 | acc:65.7132\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 65.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0765 | acc:53.4196\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5252 | acc:75.3527\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1204 | acc:53.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 53.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5533 | acc:72.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 72.35%\n",
      "[VAL Acc] Avg 62.20%\n",
      "Train Epoch: 067 Batch: 00001/00094 | Loss: 299.6067 | CE: 0.1878 | KD: 1060.1543\n",
      "Train Epoch: 067 Batch: 00002/00094 | Loss: 299.6355 | CE: 0.2153 | KD: 1060.1591\n",
      "Train Epoch: 067 Batch: 00003/00094 | Loss: 299.6050 | CE: 0.1992 | KD: 1060.1079\n",
      "Train Epoch: 067 Batch: 00004/00094 | Loss: 299.5445 | CE: 0.1625 | KD: 1060.0238\n",
      "Train Epoch: 067 Batch: 00005/00094 | Loss: 299.6327 | CE: 0.2190 | KD: 1060.1356\n",
      "Train Epoch: 067 Batch: 00006/00094 | Loss: 299.5693 | CE: 0.1862 | KD: 1060.0276\n",
      "Train Epoch: 067 Batch: 00007/00094 | Loss: 299.5376 | CE: 0.1280 | KD: 1060.1212\n",
      "Train Epoch: 067 Batch: 00008/00094 | Loss: 299.6582 | CE: 0.2424 | KD: 1060.1434\n",
      "Train Epoch: 067 Batch: 00009/00094 | Loss: 299.6873 | CE: 0.2776 | KD: 1060.1218\n",
      "Train Epoch: 067 Batch: 00010/00094 | Loss: 299.6005 | CE: 0.1869 | KD: 1060.1356\n",
      "Train Epoch: 067 Batch: 00011/00094 | Loss: 299.5561 | CE: 0.1384 | KD: 1060.1500\n",
      "Train Epoch: 067 Batch: 00012/00094 | Loss: 299.6132 | CE: 0.2120 | KD: 1060.0916\n",
      "Train Epoch: 067 Batch: 00013/00094 | Loss: 299.6812 | CE: 0.2761 | KD: 1060.1056\n",
      "Train Epoch: 067 Batch: 00014/00094 | Loss: 299.6118 | CE: 0.2217 | KD: 1060.0520\n",
      "Train Epoch: 067 Batch: 00015/00094 | Loss: 299.5751 | CE: 0.1827 | KD: 1060.0605\n",
      "Train Epoch: 067 Batch: 00016/00094 | Loss: 299.6074 | CE: 0.2075 | KD: 1060.0869\n",
      "Train Epoch: 067 Batch: 00017/00094 | Loss: 299.5530 | CE: 0.1577 | KD: 1060.0708\n",
      "Train Epoch: 067 Batch: 00018/00094 | Loss: 299.5746 | CE: 0.1976 | KD: 1060.0059\n",
      "Train Epoch: 067 Batch: 00019/00094 | Loss: 299.6423 | CE: 0.2326 | KD: 1060.1217\n",
      "Train Epoch: 067 Batch: 00020/00094 | Loss: 299.6035 | CE: 0.2134 | KD: 1060.0525\n",
      "Train Epoch: 067 Batch: 00021/00094 | Loss: 299.5656 | CE: 0.1876 | KD: 1060.0093\n",
      "Train Epoch: 067 Batch: 00022/00094 | Loss: 299.5738 | CE: 0.1753 | KD: 1060.0820\n",
      "Train Epoch: 067 Batch: 00023/00094 | Loss: 299.5497 | CE: 0.1638 | KD: 1060.0375\n",
      "Train Epoch: 067 Batch: 00024/00094 | Loss: 299.6085 | CE: 0.2120 | KD: 1060.0752\n",
      "Train Epoch: 067 Batch: 00025/00094 | Loss: 299.6378 | CE: 0.2416 | KD: 1060.0740\n",
      "Train Epoch: 067 Batch: 00026/00094 | Loss: 299.6395 | CE: 0.2231 | KD: 1060.1456\n",
      "Train Epoch: 067 Batch: 00027/00094 | Loss: 299.5283 | CE: 0.1613 | KD: 1059.9703\n",
      "Train Epoch: 067 Batch: 00028/00094 | Loss: 299.6068 | CE: 0.1913 | KD: 1060.1423\n",
      "Train Epoch: 067 Batch: 00029/00094 | Loss: 299.5116 | CE: 0.1239 | KD: 1060.0437\n",
      "Train Epoch: 067 Batch: 00030/00094 | Loss: 299.6518 | CE: 0.2184 | KD: 1060.2054\n",
      "Train Epoch: 067 Batch: 00031/00094 | Loss: 299.5569 | CE: 0.1658 | KD: 1060.0560\n",
      "Train Epoch: 067 Batch: 00032/00094 | Loss: 299.6490 | CE: 0.2169 | KD: 1060.2012\n",
      "Train Epoch: 067 Batch: 00033/00094 | Loss: 299.5550 | CE: 0.1451 | KD: 1060.1223\n",
      "Train Epoch: 067 Batch: 00034/00094 | Loss: 299.6989 | CE: 0.2670 | KD: 1060.2001\n",
      "Train Epoch: 067 Batch: 00035/00094 | Loss: 299.5175 | CE: 0.1250 | KD: 1060.0610\n",
      "Train Epoch: 067 Batch: 00036/00094 | Loss: 299.6423 | CE: 0.1976 | KD: 1060.2457\n",
      "Train Epoch: 067 Batch: 00037/00094 | Loss: 299.6093 | CE: 0.1905 | KD: 1060.1542\n",
      "Train Epoch: 067 Batch: 00038/00094 | Loss: 299.6795 | CE: 0.2368 | KD: 1060.2386\n",
      "Train Epoch: 067 Batch: 00039/00094 | Loss: 299.5572 | CE: 0.1595 | KD: 1060.0792\n",
      "Train Epoch: 067 Batch: 00040/00094 | Loss: 299.6246 | CE: 0.2208 | KD: 1060.1008\n",
      "Train Epoch: 067 Batch: 00041/00094 | Loss: 299.6024 | CE: 0.1991 | KD: 1060.0991\n",
      "Train Epoch: 067 Batch: 00042/00094 | Loss: 299.5557 | CE: 0.1497 | KD: 1060.1088\n",
      "Train Epoch: 067 Batch: 00043/00094 | Loss: 299.6003 | CE: 0.1917 | KD: 1060.1179\n",
      "Train Epoch: 067 Batch: 00044/00094 | Loss: 299.5961 | CE: 0.2018 | KD: 1060.0673\n",
      "Train Epoch: 067 Batch: 00045/00094 | Loss: 299.6261 | CE: 0.1904 | KD: 1060.2136\n",
      "Train Epoch: 067 Batch: 00046/00094 | Loss: 299.5910 | CE: 0.2265 | KD: 1059.9619\n",
      "Train Epoch: 067 Batch: 00047/00094 | Loss: 299.5741 | CE: 0.1699 | KD: 1060.1025\n",
      "Train Epoch: 067 Batch: 00048/00094 | Loss: 299.6267 | CE: 0.2121 | KD: 1060.1393\n",
      "Train Epoch: 067 Batch: 00049/00094 | Loss: 299.6147 | CE: 0.2028 | KD: 1060.1296\n",
      "Train Epoch: 067 Batch: 00050/00094 | Loss: 299.5342 | CE: 0.1491 | KD: 1060.0345\n",
      "Train Epoch: 067 Batch: 00051/00094 | Loss: 299.5695 | CE: 0.1844 | KD: 1060.0345\n",
      "Train Epoch: 067 Batch: 00052/00094 | Loss: 299.5233 | CE: 0.1273 | KD: 1060.0734\n",
      "Train Epoch: 067 Batch: 00053/00094 | Loss: 299.6451 | CE: 0.2289 | KD: 1060.1449\n",
      "Train Epoch: 067 Batch: 00054/00094 | Loss: 299.6346 | CE: 0.2233 | KD: 1060.1276\n",
      "Train Epoch: 067 Batch: 00055/00094 | Loss: 299.6384 | CE: 0.2230 | KD: 1060.1418\n",
      "Train Epoch: 067 Batch: 00056/00094 | Loss: 299.5319 | CE: 0.1571 | KD: 1059.9979\n",
      "Train Epoch: 067 Batch: 00057/00094 | Loss: 299.5991 | CE: 0.1569 | KD: 1060.2366\n",
      "Train Epoch: 067 Batch: 00058/00094 | Loss: 299.5615 | CE: 0.1714 | KD: 1060.0524\n",
      "Train Epoch: 067 Batch: 00059/00094 | Loss: 299.6407 | CE: 0.2119 | KD: 1060.1895\n",
      "Train Epoch: 067 Batch: 00060/00094 | Loss: 299.5752 | CE: 0.1643 | KD: 1060.1261\n",
      "Train Epoch: 067 Batch: 00061/00094 | Loss: 299.6576 | CE: 0.2258 | KD: 1060.2000\n",
      "Train Epoch: 067 Batch: 00062/00094 | Loss: 299.5639 | CE: 0.1675 | KD: 1060.0746\n",
      "Train Epoch: 067 Batch: 00063/00094 | Loss: 299.6170 | CE: 0.2223 | KD: 1060.0687\n",
      "Train Epoch: 067 Batch: 00064/00094 | Loss: 299.5313 | CE: 0.1289 | KD: 1060.0958\n",
      "Train Epoch: 067 Batch: 00065/00094 | Loss: 299.5480 | CE: 0.1716 | KD: 1060.0039\n",
      "Train Epoch: 067 Batch: 00066/00094 | Loss: 299.5416 | CE: 0.1249 | KD: 1060.1465\n",
      "Train Epoch: 067 Batch: 00067/00094 | Loss: 299.5983 | CE: 0.1997 | KD: 1060.0825\n",
      "Train Epoch: 067 Batch: 00068/00094 | Loss: 299.5527 | CE: 0.1716 | KD: 1060.0206\n",
      "Train Epoch: 067 Batch: 00069/00094 | Loss: 299.6407 | CE: 0.2080 | KD: 1060.2031\n",
      "Train Epoch: 067 Batch: 00070/00094 | Loss: 299.7351 | CE: 0.2835 | KD: 1060.2699\n",
      "Train Epoch: 067 Batch: 00071/00094 | Loss: 299.5382 | CE: 0.1586 | KD: 1060.0150\n",
      "Train Epoch: 067 Batch: 00072/00094 | Loss: 299.6404 | CE: 0.2419 | KD: 1060.0819\n",
      "Train Epoch: 067 Batch: 00073/00094 | Loss: 299.6037 | CE: 0.1684 | KD: 1060.2124\n",
      "Train Epoch: 067 Batch: 00074/00094 | Loss: 299.5507 | CE: 0.1657 | KD: 1060.0343\n",
      "Train Epoch: 067 Batch: 00075/00094 | Loss: 299.5372 | CE: 0.1458 | KD: 1060.0568\n",
      "Train Epoch: 067 Batch: 00076/00094 | Loss: 299.5956 | CE: 0.1928 | KD: 1060.0972\n",
      "Train Epoch: 067 Batch: 00077/00094 | Loss: 299.6477 | CE: 0.2301 | KD: 1060.1497\n",
      "Train Epoch: 067 Batch: 00078/00094 | Loss: 299.5243 | CE: 0.1226 | KD: 1060.0933\n",
      "Train Epoch: 067 Batch: 00079/00094 | Loss: 299.5754 | CE: 0.1555 | KD: 1060.1576\n",
      "Train Epoch: 067 Batch: 00080/00094 | Loss: 299.5797 | CE: 0.1813 | KD: 1060.0817\n",
      "Train Epoch: 067 Batch: 00081/00094 | Loss: 299.7132 | CE: 0.2784 | KD: 1060.2107\n",
      "Train Epoch: 067 Batch: 00082/00094 | Loss: 299.5869 | CE: 0.1721 | KD: 1060.1399\n",
      "Train Epoch: 067 Batch: 00083/00094 | Loss: 299.5605 | CE: 0.1492 | KD: 1060.1276\n",
      "Train Epoch: 067 Batch: 00084/00094 | Loss: 299.6139 | CE: 0.1821 | KD: 1060.2002\n",
      "Train Epoch: 067 Batch: 00085/00094 | Loss: 299.5828 | CE: 0.1800 | KD: 1060.0973\n",
      "Train Epoch: 067 Batch: 00086/00094 | Loss: 299.4941 | CE: 0.1099 | KD: 1060.0315\n",
      "Train Epoch: 067 Batch: 00087/00094 | Loss: 299.5299 | CE: 0.1236 | KD: 1060.1095\n",
      "Train Epoch: 067 Batch: 00088/00094 | Loss: 299.6982 | CE: 0.2395 | KD: 1060.2954\n",
      "Train Epoch: 067 Batch: 00089/00094 | Loss: 299.5479 | CE: 0.1659 | KD: 1060.0237\n",
      "Train Epoch: 067 Batch: 00090/00094 | Loss: 299.5471 | CE: 0.1421 | KD: 1060.1049\n",
      "Train Epoch: 067 Batch: 00091/00094 | Loss: 299.6024 | CE: 0.1952 | KD: 1060.1130\n",
      "Train Epoch: 067 Batch: 00092/00094 | Loss: 299.5787 | CE: 0.1764 | KD: 1060.0955\n",
      "Train Epoch: 067 Batch: 00093/00094 | Loss: 299.7497 | CE: 0.3043 | KD: 1060.2483\n",
      "Train Epoch: 067 Batch: 00094/00094 | Loss: 299.6895 | CE: 0.2558 | KD: 1060.2070\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2197 | acc:92.0500\n",
      "[VAL Acc] Target: 92.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3093 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1291 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2751 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7195 | acc:63.7147\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 63.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9634 | acc:55.8226\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5474 | acc:74.8433\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0203 | acc:55.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5658 | acc:72.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 72.30%\n",
      "[VAL Acc] Avg 62.63%\n",
      "Train Epoch: 068 Batch: 00001/00094 | Loss: 299.6778 | CE: 0.2190 | KD: 1060.2955\n",
      "Train Epoch: 068 Batch: 00002/00094 | Loss: 299.6216 | CE: 0.2242 | KD: 1060.0784\n",
      "Train Epoch: 068 Batch: 00003/00094 | Loss: 299.5987 | CE: 0.1903 | KD: 1060.1171\n",
      "Train Epoch: 068 Batch: 00004/00094 | Loss: 299.5675 | CE: 0.1591 | KD: 1060.1174\n",
      "Train Epoch: 068 Batch: 00005/00094 | Loss: 299.5627 | CE: 0.1835 | KD: 1060.0137\n",
      "Train Epoch: 068 Batch: 00006/00094 | Loss: 299.5736 | CE: 0.1752 | KD: 1060.0819\n",
      "Train Epoch: 068 Batch: 00007/00094 | Loss: 299.5690 | CE: 0.1626 | KD: 1060.1101\n",
      "Train Epoch: 068 Batch: 00008/00094 | Loss: 299.6994 | CE: 0.2907 | KD: 1060.1180\n",
      "Train Epoch: 068 Batch: 00009/00094 | Loss: 299.5784 | CE: 0.1754 | KD: 1060.0978\n",
      "Train Epoch: 068 Batch: 00010/00094 | Loss: 299.5443 | CE: 0.1502 | KD: 1060.0668\n",
      "Train Epoch: 068 Batch: 00011/00094 | Loss: 299.6198 | CE: 0.2120 | KD: 1060.1152\n",
      "Train Epoch: 068 Batch: 00012/00094 | Loss: 299.5931 | CE: 0.1570 | KD: 1060.2155\n",
      "Train Epoch: 068 Batch: 00013/00094 | Loss: 299.6285 | CE: 0.2145 | KD: 1060.1368\n",
      "Train Epoch: 068 Batch: 00014/00094 | Loss: 299.6918 | CE: 0.2330 | KD: 1060.2957\n",
      "Train Epoch: 068 Batch: 00015/00094 | Loss: 299.5574 | CE: 0.1711 | KD: 1060.0389\n",
      "Train Epoch: 068 Batch: 00016/00094 | Loss: 299.6503 | CE: 0.2555 | KD: 1060.0691\n",
      "Train Epoch: 068 Batch: 00017/00094 | Loss: 299.5869 | CE: 0.1862 | KD: 1060.0900\n",
      "Train Epoch: 068 Batch: 00018/00094 | Loss: 299.5660 | CE: 0.1705 | KD: 1060.0715\n",
      "Train Epoch: 068 Batch: 00019/00094 | Loss: 299.5269 | CE: 0.1446 | KD: 1060.0247\n",
      "Train Epoch: 068 Batch: 00020/00094 | Loss: 299.5627 | CE: 0.1631 | KD: 1060.0861\n",
      "Train Epoch: 068 Batch: 00021/00094 | Loss: 299.5714 | CE: 0.1623 | KD: 1060.1195\n",
      "Train Epoch: 068 Batch: 00022/00094 | Loss: 299.6122 | CE: 0.2134 | KD: 1060.0830\n",
      "Train Epoch: 068 Batch: 00023/00094 | Loss: 299.6525 | CE: 0.2390 | KD: 1060.1351\n",
      "Train Epoch: 068 Batch: 00024/00094 | Loss: 299.5750 | CE: 0.1655 | KD: 1060.1211\n",
      "Train Epoch: 068 Batch: 00025/00094 | Loss: 299.6734 | CE: 0.2711 | KD: 1060.0953\n",
      "Train Epoch: 068 Batch: 00026/00094 | Loss: 299.5912 | CE: 0.1952 | KD: 1060.0732\n",
      "Train Epoch: 068 Batch: 00027/00094 | Loss: 299.5844 | CE: 0.1686 | KD: 1060.1434\n",
      "Train Epoch: 068 Batch: 00028/00094 | Loss: 299.6281 | CE: 0.2313 | KD: 1060.0757\n",
      "Train Epoch: 068 Batch: 00029/00094 | Loss: 299.5955 | CE: 0.1912 | KD: 1060.1025\n",
      "Train Epoch: 068 Batch: 00030/00094 | Loss: 299.6215 | CE: 0.2036 | KD: 1060.1506\n",
      "Train Epoch: 068 Batch: 00031/00094 | Loss: 299.5865 | CE: 0.1511 | KD: 1060.2125\n",
      "Train Epoch: 068 Batch: 00032/00094 | Loss: 299.6326 | CE: 0.2315 | KD: 1060.0913\n",
      "Train Epoch: 068 Batch: 00033/00094 | Loss: 299.5400 | CE: 0.1377 | KD: 1060.0957\n",
      "Train Epoch: 068 Batch: 00034/00094 | Loss: 299.5914 | CE: 0.1865 | KD: 1060.1047\n",
      "Train Epoch: 068 Batch: 00035/00094 | Loss: 299.5904 | CE: 0.1933 | KD: 1060.0769\n",
      "Train Epoch: 068 Batch: 00036/00094 | Loss: 299.5890 | CE: 0.1855 | KD: 1060.0999\n",
      "Train Epoch: 068 Batch: 00037/00094 | Loss: 299.6700 | CE: 0.2366 | KD: 1060.2054\n",
      "Train Epoch: 068 Batch: 00038/00094 | Loss: 299.5770 | CE: 0.1826 | KD: 1060.0676\n",
      "Train Epoch: 068 Batch: 00039/00094 | Loss: 299.5562 | CE: 0.1569 | KD: 1060.0850\n",
      "Train Epoch: 068 Batch: 00040/00094 | Loss: 299.7217 | CE: 0.2936 | KD: 1060.1871\n",
      "Train Epoch: 068 Batch: 00041/00094 | Loss: 299.5501 | CE: 0.1655 | KD: 1060.0327\n",
      "Train Epoch: 068 Batch: 00042/00094 | Loss: 299.6200 | CE: 0.1943 | KD: 1060.1785\n",
      "Train Epoch: 068 Batch: 00043/00094 | Loss: 299.5627 | CE: 0.1877 | KD: 1059.9988\n",
      "Train Epoch: 068 Batch: 00044/00094 | Loss: 299.5818 | CE: 0.2014 | KD: 1060.0179\n",
      "Train Epoch: 068 Batch: 00045/00094 | Loss: 299.7447 | CE: 0.2826 | KD: 1060.3071\n",
      "Train Epoch: 068 Batch: 00046/00094 | Loss: 299.6360 | CE: 0.2263 | KD: 1060.1216\n",
      "Train Epoch: 068 Batch: 00047/00094 | Loss: 299.6008 | CE: 0.2123 | KD: 1060.0468\n",
      "Train Epoch: 068 Batch: 00048/00094 | Loss: 299.5737 | CE: 0.1649 | KD: 1060.1185\n",
      "Train Epoch: 068 Batch: 00049/00094 | Loss: 299.5835 | CE: 0.1670 | KD: 1060.1458\n",
      "Train Epoch: 068 Batch: 00050/00094 | Loss: 299.5635 | CE: 0.1806 | KD: 1060.0269\n",
      "Train Epoch: 068 Batch: 00051/00094 | Loss: 299.6133 | CE: 0.1837 | KD: 1060.1921\n",
      "Train Epoch: 068 Batch: 00052/00094 | Loss: 299.5810 | CE: 0.1672 | KD: 1060.1362\n",
      "Train Epoch: 068 Batch: 00053/00094 | Loss: 299.7362 | CE: 0.2642 | KD: 1060.3423\n",
      "Train Epoch: 068 Batch: 00054/00094 | Loss: 299.7114 | CE: 0.2771 | KD: 1060.2089\n",
      "Train Epoch: 068 Batch: 00055/00094 | Loss: 299.5339 | CE: 0.1271 | KD: 1060.1115\n",
      "Train Epoch: 068 Batch: 00056/00094 | Loss: 299.5981 | CE: 0.2075 | KD: 1060.0543\n",
      "Train Epoch: 068 Batch: 00057/00094 | Loss: 299.5604 | CE: 0.1473 | KD: 1060.1337\n",
      "Train Epoch: 068 Batch: 00058/00094 | Loss: 299.6577 | CE: 0.2421 | KD: 1060.1422\n",
      "Train Epoch: 068 Batch: 00059/00094 | Loss: 299.5578 | CE: 0.1750 | KD: 1060.0265\n",
      "Train Epoch: 068 Batch: 00060/00094 | Loss: 299.5865 | CE: 0.1571 | KD: 1060.1919\n",
      "Train Epoch: 068 Batch: 00061/00094 | Loss: 299.6085 | CE: 0.1871 | KD: 1060.1631\n",
      "Train Epoch: 068 Batch: 00062/00094 | Loss: 299.5730 | CE: 0.1678 | KD: 1060.1058\n",
      "Train Epoch: 068 Batch: 00063/00094 | Loss: 299.6366 | CE: 0.2318 | KD: 1060.1046\n",
      "Train Epoch: 068 Batch: 00064/00094 | Loss: 299.6484 | CE: 0.2238 | KD: 1060.1746\n",
      "Train Epoch: 068 Batch: 00065/00094 | Loss: 299.5201 | CE: 0.1211 | KD: 1060.0837\n",
      "Train Epoch: 068 Batch: 00066/00094 | Loss: 299.6087 | CE: 0.2151 | KD: 1060.0648\n",
      "Train Epoch: 068 Batch: 00067/00094 | Loss: 299.5693 | CE: 0.1702 | KD: 1060.0840\n",
      "Train Epoch: 068 Batch: 00068/00094 | Loss: 299.5924 | CE: 0.1880 | KD: 1060.1030\n",
      "Train Epoch: 068 Batch: 00069/00094 | Loss: 299.7121 | CE: 0.2633 | KD: 1060.2604\n",
      "Train Epoch: 068 Batch: 00070/00094 | Loss: 299.5565 | CE: 0.1684 | KD: 1060.0455\n",
      "Train Epoch: 068 Batch: 00071/00094 | Loss: 299.5755 | CE: 0.1870 | KD: 1060.0469\n",
      "Train Epoch: 068 Batch: 00072/00094 | Loss: 299.5555 | CE: 0.1397 | KD: 1060.1432\n",
      "Train Epoch: 068 Batch: 00073/00094 | Loss: 299.6058 | CE: 0.1557 | KD: 1060.2646\n",
      "Train Epoch: 068 Batch: 00074/00094 | Loss: 299.5607 | CE: 0.1920 | KD: 1059.9766\n",
      "Train Epoch: 068 Batch: 00075/00094 | Loss: 299.5821 | CE: 0.1898 | KD: 1060.0601\n",
      "Train Epoch: 068 Batch: 00076/00094 | Loss: 299.5855 | CE: 0.1761 | KD: 1060.1206\n",
      "Train Epoch: 068 Batch: 00077/00094 | Loss: 299.5465 | CE: 0.1626 | KD: 1060.0303\n",
      "Train Epoch: 068 Batch: 00078/00094 | Loss: 299.5250 | CE: 0.1217 | KD: 1060.0991\n",
      "Train Epoch: 068 Batch: 00079/00094 | Loss: 299.5054 | CE: 0.1311 | KD: 1059.9962\n",
      "Train Epoch: 068 Batch: 00080/00094 | Loss: 299.7065 | CE: 0.2960 | KD: 1060.1244\n",
      "Train Epoch: 068 Batch: 00081/00094 | Loss: 299.6292 | CE: 0.2320 | KD: 1060.0776\n",
      "Train Epoch: 068 Batch: 00082/00094 | Loss: 299.5221 | CE: 0.1362 | KD: 1060.0374\n",
      "Train Epoch: 068 Batch: 00083/00094 | Loss: 299.6461 | CE: 0.2220 | KD: 1060.1730\n",
      "Train Epoch: 068 Batch: 00084/00094 | Loss: 299.5935 | CE: 0.1709 | KD: 1060.1674\n",
      "Train Epoch: 068 Batch: 00085/00094 | Loss: 299.5894 | CE: 0.1878 | KD: 1060.0929\n",
      "Train Epoch: 068 Batch: 00086/00094 | Loss: 299.6310 | CE: 0.2248 | KD: 1060.1094\n",
      "Train Epoch: 068 Batch: 00087/00094 | Loss: 299.5128 | CE: 0.1262 | KD: 1060.0399\n",
      "Train Epoch: 068 Batch: 00088/00094 | Loss: 299.6228 | CE: 0.2164 | KD: 1060.1102\n",
      "Train Epoch: 068 Batch: 00089/00094 | Loss: 299.5610 | CE: 0.1670 | KD: 1060.0660\n",
      "Train Epoch: 068 Batch: 00090/00094 | Loss: 299.5881 | CE: 0.1729 | KD: 1060.1411\n",
      "Train Epoch: 068 Batch: 00091/00094 | Loss: 299.6311 | CE: 0.2091 | KD: 1060.1653\n",
      "Train Epoch: 068 Batch: 00092/00094 | Loss: 299.5073 | CE: 0.1215 | KD: 1060.0370\n",
      "Train Epoch: 068 Batch: 00093/00094 | Loss: 299.5770 | CE: 0.1958 | KD: 1060.0208\n",
      "Train Epoch: 068 Batch: 00094/00094 | Loss: 299.7523 | CE: 0.3241 | KD: 1060.1874\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2204 | acc:92.3000\n",
      "[VAL Acc] Target: 92.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3078 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1477 | acc:47.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 47.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2650 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6853 | acc:66.1834\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 66.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0526 | acc:54.2514\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5368 | acc:74.0204\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1049 | acc:54.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5437 | acc:72.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 72.40%\n",
      "[VAL Acc] Avg 62.16%\n",
      "Train Epoch: 069 Batch: 00001/00094 | Loss: 299.5324 | CE: 0.1228 | KD: 1060.1215\n",
      "Train Epoch: 069 Batch: 00002/00094 | Loss: 299.5639 | CE: 0.1836 | KD: 1060.0177\n",
      "Train Epoch: 069 Batch: 00003/00094 | Loss: 299.5446 | CE: 0.1601 | KD: 1060.0323\n",
      "Train Epoch: 069 Batch: 00004/00094 | Loss: 299.6058 | CE: 0.1914 | KD: 1060.1384\n",
      "Train Epoch: 069 Batch: 00005/00094 | Loss: 299.5745 | CE: 0.1596 | KD: 1060.1399\n",
      "Train Epoch: 069 Batch: 00006/00094 | Loss: 299.5769 | CE: 0.1817 | KD: 1060.0704\n",
      "Train Epoch: 069 Batch: 00007/00094 | Loss: 299.6121 | CE: 0.1584 | KD: 1060.2773\n",
      "Train Epoch: 069 Batch: 00008/00094 | Loss: 299.6461 | CE: 0.2594 | KD: 1060.0402\n",
      "Train Epoch: 069 Batch: 00009/00094 | Loss: 299.6100 | CE: 0.1750 | KD: 1060.2111\n",
      "Train Epoch: 069 Batch: 00010/00094 | Loss: 299.5811 | CE: 0.1843 | KD: 1060.0763\n",
      "Train Epoch: 069 Batch: 00011/00094 | Loss: 299.6060 | CE: 0.1936 | KD: 1060.1312\n",
      "Train Epoch: 069 Batch: 00012/00094 | Loss: 299.5394 | CE: 0.1747 | KD: 1059.9624\n",
      "Train Epoch: 069 Batch: 00013/00094 | Loss: 299.5658 | CE: 0.1669 | KD: 1060.0835\n",
      "Train Epoch: 069 Batch: 00014/00094 | Loss: 299.5666 | CE: 0.1492 | KD: 1060.1488\n",
      "Train Epoch: 069 Batch: 00015/00094 | Loss: 299.6166 | CE: 0.2073 | KD: 1060.1204\n",
      "Train Epoch: 069 Batch: 00016/00094 | Loss: 299.6320 | CE: 0.2155 | KD: 1060.1461\n",
      "Train Epoch: 069 Batch: 00017/00094 | Loss: 299.7483 | CE: 0.2929 | KD: 1060.2833\n",
      "Train Epoch: 069 Batch: 00018/00094 | Loss: 299.6074 | CE: 0.1943 | KD: 1060.1338\n",
      "Train Epoch: 069 Batch: 00019/00094 | Loss: 299.5783 | CE: 0.1757 | KD: 1060.0969\n",
      "Train Epoch: 069 Batch: 00020/00094 | Loss: 299.6046 | CE: 0.1854 | KD: 1060.1554\n",
      "Train Epoch: 069 Batch: 00021/00094 | Loss: 299.5638 | CE: 0.1568 | KD: 1060.1121\n",
      "Train Epoch: 069 Batch: 00022/00094 | Loss: 299.5835 | CE: 0.1735 | KD: 1060.1229\n",
      "Train Epoch: 069 Batch: 00023/00094 | Loss: 299.5982 | CE: 0.2067 | KD: 1060.0575\n",
      "Train Epoch: 069 Batch: 00024/00094 | Loss: 299.6195 | CE: 0.2105 | KD: 1060.1190\n",
      "Train Epoch: 069 Batch: 00025/00094 | Loss: 299.5154 | CE: 0.1336 | KD: 1060.0232\n",
      "Train Epoch: 069 Batch: 00026/00094 | Loss: 299.6365 | CE: 0.2379 | KD: 1060.0824\n",
      "Train Epoch: 069 Batch: 00027/00094 | Loss: 299.7018 | CE: 0.2695 | KD: 1060.2019\n",
      "Train Epoch: 069 Batch: 00028/00094 | Loss: 299.6226 | CE: 0.2282 | KD: 1060.0673\n",
      "Train Epoch: 069 Batch: 00029/00094 | Loss: 299.5768 | CE: 0.1748 | KD: 1060.0945\n",
      "Train Epoch: 069 Batch: 00030/00094 | Loss: 299.5199 | CE: 0.1375 | KD: 1060.0253\n",
      "Train Epoch: 069 Batch: 00031/00094 | Loss: 299.6193 | CE: 0.1930 | KD: 1060.1804\n",
      "Train Epoch: 069 Batch: 00032/00094 | Loss: 299.5466 | CE: 0.1441 | KD: 1060.0964\n",
      "Train Epoch: 069 Batch: 00033/00094 | Loss: 299.6239 | CE: 0.2150 | KD: 1060.1187\n",
      "Train Epoch: 069 Batch: 00034/00094 | Loss: 299.6785 | CE: 0.2420 | KD: 1060.2164\n",
      "Train Epoch: 069 Batch: 00035/00094 | Loss: 299.5815 | CE: 0.1676 | KD: 1060.1365\n",
      "Train Epoch: 069 Batch: 00036/00094 | Loss: 299.7991 | CE: 0.3710 | KD: 1060.1865\n",
      "Train Epoch: 069 Batch: 00037/00094 | Loss: 299.6063 | CE: 0.1987 | KD: 1060.1143\n",
      "Train Epoch: 069 Batch: 00038/00094 | Loss: 299.5676 | CE: 0.1695 | KD: 1060.0808\n",
      "Train Epoch: 069 Batch: 00039/00094 | Loss: 299.5713 | CE: 0.1803 | KD: 1060.0555\n",
      "Train Epoch: 069 Batch: 00040/00094 | Loss: 299.5955 | CE: 0.1832 | KD: 1060.1309\n",
      "Train Epoch: 069 Batch: 00041/00094 | Loss: 299.5343 | CE: 0.1370 | KD: 1060.0776\n",
      "Train Epoch: 069 Batch: 00042/00094 | Loss: 299.6173 | CE: 0.1935 | KD: 1060.1718\n",
      "Train Epoch: 069 Batch: 00043/00094 | Loss: 299.7325 | CE: 0.3269 | KD: 1060.1073\n",
      "Train Epoch: 069 Batch: 00044/00094 | Loss: 299.6104 | CE: 0.2089 | KD: 1060.0925\n",
      "Train Epoch: 069 Batch: 00045/00094 | Loss: 299.7150 | CE: 0.2574 | KD: 1060.2910\n",
      "Train Epoch: 069 Batch: 00046/00094 | Loss: 299.5945 | CE: 0.1716 | KD: 1060.1686\n",
      "Train Epoch: 069 Batch: 00047/00094 | Loss: 299.6509 | CE: 0.2035 | KD: 1060.2552\n",
      "Train Epoch: 069 Batch: 00048/00094 | Loss: 299.6081 | CE: 0.1896 | KD: 1060.1530\n",
      "Train Epoch: 069 Batch: 00049/00094 | Loss: 299.5780 | CE: 0.1855 | KD: 1060.0610\n",
      "Train Epoch: 069 Batch: 00050/00094 | Loss: 299.6349 | CE: 0.2347 | KD: 1060.0878\n",
      "Train Epoch: 069 Batch: 00051/00094 | Loss: 299.5860 | CE: 0.1936 | KD: 1060.0604\n",
      "Train Epoch: 069 Batch: 00052/00094 | Loss: 299.5018 | CE: 0.1247 | KD: 1060.0062\n",
      "Train Epoch: 069 Batch: 00053/00094 | Loss: 299.5984 | CE: 0.2043 | KD: 1060.0665\n",
      "Train Epoch: 069 Batch: 00054/00094 | Loss: 299.6050 | CE: 0.1964 | KD: 1060.1179\n",
      "Train Epoch: 069 Batch: 00055/00094 | Loss: 299.6412 | CE: 0.2273 | KD: 1060.1367\n",
      "Train Epoch: 069 Batch: 00056/00094 | Loss: 299.5993 | CE: 0.1911 | KD: 1060.1166\n",
      "Train Epoch: 069 Batch: 00057/00094 | Loss: 299.6307 | CE: 0.2020 | KD: 1060.1890\n",
      "Train Epoch: 069 Batch: 00058/00094 | Loss: 299.5624 | CE: 0.1621 | KD: 1060.0886\n",
      "Train Epoch: 069 Batch: 00059/00094 | Loss: 299.4978 | CE: 0.1203 | KD: 1060.0079\n",
      "Train Epoch: 069 Batch: 00060/00094 | Loss: 299.5879 | CE: 0.1497 | KD: 1060.2227\n",
      "Train Epoch: 069 Batch: 00061/00094 | Loss: 299.5625 | CE: 0.1762 | KD: 1060.0391\n",
      "Train Epoch: 069 Batch: 00062/00094 | Loss: 299.5659 | CE: 0.1569 | KD: 1060.1195\n",
      "Train Epoch: 069 Batch: 00063/00094 | Loss: 299.6751 | CE: 0.2674 | KD: 1060.1145\n",
      "Train Epoch: 069 Batch: 00064/00094 | Loss: 299.5698 | CE: 0.1426 | KD: 1060.1838\n",
      "Train Epoch: 069 Batch: 00065/00094 | Loss: 299.5597 | CE: 0.1562 | KD: 1060.0996\n",
      "Train Epoch: 069 Batch: 00066/00094 | Loss: 299.5653 | CE: 0.1588 | KD: 1060.1102\n",
      "Train Epoch: 069 Batch: 00067/00094 | Loss: 299.5887 | CE: 0.1514 | KD: 1060.2196\n",
      "Train Epoch: 069 Batch: 00068/00094 | Loss: 299.5471 | CE: 0.1645 | KD: 1060.0261\n",
      "Train Epoch: 069 Batch: 00069/00094 | Loss: 299.5982 | CE: 0.1896 | KD: 1060.1179\n",
      "Train Epoch: 069 Batch: 00070/00094 | Loss: 299.6079 | CE: 0.2093 | KD: 1060.0824\n",
      "Train Epoch: 069 Batch: 00071/00094 | Loss: 299.5110 | CE: 0.1377 | KD: 1059.9927\n",
      "Train Epoch: 069 Batch: 00072/00094 | Loss: 299.5394 | CE: 0.1217 | KD: 1060.1501\n",
      "Train Epoch: 069 Batch: 00073/00094 | Loss: 299.5345 | CE: 0.1057 | KD: 1060.1895\n",
      "Train Epoch: 069 Batch: 00074/00094 | Loss: 299.5173 | CE: 0.1244 | KD: 1060.0624\n",
      "Train Epoch: 069 Batch: 00075/00094 | Loss: 299.6472 | CE: 0.2058 | KD: 1060.2340\n",
      "Train Epoch: 069 Batch: 00076/00094 | Loss: 299.7128 | CE: 0.2725 | KD: 1060.2300\n",
      "Train Epoch: 069 Batch: 00077/00094 | Loss: 299.5818 | CE: 0.1884 | KD: 1060.0640\n",
      "Train Epoch: 069 Batch: 00078/00094 | Loss: 299.5100 | CE: 0.1306 | KD: 1060.0145\n",
      "Train Epoch: 069 Batch: 00079/00094 | Loss: 299.6050 | CE: 0.1928 | KD: 1060.1306\n",
      "Train Epoch: 069 Batch: 00080/00094 | Loss: 299.6491 | CE: 0.2697 | KD: 1060.0146\n",
      "Train Epoch: 069 Batch: 00081/00094 | Loss: 299.6532 | CE: 0.2487 | KD: 1060.1035\n",
      "Train Epoch: 069 Batch: 00082/00094 | Loss: 299.5910 | CE: 0.1938 | KD: 1060.0774\n",
      "Train Epoch: 069 Batch: 00083/00094 | Loss: 299.5352 | CE: 0.1443 | KD: 1060.0548\n",
      "Train Epoch: 069 Batch: 00084/00094 | Loss: 299.5887 | CE: 0.1723 | KD: 1060.1455\n",
      "Train Epoch: 069 Batch: 00085/00094 | Loss: 299.5993 | CE: 0.1774 | KD: 1060.1649\n",
      "Train Epoch: 069 Batch: 00086/00094 | Loss: 299.6517 | CE: 0.2321 | KD: 1060.1567\n",
      "Train Epoch: 069 Batch: 00087/00094 | Loss: 299.6540 | CE: 0.2379 | KD: 1060.1445\n",
      "Train Epoch: 069 Batch: 00088/00094 | Loss: 299.6993 | CE: 0.2828 | KD: 1060.1458\n",
      "Train Epoch: 069 Batch: 00089/00094 | Loss: 299.6413 | CE: 0.2031 | KD: 1060.2229\n",
      "Train Epoch: 069 Batch: 00090/00094 | Loss: 299.5734 | CE: 0.1694 | KD: 1060.1016\n",
      "Train Epoch: 069 Batch: 00091/00094 | Loss: 299.5667 | CE: 0.1613 | KD: 1060.1064\n",
      "Train Epoch: 069 Batch: 00092/00094 | Loss: 299.5885 | CE: 0.1686 | KD: 1060.1580\n",
      "Train Epoch: 069 Batch: 00093/00094 | Loss: 299.7022 | CE: 0.2921 | KD: 1060.1233\n",
      "Train Epoch: 069 Batch: 00094/00094 | Loss: 299.6409 | CE: 0.1868 | KD: 1060.2791\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2189 | acc:92.3000\n",
      "[VAL Acc] Target: 92.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2938 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1437 | acc:48.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2483 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7026 | acc:65.1254\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 65.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0015 | acc:54.8983\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5328 | acc:75.2743\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0402 | acc:55.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5360 | acc:73.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.00%\n",
      "[VAL Acc] Avg 62.64%\n",
      "Train Epoch: 070 Batch: 00001/00094 | Loss: 299.5486 | CE: 0.1742 | KD: 1059.9967\n",
      "Train Epoch: 070 Batch: 00002/00094 | Loss: 299.7056 | CE: 0.2732 | KD: 1060.2019\n",
      "Train Epoch: 070 Batch: 00003/00094 | Loss: 299.6281 | CE: 0.2294 | KD: 1060.0828\n",
      "Train Epoch: 070 Batch: 00004/00094 | Loss: 299.5649 | CE: 0.1604 | KD: 1060.1034\n",
      "Train Epoch: 070 Batch: 00005/00094 | Loss: 299.5054 | CE: 0.1340 | KD: 1059.9862\n",
      "Train Epoch: 070 Batch: 00006/00094 | Loss: 299.5908 | CE: 0.1846 | KD: 1060.1093\n",
      "Train Epoch: 070 Batch: 00007/00094 | Loss: 299.6381 | CE: 0.2335 | KD: 1060.1036\n",
      "Train Epoch: 070 Batch: 00008/00094 | Loss: 299.6130 | CE: 0.2100 | KD: 1060.0981\n",
      "Train Epoch: 070 Batch: 00009/00094 | Loss: 299.6169 | CE: 0.1990 | KD: 1060.1510\n",
      "Train Epoch: 070 Batch: 00010/00094 | Loss: 299.6830 | CE: 0.2363 | KD: 1060.2529\n",
      "Train Epoch: 070 Batch: 00011/00094 | Loss: 299.5201 | CE: 0.1129 | KD: 1060.1128\n",
      "Train Epoch: 070 Batch: 00012/00094 | Loss: 299.5773 | CE: 0.1429 | KD: 1060.2092\n",
      "Train Epoch: 070 Batch: 00013/00094 | Loss: 299.7046 | CE: 0.2731 | KD: 1060.1989\n",
      "Train Epoch: 070 Batch: 00014/00094 | Loss: 299.5520 | CE: 0.1631 | KD: 1060.0481\n",
      "Train Epoch: 070 Batch: 00015/00094 | Loss: 299.6196 | CE: 0.2135 | KD: 1060.1091\n",
      "Train Epoch: 070 Batch: 00016/00094 | Loss: 299.5294 | CE: 0.1475 | KD: 1060.0231\n",
      "Train Epoch: 070 Batch: 00017/00094 | Loss: 299.6279 | CE: 0.2298 | KD: 1060.0808\n",
      "Train Epoch: 070 Batch: 00018/00094 | Loss: 299.6455 | CE: 0.2292 | KD: 1060.1454\n",
      "Train Epoch: 070 Batch: 00019/00094 | Loss: 299.6176 | CE: 0.2166 | KD: 1060.0907\n",
      "Train Epoch: 070 Batch: 00020/00094 | Loss: 299.6159 | CE: 0.1881 | KD: 1060.1858\n",
      "Train Epoch: 070 Batch: 00021/00094 | Loss: 299.6707 | CE: 0.2570 | KD: 1060.1357\n",
      "Train Epoch: 070 Batch: 00022/00094 | Loss: 299.5454 | CE: 0.1512 | KD: 1060.0670\n",
      "Train Epoch: 070 Batch: 00023/00094 | Loss: 299.7661 | CE: 0.3073 | KD: 1060.2955\n",
      "Train Epoch: 070 Batch: 00024/00094 | Loss: 299.6142 | CE: 0.2068 | KD: 1060.1136\n",
      "Train Epoch: 070 Batch: 00025/00094 | Loss: 299.5224 | CE: 0.1491 | KD: 1059.9928\n",
      "Train Epoch: 070 Batch: 00026/00094 | Loss: 299.5416 | CE: 0.1570 | KD: 1060.0326\n",
      "Train Epoch: 070 Batch: 00027/00094 | Loss: 299.6640 | CE: 0.2315 | KD: 1060.2024\n",
      "Train Epoch: 070 Batch: 00028/00094 | Loss: 299.5651 | CE: 0.1840 | KD: 1060.0205\n",
      "Train Epoch: 070 Batch: 00029/00094 | Loss: 299.6264 | CE: 0.1975 | KD: 1060.1896\n",
      "Train Epoch: 070 Batch: 00030/00094 | Loss: 299.6006 | CE: 0.1673 | KD: 1060.2052\n",
      "Train Epoch: 070 Batch: 00031/00094 | Loss: 299.6326 | CE: 0.2297 | KD: 1060.0979\n",
      "Train Epoch: 070 Batch: 00032/00094 | Loss: 299.7237 | CE: 0.2664 | KD: 1060.2904\n",
      "Train Epoch: 070 Batch: 00033/00094 | Loss: 299.5822 | CE: 0.1737 | KD: 1060.1176\n",
      "Train Epoch: 070 Batch: 00034/00094 | Loss: 299.5943 | CE: 0.2152 | KD: 1060.0137\n",
      "Train Epoch: 070 Batch: 00035/00094 | Loss: 299.6278 | CE: 0.2002 | KD: 1060.1854\n",
      "Train Epoch: 070 Batch: 00036/00094 | Loss: 299.6133 | CE: 0.1803 | KD: 1060.2043\n",
      "Train Epoch: 070 Batch: 00037/00094 | Loss: 299.5673 | CE: 0.1743 | KD: 1060.0627\n",
      "Train Epoch: 070 Batch: 00038/00094 | Loss: 299.5269 | CE: 0.1282 | KD: 1060.0829\n",
      "Train Epoch: 070 Batch: 00039/00094 | Loss: 299.6435 | CE: 0.1970 | KD: 1060.2521\n",
      "Train Epoch: 070 Batch: 00040/00094 | Loss: 299.6504 | CE: 0.2601 | KD: 1060.0531\n",
      "Train Epoch: 070 Batch: 00041/00094 | Loss: 299.6217 | CE: 0.1976 | KD: 1060.1726\n",
      "Train Epoch: 070 Batch: 00042/00094 | Loss: 299.5918 | CE: 0.1740 | KD: 1060.1503\n",
      "Train Epoch: 070 Batch: 00043/00094 | Loss: 299.6476 | CE: 0.2384 | KD: 1060.1200\n",
      "Train Epoch: 070 Batch: 00044/00094 | Loss: 299.5887 | CE: 0.1628 | KD: 1060.1791\n",
      "Train Epoch: 070 Batch: 00045/00094 | Loss: 299.5825 | CE: 0.1997 | KD: 1060.0266\n",
      "Train Epoch: 070 Batch: 00046/00094 | Loss: 299.6040 | CE: 0.1845 | KD: 1060.1562\n",
      "Train Epoch: 070 Batch: 00047/00094 | Loss: 299.6060 | CE: 0.1799 | KD: 1060.1797\n",
      "Train Epoch: 070 Batch: 00048/00094 | Loss: 299.5519 | CE: 0.1487 | KD: 1060.0986\n",
      "Train Epoch: 070 Batch: 00049/00094 | Loss: 299.5691 | CE: 0.1702 | KD: 1060.0836\n",
      "Train Epoch: 070 Batch: 00050/00094 | Loss: 299.6151 | CE: 0.2259 | KD: 1060.0492\n",
      "Train Epoch: 070 Batch: 00051/00094 | Loss: 299.5711 | CE: 0.1840 | KD: 1060.0417\n",
      "Train Epoch: 070 Batch: 00052/00094 | Loss: 299.5790 | CE: 0.1748 | KD: 1060.1021\n",
      "Train Epoch: 070 Batch: 00053/00094 | Loss: 299.5444 | CE: 0.1591 | KD: 1060.0354\n",
      "Train Epoch: 070 Batch: 00054/00094 | Loss: 299.6301 | CE: 0.2100 | KD: 1060.1586\n",
      "Train Epoch: 070 Batch: 00055/00094 | Loss: 299.5947 | CE: 0.1712 | KD: 1060.1708\n",
      "Train Epoch: 070 Batch: 00056/00094 | Loss: 299.5554 | CE: 0.1534 | KD: 1060.0942\n",
      "Train Epoch: 070 Batch: 00057/00094 | Loss: 299.5530 | CE: 0.1583 | KD: 1060.0686\n",
      "Train Epoch: 070 Batch: 00058/00094 | Loss: 299.5571 | CE: 0.1886 | KD: 1059.9757\n",
      "Train Epoch: 070 Batch: 00059/00094 | Loss: 299.6021 | CE: 0.2127 | KD: 1060.0498\n",
      "Train Epoch: 070 Batch: 00060/00094 | Loss: 299.5184 | CE: 0.1289 | KD: 1060.0502\n",
      "Train Epoch: 070 Batch: 00061/00094 | Loss: 299.6194 | CE: 0.2215 | KD: 1060.0801\n",
      "Train Epoch: 070 Batch: 00062/00094 | Loss: 299.6527 | CE: 0.2067 | KD: 1060.2501\n",
      "Train Epoch: 070 Batch: 00063/00094 | Loss: 299.5819 | CE: 0.1573 | KD: 1060.1746\n",
      "Train Epoch: 070 Batch: 00064/00094 | Loss: 299.6101 | CE: 0.1585 | KD: 1060.2700\n",
      "Train Epoch: 070 Batch: 00065/00094 | Loss: 299.6348 | CE: 0.2127 | KD: 1060.1656\n",
      "Train Epoch: 070 Batch: 00066/00094 | Loss: 299.6207 | CE: 0.2053 | KD: 1060.1421\n",
      "Train Epoch: 070 Batch: 00067/00094 | Loss: 299.6709 | CE: 0.2447 | KD: 1060.1802\n",
      "Train Epoch: 070 Batch: 00068/00094 | Loss: 299.6118 | CE: 0.1995 | KD: 1060.1310\n",
      "Train Epoch: 070 Batch: 00069/00094 | Loss: 299.6256 | CE: 0.2158 | KD: 1060.1221\n",
      "Train Epoch: 070 Batch: 00070/00094 | Loss: 299.5823 | CE: 0.1899 | KD: 1060.0607\n",
      "Train Epoch: 070 Batch: 00071/00094 | Loss: 299.5776 | CE: 0.1813 | KD: 1060.0741\n",
      "Train Epoch: 070 Batch: 00072/00094 | Loss: 299.5479 | CE: 0.1567 | KD: 1060.0562\n",
      "Train Epoch: 070 Batch: 00073/00094 | Loss: 299.6346 | CE: 0.2242 | KD: 1060.1241\n",
      "Train Epoch: 070 Batch: 00074/00094 | Loss: 299.6649 | CE: 0.2617 | KD: 1060.0986\n",
      "Train Epoch: 070 Batch: 00075/00094 | Loss: 299.5396 | CE: 0.1291 | KD: 1060.1246\n",
      "Train Epoch: 070 Batch: 00076/00094 | Loss: 299.5993 | CE: 0.1987 | KD: 1060.0895\n",
      "Train Epoch: 070 Batch: 00077/00094 | Loss: 299.5871 | CE: 0.1950 | KD: 1060.0594\n",
      "Train Epoch: 070 Batch: 00078/00094 | Loss: 299.6411 | CE: 0.2178 | KD: 1060.1697\n",
      "Train Epoch: 070 Batch: 00079/00094 | Loss: 299.5317 | CE: 0.1294 | KD: 1060.0955\n",
      "Train Epoch: 070 Batch: 00080/00094 | Loss: 299.5499 | CE: 0.1716 | KD: 1060.0107\n",
      "Train Epoch: 070 Batch: 00081/00094 | Loss: 299.5724 | CE: 0.1736 | KD: 1060.0829\n",
      "Train Epoch: 070 Batch: 00082/00094 | Loss: 299.6141 | CE: 0.2067 | KD: 1060.1136\n",
      "Train Epoch: 070 Batch: 00083/00094 | Loss: 299.5335 | CE: 0.1452 | KD: 1060.0460\n",
      "Train Epoch: 070 Batch: 00084/00094 | Loss: 299.5751 | CE: 0.1765 | KD: 1060.0824\n",
      "Train Epoch: 070 Batch: 00085/00094 | Loss: 299.6544 | CE: 0.2343 | KD: 1060.1584\n",
      "Train Epoch: 070 Batch: 00086/00094 | Loss: 299.5864 | CE: 0.1831 | KD: 1060.0990\n",
      "Train Epoch: 070 Batch: 00087/00094 | Loss: 299.6002 | CE: 0.1738 | KD: 1060.1807\n",
      "Train Epoch: 070 Batch: 00088/00094 | Loss: 299.6829 | CE: 0.2483 | KD: 1060.2098\n",
      "Train Epoch: 070 Batch: 00089/00094 | Loss: 299.5414 | CE: 0.1552 | KD: 1060.0386\n",
      "Train Epoch: 070 Batch: 00090/00094 | Loss: 299.6695 | CE: 0.2466 | KD: 1060.1685\n",
      "Train Epoch: 070 Batch: 00091/00094 | Loss: 299.5160 | CE: 0.1184 | KD: 1060.0789\n",
      "Train Epoch: 070 Batch: 00092/00094 | Loss: 299.5852 | CE: 0.1956 | KD: 1060.0507\n",
      "Train Epoch: 070 Batch: 00093/00094 | Loss: 299.5584 | CE: 0.1508 | KD: 1060.1145\n",
      "Train Epoch: 070 Batch: 00094/00094 | Loss: 299.6284 | CE: 0.1950 | KD: 1060.2057\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2205 | acc:92.7000\n",
      "[VAL Acc] Target: 92.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2671 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1648 | acc:46.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 46.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2365 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6997 | acc:63.5972\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 63.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0008 | acc:54.6211\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5375 | acc:73.3542\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0310 | acc:55.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5515 | acc:72.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 72.10%\n",
      "[VAL Acc] Avg 61.87%\n",
      "Train Epoch: 071 Batch: 00001/00094 | Loss: 269.5871 | CE: 0.1231 | KD: 1060.1033\n",
      "Train Epoch: 071 Batch: 00002/00094 | Loss: 269.5659 | CE: 0.1318 | KD: 1059.9857\n",
      "Train Epoch: 071 Batch: 00003/00094 | Loss: 269.6784 | CE: 0.2162 | KD: 1060.0963\n",
      "Train Epoch: 071 Batch: 00004/00094 | Loss: 269.6516 | CE: 0.1796 | KD: 1060.1348\n",
      "Train Epoch: 071 Batch: 00005/00094 | Loss: 269.7050 | CE: 0.2374 | KD: 1060.1173\n",
      "Train Epoch: 071 Batch: 00006/00094 | Loss: 269.6156 | CE: 0.1455 | KD: 1060.1273\n",
      "Train Epoch: 071 Batch: 00007/00094 | Loss: 269.6244 | CE: 0.1830 | KD: 1060.0142\n",
      "Train Epoch: 071 Batch: 00008/00094 | Loss: 269.6169 | CE: 0.1651 | KD: 1060.0555\n",
      "Train Epoch: 071 Batch: 00009/00094 | Loss: 269.6628 | CE: 0.2006 | KD: 1060.0959\n",
      "Train Epoch: 071 Batch: 00010/00094 | Loss: 269.6429 | CE: 0.1648 | KD: 1060.1588\n",
      "Train Epoch: 071 Batch: 00011/00094 | Loss: 269.6777 | CE: 0.2118 | KD: 1060.1110\n",
      "Train Epoch: 071 Batch: 00012/00094 | Loss: 269.6292 | CE: 0.1573 | KD: 1060.1345\n",
      "Train Epoch: 071 Batch: 00013/00094 | Loss: 269.6693 | CE: 0.1878 | KD: 1060.1720\n",
      "Train Epoch: 071 Batch: 00014/00094 | Loss: 269.6158 | CE: 0.1658 | KD: 1060.0479\n",
      "Train Epoch: 071 Batch: 00015/00094 | Loss: 269.6909 | CE: 0.2241 | KD: 1060.1140\n",
      "Train Epoch: 071 Batch: 00016/00094 | Loss: 269.6137 | CE: 0.1504 | KD: 1060.1006\n",
      "Train Epoch: 071 Batch: 00017/00094 | Loss: 269.6447 | CE: 0.1661 | KD: 1060.1605\n",
      "Train Epoch: 071 Batch: 00018/00094 | Loss: 269.7232 | CE: 0.2245 | KD: 1060.2397\n",
      "Train Epoch: 071 Batch: 00019/00094 | Loss: 269.6573 | CE: 0.1998 | KD: 1060.0779\n",
      "Train Epoch: 071 Batch: 00020/00094 | Loss: 269.5862 | CE: 0.1451 | KD: 1060.0132\n",
      "Train Epoch: 071 Batch: 00021/00094 | Loss: 269.6228 | CE: 0.1675 | KD: 1060.0688\n",
      "Train Epoch: 071 Batch: 00022/00094 | Loss: 269.6419 | CE: 0.1689 | KD: 1060.1384\n",
      "Train Epoch: 071 Batch: 00023/00094 | Loss: 269.5573 | CE: 0.1172 | KD: 1060.0092\n",
      "Train Epoch: 071 Batch: 00024/00094 | Loss: 269.6710 | CE: 0.2070 | KD: 1060.1030\n",
      "Train Epoch: 071 Batch: 00025/00094 | Loss: 269.6052 | CE: 0.1197 | KD: 1060.1877\n",
      "Train Epoch: 071 Batch: 00026/00094 | Loss: 269.5881 | CE: 0.1239 | KD: 1060.1041\n",
      "Train Epoch: 071 Batch: 00027/00094 | Loss: 269.5962 | CE: 0.1340 | KD: 1060.0963\n",
      "Train Epoch: 071 Batch: 00028/00094 | Loss: 269.6801 | CE: 0.1997 | KD: 1060.1675\n",
      "Train Epoch: 071 Batch: 00029/00094 | Loss: 269.6213 | CE: 0.1576 | KD: 1060.1021\n",
      "Train Epoch: 071 Batch: 00030/00094 | Loss: 269.6718 | CE: 0.1966 | KD: 1060.1475\n",
      "Train Epoch: 071 Batch: 00031/00094 | Loss: 269.6216 | CE: 0.1728 | KD: 1060.0436\n",
      "Train Epoch: 071 Batch: 00032/00094 | Loss: 269.5912 | CE: 0.1266 | KD: 1060.1058\n",
      "Train Epoch: 071 Batch: 00033/00094 | Loss: 269.6738 | CE: 0.2011 | KD: 1060.1373\n",
      "Train Epoch: 071 Batch: 00034/00094 | Loss: 269.6815 | CE: 0.2041 | KD: 1060.1561\n",
      "Train Epoch: 071 Batch: 00035/00094 | Loss: 269.6597 | CE: 0.1902 | KD: 1060.1249\n",
      "Train Epoch: 071 Batch: 00036/00094 | Loss: 269.5957 | CE: 0.1294 | KD: 1060.1125\n",
      "Train Epoch: 071 Batch: 00037/00094 | Loss: 269.6875 | CE: 0.2228 | KD: 1060.1062\n",
      "Train Epoch: 071 Batch: 00038/00094 | Loss: 269.6054 | CE: 0.1522 | KD: 1060.0605\n",
      "Train Epoch: 071 Batch: 00039/00094 | Loss: 269.6617 | CE: 0.2032 | KD: 1060.0813\n",
      "Train Epoch: 071 Batch: 00040/00094 | Loss: 269.6419 | CE: 0.1955 | KD: 1060.0341\n",
      "Train Epoch: 071 Batch: 00041/00094 | Loss: 269.6541 | CE: 0.1788 | KD: 1060.1473\n",
      "Train Epoch: 071 Batch: 00042/00094 | Loss: 269.6883 | CE: 0.2381 | KD: 1060.0490\n",
      "Train Epoch: 071 Batch: 00043/00094 | Loss: 269.6496 | CE: 0.1812 | KD: 1060.1204\n",
      "Train Epoch: 071 Batch: 00044/00094 | Loss: 269.6346 | CE: 0.1715 | KD: 1060.0999\n",
      "Train Epoch: 071 Batch: 00045/00094 | Loss: 269.6800 | CE: 0.2112 | KD: 1060.1222\n",
      "Train Epoch: 071 Batch: 00046/00094 | Loss: 269.6307 | CE: 0.1631 | KD: 1060.1172\n",
      "Train Epoch: 071 Batch: 00047/00094 | Loss: 269.6115 | CE: 0.1490 | KD: 1060.0977\n",
      "Train Epoch: 071 Batch: 00048/00094 | Loss: 269.7178 | CE: 0.2322 | KD: 1060.1881\n",
      "Train Epoch: 071 Batch: 00049/00094 | Loss: 269.5912 | CE: 0.1233 | KD: 1060.1185\n",
      "Train Epoch: 071 Batch: 00050/00094 | Loss: 269.6281 | CE: 0.1877 | KD: 1060.0101\n",
      "Train Epoch: 071 Batch: 00051/00094 | Loss: 269.6069 | CE: 0.1538 | KD: 1060.0602\n",
      "Train Epoch: 071 Batch: 00052/00094 | Loss: 269.6644 | CE: 0.1933 | KD: 1060.1313\n",
      "Train Epoch: 071 Batch: 00053/00094 | Loss: 269.6160 | CE: 0.1715 | KD: 1060.0265\n",
      "Train Epoch: 071 Batch: 00054/00094 | Loss: 269.6232 | CE: 0.1592 | KD: 1060.1031\n",
      "Train Epoch: 071 Batch: 00055/00094 | Loss: 269.6755 | CE: 0.2245 | KD: 1060.0520\n",
      "Train Epoch: 071 Batch: 00056/00094 | Loss: 269.6691 | CE: 0.2065 | KD: 1060.0977\n",
      "Train Epoch: 071 Batch: 00057/00094 | Loss: 269.6702 | CE: 0.2171 | KD: 1060.0604\n",
      "Train Epoch: 071 Batch: 00058/00094 | Loss: 269.7349 | CE: 0.2558 | KD: 1060.1628\n",
      "Train Epoch: 071 Batch: 00059/00094 | Loss: 269.6107 | CE: 0.1568 | KD: 1060.0638\n",
      "Train Epoch: 071 Batch: 00060/00094 | Loss: 269.6456 | CE: 0.1788 | KD: 1060.1143\n",
      "Train Epoch: 071 Batch: 00061/00094 | Loss: 269.6036 | CE: 0.1543 | KD: 1060.0454\n",
      "Train Epoch: 071 Batch: 00062/00094 | Loss: 269.6003 | CE: 0.1315 | KD: 1060.1222\n",
      "Train Epoch: 071 Batch: 00063/00094 | Loss: 269.6226 | CE: 0.1646 | KD: 1060.0795\n",
      "Train Epoch: 071 Batch: 00064/00094 | Loss: 269.6050 | CE: 0.1515 | KD: 1060.0621\n",
      "Train Epoch: 071 Batch: 00065/00094 | Loss: 269.6327 | CE: 0.1783 | KD: 1060.0656\n",
      "Train Epoch: 071 Batch: 00066/00094 | Loss: 269.6309 | CE: 0.1754 | KD: 1060.0701\n",
      "Train Epoch: 071 Batch: 00067/00094 | Loss: 269.6746 | CE: 0.2119 | KD: 1060.0980\n",
      "Train Epoch: 071 Batch: 00068/00094 | Loss: 269.7023 | CE: 0.2300 | KD: 1060.1360\n",
      "Train Epoch: 071 Batch: 00069/00094 | Loss: 269.6151 | CE: 0.1570 | KD: 1060.0803\n",
      "Train Epoch: 071 Batch: 00070/00094 | Loss: 269.6349 | CE: 0.1799 | KD: 1060.0675\n",
      "Train Epoch: 071 Batch: 00071/00094 | Loss: 269.6295 | CE: 0.1745 | KD: 1060.0680\n",
      "Train Epoch: 071 Batch: 00072/00094 | Loss: 269.5867 | CE: 0.1374 | KD: 1060.0453\n",
      "Train Epoch: 071 Batch: 00073/00094 | Loss: 269.6237 | CE: 0.1463 | KD: 1060.1561\n",
      "Train Epoch: 071 Batch: 00074/00094 | Loss: 269.5973 | CE: 0.1508 | KD: 1060.0341\n",
      "Train Epoch: 071 Batch: 00075/00094 | Loss: 269.5621 | CE: 0.1189 | KD: 1060.0212\n",
      "Train Epoch: 071 Batch: 00076/00094 | Loss: 269.7004 | CE: 0.2133 | KD: 1060.1941\n",
      "Train Epoch: 071 Batch: 00077/00094 | Loss: 269.5979 | CE: 0.1194 | KD: 1060.1600\n",
      "Train Epoch: 071 Batch: 00078/00094 | Loss: 269.6599 | CE: 0.2000 | KD: 1060.0872\n",
      "Train Epoch: 071 Batch: 00079/00094 | Loss: 269.7185 | CE: 0.2083 | KD: 1060.2850\n",
      "Train Epoch: 071 Batch: 00080/00094 | Loss: 269.7089 | CE: 0.2321 | KD: 1060.1534\n",
      "Train Epoch: 071 Batch: 00081/00094 | Loss: 269.6671 | CE: 0.1727 | KD: 1060.2230\n",
      "Train Epoch: 071 Batch: 00082/00094 | Loss: 269.6711 | CE: 0.1899 | KD: 1060.1708\n",
      "Train Epoch: 071 Batch: 00083/00094 | Loss: 269.6204 | CE: 0.1465 | KD: 1060.1421\n",
      "Train Epoch: 071 Batch: 00084/00094 | Loss: 269.6096 | CE: 0.1690 | KD: 1060.0111\n",
      "Train Epoch: 071 Batch: 00085/00094 | Loss: 269.7194 | CE: 0.2333 | KD: 1060.1899\n",
      "Train Epoch: 071 Batch: 00086/00094 | Loss: 269.6504 | CE: 0.1909 | KD: 1060.0856\n",
      "Train Epoch: 071 Batch: 00087/00094 | Loss: 269.6013 | CE: 0.1659 | KD: 1059.9907\n",
      "Train Epoch: 071 Batch: 00088/00094 | Loss: 269.6375 | CE: 0.1751 | KD: 1060.0967\n",
      "Train Epoch: 071 Batch: 00089/00094 | Loss: 269.6969 | CE: 0.2460 | KD: 1060.0515\n",
      "Train Epoch: 071 Batch: 00090/00094 | Loss: 269.6524 | CE: 0.1952 | KD: 1060.0765\n",
      "Train Epoch: 071 Batch: 00091/00094 | Loss: 269.6681 | CE: 0.1691 | KD: 1060.2410\n",
      "Train Epoch: 071 Batch: 00092/00094 | Loss: 269.6357 | CE: 0.1897 | KD: 1060.0325\n",
      "Train Epoch: 071 Batch: 00093/00094 | Loss: 269.6319 | CE: 0.1708 | KD: 1060.0919\n",
      "Train Epoch: 071 Batch: 00094/00094 | Loss: 269.6983 | CE: 0.2012 | KD: 1060.2334\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2086 | acc:93.3000\n",
      "[VAL Acc] Target: 93.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2751 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1577 | acc:46.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 46.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2182 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7275 | acc:62.5784\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0507 | acc:54.1590\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5581 | acc:73.1975\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 73.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0705 | acc:55.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5438 | acc:72.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 72.05%\n",
      "[VAL Acc] Avg 61.93%\n",
      "Train Epoch: 072 Batch: 00001/00094 | Loss: 269.6489 | CE: 0.2097 | KD: 1060.0059\n",
      "Train Epoch: 072 Batch: 00002/00094 | Loss: 269.6136 | CE: 0.1461 | KD: 1060.1167\n",
      "Train Epoch: 072 Batch: 00003/00094 | Loss: 269.6909 | CE: 0.1770 | KD: 1060.2994\n",
      "Train Epoch: 072 Batch: 00004/00094 | Loss: 269.6614 | CE: 0.1987 | KD: 1060.0981\n",
      "Train Epoch: 072 Batch: 00005/00094 | Loss: 269.6104 | CE: 0.1496 | KD: 1060.0907\n",
      "Train Epoch: 072 Batch: 00006/00094 | Loss: 269.6899 | CE: 0.2301 | KD: 1060.0869\n",
      "Train Epoch: 072 Batch: 00007/00094 | Loss: 269.6231 | CE: 0.1606 | KD: 1060.0973\n",
      "Train Epoch: 072 Batch: 00008/00094 | Loss: 269.7106 | CE: 0.2387 | KD: 1060.1345\n",
      "Train Epoch: 072 Batch: 00009/00094 | Loss: 269.6646 | CE: 0.1931 | KD: 1060.1324\n",
      "Train Epoch: 072 Batch: 00010/00094 | Loss: 269.6485 | CE: 0.1736 | KD: 1060.1461\n",
      "Train Epoch: 072 Batch: 00011/00094 | Loss: 269.6472 | CE: 0.1796 | KD: 1060.1174\n",
      "Train Epoch: 072 Batch: 00012/00094 | Loss: 269.6216 | CE: 0.1732 | KD: 1060.0421\n",
      "Train Epoch: 072 Batch: 00013/00094 | Loss: 269.6577 | CE: 0.1670 | KD: 1060.2085\n",
      "Train Epoch: 072 Batch: 00014/00094 | Loss: 269.6459 | CE: 0.1801 | KD: 1060.1101\n",
      "Train Epoch: 072 Batch: 00015/00094 | Loss: 269.6348 | CE: 0.1807 | KD: 1060.0643\n",
      "Train Epoch: 072 Batch: 00016/00094 | Loss: 269.6225 | CE: 0.1570 | KD: 1060.1091\n",
      "Train Epoch: 072 Batch: 00017/00094 | Loss: 269.6431 | CE: 0.1960 | KD: 1060.0367\n",
      "Train Epoch: 072 Batch: 00018/00094 | Loss: 269.5673 | CE: 0.1236 | KD: 1060.0232\n",
      "Train Epoch: 072 Batch: 00019/00094 | Loss: 269.6425 | CE: 0.1634 | KD: 1060.1626\n",
      "Train Epoch: 072 Batch: 00020/00094 | Loss: 269.6760 | CE: 0.2040 | KD: 1060.1348\n",
      "Train Epoch: 072 Batch: 00021/00094 | Loss: 269.6547 | CE: 0.1978 | KD: 1060.0754\n",
      "Train Epoch: 072 Batch: 00022/00094 | Loss: 269.6641 | CE: 0.1830 | KD: 1060.1705\n",
      "Train Epoch: 072 Batch: 00023/00094 | Loss: 269.7162 | CE: 0.2454 | KD: 1060.1302\n",
      "Train Epoch: 072 Batch: 00024/00094 | Loss: 269.7072 | CE: 0.2253 | KD: 1060.1740\n",
      "Train Epoch: 072 Batch: 00025/00094 | Loss: 269.6831 | CE: 0.2204 | KD: 1060.0983\n",
      "Train Epoch: 072 Batch: 00026/00094 | Loss: 269.7090 | CE: 0.2177 | KD: 1060.2104\n",
      "Train Epoch: 072 Batch: 00027/00094 | Loss: 269.6925 | CE: 0.2169 | KD: 1060.1488\n",
      "Train Epoch: 072 Batch: 00028/00094 | Loss: 269.7092 | CE: 0.2001 | KD: 1060.2808\n",
      "Train Epoch: 072 Batch: 00029/00094 | Loss: 269.6057 | CE: 0.1558 | KD: 1060.0476\n",
      "Train Epoch: 072 Batch: 00030/00094 | Loss: 269.5950 | CE: 0.1395 | KD: 1060.0699\n",
      "Train Epoch: 072 Batch: 00031/00094 | Loss: 269.6496 | CE: 0.1691 | KD: 1060.1681\n",
      "Train Epoch: 072 Batch: 00032/00094 | Loss: 269.6609 | CE: 0.1616 | KD: 1060.2419\n",
      "Train Epoch: 072 Batch: 00033/00094 | Loss: 269.7082 | CE: 0.2520 | KD: 1060.0724\n",
      "Train Epoch: 072 Batch: 00034/00094 | Loss: 269.5938 | CE: 0.1460 | KD: 1060.0394\n",
      "Train Epoch: 072 Batch: 00035/00094 | Loss: 269.6482 | CE: 0.1982 | KD: 1060.0481\n",
      "Train Epoch: 072 Batch: 00036/00094 | Loss: 269.6393 | CE: 0.1576 | KD: 1060.1726\n",
      "Train Epoch: 072 Batch: 00037/00094 | Loss: 269.6322 | CE: 0.1746 | KD: 1060.0779\n",
      "Train Epoch: 072 Batch: 00038/00094 | Loss: 269.6547 | CE: 0.1906 | KD: 1060.1036\n",
      "Train Epoch: 072 Batch: 00039/00094 | Loss: 269.6567 | CE: 0.1951 | KD: 1060.0940\n",
      "Train Epoch: 072 Batch: 00040/00094 | Loss: 269.6334 | CE: 0.1700 | KD: 1060.1008\n",
      "Train Epoch: 072 Batch: 00041/00094 | Loss: 269.6245 | CE: 0.1623 | KD: 1060.0963\n",
      "Train Epoch: 072 Batch: 00042/00094 | Loss: 269.7034 | CE: 0.2053 | KD: 1060.2374\n",
      "Train Epoch: 072 Batch: 00043/00094 | Loss: 269.5696 | CE: 0.1074 | KD: 1060.0964\n",
      "Train Epoch: 072 Batch: 00044/00094 | Loss: 269.6024 | CE: 0.1592 | KD: 1060.0211\n",
      "Train Epoch: 072 Batch: 00045/00094 | Loss: 269.6453 | CE: 0.2009 | KD: 1060.0260\n",
      "Train Epoch: 072 Batch: 00046/00094 | Loss: 269.6405 | CE: 0.1782 | KD: 1060.0968\n",
      "Train Epoch: 072 Batch: 00047/00094 | Loss: 269.7380 | CE: 0.2748 | KD: 1060.0999\n",
      "Train Epoch: 072 Batch: 00048/00094 | Loss: 269.6148 | CE: 0.1416 | KD: 1060.1396\n",
      "Train Epoch: 072 Batch: 00049/00094 | Loss: 269.7530 | CE: 0.2857 | KD: 1060.1160\n",
      "Train Epoch: 072 Batch: 00050/00094 | Loss: 269.7217 | CE: 0.2575 | KD: 1060.1039\n",
      "Train Epoch: 072 Batch: 00051/00094 | Loss: 269.6512 | CE: 0.1689 | KD: 1060.1750\n",
      "Train Epoch: 072 Batch: 00052/00094 | Loss: 269.6962 | CE: 0.2189 | KD: 1060.1555\n",
      "Train Epoch: 072 Batch: 00053/00094 | Loss: 269.6246 | CE: 0.1652 | KD: 1060.0850\n",
      "Train Epoch: 072 Batch: 00054/00094 | Loss: 269.7438 | CE: 0.2597 | KD: 1060.1824\n",
      "Train Epoch: 072 Batch: 00055/00094 | Loss: 269.6372 | CE: 0.1892 | KD: 1060.0404\n",
      "Train Epoch: 072 Batch: 00056/00094 | Loss: 269.6451 | CE: 0.1504 | KD: 1060.2241\n",
      "Train Epoch: 072 Batch: 00057/00094 | Loss: 269.6606 | CE: 0.1609 | KD: 1060.2435\n",
      "Train Epoch: 072 Batch: 00058/00094 | Loss: 269.5785 | CE: 0.1259 | KD: 1060.0586\n",
      "Train Epoch: 072 Batch: 00059/00094 | Loss: 269.5822 | CE: 0.1395 | KD: 1060.0197\n",
      "Train Epoch: 072 Batch: 00060/00094 | Loss: 269.6455 | CE: 0.1764 | KD: 1060.1233\n",
      "Train Epoch: 072 Batch: 00061/00094 | Loss: 269.6187 | CE: 0.1427 | KD: 1060.1504\n",
      "Train Epoch: 072 Batch: 00062/00094 | Loss: 269.6098 | CE: 0.1496 | KD: 1060.0880\n",
      "Train Epoch: 072 Batch: 00063/00094 | Loss: 269.6190 | CE: 0.1590 | KD: 1060.0875\n",
      "Train Epoch: 072 Batch: 00064/00094 | Loss: 269.6929 | CE: 0.2323 | KD: 1060.0898\n",
      "Train Epoch: 072 Batch: 00065/00094 | Loss: 269.5970 | CE: 0.1303 | KD: 1060.1138\n",
      "Train Epoch: 072 Batch: 00066/00094 | Loss: 269.7542 | CE: 0.2654 | KD: 1060.2009\n",
      "Train Epoch: 072 Batch: 00067/00094 | Loss: 269.5767 | CE: 0.1378 | KD: 1060.0045\n",
      "Train Epoch: 072 Batch: 00068/00094 | Loss: 269.6727 | CE: 0.2126 | KD: 1060.0880\n",
      "Train Epoch: 072 Batch: 00069/00094 | Loss: 269.6229 | CE: 0.1608 | KD: 1060.0958\n",
      "Train Epoch: 072 Batch: 00070/00094 | Loss: 269.6320 | CE: 0.1513 | KD: 1060.1689\n",
      "Train Epoch: 072 Batch: 00071/00094 | Loss: 269.6908 | CE: 0.2095 | KD: 1060.1714\n",
      "Train Epoch: 072 Batch: 00072/00094 | Loss: 269.7536 | CE: 0.2317 | KD: 1060.3313\n",
      "Train Epoch: 072 Batch: 00073/00094 | Loss: 269.5634 | CE: 0.1132 | KD: 1060.0488\n",
      "Train Epoch: 072 Batch: 00074/00094 | Loss: 269.5775 | CE: 0.1238 | KD: 1060.0626\n",
      "Train Epoch: 072 Batch: 00075/00094 | Loss: 269.6638 | CE: 0.1714 | KD: 1060.2150\n",
      "Train Epoch: 072 Batch: 00076/00094 | Loss: 269.5635 | CE: 0.1165 | KD: 1060.0364\n",
      "Train Epoch: 072 Batch: 00077/00094 | Loss: 269.5992 | CE: 0.1389 | KD: 1060.0889\n",
      "Train Epoch: 072 Batch: 00078/00094 | Loss: 269.6226 | CE: 0.1477 | KD: 1060.1461\n",
      "Train Epoch: 072 Batch: 00079/00094 | Loss: 269.7237 | CE: 0.2277 | KD: 1060.2294\n",
      "Train Epoch: 072 Batch: 00080/00094 | Loss: 269.6248 | CE: 0.1528 | KD: 1060.1345\n",
      "Train Epoch: 072 Batch: 00081/00094 | Loss: 269.6562 | CE: 0.2081 | KD: 1060.0406\n",
      "Train Epoch: 072 Batch: 00082/00094 | Loss: 269.7176 | CE: 0.2459 | KD: 1060.1333\n",
      "Train Epoch: 072 Batch: 00083/00094 | Loss: 269.6679 | CE: 0.1844 | KD: 1060.1798\n",
      "Train Epoch: 072 Batch: 00084/00094 | Loss: 269.5930 | CE: 0.1464 | KD: 1060.0348\n",
      "Train Epoch: 072 Batch: 00085/00094 | Loss: 269.5953 | CE: 0.1433 | KD: 1060.0560\n",
      "Train Epoch: 072 Batch: 00086/00094 | Loss: 269.7204 | CE: 0.2295 | KD: 1060.2090\n",
      "Train Epoch: 072 Batch: 00087/00094 | Loss: 269.6496 | CE: 0.1902 | KD: 1060.0852\n",
      "Train Epoch: 072 Batch: 00088/00094 | Loss: 269.6316 | CE: 0.1579 | KD: 1060.1415\n",
      "Train Epoch: 072 Batch: 00089/00094 | Loss: 269.6914 | CE: 0.2063 | KD: 1060.1864\n",
      "Train Epoch: 072 Batch: 00090/00094 | Loss: 269.6648 | CE: 0.2066 | KD: 1060.0803\n",
      "Train Epoch: 072 Batch: 00091/00094 | Loss: 269.5961 | CE: 0.1482 | KD: 1060.0398\n",
      "Train Epoch: 072 Batch: 00092/00094 | Loss: 269.8132 | CE: 0.3318 | KD: 1060.1718\n",
      "Train Epoch: 072 Batch: 00093/00094 | Loss: 269.6148 | CE: 0.1448 | KD: 1060.1271\n",
      "Train Epoch: 072 Batch: 00094/00094 | Loss: 269.6528 | CE: 0.1713 | KD: 1060.1722\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2232 | acc:92.0000\n",
      "[VAL Acc] Target: 92.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2565 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1338 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1786 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6912 | acc:64.2241\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 64.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0705 | acc:53.8817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5389 | acc:74.4122\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0954 | acc:55.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5299 | acc:73.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.00%\n",
      "[VAL Acc] Avg 62.17%\n",
      "Train Epoch: 073 Batch: 00001/00094 | Loss: 269.7073 | CE: 0.2317 | KD: 1060.1489\n",
      "Train Epoch: 073 Batch: 00002/00094 | Loss: 269.6526 | CE: 0.1884 | KD: 1060.1042\n",
      "Train Epoch: 073 Batch: 00003/00094 | Loss: 269.6111 | CE: 0.1249 | KD: 1060.1908\n",
      "Train Epoch: 073 Batch: 00004/00094 | Loss: 269.6726 | CE: 0.2039 | KD: 1060.1218\n",
      "Train Epoch: 073 Batch: 00005/00094 | Loss: 269.7263 | CE: 0.2264 | KD: 1060.2448\n",
      "Train Epoch: 073 Batch: 00006/00094 | Loss: 269.7012 | CE: 0.2368 | KD: 1060.1047\n",
      "Train Epoch: 073 Batch: 00007/00094 | Loss: 269.6985 | CE: 0.2031 | KD: 1060.2268\n",
      "Train Epoch: 073 Batch: 00008/00094 | Loss: 269.6723 | CE: 0.2133 | KD: 1060.0836\n",
      "Train Epoch: 073 Batch: 00009/00094 | Loss: 269.6187 | CE: 0.1817 | KD: 1059.9968\n",
      "Train Epoch: 073 Batch: 00010/00094 | Loss: 269.6965 | CE: 0.2111 | KD: 1060.1875\n",
      "Train Epoch: 073 Batch: 00011/00094 | Loss: 269.6779 | CE: 0.2077 | KD: 1060.1276\n",
      "Train Epoch: 073 Batch: 00012/00094 | Loss: 269.5643 | CE: 0.1152 | KD: 1060.0447\n",
      "Train Epoch: 073 Batch: 00013/00094 | Loss: 269.6471 | CE: 0.2034 | KD: 1060.0234\n",
      "Train Epoch: 073 Batch: 00014/00094 | Loss: 269.6594 | CE: 0.2060 | KD: 1060.0615\n",
      "Train Epoch: 073 Batch: 00015/00094 | Loss: 269.6781 | CE: 0.2039 | KD: 1060.1433\n",
      "Train Epoch: 073 Batch: 00016/00094 | Loss: 269.7188 | CE: 0.2642 | KD: 1060.0662\n",
      "Train Epoch: 073 Batch: 00017/00094 | Loss: 269.6686 | CE: 0.1632 | KD: 1060.2660\n",
      "Train Epoch: 073 Batch: 00018/00094 | Loss: 269.6681 | CE: 0.1733 | KD: 1060.2245\n",
      "Train Epoch: 073 Batch: 00019/00094 | Loss: 269.6006 | CE: 0.1379 | KD: 1060.0983\n",
      "Train Epoch: 073 Batch: 00020/00094 | Loss: 269.5973 | CE: 0.1436 | KD: 1060.0629\n",
      "Train Epoch: 073 Batch: 00021/00094 | Loss: 269.8812 | CE: 0.3845 | KD: 1060.2318\n",
      "Train Epoch: 073 Batch: 00022/00094 | Loss: 269.6647 | CE: 0.1829 | KD: 1060.1733\n",
      "Train Epoch: 073 Batch: 00023/00094 | Loss: 269.6660 | CE: 0.2228 | KD: 1060.0216\n",
      "Train Epoch: 073 Batch: 00024/00094 | Loss: 269.6302 | CE: 0.1676 | KD: 1060.0979\n",
      "Train Epoch: 073 Batch: 00025/00094 | Loss: 269.6272 | CE: 0.1801 | KD: 1060.0366\n",
      "Train Epoch: 073 Batch: 00026/00094 | Loss: 269.6519 | CE: 0.1961 | KD: 1060.0712\n",
      "Train Epoch: 073 Batch: 00027/00094 | Loss: 269.6657 | CE: 0.1834 | KD: 1060.1754\n",
      "Train Epoch: 073 Batch: 00028/00094 | Loss: 269.6565 | CE: 0.1820 | KD: 1060.1443\n",
      "Train Epoch: 073 Batch: 00029/00094 | Loss: 269.6017 | CE: 0.1520 | KD: 1060.0468\n",
      "Train Epoch: 073 Batch: 00030/00094 | Loss: 269.7490 | CE: 0.2659 | KD: 1060.1782\n",
      "Train Epoch: 073 Batch: 00031/00094 | Loss: 269.7051 | CE: 0.2408 | KD: 1060.1042\n",
      "Train Epoch: 073 Batch: 00032/00094 | Loss: 269.6406 | CE: 0.1733 | KD: 1060.1162\n",
      "Train Epoch: 073 Batch: 00033/00094 | Loss: 269.5938 | CE: 0.1262 | KD: 1060.1177\n",
      "Train Epoch: 073 Batch: 00034/00094 | Loss: 269.6120 | CE: 0.1373 | KD: 1060.1453\n",
      "Train Epoch: 073 Batch: 00035/00094 | Loss: 269.6326 | CE: 0.1696 | KD: 1060.0994\n",
      "Train Epoch: 073 Batch: 00036/00094 | Loss: 269.5983 | CE: 0.1574 | KD: 1060.0125\n",
      "Train Epoch: 073 Batch: 00037/00094 | Loss: 269.5965 | CE: 0.1437 | KD: 1060.0592\n",
      "Train Epoch: 073 Batch: 00038/00094 | Loss: 269.5804 | CE: 0.1363 | KD: 1060.0245\n",
      "Train Epoch: 073 Batch: 00039/00094 | Loss: 269.6420 | CE: 0.2095 | KD: 1059.9794\n",
      "Train Epoch: 073 Batch: 00040/00094 | Loss: 269.6119 | CE: 0.1652 | KD: 1060.0353\n",
      "Train Epoch: 073 Batch: 00041/00094 | Loss: 269.6706 | CE: 0.1925 | KD: 1060.1587\n",
      "Train Epoch: 073 Batch: 00042/00094 | Loss: 269.6697 | CE: 0.2040 | KD: 1060.1099\n",
      "Train Epoch: 073 Batch: 00043/00094 | Loss: 269.6499 | CE: 0.1736 | KD: 1060.1517\n",
      "Train Epoch: 073 Batch: 00044/00094 | Loss: 269.5764 | CE: 0.1066 | KD: 1060.1259\n",
      "Train Epoch: 073 Batch: 00045/00094 | Loss: 269.6329 | CE: 0.1361 | KD: 1060.2321\n",
      "Train Epoch: 073 Batch: 00046/00094 | Loss: 269.5823 | CE: 0.1493 | KD: 1059.9810\n",
      "Train Epoch: 073 Batch: 00047/00094 | Loss: 269.6782 | CE: 0.2055 | KD: 1060.1372\n",
      "Train Epoch: 073 Batch: 00048/00094 | Loss: 269.6427 | CE: 0.1914 | KD: 1060.0532\n",
      "Train Epoch: 073 Batch: 00049/00094 | Loss: 269.6459 | CE: 0.1562 | KD: 1060.2043\n",
      "Train Epoch: 073 Batch: 00050/00094 | Loss: 269.6525 | CE: 0.2004 | KD: 1060.0565\n",
      "Train Epoch: 073 Batch: 00051/00094 | Loss: 269.6137 | CE: 0.1531 | KD: 1060.0898\n",
      "Train Epoch: 073 Batch: 00052/00094 | Loss: 269.6524 | CE: 0.1524 | KD: 1060.2449\n",
      "Train Epoch: 073 Batch: 00053/00094 | Loss: 269.6496 | CE: 0.1897 | KD: 1060.0870\n",
      "Train Epoch: 073 Batch: 00054/00094 | Loss: 269.6247 | CE: 0.1489 | KD: 1060.1497\n",
      "Train Epoch: 073 Batch: 00055/00094 | Loss: 269.6195 | CE: 0.1632 | KD: 1060.0728\n",
      "Train Epoch: 073 Batch: 00056/00094 | Loss: 269.6582 | CE: 0.1995 | KD: 1060.0823\n",
      "Train Epoch: 073 Batch: 00057/00094 | Loss: 269.6595 | CE: 0.1705 | KD: 1060.2013\n",
      "Train Epoch: 073 Batch: 00058/00094 | Loss: 269.6312 | CE: 0.1603 | KD: 1060.1301\n",
      "Train Epoch: 073 Batch: 00059/00094 | Loss: 269.5811 | CE: 0.1110 | KD: 1060.1276\n",
      "Train Epoch: 073 Batch: 00060/00094 | Loss: 269.6211 | CE: 0.1354 | KD: 1060.1886\n",
      "Train Epoch: 073 Batch: 00061/00094 | Loss: 269.6052 | CE: 0.1608 | KD: 1060.0261\n",
      "Train Epoch: 073 Batch: 00062/00094 | Loss: 269.6119 | CE: 0.1532 | KD: 1060.0824\n",
      "Train Epoch: 073 Batch: 00063/00094 | Loss: 269.7076 | CE: 0.2396 | KD: 1060.1188\n",
      "Train Epoch: 073 Batch: 00064/00094 | Loss: 269.6195 | CE: 0.1758 | KD: 1060.0234\n",
      "Train Epoch: 073 Batch: 00065/00094 | Loss: 269.6810 | CE: 0.1987 | KD: 1060.1753\n",
      "Train Epoch: 073 Batch: 00066/00094 | Loss: 269.6185 | CE: 0.1660 | KD: 1060.0579\n",
      "Train Epoch: 073 Batch: 00067/00094 | Loss: 269.6974 | CE: 0.2483 | KD: 1060.0447\n",
      "Train Epoch: 073 Batch: 00068/00094 | Loss: 269.6250 | CE: 0.1408 | KD: 1060.1825\n",
      "Train Epoch: 073 Batch: 00069/00094 | Loss: 269.7006 | CE: 0.2378 | KD: 1060.0984\n",
      "Train Epoch: 073 Batch: 00070/00094 | Loss: 269.6725 | CE: 0.2015 | KD: 1060.1305\n",
      "Train Epoch: 073 Batch: 00071/00094 | Loss: 269.6340 | CE: 0.1827 | KD: 1060.0532\n",
      "Train Epoch: 073 Batch: 00072/00094 | Loss: 269.6108 | CE: 0.1695 | KD: 1060.0139\n",
      "Train Epoch: 073 Batch: 00073/00094 | Loss: 269.6937 | CE: 0.2424 | KD: 1060.0530\n",
      "Train Epoch: 073 Batch: 00074/00094 | Loss: 269.6359 | CE: 0.1646 | KD: 1060.1320\n",
      "Train Epoch: 073 Batch: 00075/00094 | Loss: 269.7231 | CE: 0.2321 | KD: 1060.2094\n",
      "Train Epoch: 073 Batch: 00076/00094 | Loss: 269.6569 | CE: 0.1721 | KD: 1060.1851\n",
      "Train Epoch: 073 Batch: 00077/00094 | Loss: 269.6645 | CE: 0.1704 | KD: 1060.2217\n",
      "Train Epoch: 073 Batch: 00078/00094 | Loss: 269.6741 | CE: 0.1909 | KD: 1060.1790\n",
      "Train Epoch: 073 Batch: 00079/00094 | Loss: 269.6117 | CE: 0.1377 | KD: 1060.1423\n",
      "Train Epoch: 073 Batch: 00080/00094 | Loss: 269.6477 | CE: 0.1878 | KD: 1060.0873\n",
      "Train Epoch: 073 Batch: 00081/00094 | Loss: 269.6515 | CE: 0.1622 | KD: 1060.2025\n",
      "Train Epoch: 073 Batch: 00082/00094 | Loss: 269.6345 | CE: 0.1758 | KD: 1060.0825\n",
      "Train Epoch: 073 Batch: 00083/00094 | Loss: 269.6630 | CE: 0.1897 | KD: 1060.1400\n",
      "Train Epoch: 073 Batch: 00084/00094 | Loss: 269.6058 | CE: 0.1645 | KD: 1060.0138\n",
      "Train Epoch: 073 Batch: 00085/00094 | Loss: 269.7068 | CE: 0.2515 | KD: 1060.0691\n",
      "Train Epoch: 073 Batch: 00086/00094 | Loss: 269.6703 | CE: 0.2208 | KD: 1060.0461\n",
      "Train Epoch: 073 Batch: 00087/00094 | Loss: 269.6080 | CE: 0.1518 | KD: 1060.0725\n",
      "Train Epoch: 073 Batch: 00088/00094 | Loss: 269.6347 | CE: 0.1643 | KD: 1060.1283\n",
      "Train Epoch: 073 Batch: 00089/00094 | Loss: 269.6440 | CE: 0.1613 | KD: 1060.1766\n",
      "Train Epoch: 073 Batch: 00090/00094 | Loss: 269.7042 | CE: 0.2238 | KD: 1060.1676\n",
      "Train Epoch: 073 Batch: 00091/00094 | Loss: 269.6956 | CE: 0.2354 | KD: 1060.0881\n",
      "Train Epoch: 073 Batch: 00092/00094 | Loss: 269.5522 | CE: 0.1252 | KD: 1059.9576\n",
      "Train Epoch: 073 Batch: 00093/00094 | Loss: 269.6400 | CE: 0.1856 | KD: 1060.0656\n",
      "Train Epoch: 073 Batch: 00094/00094 | Loss: 269.6595 | CE: 0.1660 | KD: 1060.2196\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2089 | acc:93.3500\n",
      "[VAL Acc] Target: 93.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2654 | acc:50.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1264 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2204 | acc:47.9008\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6850 | acc:65.0078\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 65.01%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9672 | acc:54.8983\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5324 | acc:75.4310\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0328 | acc:55.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.5218 | acc:73.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 73.30%\n",
      "[VAL Acc] Avg 62.70%\n",
      "Early stopping ...\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : t18_diff_elsa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/6942265c854546f181de957f3116e751\u001b[0m\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/biggan_val_acc [73]         : (46.25, 53.25)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/crn_val_acc [73]            : (56.112852664576806, 81.54388714733543)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/cyclegan_val_acc [73]       : (46.37404580152672, 51.717557251908396)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/diffusionshort_val_acc [73] : (68.35, 82.8)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/faceforensics_val_acc [73]  : (52.2181146025878, 60.998151571164506)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/gaugan_val_acc [73]         : (48.949999999999996, 51.7)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/imle_val_acc [73]           : (50.43103448275862, 70.45454545454545)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/wild_val_acc [73]           : (52.5, 59.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/target_val_acc [73]                                              : (82.85, 93.4)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/val_acc [73]                                                     : (58.599399852974365, 63.452941207621194)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [1779]                                                          : (269.58624267578125, 1060.759033203125)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss [6862]                                                   : (269.55224609375, 1062.0045166015625)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_kd [6862]                                                : (1059.867431640625, 1060.7860107421875)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_main [6862]                                              : (0.10528656840324402, 1.2185229063034058)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start_acc                                                            : 61.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : t18_diff_elsa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size           : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config_file          : model_config.conf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decay_factor         : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stop           : 35\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs               : 250\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flip                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kd_alpha             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr                   : 0.005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_schedule          : cosine\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_gpu              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     output_model_version : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resolution           : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (410.92 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1 (7.34 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\", \"biggan\", \"cyclegan\", \"imle\", \"faceforensics\", \"crn\", \"wild\", \"diffusionshort\"] \n",
    "TARGET_DATASET = \"elsa\"\n",
    "NETWORK = \"ResNet18\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/KD_r18/diff\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/KD_r18/diff\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "COMET_NAME = \"t18_diff_elsa\"\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='model_config.conf', network='Xception', input_model='../KD-AIGC-Detection/checkpoints/KD_Xception/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort', output_dir='../KD-AIGC-Detection/checkpoints/KD_Xception/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort_elsa', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild,../KD-AIGC-Detection/datasets/custom/diffusionshort', target_dataset='../KD-AIGC-Detection/datasets/custom/elsa', use_comet=True, comet_name='txc_diff_elsa', num_gpu='0', output_model_version='3', epochs='250', early_stop='35', batch_size='64', resolution='128', lr_schedule='cosine', lr='0.005', kd_alpha='1', decay_factor='0.9', flip='False')\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/0cd1a97b7b0b4523bf338e802aae1142\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/imle\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/crn\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/wild\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild,../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/elsa/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/elsa/train\n",
      "Dataset available in train_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan / ../KD-AIGC-Detection/datasets/custom/biggan / ../KD-AIGC-Detection/datasets/custom/cyclegan / ../KD-AIGC-Detection/datasets/custom/imle / ../KD-AIGC-Detection/datasets/custom/faceforensics / ../KD-AIGC-Detection/datasets/custom/crn / ../KD-AIGC-Detection/datasets/custom/wild / ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Loading Xception from ../KD-AIGC-Detection/checkpoints/KD_Xception/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort/model_best_accuracy.pth\n",
      "Apply Cosine learning rate schedule\n",
      "/home/fra/miniconda3/envs/paper/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:1.6500 | acc:49.7500\n",
      "Start Target Validation ACC: 49.75%\n",
      "Start training in 250 epochs\n",
      "Train Epoch: 001 Batch: 00001/00094 | Loss: 1061.9781 | CE: 0.8622 | KD: 1061.1160\n",
      "Train Epoch: 001 Batch: 00002/00094 | Loss: 1062.1224 | CE: 1.1597 | KD: 1060.9626\n",
      "Train Epoch: 001 Batch: 00003/00094 | Loss: 1061.5453 | CE: 0.8495 | KD: 1060.6958\n",
      "Train Epoch: 001 Batch: 00004/00094 | Loss: 1061.5304 | CE: 0.7596 | KD: 1060.7709\n",
      "Train Epoch: 001 Batch: 00005/00094 | Loss: 1061.6649 | CE: 0.8738 | KD: 1060.7911\n",
      "Train Epoch: 001 Batch: 00006/00094 | Loss: 1061.3807 | CE: 0.6699 | KD: 1060.7109\n",
      "Train Epoch: 001 Batch: 00007/00094 | Loss: 1061.6335 | CE: 0.6392 | KD: 1060.9944\n",
      "Train Epoch: 001 Batch: 00008/00094 | Loss: 1061.0863 | CE: 0.5468 | KD: 1060.5396\n",
      "Train Epoch: 001 Batch: 00009/00094 | Loss: 1061.2815 | CE: 0.6304 | KD: 1060.6510\n",
      "Train Epoch: 001 Batch: 00010/00094 | Loss: 1061.1937 | CE: 0.6688 | KD: 1060.5249\n",
      "Train Epoch: 001 Batch: 00011/00094 | Loss: 1061.3350 | CE: 0.6840 | KD: 1060.6509\n",
      "Train Epoch: 001 Batch: 00012/00094 | Loss: 1061.2422 | CE: 0.5143 | KD: 1060.7279\n",
      "Train Epoch: 001 Batch: 00013/00094 | Loss: 1062.3254 | CE: 0.7414 | KD: 1061.5841\n",
      "Train Epoch: 001 Batch: 00014/00094 | Loss: 1061.9008 | CE: 0.6950 | KD: 1061.2057\n",
      "Train Epoch: 001 Batch: 00015/00094 | Loss: 1061.0001 | CE: 0.4758 | KD: 1060.5243\n",
      "Train Epoch: 001 Batch: 00016/00094 | Loss: 1061.1072 | CE: 0.6778 | KD: 1060.4294\n",
      "Train Epoch: 001 Batch: 00017/00094 | Loss: 1061.1189 | CE: 0.5381 | KD: 1060.5808\n",
      "Train Epoch: 001 Batch: 00018/00094 | Loss: 1061.6755 | CE: 0.6172 | KD: 1061.0583\n",
      "Train Epoch: 001 Batch: 00019/00094 | Loss: 1061.1595 | CE: 0.6342 | KD: 1060.5254\n",
      "Train Epoch: 001 Batch: 00020/00094 | Loss: 1061.0289 | CE: 0.6147 | KD: 1060.4142\n",
      "Train Epoch: 001 Batch: 00021/00094 | Loss: 1061.1857 | CE: 0.6602 | KD: 1060.5254\n",
      "Train Epoch: 001 Batch: 00022/00094 | Loss: 1061.0989 | CE: 0.6080 | KD: 1060.4910\n",
      "Train Epoch: 001 Batch: 00023/00094 | Loss: 1061.5699 | CE: 0.6025 | KD: 1060.9675\n",
      "Train Epoch: 001 Batch: 00024/00094 | Loss: 1061.6100 | CE: 0.5664 | KD: 1061.0436\n",
      "Train Epoch: 001 Batch: 00025/00094 | Loss: 1061.3282 | CE: 0.6283 | KD: 1060.7000\n",
      "Train Epoch: 001 Batch: 00026/00094 | Loss: 1061.0433 | CE: 0.5775 | KD: 1060.4658\n",
      "Train Epoch: 001 Batch: 00027/00094 | Loss: 1061.0393 | CE: 0.6258 | KD: 1060.4135\n",
      "Train Epoch: 001 Batch: 00028/00094 | Loss: 1061.3197 | CE: 0.6606 | KD: 1060.6592\n",
      "Train Epoch: 001 Batch: 00029/00094 | Loss: 1061.3986 | CE: 0.6091 | KD: 1060.7894\n",
      "Train Epoch: 001 Batch: 00030/00094 | Loss: 1061.0057 | CE: 0.5059 | KD: 1060.4999\n",
      "Train Epoch: 001 Batch: 00031/00094 | Loss: 1061.2803 | CE: 0.4718 | KD: 1060.8085\n",
      "Train Epoch: 001 Batch: 00032/00094 | Loss: 1061.2521 | CE: 0.6032 | KD: 1060.6488\n",
      "Train Epoch: 001 Batch: 00033/00094 | Loss: 1061.0748 | CE: 0.6304 | KD: 1060.4443\n",
      "Train Epoch: 001 Batch: 00034/00094 | Loss: 1060.9462 | CE: 0.6628 | KD: 1060.2833\n",
      "Train Epoch: 001 Batch: 00035/00094 | Loss: 1060.9229 | CE: 0.4769 | KD: 1060.4459\n",
      "Train Epoch: 001 Batch: 00036/00094 | Loss: 1061.2445 | CE: 0.5558 | KD: 1060.6887\n",
      "Train Epoch: 001 Batch: 00037/00094 | Loss: 1060.9939 | CE: 0.5263 | KD: 1060.4677\n",
      "Train Epoch: 001 Batch: 00038/00094 | Loss: 1061.1755 | CE: 0.6691 | KD: 1060.5065\n",
      "Train Epoch: 001 Batch: 00039/00094 | Loss: 1060.9454 | CE: 0.4372 | KD: 1060.5083\n",
      "Train Epoch: 001 Batch: 00040/00094 | Loss: 1060.7322 | CE: 0.4068 | KD: 1060.3254\n",
      "Train Epoch: 001 Batch: 00041/00094 | Loss: 1061.4249 | CE: 0.5657 | KD: 1060.8593\n",
      "Train Epoch: 001 Batch: 00042/00094 | Loss: 1060.9210 | CE: 0.4295 | KD: 1060.4916\n",
      "Train Epoch: 001 Batch: 00043/00094 | Loss: 1061.2263 | CE: 0.6030 | KD: 1060.6234\n",
      "Train Epoch: 001 Batch: 00044/00094 | Loss: 1060.6759 | CE: 0.5248 | KD: 1060.1511\n",
      "Train Epoch: 001 Batch: 00045/00094 | Loss: 1061.4576 | CE: 0.6109 | KD: 1060.8467\n",
      "Train Epoch: 001 Batch: 00046/00094 | Loss: 1061.1552 | CE: 0.5925 | KD: 1060.5626\n",
      "Train Epoch: 001 Batch: 00047/00094 | Loss: 1061.5673 | CE: 0.6859 | KD: 1060.8813\n",
      "Train Epoch: 001 Batch: 00048/00094 | Loss: 1061.3462 | CE: 0.5749 | KD: 1060.7712\n",
      "Train Epoch: 001 Batch: 00049/00094 | Loss: 1060.9860 | CE: 0.5446 | KD: 1060.4413\n",
      "Train Epoch: 001 Batch: 00050/00094 | Loss: 1061.1934 | CE: 0.6081 | KD: 1060.5853\n",
      "Train Epoch: 001 Batch: 00051/00094 | Loss: 1062.3489 | CE: 0.7243 | KD: 1061.6245\n",
      "Train Epoch: 001 Batch: 00052/00094 | Loss: 1060.8448 | CE: 0.4539 | KD: 1060.3910\n",
      "Train Epoch: 001 Batch: 00053/00094 | Loss: 1061.1364 | CE: 0.5740 | KD: 1060.5624\n",
      "Train Epoch: 001 Batch: 00054/00094 | Loss: 1061.3223 | CE: 0.6141 | KD: 1060.7081\n",
      "Train Epoch: 001 Batch: 00055/00094 | Loss: 1060.8351 | CE: 0.4541 | KD: 1060.3810\n",
      "Train Epoch: 001 Batch: 00056/00094 | Loss: 1060.6567 | CE: 0.4485 | KD: 1060.2083\n",
      "Train Epoch: 001 Batch: 00057/00094 | Loss: 1061.1289 | CE: 0.4636 | KD: 1060.6653\n",
      "Train Epoch: 001 Batch: 00058/00094 | Loss: 1061.4412 | CE: 0.4720 | KD: 1060.9691\n",
      "Train Epoch: 001 Batch: 00059/00094 | Loss: 1061.1498 | CE: 0.5582 | KD: 1060.5916\n",
      "Train Epoch: 001 Batch: 00060/00094 | Loss: 1061.4041 | CE: 0.6472 | KD: 1060.7568\n",
      "Train Epoch: 001 Batch: 00061/00094 | Loss: 1061.0404 | CE: 0.6372 | KD: 1060.4032\n",
      "Train Epoch: 001 Batch: 00062/00094 | Loss: 1061.0583 | CE: 0.4817 | KD: 1060.5767\n",
      "Train Epoch: 001 Batch: 00063/00094 | Loss: 1061.4288 | CE: 0.5508 | KD: 1060.8781\n",
      "Train Epoch: 001 Batch: 00064/00094 | Loss: 1060.8079 | CE: 0.4252 | KD: 1060.3827\n",
      "Train Epoch: 001 Batch: 00065/00094 | Loss: 1061.2250 | CE: 0.6811 | KD: 1060.5439\n",
      "Train Epoch: 001 Batch: 00066/00094 | Loss: 1060.9142 | CE: 0.4304 | KD: 1060.4838\n",
      "Train Epoch: 001 Batch: 00067/00094 | Loss: 1061.3566 | CE: 0.6615 | KD: 1060.6951\n",
      "Train Epoch: 001 Batch: 00068/00094 | Loss: 1061.1146 | CE: 0.6645 | KD: 1060.4502\n",
      "Train Epoch: 001 Batch: 00069/00094 | Loss: 1061.3392 | CE: 0.5559 | KD: 1060.7833\n",
      "Train Epoch: 001 Batch: 00070/00094 | Loss: 1061.2662 | CE: 0.6698 | KD: 1060.5964\n",
      "Train Epoch: 001 Batch: 00071/00094 | Loss: 1060.9379 | CE: 0.4578 | KD: 1060.4801\n",
      "Train Epoch: 001 Batch: 00072/00094 | Loss: 1060.8635 | CE: 0.4943 | KD: 1060.3691\n",
      "Train Epoch: 001 Batch: 00073/00094 | Loss: 1061.1196 | CE: 0.6234 | KD: 1060.4962\n",
      "Train Epoch: 001 Batch: 00074/00094 | Loss: 1061.1448 | CE: 0.5786 | KD: 1060.5662\n",
      "Train Epoch: 001 Batch: 00075/00094 | Loss: 1061.4324 | CE: 0.6933 | KD: 1060.7391\n",
      "Train Epoch: 001 Batch: 00076/00094 | Loss: 1061.1260 | CE: 0.5595 | KD: 1060.5665\n",
      "Train Epoch: 001 Batch: 00077/00094 | Loss: 1061.2804 | CE: 0.5761 | KD: 1060.7043\n",
      "Train Epoch: 001 Batch: 00078/00094 | Loss: 1061.1650 | CE: 0.4767 | KD: 1060.6884\n",
      "Train Epoch: 001 Batch: 00079/00094 | Loss: 1060.7433 | CE: 0.4844 | KD: 1060.2589\n",
      "Train Epoch: 001 Batch: 00080/00094 | Loss: 1061.0447 | CE: 0.4237 | KD: 1060.6210\n",
      "Train Epoch: 001 Batch: 00081/00094 | Loss: 1061.0090 | CE: 0.5510 | KD: 1060.4581\n",
      "Train Epoch: 001 Batch: 00082/00094 | Loss: 1061.6803 | CE: 0.8163 | KD: 1060.8640\n",
      "Train Epoch: 001 Batch: 00083/00094 | Loss: 1061.9153 | CE: 0.3880 | KD: 1061.5273\n",
      "Train Epoch: 001 Batch: 00084/00094 | Loss: 1060.9353 | CE: 0.4305 | KD: 1060.5049\n",
      "Train Epoch: 001 Batch: 00085/00094 | Loss: 1061.1360 | CE: 0.7607 | KD: 1060.3752\n",
      "Train Epoch: 001 Batch: 00086/00094 | Loss: 1061.4128 | CE: 0.5936 | KD: 1060.8192\n",
      "Train Epoch: 001 Batch: 00087/00094 | Loss: 1060.8674 | CE: 0.4087 | KD: 1060.4587\n",
      "Train Epoch: 001 Batch: 00088/00094 | Loss: 1061.4159 | CE: 0.6342 | KD: 1060.7817\n",
      "Train Epoch: 001 Batch: 00089/00094 | Loss: 1061.2139 | CE: 0.3739 | KD: 1060.8400\n",
      "Train Epoch: 001 Batch: 00090/00094 | Loss: 1061.1344 | CE: 0.6495 | KD: 1060.4850\n",
      "Train Epoch: 001 Batch: 00091/00094 | Loss: 1060.6522 | CE: 0.4583 | KD: 1060.1940\n",
      "Train Epoch: 001 Batch: 00092/00094 | Loss: 1060.7638 | CE: 0.3777 | KD: 1060.3861\n",
      "Train Epoch: 001 Batch: 00093/00094 | Loss: 1060.8335 | CE: 0.5894 | KD: 1060.2441\n",
      "Train Epoch: 001 Batch: 00094/00094 | Loss: 1061.0153 | CE: 0.4621 | KD: 1060.5532\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.5563 | acc:77.1500\n",
      "[VAL Acc] Target: 77.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1412 | acc:50.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8085 | acc:59.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2115 | acc:51.5267\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 51.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7112 | acc:57.2492\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7941 | acc:53.1423\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5584 | acc:69.5141\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8701 | acc:47.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 47.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7423 | acc:62.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 62.45%\n",
      "[VAL Acc] Avg 58.78%\n",
      "VAL Acc improve from 0.00% to 58.78%\n",
      "Save best model\n",
      "Train Epoch: 002 Batch: 00001/00094 | Loss: 1060.9354 | CE: 0.4806 | KD: 1060.4548\n",
      "Train Epoch: 002 Batch: 00002/00094 | Loss: 1060.8783 | CE: 0.5378 | KD: 1060.3405\n",
      "Train Epoch: 002 Batch: 00003/00094 | Loss: 1061.0217 | CE: 0.6107 | KD: 1060.4110\n",
      "Train Epoch: 002 Batch: 00004/00094 | Loss: 1061.2432 | CE: 0.5469 | KD: 1060.6963\n",
      "Train Epoch: 002 Batch: 00005/00094 | Loss: 1061.2622 | CE: 0.4716 | KD: 1060.7906\n",
      "Train Epoch: 002 Batch: 00006/00094 | Loss: 1061.0020 | CE: 0.4756 | KD: 1060.5264\n",
      "Train Epoch: 002 Batch: 00007/00094 | Loss: 1060.8832 | CE: 0.4749 | KD: 1060.4082\n",
      "Train Epoch: 002 Batch: 00008/00094 | Loss: 1061.3044 | CE: 0.4502 | KD: 1060.8542\n",
      "Train Epoch: 002 Batch: 00009/00094 | Loss: 1061.2861 | CE: 0.5082 | KD: 1060.7778\n",
      "Train Epoch: 002 Batch: 00010/00094 | Loss: 1061.1862 | CE: 0.6349 | KD: 1060.5513\n",
      "Train Epoch: 002 Batch: 00011/00094 | Loss: 1061.2375 | CE: 0.4706 | KD: 1060.7670\n",
      "Train Epoch: 002 Batch: 00012/00094 | Loss: 1061.3519 | CE: 0.4393 | KD: 1060.9126\n",
      "Train Epoch: 002 Batch: 00013/00094 | Loss: 1061.4645 | CE: 0.5002 | KD: 1060.9642\n",
      "Train Epoch: 002 Batch: 00014/00094 | Loss: 1061.2823 | CE: 0.5581 | KD: 1060.7242\n",
      "Train Epoch: 002 Batch: 00015/00094 | Loss: 1061.0371 | CE: 0.4627 | KD: 1060.5743\n",
      "Train Epoch: 002 Batch: 00016/00094 | Loss: 1061.2148 | CE: 0.5885 | KD: 1060.6263\n",
      "Train Epoch: 002 Batch: 00017/00094 | Loss: 1060.9900 | CE: 0.5143 | KD: 1060.4757\n",
      "Train Epoch: 002 Batch: 00018/00094 | Loss: 1060.8168 | CE: 0.4939 | KD: 1060.3229\n",
      "Train Epoch: 002 Batch: 00019/00094 | Loss: 1061.1519 | CE: 0.5992 | KD: 1060.5526\n",
      "Train Epoch: 002 Batch: 00020/00094 | Loss: 1061.5201 | CE: 0.5818 | KD: 1060.9384\n",
      "Train Epoch: 002 Batch: 00021/00094 | Loss: 1061.1298 | CE: 0.7068 | KD: 1060.4230\n",
      "Train Epoch: 002 Batch: 00022/00094 | Loss: 1061.4362 | CE: 0.6144 | KD: 1060.8218\n",
      "Train Epoch: 002 Batch: 00023/00094 | Loss: 1061.0336 | CE: 0.3932 | KD: 1060.6404\n",
      "Train Epoch: 002 Batch: 00024/00094 | Loss: 1060.7163 | CE: 0.2951 | KD: 1060.4213\n",
      "Train Epoch: 002 Batch: 00025/00094 | Loss: 1061.1554 | CE: 0.5964 | KD: 1060.5590\n",
      "Train Epoch: 002 Batch: 00026/00094 | Loss: 1061.0177 | CE: 0.5962 | KD: 1060.4215\n",
      "Train Epoch: 002 Batch: 00027/00094 | Loss: 1060.7642 | CE: 0.5391 | KD: 1060.2251\n",
      "Train Epoch: 002 Batch: 00028/00094 | Loss: 1060.8060 | CE: 0.5251 | KD: 1060.2809\n",
      "Train Epoch: 002 Batch: 00029/00094 | Loss: 1061.0508 | CE: 0.4902 | KD: 1060.5605\n",
      "Train Epoch: 002 Batch: 00030/00094 | Loss: 1060.9993 | CE: 0.3872 | KD: 1060.6121\n",
      "Train Epoch: 002 Batch: 00031/00094 | Loss: 1061.0787 | CE: 0.5632 | KD: 1060.5155\n",
      "Train Epoch: 002 Batch: 00032/00094 | Loss: 1061.0360 | CE: 0.6194 | KD: 1060.4166\n",
      "Train Epoch: 002 Batch: 00033/00094 | Loss: 1060.7329 | CE: 0.4349 | KD: 1060.2980\n",
      "Train Epoch: 002 Batch: 00034/00094 | Loss: 1060.6194 | CE: 0.4685 | KD: 1060.1509\n",
      "Train Epoch: 002 Batch: 00035/00094 | Loss: 1061.4326 | CE: 0.5776 | KD: 1060.8551\n",
      "Train Epoch: 002 Batch: 00036/00094 | Loss: 1060.9081 | CE: 0.5229 | KD: 1060.3851\n",
      "Train Epoch: 002 Batch: 00037/00094 | Loss: 1061.1960 | CE: 0.6030 | KD: 1060.5930\n",
      "Train Epoch: 002 Batch: 00038/00094 | Loss: 1060.9364 | CE: 0.3650 | KD: 1060.5714\n",
      "Train Epoch: 002 Batch: 00039/00094 | Loss: 1060.9905 | CE: 0.4126 | KD: 1060.5779\n",
      "Train Epoch: 002 Batch: 00040/00094 | Loss: 1060.8488 | CE: 0.4974 | KD: 1060.3514\n",
      "Train Epoch: 002 Batch: 00041/00094 | Loss: 1061.0784 | CE: 0.5471 | KD: 1060.5312\n",
      "Train Epoch: 002 Batch: 00042/00094 | Loss: 1060.8260 | CE: 0.5098 | KD: 1060.3163\n",
      "Train Epoch: 002 Batch: 00043/00094 | Loss: 1060.7382 | CE: 0.4247 | KD: 1060.3135\n",
      "Train Epoch: 002 Batch: 00044/00094 | Loss: 1061.5426 | CE: 0.5928 | KD: 1060.9498\n",
      "Train Epoch: 002 Batch: 00045/00094 | Loss: 1061.4669 | CE: 0.5822 | KD: 1060.8846\n",
      "Train Epoch: 002 Batch: 00046/00094 | Loss: 1061.2594 | CE: 0.5589 | KD: 1060.7006\n",
      "Train Epoch: 002 Batch: 00047/00094 | Loss: 1060.8132 | CE: 0.4377 | KD: 1060.3755\n",
      "Train Epoch: 002 Batch: 00048/00094 | Loss: 1060.8954 | CE: 0.4969 | KD: 1060.3986\n",
      "Train Epoch: 002 Batch: 00049/00094 | Loss: 1060.8214 | CE: 0.5170 | KD: 1060.3044\n",
      "Train Epoch: 002 Batch: 00050/00094 | Loss: 1060.7291 | CE: 0.4635 | KD: 1060.2656\n",
      "Train Epoch: 002 Batch: 00051/00094 | Loss: 1060.9602 | CE: 0.4275 | KD: 1060.5327\n",
      "Train Epoch: 002 Batch: 00052/00094 | Loss: 1061.0380 | CE: 0.4710 | KD: 1060.5670\n",
      "Train Epoch: 002 Batch: 00053/00094 | Loss: 1060.8792 | CE: 0.4507 | KD: 1060.4285\n",
      "Train Epoch: 002 Batch: 00054/00094 | Loss: 1060.7539 | CE: 0.3976 | KD: 1060.3563\n",
      "Train Epoch: 002 Batch: 00055/00094 | Loss: 1061.2604 | CE: 0.4702 | KD: 1060.7902\n",
      "Train Epoch: 002 Batch: 00056/00094 | Loss: 1060.9291 | CE: 0.4951 | KD: 1060.4340\n",
      "Train Epoch: 002 Batch: 00057/00094 | Loss: 1060.7716 | CE: 0.4950 | KD: 1060.2766\n",
      "Train Epoch: 002 Batch: 00058/00094 | Loss: 1061.2969 | CE: 0.5783 | KD: 1060.7185\n",
      "Train Epoch: 002 Batch: 00059/00094 | Loss: 1060.9263 | CE: 0.4932 | KD: 1060.4331\n",
      "Train Epoch: 002 Batch: 00060/00094 | Loss: 1060.9948 | CE: 0.5251 | KD: 1060.4696\n",
      "Train Epoch: 002 Batch: 00061/00094 | Loss: 1060.8250 | CE: 0.4521 | KD: 1060.3728\n",
      "Train Epoch: 002 Batch: 00062/00094 | Loss: 1061.7601 | CE: 0.4511 | KD: 1061.3090\n",
      "Train Epoch: 002 Batch: 00063/00094 | Loss: 1060.9813 | CE: 0.5797 | KD: 1060.4016\n",
      "Train Epoch: 002 Batch: 00064/00094 | Loss: 1060.8887 | CE: 0.4613 | KD: 1060.4274\n",
      "Train Epoch: 002 Batch: 00065/00094 | Loss: 1061.5325 | CE: 0.6097 | KD: 1060.9227\n",
      "Train Epoch: 002 Batch: 00066/00094 | Loss: 1060.7871 | CE: 0.4361 | KD: 1060.3511\n",
      "Train Epoch: 002 Batch: 00067/00094 | Loss: 1060.9789 | CE: 0.4056 | KD: 1060.5732\n",
      "Train Epoch: 002 Batch: 00068/00094 | Loss: 1060.9658 | CE: 0.5805 | KD: 1060.3853\n",
      "Train Epoch: 002 Batch: 00069/00094 | Loss: 1061.4282 | CE: 0.6939 | KD: 1060.7344\n",
      "Train Epoch: 002 Batch: 00070/00094 | Loss: 1061.7443 | CE: 0.4730 | KD: 1061.2712\n",
      "Train Epoch: 002 Batch: 00071/00094 | Loss: 1061.0273 | CE: 0.5512 | KD: 1060.4762\n",
      "Train Epoch: 002 Batch: 00072/00094 | Loss: 1061.3317 | CE: 0.5898 | KD: 1060.7418\n",
      "Train Epoch: 002 Batch: 00073/00094 | Loss: 1060.9196 | CE: 0.4788 | KD: 1060.4407\n",
      "Train Epoch: 002 Batch: 00074/00094 | Loss: 1061.1951 | CE: 0.4475 | KD: 1060.7476\n",
      "Train Epoch: 002 Batch: 00075/00094 | Loss: 1060.9648 | CE: 0.5707 | KD: 1060.3940\n",
      "Train Epoch: 002 Batch: 00076/00094 | Loss: 1061.0020 | CE: 0.4113 | KD: 1060.5907\n",
      "Train Epoch: 002 Batch: 00077/00094 | Loss: 1061.7374 | CE: 0.5944 | KD: 1061.1431\n",
      "Train Epoch: 002 Batch: 00078/00094 | Loss: 1061.0194 | CE: 0.6135 | KD: 1060.4059\n",
      "Train Epoch: 002 Batch: 00079/00094 | Loss: 1060.8483 | CE: 0.5013 | KD: 1060.3470\n",
      "Train Epoch: 002 Batch: 00080/00094 | Loss: 1060.9103 | CE: 0.5198 | KD: 1060.3905\n",
      "Train Epoch: 002 Batch: 00081/00094 | Loss: 1061.1166 | CE: 0.4823 | KD: 1060.6343\n",
      "Train Epoch: 002 Batch: 00082/00094 | Loss: 1061.1458 | CE: 0.3400 | KD: 1060.8058\n",
      "Train Epoch: 002 Batch: 00083/00094 | Loss: 1060.8131 | CE: 0.3577 | KD: 1060.4554\n",
      "Train Epoch: 002 Batch: 00084/00094 | Loss: 1061.3176 | CE: 0.6471 | KD: 1060.6705\n",
      "Train Epoch: 002 Batch: 00085/00094 | Loss: 1061.1426 | CE: 0.6584 | KD: 1060.4843\n",
      "Train Epoch: 002 Batch: 00086/00094 | Loss: 1061.1752 | CE: 0.7050 | KD: 1060.4702\n",
      "Train Epoch: 002 Batch: 00087/00094 | Loss: 1061.0670 | CE: 0.5064 | KD: 1060.5605\n",
      "Train Epoch: 002 Batch: 00088/00094 | Loss: 1061.3158 | CE: 0.5402 | KD: 1060.7755\n",
      "Train Epoch: 002 Batch: 00089/00094 | Loss: 1061.5642 | CE: 0.7855 | KD: 1060.7787\n",
      "Train Epoch: 002 Batch: 00090/00094 | Loss: 1061.0414 | CE: 0.5552 | KD: 1060.4861\n",
      "Train Epoch: 002 Batch: 00091/00094 | Loss: 1060.9736 | CE: 0.5058 | KD: 1060.4679\n",
      "Train Epoch: 002 Batch: 00092/00094 | Loss: 1061.4396 | CE: 0.5421 | KD: 1060.8975\n",
      "Train Epoch: 002 Batch: 00093/00094 | Loss: 1061.0626 | CE: 0.5060 | KD: 1060.5566\n",
      "Train Epoch: 002 Batch: 00094/00094 | Loss: 1060.6586 | CE: 0.4299 | KD: 1060.2286\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.6127 | acc:75.2000\n",
      "[VAL Acc] Target: 75.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2395 | acc:49.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8858 | acc:51.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2362 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8724 | acc:51.4107\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7897 | acc:51.3863\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6426 | acc:58.3856\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 58.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8247 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.8571 | acc:58.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 58.15%\n",
      "[VAL Acc] Avg 54.82%\n",
      "Train Epoch: 003 Batch: 00001/00094 | Loss: 1061.5720 | CE: 0.7536 | KD: 1060.8185\n",
      "Train Epoch: 003 Batch: 00002/00094 | Loss: 1061.2098 | CE: 0.5381 | KD: 1060.6718\n",
      "Train Epoch: 003 Batch: 00003/00094 | Loss: 1060.7362 | CE: 0.4316 | KD: 1060.3046\n",
      "Train Epoch: 003 Batch: 00004/00094 | Loss: 1060.6237 | CE: 0.4661 | KD: 1060.1576\n",
      "Train Epoch: 003 Batch: 00005/00094 | Loss: 1060.8923 | CE: 0.5526 | KD: 1060.3397\n",
      "Train Epoch: 003 Batch: 00006/00094 | Loss: 1061.3201 | CE: 0.4695 | KD: 1060.8506\n",
      "Train Epoch: 003 Batch: 00007/00094 | Loss: 1061.4309 | CE: 0.4788 | KD: 1060.9520\n",
      "Train Epoch: 003 Batch: 00008/00094 | Loss: 1061.0367 | CE: 0.5491 | KD: 1060.4877\n",
      "Train Epoch: 003 Batch: 00009/00094 | Loss: 1061.1544 | CE: 0.5218 | KD: 1060.6327\n",
      "Train Epoch: 003 Batch: 00010/00094 | Loss: 1061.0295 | CE: 0.5924 | KD: 1060.4371\n",
      "Train Epoch: 003 Batch: 00011/00094 | Loss: 1061.0946 | CE: 0.4814 | KD: 1060.6133\n",
      "Train Epoch: 003 Batch: 00012/00094 | Loss: 1060.5137 | CE: 0.3633 | KD: 1060.1504\n",
      "Train Epoch: 003 Batch: 00013/00094 | Loss: 1061.3265 | CE: 0.5447 | KD: 1060.7819\n",
      "Train Epoch: 003 Batch: 00014/00094 | Loss: 1061.4176 | CE: 0.5850 | KD: 1060.8326\n",
      "Train Epoch: 003 Batch: 00015/00094 | Loss: 1060.7092 | CE: 0.3923 | KD: 1060.3170\n",
      "Train Epoch: 003 Batch: 00016/00094 | Loss: 1061.3284 | CE: 0.5150 | KD: 1060.8134\n",
      "Train Epoch: 003 Batch: 00017/00094 | Loss: 1061.0521 | CE: 0.5113 | KD: 1060.5408\n",
      "Train Epoch: 003 Batch: 00018/00094 | Loss: 1060.9623 | CE: 0.6146 | KD: 1060.3477\n",
      "Train Epoch: 003 Batch: 00019/00094 | Loss: 1060.9032 | CE: 0.5717 | KD: 1060.3315\n",
      "Train Epoch: 003 Batch: 00020/00094 | Loss: 1061.6466 | CE: 0.6471 | KD: 1060.9995\n",
      "Train Epoch: 003 Batch: 00021/00094 | Loss: 1061.2164 | CE: 0.5788 | KD: 1060.6377\n",
      "Train Epoch: 003 Batch: 00022/00094 | Loss: 1060.8783 | CE: 0.4178 | KD: 1060.4604\n",
      "Train Epoch: 003 Batch: 00023/00094 | Loss: 1060.8611 | CE: 0.4506 | KD: 1060.4105\n",
      "Train Epoch: 003 Batch: 00024/00094 | Loss: 1061.0084 | CE: 0.6114 | KD: 1060.3970\n",
      "Train Epoch: 003 Batch: 00025/00094 | Loss: 1061.0648 | CE: 0.5188 | KD: 1060.5460\n",
      "Train Epoch: 003 Batch: 00026/00094 | Loss: 1061.0674 | CE: 0.5567 | KD: 1060.5107\n",
      "Train Epoch: 003 Batch: 00027/00094 | Loss: 1061.2054 | CE: 0.5612 | KD: 1060.6442\n",
      "Train Epoch: 003 Batch: 00028/00094 | Loss: 1061.2693 | CE: 0.5254 | KD: 1060.7439\n",
      "Train Epoch: 003 Batch: 00029/00094 | Loss: 1061.0094 | CE: 0.4987 | KD: 1060.5107\n",
      "Train Epoch: 003 Batch: 00030/00094 | Loss: 1061.0684 | CE: 0.5645 | KD: 1060.5039\n",
      "Train Epoch: 003 Batch: 00031/00094 | Loss: 1061.0099 | CE: 0.5812 | KD: 1060.4287\n",
      "Train Epoch: 003 Batch: 00032/00094 | Loss: 1060.7428 | CE: 0.4836 | KD: 1060.2592\n",
      "Train Epoch: 003 Batch: 00033/00094 | Loss: 1061.3665 | CE: 0.5316 | KD: 1060.8348\n",
      "Train Epoch: 003 Batch: 00034/00094 | Loss: 1061.6060 | CE: 0.5873 | KD: 1061.0187\n",
      "Train Epoch: 003 Batch: 00035/00094 | Loss: 1061.1731 | CE: 0.6999 | KD: 1060.4731\n",
      "Train Epoch: 003 Batch: 00036/00094 | Loss: 1061.0352 | CE: 0.5814 | KD: 1060.4537\n",
      "Train Epoch: 003 Batch: 00037/00094 | Loss: 1061.6249 | CE: 0.6428 | KD: 1060.9821\n",
      "Train Epoch: 003 Batch: 00038/00094 | Loss: 1060.6837 | CE: 0.5285 | KD: 1060.1552\n",
      "Train Epoch: 003 Batch: 00039/00094 | Loss: 1060.5636 | CE: 0.3456 | KD: 1060.2180\n",
      "Train Epoch: 003 Batch: 00040/00094 | Loss: 1060.9597 | CE: 0.4692 | KD: 1060.4905\n",
      "Train Epoch: 003 Batch: 00041/00094 | Loss: 1061.2141 | CE: 0.5801 | KD: 1060.6340\n",
      "Train Epoch: 003 Batch: 00042/00094 | Loss: 1061.1113 | CE: 0.5039 | KD: 1060.6074\n",
      "Train Epoch: 003 Batch: 00043/00094 | Loss: 1060.9099 | CE: 0.5427 | KD: 1060.3672\n",
      "Train Epoch: 003 Batch: 00044/00094 | Loss: 1060.6067 | CE: 0.3625 | KD: 1060.2443\n",
      "Train Epoch: 003 Batch: 00045/00094 | Loss: 1060.8799 | CE: 0.4628 | KD: 1060.4170\n",
      "Train Epoch: 003 Batch: 00046/00094 | Loss: 1060.6155 | CE: 0.4293 | KD: 1060.1862\n",
      "Train Epoch: 003 Batch: 00047/00094 | Loss: 1061.0117 | CE: 0.6386 | KD: 1060.3730\n",
      "Train Epoch: 003 Batch: 00048/00094 | Loss: 1061.9033 | CE: 0.4797 | KD: 1061.4236\n",
      "Train Epoch: 003 Batch: 00049/00094 | Loss: 1060.8462 | CE: 0.5394 | KD: 1060.3068\n",
      "Train Epoch: 003 Batch: 00050/00094 | Loss: 1061.1960 | CE: 0.5712 | KD: 1060.6249\n",
      "Train Epoch: 003 Batch: 00051/00094 | Loss: 1061.0659 | CE: 0.5503 | KD: 1060.5156\n",
      "Train Epoch: 003 Batch: 00052/00094 | Loss: 1061.1527 | CE: 0.4392 | KD: 1060.7135\n",
      "Train Epoch: 003 Batch: 00053/00094 | Loss: 1060.9818 | CE: 0.5086 | KD: 1060.4733\n",
      "Train Epoch: 003 Batch: 00054/00094 | Loss: 1061.0291 | CE: 0.5767 | KD: 1060.4524\n",
      "Train Epoch: 003 Batch: 00055/00094 | Loss: 1061.2728 | CE: 0.5762 | KD: 1060.6967\n",
      "Train Epoch: 003 Batch: 00056/00094 | Loss: 1060.8268 | CE: 0.4021 | KD: 1060.4247\n",
      "Train Epoch: 003 Batch: 00057/00094 | Loss: 1060.9915 | CE: 0.4341 | KD: 1060.5574\n",
      "Train Epoch: 003 Batch: 00058/00094 | Loss: 1060.5315 | CE: 0.3986 | KD: 1060.1329\n",
      "Train Epoch: 003 Batch: 00059/00094 | Loss: 1060.9707 | CE: 0.4894 | KD: 1060.4813\n",
      "Train Epoch: 003 Batch: 00060/00094 | Loss: 1061.1716 | CE: 0.4606 | KD: 1060.7111\n",
      "Train Epoch: 003 Batch: 00061/00094 | Loss: 1061.3300 | CE: 0.6391 | KD: 1060.6909\n",
      "Train Epoch: 003 Batch: 00062/00094 | Loss: 1061.0745 | CE: 0.5363 | KD: 1060.5382\n",
      "Train Epoch: 003 Batch: 00063/00094 | Loss: 1061.3979 | CE: 0.6626 | KD: 1060.7354\n",
      "Train Epoch: 003 Batch: 00064/00094 | Loss: 1061.1774 | CE: 0.5936 | KD: 1060.5837\n",
      "Train Epoch: 003 Batch: 00065/00094 | Loss: 1060.6158 | CE: 0.3985 | KD: 1060.2173\n",
      "Train Epoch: 003 Batch: 00066/00094 | Loss: 1061.2605 | CE: 0.6428 | KD: 1060.6177\n",
      "Train Epoch: 003 Batch: 00067/00094 | Loss: 1060.8049 | CE: 0.5280 | KD: 1060.2770\n",
      "Train Epoch: 003 Batch: 00068/00094 | Loss: 1060.7008 | CE: 0.4195 | KD: 1060.2812\n",
      "Train Epoch: 003 Batch: 00069/00094 | Loss: 1060.9973 | CE: 0.4288 | KD: 1060.5685\n",
      "Train Epoch: 003 Batch: 00070/00094 | Loss: 1060.7322 | CE: 0.4467 | KD: 1060.2854\n",
      "Train Epoch: 003 Batch: 00071/00094 | Loss: 1061.2830 | CE: 0.4180 | KD: 1060.8649\n",
      "Train Epoch: 003 Batch: 00072/00094 | Loss: 1060.9631 | CE: 0.5251 | KD: 1060.4380\n",
      "Train Epoch: 003 Batch: 00073/00094 | Loss: 1061.1962 | CE: 0.6515 | KD: 1060.5447\n",
      "Train Epoch: 003 Batch: 00074/00094 | Loss: 1061.3051 | CE: 0.6215 | KD: 1060.6836\n",
      "Train Epoch: 003 Batch: 00075/00094 | Loss: 1060.7853 | CE: 0.4375 | KD: 1060.3478\n",
      "Train Epoch: 003 Batch: 00076/00094 | Loss: 1060.8269 | CE: 0.5778 | KD: 1060.2491\n",
      "Train Epoch: 003 Batch: 00077/00094 | Loss: 1061.2388 | CE: 0.4316 | KD: 1060.8071\n",
      "Train Epoch: 003 Batch: 00078/00094 | Loss: 1060.7152 | CE: 0.4426 | KD: 1060.2726\n",
      "Train Epoch: 003 Batch: 00079/00094 | Loss: 1061.5071 | CE: 0.5524 | KD: 1060.9547\n",
      "Train Epoch: 003 Batch: 00080/00094 | Loss: 1061.6481 | CE: 0.5060 | KD: 1061.1421\n",
      "Train Epoch: 003 Batch: 00081/00094 | Loss: 1060.6838 | CE: 0.3854 | KD: 1060.2985\n",
      "Train Epoch: 003 Batch: 00082/00094 | Loss: 1060.9441 | CE: 0.5307 | KD: 1060.4135\n",
      "Train Epoch: 003 Batch: 00083/00094 | Loss: 1061.1674 | CE: 0.5591 | KD: 1060.6083\n",
      "Train Epoch: 003 Batch: 00084/00094 | Loss: 1061.2922 | CE: 0.5837 | KD: 1060.7086\n",
      "Train Epoch: 003 Batch: 00085/00094 | Loss: 1061.3242 | CE: 0.6748 | KD: 1060.6494\n",
      "Train Epoch: 003 Batch: 00086/00094 | Loss: 1061.2002 | CE: 0.4269 | KD: 1060.7732\n",
      "Train Epoch: 003 Batch: 00087/00094 | Loss: 1060.6350 | CE: 0.3592 | KD: 1060.2758\n",
      "Train Epoch: 003 Batch: 00088/00094 | Loss: 1060.6696 | CE: 0.4505 | KD: 1060.2191\n",
      "Train Epoch: 003 Batch: 00089/00094 | Loss: 1061.2386 | CE: 0.5051 | KD: 1060.7335\n",
      "Train Epoch: 003 Batch: 00090/00094 | Loss: 1060.8085 | CE: 0.4442 | KD: 1060.3643\n",
      "Train Epoch: 003 Batch: 00091/00094 | Loss: 1061.4366 | CE: 0.5084 | KD: 1060.9282\n",
      "Train Epoch: 003 Batch: 00092/00094 | Loss: 1061.0889 | CE: 0.5381 | KD: 1060.5508\n",
      "Train Epoch: 003 Batch: 00093/00094 | Loss: 1060.8813 | CE: 0.5251 | KD: 1060.3563\n",
      "Train Epoch: 003 Batch: 00094/00094 | Loss: 1061.1257 | CE: 0.4099 | KD: 1060.7158\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.6906 | acc:74.6500\n",
      "[VAL Acc] Target: 74.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3943 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1205 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3188 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.0030 | acc:50.4310\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8051 | acc:52.0333\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.7317 | acc:54.4671\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 54.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8416 | acc:48.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.9937 | acc:56.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 56.20%\n",
      "[VAL Acc] Avg 54.12%\n",
      "Train Epoch: 004 Batch: 00001/00094 | Loss: 1061.2347 | CE: 0.4055 | KD: 1060.8292\n",
      "Train Epoch: 004 Batch: 00002/00094 | Loss: 1060.8823 | CE: 0.5008 | KD: 1060.3815\n",
      "Train Epoch: 004 Batch: 00003/00094 | Loss: 1061.1061 | CE: 0.4814 | KD: 1060.6246\n",
      "Train Epoch: 004 Batch: 00004/00094 | Loss: 1061.3121 | CE: 0.5123 | KD: 1060.7998\n",
      "Train Epoch: 004 Batch: 00005/00094 | Loss: 1061.0845 | CE: 0.4692 | KD: 1060.6152\n",
      "Train Epoch: 004 Batch: 00006/00094 | Loss: 1061.8428 | CE: 0.5617 | KD: 1061.2811\n",
      "Train Epoch: 004 Batch: 00007/00094 | Loss: 1060.7509 | CE: 0.5124 | KD: 1060.2384\n",
      "Train Epoch: 004 Batch: 00008/00094 | Loss: 1061.2322 | CE: 0.6109 | KD: 1060.6212\n",
      "Train Epoch: 004 Batch: 00009/00094 | Loss: 1062.2109 | CE: 0.6572 | KD: 1061.5537\n",
      "Train Epoch: 004 Batch: 00010/00094 | Loss: 1060.4230 | CE: 0.3370 | KD: 1060.0861\n",
      "Train Epoch: 004 Batch: 00011/00094 | Loss: 1061.0192 | CE: 0.4967 | KD: 1060.5225\n",
      "Train Epoch: 004 Batch: 00012/00094 | Loss: 1061.3013 | CE: 0.4762 | KD: 1060.8251\n",
      "Train Epoch: 004 Batch: 00013/00094 | Loss: 1060.6722 | CE: 0.4324 | KD: 1060.2399\n",
      "Train Epoch: 004 Batch: 00014/00094 | Loss: 1060.9247 | CE: 0.4983 | KD: 1060.4264\n",
      "Train Epoch: 004 Batch: 00015/00094 | Loss: 1060.9479 | CE: 0.4762 | KD: 1060.4717\n",
      "Train Epoch: 004 Batch: 00016/00094 | Loss: 1061.1853 | CE: 0.4288 | KD: 1060.7566\n",
      "Train Epoch: 004 Batch: 00017/00094 | Loss: 1060.8682 | CE: 0.3773 | KD: 1060.4908\n",
      "Train Epoch: 004 Batch: 00018/00094 | Loss: 1060.7736 | CE: 0.4672 | KD: 1060.3064\n",
      "Train Epoch: 004 Batch: 00019/00094 | Loss: 1061.1123 | CE: 0.5630 | KD: 1060.5493\n",
      "Train Epoch: 004 Batch: 00020/00094 | Loss: 1060.9504 | CE: 0.3198 | KD: 1060.6306\n",
      "Train Epoch: 004 Batch: 00021/00094 | Loss: 1061.2708 | CE: 0.5399 | KD: 1060.7308\n",
      "Train Epoch: 004 Batch: 00022/00094 | Loss: 1061.0387 | CE: 0.4703 | KD: 1060.5684\n",
      "Train Epoch: 004 Batch: 00023/00094 | Loss: 1061.3707 | CE: 0.5466 | KD: 1060.8241\n",
      "Train Epoch: 004 Batch: 00024/00094 | Loss: 1061.1678 | CE: 0.5745 | KD: 1060.5934\n",
      "Train Epoch: 004 Batch: 00025/00094 | Loss: 1061.4027 | CE: 0.6804 | KD: 1060.7223\n",
      "Train Epoch: 004 Batch: 00026/00094 | Loss: 1060.7937 | CE: 0.4408 | KD: 1060.3529\n",
      "Train Epoch: 004 Batch: 00027/00094 | Loss: 1061.5042 | CE: 0.4956 | KD: 1061.0085\n",
      "Train Epoch: 004 Batch: 00028/00094 | Loss: 1061.1304 | CE: 0.5563 | KD: 1060.5741\n",
      "Train Epoch: 004 Batch: 00029/00094 | Loss: 1061.3173 | CE: 0.5214 | KD: 1060.7959\n",
      "Train Epoch: 004 Batch: 00030/00094 | Loss: 1060.9961 | CE: 0.4372 | KD: 1060.5590\n",
      "Train Epoch: 004 Batch: 00031/00094 | Loss: 1060.9727 | CE: 0.4923 | KD: 1060.4803\n",
      "Train Epoch: 004 Batch: 00032/00094 | Loss: 1060.9210 | CE: 0.4289 | KD: 1060.4921\n",
      "Train Epoch: 004 Batch: 00033/00094 | Loss: 1061.3395 | CE: 0.5081 | KD: 1060.8313\n",
      "Train Epoch: 004 Batch: 00034/00094 | Loss: 1060.8362 | CE: 0.4419 | KD: 1060.3943\n",
      "Train Epoch: 004 Batch: 00035/00094 | Loss: 1060.7322 | CE: 0.4884 | KD: 1060.2438\n",
      "Train Epoch: 004 Batch: 00036/00094 | Loss: 1061.4612 | CE: 0.5122 | KD: 1060.9490\n",
      "Train Epoch: 004 Batch: 00037/00094 | Loss: 1061.3099 | CE: 0.4880 | KD: 1060.8219\n",
      "Train Epoch: 004 Batch: 00038/00094 | Loss: 1061.0194 | CE: 0.5121 | KD: 1060.5073\n",
      "Train Epoch: 004 Batch: 00039/00094 | Loss: 1061.0378 | CE: 0.4616 | KD: 1060.5763\n",
      "Train Epoch: 004 Batch: 00040/00094 | Loss: 1060.9192 | CE: 0.4771 | KD: 1060.4421\n",
      "Train Epoch: 004 Batch: 00041/00094 | Loss: 1060.9435 | CE: 0.4906 | KD: 1060.4529\n",
      "Train Epoch: 004 Batch: 00042/00094 | Loss: 1060.9451 | CE: 0.4171 | KD: 1060.5280\n",
      "Train Epoch: 004 Batch: 00043/00094 | Loss: 1061.1460 | CE: 0.5306 | KD: 1060.6154\n",
      "Train Epoch: 004 Batch: 00044/00094 | Loss: 1060.8273 | CE: 0.3776 | KD: 1060.4497\n",
      "Train Epoch: 004 Batch: 00045/00094 | Loss: 1060.9231 | CE: 0.5924 | KD: 1060.3307\n",
      "Train Epoch: 004 Batch: 00046/00094 | Loss: 1060.9529 | CE: 0.5746 | KD: 1060.3783\n",
      "Train Epoch: 004 Batch: 00047/00094 | Loss: 1060.5713 | CE: 0.4140 | KD: 1060.1572\n",
      "Train Epoch: 004 Batch: 00048/00094 | Loss: 1061.2100 | CE: 0.6326 | KD: 1060.5774\n",
      "Train Epoch: 004 Batch: 00049/00094 | Loss: 1061.3217 | CE: 0.6209 | KD: 1060.7008\n",
      "Train Epoch: 004 Batch: 00050/00094 | Loss: 1060.9960 | CE: 0.5390 | KD: 1060.4570\n",
      "Train Epoch: 004 Batch: 00051/00094 | Loss: 1060.5355 | CE: 0.3968 | KD: 1060.1387\n",
      "Train Epoch: 004 Batch: 00052/00094 | Loss: 1060.9991 | CE: 0.4355 | KD: 1060.5637\n",
      "Train Epoch: 004 Batch: 00053/00094 | Loss: 1060.8456 | CE: 0.4358 | KD: 1060.4098\n",
      "Train Epoch: 004 Batch: 00054/00094 | Loss: 1061.4596 | CE: 0.5543 | KD: 1060.9053\n",
      "Train Epoch: 004 Batch: 00055/00094 | Loss: 1060.8502 | CE: 0.4858 | KD: 1060.3645\n",
      "Train Epoch: 004 Batch: 00056/00094 | Loss: 1060.7921 | CE: 0.5075 | KD: 1060.2847\n",
      "Train Epoch: 004 Batch: 00057/00094 | Loss: 1061.3546 | CE: 0.3948 | KD: 1060.9598\n",
      "Train Epoch: 004 Batch: 00058/00094 | Loss: 1060.8352 | CE: 0.5118 | KD: 1060.3234\n",
      "Train Epoch: 004 Batch: 00059/00094 | Loss: 1061.2609 | CE: 0.3588 | KD: 1060.9020\n",
      "Train Epoch: 004 Batch: 00060/00094 | Loss: 1061.1647 | CE: 0.4085 | KD: 1060.7562\n",
      "Train Epoch: 004 Batch: 00061/00094 | Loss: 1061.3745 | CE: 0.8140 | KD: 1060.5605\n",
      "Train Epoch: 004 Batch: 00062/00094 | Loss: 1060.9823 | CE: 0.5403 | KD: 1060.4420\n",
      "Train Epoch: 004 Batch: 00063/00094 | Loss: 1061.0209 | CE: 0.4663 | KD: 1060.5546\n",
      "Train Epoch: 004 Batch: 00064/00094 | Loss: 1060.9922 | CE: 0.4859 | KD: 1060.5063\n",
      "Train Epoch: 004 Batch: 00065/00094 | Loss: 1060.5020 | CE: 0.3753 | KD: 1060.1266\n",
      "Train Epoch: 004 Batch: 00066/00094 | Loss: 1061.2135 | CE: 0.5910 | KD: 1060.6226\n",
      "Train Epoch: 004 Batch: 00067/00094 | Loss: 1060.9427 | CE: 0.5352 | KD: 1060.4076\n",
      "Train Epoch: 004 Batch: 00068/00094 | Loss: 1061.0378 | CE: 0.4126 | KD: 1060.6252\n",
      "Train Epoch: 004 Batch: 00069/00094 | Loss: 1060.9225 | CE: 0.4571 | KD: 1060.4653\n",
      "Train Epoch: 004 Batch: 00070/00094 | Loss: 1060.9503 | CE: 0.5256 | KD: 1060.4247\n",
      "Train Epoch: 004 Batch: 00071/00094 | Loss: 1060.8687 | CE: 0.4596 | KD: 1060.4091\n",
      "Train Epoch: 004 Batch: 00072/00094 | Loss: 1061.4318 | CE: 0.5621 | KD: 1060.8696\n",
      "Train Epoch: 004 Batch: 00073/00094 | Loss: 1061.0371 | CE: 0.5431 | KD: 1060.4940\n",
      "Train Epoch: 004 Batch: 00074/00094 | Loss: 1061.4048 | CE: 0.6422 | KD: 1060.7626\n",
      "Train Epoch: 004 Batch: 00075/00094 | Loss: 1060.9468 | CE: 0.4251 | KD: 1060.5216\n",
      "Train Epoch: 004 Batch: 00076/00094 | Loss: 1060.6562 | CE: 0.4190 | KD: 1060.2372\n",
      "Train Epoch: 004 Batch: 00077/00094 | Loss: 1060.8970 | CE: 0.5558 | KD: 1060.3412\n",
      "Train Epoch: 004 Batch: 00078/00094 | Loss: 1060.9541 | CE: 0.4235 | KD: 1060.5306\n",
      "Train Epoch: 004 Batch: 00079/00094 | Loss: 1060.8831 | CE: 0.4692 | KD: 1060.4138\n",
      "Train Epoch: 004 Batch: 00080/00094 | Loss: 1060.8932 | CE: 0.3886 | KD: 1060.5046\n",
      "Train Epoch: 004 Batch: 00081/00094 | Loss: 1060.9341 | CE: 0.4475 | KD: 1060.4866\n",
      "Train Epoch: 004 Batch: 00082/00094 | Loss: 1060.9983 | CE: 0.5512 | KD: 1060.4471\n",
      "Train Epoch: 004 Batch: 00083/00094 | Loss: 1061.0404 | CE: 0.4533 | KD: 1060.5870\n",
      "Train Epoch: 004 Batch: 00084/00094 | Loss: 1060.8481 | CE: 0.3760 | KD: 1060.4722\n",
      "Train Epoch: 004 Batch: 00085/00094 | Loss: 1060.8074 | CE: 0.4989 | KD: 1060.3085\n",
      "Train Epoch: 004 Batch: 00086/00094 | Loss: 1060.8007 | CE: 0.5026 | KD: 1060.2981\n",
      "Train Epoch: 004 Batch: 00087/00094 | Loss: 1060.9761 | CE: 0.5052 | KD: 1060.4708\n",
      "Train Epoch: 004 Batch: 00088/00094 | Loss: 1060.7157 | CE: 0.4564 | KD: 1060.2593\n",
      "Train Epoch: 004 Batch: 00089/00094 | Loss: 1060.7723 | CE: 0.4697 | KD: 1060.3026\n",
      "Train Epoch: 004 Batch: 00090/00094 | Loss: 1061.1052 | CE: 0.6468 | KD: 1060.4584\n",
      "Train Epoch: 004 Batch: 00091/00094 | Loss: 1061.4391 | CE: 0.6430 | KD: 1060.7961\n",
      "Train Epoch: 004 Batch: 00092/00094 | Loss: 1060.9281 | CE: 0.5212 | KD: 1060.4070\n",
      "Train Epoch: 004 Batch: 00093/00094 | Loss: 1061.3522 | CE: 0.7474 | KD: 1060.6047\n",
      "Train Epoch: 004 Batch: 00094/00094 | Loss: 1061.5936 | CE: 0.4533 | KD: 1061.1403\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.6783 | acc:76.8500\n",
      "[VAL Acc] Target: 76.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1414 | acc:49.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8070 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0950 | acc:50.9542\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8789 | acc:50.6270\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7178 | acc:53.4196\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6282 | acc:58.1505\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 58.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.7528 | acc:50.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 50.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7668 | acc:60.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.50%\n",
      "[VAL Acc] Avg 56.09%\n",
      "Train Epoch: 005 Batch: 00001/00094 | Loss: 1061.0011 | CE: 0.6312 | KD: 1060.3699\n",
      "Train Epoch: 005 Batch: 00002/00094 | Loss: 1061.2064 | CE: 0.4905 | KD: 1060.7159\n",
      "Train Epoch: 005 Batch: 00003/00094 | Loss: 1060.9246 | CE: 0.4035 | KD: 1060.5211\n",
      "Train Epoch: 005 Batch: 00004/00094 | Loss: 1060.9285 | CE: 0.5605 | KD: 1060.3679\n",
      "Train Epoch: 005 Batch: 00005/00094 | Loss: 1061.5131 | CE: 0.4974 | KD: 1061.0156\n",
      "Train Epoch: 005 Batch: 00006/00094 | Loss: 1060.7305 | CE: 0.4610 | KD: 1060.2694\n",
      "Train Epoch: 005 Batch: 00007/00094 | Loss: 1061.0637 | CE: 0.5630 | KD: 1060.5007\n",
      "Train Epoch: 005 Batch: 00008/00094 | Loss: 1061.1165 | CE: 0.4004 | KD: 1060.7161\n",
      "Train Epoch: 005 Batch: 00009/00094 | Loss: 1060.8335 | CE: 0.4776 | KD: 1060.3558\n",
      "Train Epoch: 005 Batch: 00010/00094 | Loss: 1060.6691 | CE: 0.5057 | KD: 1060.1633\n",
      "Train Epoch: 005 Batch: 00011/00094 | Loss: 1060.9037 | CE: 0.4083 | KD: 1060.4954\n",
      "Train Epoch: 005 Batch: 00012/00094 | Loss: 1060.9937 | CE: 0.4318 | KD: 1060.5619\n",
      "Train Epoch: 005 Batch: 00013/00094 | Loss: 1061.0486 | CE: 0.4640 | KD: 1060.5846\n",
      "Train Epoch: 005 Batch: 00014/00094 | Loss: 1061.1151 | CE: 0.5350 | KD: 1060.5801\n",
      "Train Epoch: 005 Batch: 00015/00094 | Loss: 1061.0110 | CE: 0.6057 | KD: 1060.4053\n",
      "Train Epoch: 005 Batch: 00016/00094 | Loss: 1060.7355 | CE: 0.4675 | KD: 1060.2679\n",
      "Train Epoch: 005 Batch: 00017/00094 | Loss: 1060.6321 | CE: 0.3387 | KD: 1060.2933\n",
      "Train Epoch: 005 Batch: 00018/00094 | Loss: 1060.7452 | CE: 0.5199 | KD: 1060.2253\n",
      "Train Epoch: 005 Batch: 00019/00094 | Loss: 1060.6216 | CE: 0.3979 | KD: 1060.2236\n",
      "Train Epoch: 005 Batch: 00020/00094 | Loss: 1060.9661 | CE: 0.5332 | KD: 1060.4329\n",
      "Train Epoch: 005 Batch: 00021/00094 | Loss: 1061.2676 | CE: 0.5692 | KD: 1060.6984\n",
      "Train Epoch: 005 Batch: 00022/00094 | Loss: 1060.7073 | CE: 0.4684 | KD: 1060.2389\n",
      "Train Epoch: 005 Batch: 00023/00094 | Loss: 1060.8738 | CE: 0.4071 | KD: 1060.4667\n",
      "Train Epoch: 005 Batch: 00024/00094 | Loss: 1061.0985 | CE: 0.5874 | KD: 1060.5111\n",
      "Train Epoch: 005 Batch: 00025/00094 | Loss: 1060.9619 | CE: 0.4985 | KD: 1060.4634\n",
      "Train Epoch: 005 Batch: 00026/00094 | Loss: 1060.8506 | CE: 0.4245 | KD: 1060.4261\n",
      "Train Epoch: 005 Batch: 00027/00094 | Loss: 1061.1123 | CE: 0.4249 | KD: 1060.6874\n",
      "Train Epoch: 005 Batch: 00028/00094 | Loss: 1060.6490 | CE: 0.3728 | KD: 1060.2762\n",
      "Train Epoch: 005 Batch: 00029/00094 | Loss: 1061.3326 | CE: 0.5854 | KD: 1060.7472\n",
      "Train Epoch: 005 Batch: 00030/00094 | Loss: 1060.6139 | CE: 0.3785 | KD: 1060.2354\n",
      "Train Epoch: 005 Batch: 00031/00094 | Loss: 1060.8280 | CE: 0.4639 | KD: 1060.3641\n",
      "Train Epoch: 005 Batch: 00032/00094 | Loss: 1061.1616 | CE: 0.5337 | KD: 1060.6279\n",
      "Train Epoch: 005 Batch: 00033/00094 | Loss: 1061.2378 | CE: 0.4727 | KD: 1060.7651\n",
      "Train Epoch: 005 Batch: 00034/00094 | Loss: 1060.9271 | CE: 0.4480 | KD: 1060.4791\n",
      "Train Epoch: 005 Batch: 00035/00094 | Loss: 1061.3058 | CE: 0.6576 | KD: 1060.6482\n",
      "Train Epoch: 005 Batch: 00036/00094 | Loss: 1061.4233 | CE: 0.5239 | KD: 1060.8994\n",
      "Train Epoch: 005 Batch: 00037/00094 | Loss: 1060.7469 | CE: 0.4060 | KD: 1060.3409\n",
      "Train Epoch: 005 Batch: 00038/00094 | Loss: 1061.2333 | CE: 0.5909 | KD: 1060.6423\n",
      "Train Epoch: 005 Batch: 00039/00094 | Loss: 1060.8574 | CE: 0.4201 | KD: 1060.4374\n",
      "Train Epoch: 005 Batch: 00040/00094 | Loss: 1061.2764 | CE: 0.4936 | KD: 1060.7827\n",
      "Train Epoch: 005 Batch: 00041/00094 | Loss: 1060.9362 | CE: 0.4514 | KD: 1060.4847\n",
      "Train Epoch: 005 Batch: 00042/00094 | Loss: 1060.8783 | CE: 0.3964 | KD: 1060.4819\n",
      "Train Epoch: 005 Batch: 00043/00094 | Loss: 1060.7346 | CE: 0.4497 | KD: 1060.2849\n",
      "Train Epoch: 005 Batch: 00044/00094 | Loss: 1061.0765 | CE: 0.4353 | KD: 1060.6412\n",
      "Train Epoch: 005 Batch: 00045/00094 | Loss: 1061.3459 | CE: 0.5852 | KD: 1060.7607\n",
      "Train Epoch: 005 Batch: 00046/00094 | Loss: 1060.9354 | CE: 0.4672 | KD: 1060.4683\n",
      "Train Epoch: 005 Batch: 00047/00094 | Loss: 1061.0110 | CE: 0.6254 | KD: 1060.3856\n",
      "Train Epoch: 005 Batch: 00048/00094 | Loss: 1060.9034 | CE: 0.4565 | KD: 1060.4469\n",
      "Train Epoch: 005 Batch: 00049/00094 | Loss: 1061.5858 | CE: 0.6019 | KD: 1060.9839\n",
      "Train Epoch: 005 Batch: 00050/00094 | Loss: 1060.7552 | CE: 0.4528 | KD: 1060.3024\n",
      "Train Epoch: 005 Batch: 00051/00094 | Loss: 1060.8151 | CE: 0.4804 | KD: 1060.3347\n",
      "Train Epoch: 005 Batch: 00052/00094 | Loss: 1062.3898 | CE: 0.7506 | KD: 1061.6392\n",
      "Train Epoch: 005 Batch: 00053/00094 | Loss: 1061.5331 | CE: 0.5147 | KD: 1061.0184\n",
      "Train Epoch: 005 Batch: 00054/00094 | Loss: 1060.5765 | CE: 0.3787 | KD: 1060.1979\n",
      "Train Epoch: 005 Batch: 00055/00094 | Loss: 1060.9161 | CE: 0.4660 | KD: 1060.4502\n",
      "Train Epoch: 005 Batch: 00056/00094 | Loss: 1061.2133 | CE: 0.5394 | KD: 1060.6738\n",
      "Train Epoch: 005 Batch: 00057/00094 | Loss: 1061.3739 | CE: 0.4840 | KD: 1060.8899\n",
      "Train Epoch: 005 Batch: 00058/00094 | Loss: 1061.1318 | CE: 0.5113 | KD: 1060.6206\n",
      "Train Epoch: 005 Batch: 00059/00094 | Loss: 1060.8799 | CE: 0.3765 | KD: 1060.5034\n",
      "Train Epoch: 005 Batch: 00060/00094 | Loss: 1061.0939 | CE: 0.6164 | KD: 1060.4775\n",
      "Train Epoch: 005 Batch: 00061/00094 | Loss: 1061.3348 | CE: 0.5245 | KD: 1060.8103\n",
      "Train Epoch: 005 Batch: 00062/00094 | Loss: 1060.9397 | CE: 0.5798 | KD: 1060.3600\n",
      "Train Epoch: 005 Batch: 00063/00094 | Loss: 1061.4924 | CE: 0.6306 | KD: 1060.8618\n",
      "Train Epoch: 005 Batch: 00064/00094 | Loss: 1060.7582 | CE: 0.4444 | KD: 1060.3137\n",
      "Train Epoch: 005 Batch: 00065/00094 | Loss: 1061.2954 | CE: 0.4595 | KD: 1060.8359\n",
      "Train Epoch: 005 Batch: 00066/00094 | Loss: 1060.8040 | CE: 0.3871 | KD: 1060.4169\n",
      "Train Epoch: 005 Batch: 00067/00094 | Loss: 1061.3523 | CE: 0.5078 | KD: 1060.8445\n",
      "Train Epoch: 005 Batch: 00068/00094 | Loss: 1060.8590 | CE: 0.4820 | KD: 1060.3770\n",
      "Train Epoch: 005 Batch: 00069/00094 | Loss: 1060.9154 | CE: 0.3532 | KD: 1060.5623\n",
      "Train Epoch: 005 Batch: 00070/00094 | Loss: 1060.9030 | CE: 0.4355 | KD: 1060.4674\n",
      "Train Epoch: 005 Batch: 00071/00094 | Loss: 1061.1254 | CE: 0.6061 | KD: 1060.5193\n",
      "Train Epoch: 005 Batch: 00072/00094 | Loss: 1061.0365 | CE: 0.3939 | KD: 1060.6426\n",
      "Train Epoch: 005 Batch: 00073/00094 | Loss: 1060.9407 | CE: 0.6133 | KD: 1060.3274\n",
      "Train Epoch: 005 Batch: 00074/00094 | Loss: 1061.1700 | CE: 0.6687 | KD: 1060.5013\n",
      "Train Epoch: 005 Batch: 00075/00094 | Loss: 1061.1761 | CE: 0.5876 | KD: 1060.5885\n",
      "Train Epoch: 005 Batch: 00076/00094 | Loss: 1061.5424 | CE: 0.6643 | KD: 1060.8781\n",
      "Train Epoch: 005 Batch: 00077/00094 | Loss: 1060.7332 | CE: 0.4816 | KD: 1060.2516\n",
      "Train Epoch: 005 Batch: 00078/00094 | Loss: 1061.6038 | CE: 0.5416 | KD: 1061.0621\n",
      "Train Epoch: 005 Batch: 00079/00094 | Loss: 1061.0247 | CE: 0.6358 | KD: 1060.3889\n",
      "Train Epoch: 005 Batch: 00080/00094 | Loss: 1061.1946 | CE: 0.4903 | KD: 1060.7043\n",
      "Train Epoch: 005 Batch: 00081/00094 | Loss: 1060.6958 | CE: 0.4559 | KD: 1060.2399\n",
      "Train Epoch: 005 Batch: 00082/00094 | Loss: 1061.3225 | CE: 0.4626 | KD: 1060.8600\n",
      "Train Epoch: 005 Batch: 00083/00094 | Loss: 1060.6260 | CE: 0.4389 | KD: 1060.1870\n",
      "Train Epoch: 005 Batch: 00084/00094 | Loss: 1061.5803 | CE: 0.4032 | KD: 1061.1771\n",
      "Train Epoch: 005 Batch: 00085/00094 | Loss: 1061.6096 | CE: 0.7235 | KD: 1060.8861\n",
      "Train Epoch: 005 Batch: 00086/00094 | Loss: 1061.4033 | CE: 0.3988 | KD: 1061.0045\n",
      "Train Epoch: 005 Batch: 00087/00094 | Loss: 1060.9666 | CE: 0.5048 | KD: 1060.4618\n",
      "Train Epoch: 005 Batch: 00088/00094 | Loss: 1061.1230 | CE: 0.4707 | KD: 1060.6523\n",
      "Train Epoch: 005 Batch: 00089/00094 | Loss: 1060.7985 | CE: 0.5332 | KD: 1060.2653\n",
      "Train Epoch: 005 Batch: 00090/00094 | Loss: 1061.0721 | CE: 0.4890 | KD: 1060.5831\n",
      "Train Epoch: 005 Batch: 00091/00094 | Loss: 1060.9303 | CE: 0.4148 | KD: 1060.5155\n",
      "Train Epoch: 005 Batch: 00092/00094 | Loss: 1061.3401 | CE: 0.4993 | KD: 1060.8408\n",
      "Train Epoch: 005 Batch: 00093/00094 | Loss: 1060.6871 | CE: 0.3753 | KD: 1060.3119\n",
      "Train Epoch: 005 Batch: 00094/00094 | Loss: 1061.6370 | CE: 0.4397 | KD: 1061.1973\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.6035 | acc:78.7500\n",
      "[VAL Acc] Target: 78.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2473 | acc:49.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9260 | acc:53.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2204 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8629 | acc:51.5674\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8471 | acc:51.0166\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5840 | acc:64.8119\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8935 | acc:48.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.8018 | acc:60.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.75%\n",
      "[VAL Acc] Avg 56.46%\n",
      "Train Epoch: 006 Batch: 00001/00094 | Loss: 954.8994 | CE: 0.4519 | KD: 1060.4973\n",
      "Train Epoch: 006 Batch: 00002/00094 | Loss: 954.7744 | CE: 0.5124 | KD: 1060.2911\n",
      "Train Epoch: 006 Batch: 00003/00094 | Loss: 955.6927 | CE: 0.6800 | KD: 1061.1254\n",
      "Train Epoch: 006 Batch: 00004/00094 | Loss: 955.2477 | CE: 0.4127 | KD: 1060.9279\n",
      "Train Epoch: 006 Batch: 00005/00094 | Loss: 954.8751 | CE: 0.4941 | KD: 1060.4233\n",
      "Train Epoch: 006 Batch: 00006/00094 | Loss: 955.0383 | CE: 0.6685 | KD: 1060.4109\n",
      "Train Epoch: 006 Batch: 00007/00094 | Loss: 955.0059 | CE: 0.3261 | KD: 1060.7554\n",
      "Train Epoch: 006 Batch: 00008/00094 | Loss: 954.9460 | CE: 0.4750 | KD: 1060.5234\n",
      "Train Epoch: 006 Batch: 00009/00094 | Loss: 954.8972 | CE: 0.5424 | KD: 1060.3943\n",
      "Train Epoch: 006 Batch: 00010/00094 | Loss: 955.3945 | CE: 0.5092 | KD: 1060.9836\n",
      "Train Epoch: 006 Batch: 00011/00094 | Loss: 955.1131 | CE: 0.5061 | KD: 1060.6744\n",
      "Train Epoch: 006 Batch: 00012/00094 | Loss: 955.2162 | CE: 0.5043 | KD: 1060.7910\n",
      "Train Epoch: 006 Batch: 00013/00094 | Loss: 955.3892 | CE: 0.6993 | KD: 1060.7666\n",
      "Train Epoch: 006 Batch: 00014/00094 | Loss: 954.5872 | CE: 0.3047 | KD: 1060.3140\n",
      "Train Epoch: 006 Batch: 00015/00094 | Loss: 954.8239 | CE: 0.5524 | KD: 1060.3018\n",
      "Train Epoch: 006 Batch: 00016/00094 | Loss: 955.2882 | CE: 0.6213 | KD: 1060.7411\n",
      "Train Epoch: 006 Batch: 00017/00094 | Loss: 955.0092 | CE: 0.5494 | KD: 1060.5110\n",
      "Train Epoch: 006 Batch: 00018/00094 | Loss: 954.8073 | CE: 0.3837 | KD: 1060.4707\n",
      "Train Epoch: 006 Batch: 00019/00094 | Loss: 955.2949 | CE: 0.5771 | KD: 1060.7976\n",
      "Train Epoch: 006 Batch: 00020/00094 | Loss: 954.8777 | CE: 0.4797 | KD: 1060.4423\n",
      "Train Epoch: 006 Batch: 00021/00094 | Loss: 955.2355 | CE: 0.4325 | KD: 1060.8922\n",
      "Train Epoch: 006 Batch: 00022/00094 | Loss: 954.9967 | CE: 0.5737 | KD: 1060.4701\n",
      "Train Epoch: 006 Batch: 00023/00094 | Loss: 954.9742 | CE: 0.5018 | KD: 1060.5249\n",
      "Train Epoch: 006 Batch: 00024/00094 | Loss: 955.0170 | CE: 0.4669 | KD: 1060.6112\n",
      "Train Epoch: 006 Batch: 00025/00094 | Loss: 954.7258 | CE: 0.4509 | KD: 1060.3055\n",
      "Train Epoch: 006 Batch: 00026/00094 | Loss: 954.9350 | CE: 0.5406 | KD: 1060.4382\n",
      "Train Epoch: 006 Batch: 00027/00094 | Loss: 954.8532 | CE: 0.5663 | KD: 1060.3188\n",
      "Train Epoch: 006 Batch: 00028/00094 | Loss: 955.6443 | CE: 0.6624 | KD: 1061.0911\n",
      "Train Epoch: 006 Batch: 00029/00094 | Loss: 955.0457 | CE: 0.4962 | KD: 1060.6106\n",
      "Train Epoch: 006 Batch: 00030/00094 | Loss: 955.2484 | CE: 0.7282 | KD: 1060.5780\n",
      "Train Epoch: 006 Batch: 00031/00094 | Loss: 954.8096 | CE: 0.4569 | KD: 1060.3918\n",
      "Train Epoch: 006 Batch: 00032/00094 | Loss: 954.6553 | CE: 0.3987 | KD: 1060.2850\n",
      "Train Epoch: 006 Batch: 00033/00094 | Loss: 954.9178 | CE: 0.6096 | KD: 1060.3425\n",
      "Train Epoch: 006 Batch: 00034/00094 | Loss: 954.9341 | CE: 0.4558 | KD: 1060.5315\n",
      "Train Epoch: 006 Batch: 00035/00094 | Loss: 954.6964 | CE: 0.3552 | KD: 1060.3792\n",
      "Train Epoch: 006 Batch: 00036/00094 | Loss: 954.7109 | CE: 0.4324 | KD: 1060.3094\n",
      "Train Epoch: 006 Batch: 00037/00094 | Loss: 954.6087 | CE: 0.3602 | KD: 1060.2761\n",
      "Train Epoch: 006 Batch: 00038/00094 | Loss: 954.7462 | CE: 0.3556 | KD: 1060.4340\n",
      "Train Epoch: 006 Batch: 00039/00094 | Loss: 955.0491 | CE: 0.4392 | KD: 1060.6777\n",
      "Train Epoch: 006 Batch: 00040/00094 | Loss: 954.9589 | CE: 0.5590 | KD: 1060.4443\n",
      "Train Epoch: 006 Batch: 00041/00094 | Loss: 954.9527 | CE: 0.5053 | KD: 1060.4972\n",
      "Train Epoch: 006 Batch: 00042/00094 | Loss: 954.7030 | CE: 0.4603 | KD: 1060.2697\n",
      "Train Epoch: 006 Batch: 00043/00094 | Loss: 955.2226 | CE: 0.4839 | KD: 1060.8208\n",
      "Train Epoch: 006 Batch: 00044/00094 | Loss: 954.7715 | CE: 0.5091 | KD: 1060.2916\n",
      "Train Epoch: 006 Batch: 00045/00094 | Loss: 954.9290 | CE: 0.3981 | KD: 1060.5900\n",
      "Train Epoch: 006 Batch: 00046/00094 | Loss: 954.5594 | CE: 0.3185 | KD: 1060.2677\n",
      "Train Epoch: 006 Batch: 00047/00094 | Loss: 954.7623 | CE: 0.4379 | KD: 1060.3605\n",
      "Train Epoch: 006 Batch: 00048/00094 | Loss: 955.6345 | CE: 0.5229 | KD: 1061.2351\n",
      "Train Epoch: 006 Batch: 00049/00094 | Loss: 955.3911 | CE: 0.4081 | KD: 1061.0923\n",
      "Train Epoch: 006 Batch: 00050/00094 | Loss: 955.2281 | CE: 0.5126 | KD: 1060.7950\n",
      "Train Epoch: 006 Batch: 00051/00094 | Loss: 954.8145 | CE: 0.5546 | KD: 1060.2887\n",
      "Train Epoch: 006 Batch: 00052/00094 | Loss: 955.3307 | CE: 0.6051 | KD: 1060.8063\n",
      "Train Epoch: 006 Batch: 00053/00094 | Loss: 955.1867 | CE: 0.5817 | KD: 1060.6722\n",
      "Train Epoch: 006 Batch: 00054/00094 | Loss: 955.0836 | CE: 0.5211 | KD: 1060.6250\n",
      "Train Epoch: 006 Batch: 00055/00094 | Loss: 955.1236 | CE: 0.5506 | KD: 1060.6367\n",
      "Train Epoch: 006 Batch: 00056/00094 | Loss: 954.7773 | CE: 0.4650 | KD: 1060.3470\n",
      "Train Epoch: 006 Batch: 00057/00094 | Loss: 955.0049 | CE: 0.4032 | KD: 1060.6687\n",
      "Train Epoch: 006 Batch: 00058/00094 | Loss: 954.7865 | CE: 0.4121 | KD: 1060.4160\n",
      "Train Epoch: 006 Batch: 00059/00094 | Loss: 954.6900 | CE: 0.4447 | KD: 1060.2726\n",
      "Train Epoch: 006 Batch: 00060/00094 | Loss: 954.6349 | CE: 0.4238 | KD: 1060.2346\n",
      "Train Epoch: 006 Batch: 00061/00094 | Loss: 954.9105 | CE: 0.4749 | KD: 1060.4840\n",
      "Train Epoch: 006 Batch: 00062/00094 | Loss: 954.6590 | CE: 0.3486 | KD: 1060.3450\n",
      "Train Epoch: 006 Batch: 00063/00094 | Loss: 955.2029 | CE: 0.4470 | KD: 1060.8400\n",
      "Train Epoch: 006 Batch: 00064/00094 | Loss: 954.5173 | CE: 0.3831 | KD: 1060.1492\n",
      "Train Epoch: 006 Batch: 00065/00094 | Loss: 955.0694 | CE: 0.4576 | KD: 1060.6798\n",
      "Train Epoch: 006 Batch: 00066/00094 | Loss: 954.7189 | CE: 0.4042 | KD: 1060.3497\n",
      "Train Epoch: 006 Batch: 00067/00094 | Loss: 954.7583 | CE: 0.4529 | KD: 1060.3394\n",
      "Train Epoch: 006 Batch: 00068/00094 | Loss: 954.7273 | CE: 0.4637 | KD: 1060.2928\n",
      "Train Epoch: 006 Batch: 00069/00094 | Loss: 954.7352 | CE: 0.4461 | KD: 1060.3213\n",
      "Train Epoch: 006 Batch: 00070/00094 | Loss: 955.4150 | CE: 0.5756 | KD: 1060.9327\n",
      "Train Epoch: 006 Batch: 00071/00094 | Loss: 954.6651 | CE: 0.3989 | KD: 1060.2958\n",
      "Train Epoch: 006 Batch: 00072/00094 | Loss: 954.4994 | CE: 0.3066 | KD: 1060.2142\n",
      "Train Epoch: 006 Batch: 00073/00094 | Loss: 955.0316 | CE: 0.4963 | KD: 1060.5947\n",
      "Train Epoch: 006 Batch: 00074/00094 | Loss: 954.8938 | CE: 0.5238 | KD: 1060.4111\n",
      "Train Epoch: 006 Batch: 00075/00094 | Loss: 954.9286 | CE: 0.3780 | KD: 1060.6118\n",
      "Train Epoch: 006 Batch: 00076/00094 | Loss: 954.8348 | CE: 0.4596 | KD: 1060.4169\n",
      "Train Epoch: 006 Batch: 00077/00094 | Loss: 954.7513 | CE: 0.4892 | KD: 1060.2911\n",
      "Train Epoch: 006 Batch: 00078/00094 | Loss: 955.1793 | CE: 0.3808 | KD: 1060.8872\n",
      "Train Epoch: 006 Batch: 00079/00094 | Loss: 954.8195 | CE: 0.5479 | KD: 1060.3018\n",
      "Train Epoch: 006 Batch: 00080/00094 | Loss: 955.1815 | CE: 0.5522 | KD: 1060.6993\n",
      "Train Epoch: 006 Batch: 00081/00094 | Loss: 954.7517 | CE: 0.4622 | KD: 1060.3217\n",
      "Train Epoch: 006 Batch: 00082/00094 | Loss: 955.0526 | CE: 0.5116 | KD: 1060.6011\n",
      "Train Epoch: 006 Batch: 00083/00094 | Loss: 955.0443 | CE: 0.5356 | KD: 1060.5652\n",
      "Train Epoch: 006 Batch: 00084/00094 | Loss: 954.9328 | CE: 0.6203 | KD: 1060.3473\n",
      "Train Epoch: 006 Batch: 00085/00094 | Loss: 955.0914 | CE: 0.4846 | KD: 1060.6742\n",
      "Train Epoch: 006 Batch: 00086/00094 | Loss: 955.1415 | CE: 0.5565 | KD: 1060.6500\n",
      "Train Epoch: 006 Batch: 00087/00094 | Loss: 955.5923 | CE: 0.4432 | KD: 1061.2767\n",
      "Train Epoch: 006 Batch: 00088/00094 | Loss: 955.1909 | CE: 0.4515 | KD: 1060.8215\n",
      "Train Epoch: 006 Batch: 00089/00094 | Loss: 954.8214 | CE: 0.5128 | KD: 1060.3429\n",
      "Train Epoch: 006 Batch: 00090/00094 | Loss: 955.0995 | CE: 0.4321 | KD: 1060.7417\n",
      "Train Epoch: 006 Batch: 00091/00094 | Loss: 954.9761 | CE: 0.5332 | KD: 1060.4922\n",
      "Train Epoch: 006 Batch: 00092/00094 | Loss: 955.1790 | CE: 0.6048 | KD: 1060.6381\n",
      "Train Epoch: 006 Batch: 00093/00094 | Loss: 954.8713 | CE: 0.4498 | KD: 1060.4684\n",
      "Train Epoch: 006 Batch: 00094/00094 | Loss: 954.8221 | CE: 0.4636 | KD: 1060.3984\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.5293 | acc:80.1000\n",
      "[VAL Acc] Target: 80.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2012 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8819 | acc:54.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1742 | acc:50.3817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8508 | acc:51.2539\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8062 | acc:51.7560\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6052 | acc:63.6755\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8608 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7322 | acc:60.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.15%\n",
      "[VAL Acc] Avg 56.70%\n",
      "Train Epoch: 007 Batch: 00001/00094 | Loss: 955.0496 | CE: 0.4886 | KD: 1060.6234\n",
      "Train Epoch: 007 Batch: 00002/00094 | Loss: 954.9298 | CE: 0.5156 | KD: 1060.4602\n",
      "Train Epoch: 007 Batch: 00003/00094 | Loss: 955.1133 | CE: 0.5421 | KD: 1060.6348\n",
      "Train Epoch: 007 Batch: 00004/00094 | Loss: 955.2617 | CE: 0.3731 | KD: 1060.9873\n",
      "Train Epoch: 007 Batch: 00005/00094 | Loss: 955.2413 | CE: 0.5220 | KD: 1060.7993\n",
      "Train Epoch: 007 Batch: 00006/00094 | Loss: 955.0924 | CE: 0.5720 | KD: 1060.5782\n",
      "Train Epoch: 007 Batch: 00007/00094 | Loss: 954.6080 | CE: 0.4319 | KD: 1060.1957\n",
      "Train Epoch: 007 Batch: 00008/00094 | Loss: 954.9175 | CE: 0.5182 | KD: 1060.4437\n",
      "Train Epoch: 007 Batch: 00009/00094 | Loss: 954.8357 | CE: 0.4659 | KD: 1060.4109\n",
      "Train Epoch: 007 Batch: 00010/00094 | Loss: 954.9319 | CE: 0.4525 | KD: 1060.5327\n",
      "Train Epoch: 007 Batch: 00011/00094 | Loss: 954.9835 | CE: 0.5625 | KD: 1060.4679\n",
      "Train Epoch: 007 Batch: 00012/00094 | Loss: 955.2794 | CE: 0.5936 | KD: 1060.7621\n",
      "Train Epoch: 007 Batch: 00013/00094 | Loss: 954.9044 | CE: 0.3636 | KD: 1060.6010\n",
      "Train Epoch: 007 Batch: 00014/00094 | Loss: 955.3358 | CE: 0.5212 | KD: 1060.9052\n",
      "Train Epoch: 007 Batch: 00015/00094 | Loss: 954.7437 | CE: 0.4208 | KD: 1060.3588\n",
      "Train Epoch: 007 Batch: 00016/00094 | Loss: 955.0437 | CE: 0.6136 | KD: 1060.4779\n",
      "Train Epoch: 007 Batch: 00017/00094 | Loss: 954.7456 | CE: 0.4406 | KD: 1060.3390\n",
      "Train Epoch: 007 Batch: 00018/00094 | Loss: 955.1826 | CE: 0.5339 | KD: 1060.7207\n",
      "Train Epoch: 007 Batch: 00019/00094 | Loss: 954.9787 | CE: 0.3393 | KD: 1060.7106\n",
      "Train Epoch: 007 Batch: 00020/00094 | Loss: 954.9351 | CE: 0.5244 | KD: 1060.4563\n",
      "Train Epoch: 007 Batch: 00021/00094 | Loss: 954.7567 | CE: 0.4246 | KD: 1060.3690\n",
      "Train Epoch: 007 Batch: 00022/00094 | Loss: 955.4274 | CE: 0.5307 | KD: 1060.9965\n",
      "Train Epoch: 007 Batch: 00023/00094 | Loss: 954.9921 | CE: 0.4307 | KD: 1060.6239\n",
      "Train Epoch: 007 Batch: 00024/00094 | Loss: 954.8323 | CE: 0.5408 | KD: 1060.3239\n",
      "Train Epoch: 007 Batch: 00025/00094 | Loss: 954.8279 | CE: 0.4549 | KD: 1060.4146\n",
      "Train Epoch: 007 Batch: 00026/00094 | Loss: 955.2209 | CE: 0.4149 | KD: 1060.8956\n",
      "Train Epoch: 007 Batch: 00027/00094 | Loss: 954.5777 | CE: 0.3971 | KD: 1060.2007\n",
      "Train Epoch: 007 Batch: 00028/00094 | Loss: 954.7412 | CE: 0.4635 | KD: 1060.3086\n",
      "Train Epoch: 007 Batch: 00029/00094 | Loss: 954.8385 | CE: 0.4675 | KD: 1060.4122\n",
      "Train Epoch: 007 Batch: 00030/00094 | Loss: 955.3644 | CE: 0.5283 | KD: 1060.9290\n",
      "Train Epoch: 007 Batch: 00031/00094 | Loss: 955.1597 | CE: 0.4103 | KD: 1060.8328\n",
      "Train Epoch: 007 Batch: 00032/00094 | Loss: 954.7273 | CE: 0.5042 | KD: 1060.2479\n",
      "Train Epoch: 007 Batch: 00033/00094 | Loss: 954.9429 | CE: 0.5071 | KD: 1060.4843\n",
      "Train Epoch: 007 Batch: 00034/00094 | Loss: 954.8229 | CE: 0.4433 | KD: 1060.4218\n",
      "Train Epoch: 007 Batch: 00035/00094 | Loss: 955.2285 | CE: 0.4804 | KD: 1060.8313\n",
      "Train Epoch: 007 Batch: 00036/00094 | Loss: 954.6757 | CE: 0.4317 | KD: 1060.2710\n",
      "Train Epoch: 007 Batch: 00037/00094 | Loss: 955.4222 | CE: 0.6173 | KD: 1060.8944\n",
      "Train Epoch: 007 Batch: 00038/00094 | Loss: 954.7263 | CE: 0.3730 | KD: 1060.3926\n",
      "Train Epoch: 007 Batch: 00039/00094 | Loss: 954.6642 | CE: 0.4144 | KD: 1060.2776\n",
      "Train Epoch: 007 Batch: 00040/00094 | Loss: 955.2852 | CE: 0.6513 | KD: 1060.7043\n",
      "Train Epoch: 007 Batch: 00041/00094 | Loss: 954.6974 | CE: 0.4843 | KD: 1060.2368\n",
      "Train Epoch: 007 Batch: 00042/00094 | Loss: 955.1673 | CE: 0.5179 | KD: 1060.7216\n",
      "Train Epoch: 007 Batch: 00043/00094 | Loss: 954.8401 | CE: 0.3810 | KD: 1060.5101\n",
      "Train Epoch: 007 Batch: 00044/00094 | Loss: 954.7978 | CE: 0.4219 | KD: 1060.4177\n",
      "Train Epoch: 007 Batch: 00045/00094 | Loss: 955.0876 | CE: 0.4044 | KD: 1060.7592\n",
      "Train Epoch: 007 Batch: 00046/00094 | Loss: 954.6720 | CE: 0.3851 | KD: 1060.3188\n",
      "Train Epoch: 007 Batch: 00047/00094 | Loss: 955.6097 | CE: 0.5019 | KD: 1061.2308\n",
      "Train Epoch: 007 Batch: 00048/00094 | Loss: 955.0161 | CE: 0.4656 | KD: 1060.6117\n",
      "Train Epoch: 007 Batch: 00049/00094 | Loss: 954.8687 | CE: 0.5544 | KD: 1060.3492\n",
      "Train Epoch: 007 Batch: 00050/00094 | Loss: 954.7989 | CE: 0.3936 | KD: 1060.4503\n",
      "Train Epoch: 007 Batch: 00051/00094 | Loss: 954.7847 | CE: 0.4930 | KD: 1060.3241\n",
      "Train Epoch: 007 Batch: 00052/00094 | Loss: 955.2388 | CE: 0.3331 | KD: 1061.0063\n",
      "Train Epoch: 007 Batch: 00053/00094 | Loss: 954.7770 | CE: 0.4788 | KD: 1060.3314\n",
      "Train Epoch: 007 Batch: 00054/00094 | Loss: 954.8945 | CE: 0.4649 | KD: 1060.4773\n",
      "Train Epoch: 007 Batch: 00055/00094 | Loss: 954.7527 | CE: 0.4900 | KD: 1060.2920\n",
      "Train Epoch: 007 Batch: 00056/00094 | Loss: 954.5770 | CE: 0.4272 | KD: 1060.1665\n",
      "Train Epoch: 007 Batch: 00057/00094 | Loss: 955.3466 | CE: 0.5487 | KD: 1060.8865\n",
      "Train Epoch: 007 Batch: 00058/00094 | Loss: 954.9822 | CE: 0.4332 | KD: 1060.6101\n",
      "Train Epoch: 007 Batch: 00059/00094 | Loss: 954.7397 | CE: 0.4336 | KD: 1060.3402\n",
      "Train Epoch: 007 Batch: 00060/00094 | Loss: 954.7894 | CE: 0.4037 | KD: 1060.4286\n",
      "Train Epoch: 007 Batch: 00061/00094 | Loss: 954.6816 | CE: 0.3648 | KD: 1060.3521\n",
      "Train Epoch: 007 Batch: 00062/00094 | Loss: 955.6891 | CE: 0.6123 | KD: 1061.1965\n",
      "Train Epoch: 007 Batch: 00063/00094 | Loss: 955.1632 | CE: 0.5475 | KD: 1060.6841\n",
      "Train Epoch: 007 Batch: 00064/00094 | Loss: 954.6500 | CE: 0.4423 | KD: 1060.2307\n",
      "Train Epoch: 007 Batch: 00065/00094 | Loss: 955.5103 | CE: 0.5931 | KD: 1061.0192\n",
      "Train Epoch: 007 Batch: 00066/00094 | Loss: 955.2145 | CE: 0.5868 | KD: 1060.6974\n",
      "Train Epoch: 007 Batch: 00067/00094 | Loss: 954.6426 | CE: 0.5078 | KD: 1060.1498\n",
      "Train Epoch: 007 Batch: 00068/00094 | Loss: 955.4869 | CE: 0.6340 | KD: 1060.9476\n",
      "Train Epoch: 007 Batch: 00069/00094 | Loss: 954.8881 | CE: 0.3995 | KD: 1060.5430\n",
      "Train Epoch: 007 Batch: 00070/00094 | Loss: 954.9536 | CE: 0.3881 | KD: 1060.6284\n",
      "Train Epoch: 007 Batch: 00071/00094 | Loss: 955.4675 | CE: 0.4742 | KD: 1061.1038\n",
      "Train Epoch: 007 Batch: 00072/00094 | Loss: 954.8137 | CE: 0.4526 | KD: 1060.4012\n",
      "Train Epoch: 007 Batch: 00073/00094 | Loss: 954.9591 | CE: 0.4867 | KD: 1060.5249\n",
      "Train Epoch: 007 Batch: 00074/00094 | Loss: 954.6889 | CE: 0.4815 | KD: 1060.2305\n",
      "Train Epoch: 007 Batch: 00075/00094 | Loss: 955.0179 | CE: 0.6520 | KD: 1060.4066\n",
      "Train Epoch: 007 Batch: 00076/00094 | Loss: 954.7369 | CE: 0.4584 | KD: 1060.3094\n",
      "Train Epoch: 007 Batch: 00077/00094 | Loss: 954.9404 | CE: 0.4274 | KD: 1060.5699\n",
      "Train Epoch: 007 Batch: 00078/00094 | Loss: 954.8616 | CE: 0.5682 | KD: 1060.3260\n",
      "Train Epoch: 007 Batch: 00079/00094 | Loss: 954.9557 | CE: 0.4565 | KD: 1060.5547\n",
      "Train Epoch: 007 Batch: 00080/00094 | Loss: 955.0046 | CE: 0.5238 | KD: 1060.5343\n",
      "Train Epoch: 007 Batch: 00081/00094 | Loss: 954.6459 | CE: 0.4756 | KD: 1060.1893\n",
      "Train Epoch: 007 Batch: 00082/00094 | Loss: 955.0757 | CE: 0.5952 | KD: 1060.5339\n",
      "Train Epoch: 007 Batch: 00083/00094 | Loss: 955.2350 | CE: 0.5788 | KD: 1060.7291\n",
      "Train Epoch: 007 Batch: 00084/00094 | Loss: 954.9100 | CE: 0.3569 | KD: 1060.6146\n",
      "Train Epoch: 007 Batch: 00085/00094 | Loss: 954.7752 | CE: 0.5098 | KD: 1060.2949\n",
      "Train Epoch: 007 Batch: 00086/00094 | Loss: 954.9417 | CE: 0.5606 | KD: 1060.4235\n",
      "Train Epoch: 007 Batch: 00087/00094 | Loss: 955.2542 | CE: 0.5886 | KD: 1060.7395\n",
      "Train Epoch: 007 Batch: 00088/00094 | Loss: 954.7454 | CE: 0.3954 | KD: 1060.3889\n",
      "Train Epoch: 007 Batch: 00089/00094 | Loss: 954.9068 | CE: 0.4970 | KD: 1060.4554\n",
      "Train Epoch: 007 Batch: 00090/00094 | Loss: 955.1610 | CE: 0.4264 | KD: 1060.8163\n",
      "Train Epoch: 007 Batch: 00091/00094 | Loss: 954.7527 | CE: 0.3935 | KD: 1060.3992\n",
      "Train Epoch: 007 Batch: 00092/00094 | Loss: 955.5095 | CE: 0.6615 | KD: 1060.9421\n",
      "Train Epoch: 007 Batch: 00093/00094 | Loss: 955.0497 | CE: 0.6135 | KD: 1060.4847\n",
      "Train Epoch: 007 Batch: 00094/00094 | Loss: 954.9462 | CE: 0.3788 | KD: 1060.6306\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.5312 | acc:80.1000\n",
      "[VAL Acc] Target: 80.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2219 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8914 | acc:53.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1315 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9000 | acc:50.9796\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.98%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7820 | acc:51.9409\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6865 | acc:59.4044\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 59.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8079 | acc:49.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7306 | acc:60.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.35%\n",
      "[VAL Acc] Avg 56.02%\n",
      "Train Epoch: 008 Batch: 00001/00094 | Loss: 954.6763 | CE: 0.4296 | KD: 1060.2742\n",
      "Train Epoch: 008 Batch: 00002/00094 | Loss: 954.7759 | CE: 0.4982 | KD: 1060.3086\n",
      "Train Epoch: 008 Batch: 00003/00094 | Loss: 954.9559 | CE: 0.5814 | KD: 1060.4161\n",
      "Train Epoch: 008 Batch: 00004/00094 | Loss: 955.1014 | CE: 0.4032 | KD: 1060.7759\n",
      "Train Epoch: 008 Batch: 00005/00094 | Loss: 954.8888 | CE: 0.4170 | KD: 1060.5243\n",
      "Train Epoch: 008 Batch: 00006/00094 | Loss: 955.2118 | CE: 0.5683 | KD: 1060.7151\n",
      "Train Epoch: 008 Batch: 00007/00094 | Loss: 955.1658 | CE: 0.4405 | KD: 1060.8059\n",
      "Train Epoch: 008 Batch: 00008/00094 | Loss: 955.0584 | CE: 0.4131 | KD: 1060.7170\n",
      "Train Epoch: 008 Batch: 00009/00094 | Loss: 955.0524 | CE: 0.6362 | KD: 1060.4624\n",
      "Train Epoch: 008 Batch: 00010/00094 | Loss: 954.8831 | CE: 0.5158 | KD: 1060.4081\n",
      "Train Epoch: 008 Batch: 00011/00094 | Loss: 954.9617 | CE: 0.4590 | KD: 1060.5586\n",
      "Train Epoch: 008 Batch: 00012/00094 | Loss: 954.9636 | CE: 0.5117 | KD: 1060.5021\n",
      "Train Epoch: 008 Batch: 00013/00094 | Loss: 954.9349 | CE: 0.5988 | KD: 1060.3735\n",
      "Train Epoch: 008 Batch: 00014/00094 | Loss: 954.8014 | CE: 0.4135 | KD: 1060.4310\n",
      "Train Epoch: 008 Batch: 00015/00094 | Loss: 954.6648 | CE: 0.3589 | KD: 1060.3398\n",
      "Train Epoch: 008 Batch: 00016/00094 | Loss: 954.7680 | CE: 0.4979 | KD: 1060.3002\n",
      "Train Epoch: 008 Batch: 00017/00094 | Loss: 954.9731 | CE: 0.4056 | KD: 1060.6306\n",
      "Train Epoch: 008 Batch: 00018/00094 | Loss: 955.0809 | CE: 0.5243 | KD: 1060.6185\n",
      "Train Epoch: 008 Batch: 00019/00094 | Loss: 955.4123 | CE: 0.4746 | KD: 1061.0420\n",
      "Train Epoch: 008 Batch: 00020/00094 | Loss: 955.3474 | CE: 0.5382 | KD: 1060.8990\n",
      "Train Epoch: 008 Batch: 00021/00094 | Loss: 955.4061 | CE: 0.5964 | KD: 1060.8998\n",
      "Train Epoch: 008 Batch: 00022/00094 | Loss: 954.7463 | CE: 0.4048 | KD: 1060.3794\n",
      "Train Epoch: 008 Batch: 00023/00094 | Loss: 954.6703 | CE: 0.4176 | KD: 1060.2808\n",
      "Train Epoch: 008 Batch: 00024/00094 | Loss: 954.6882 | CE: 0.4503 | KD: 1060.2644\n",
      "Train Epoch: 008 Batch: 00025/00094 | Loss: 955.4266 | CE: 0.4925 | KD: 1061.0380\n",
      "Train Epoch: 008 Batch: 00026/00094 | Loss: 954.6873 | CE: 0.3319 | KD: 1060.3949\n",
      "Train Epoch: 008 Batch: 00027/00094 | Loss: 954.9169 | CE: 0.5472 | KD: 1060.4108\n",
      "Train Epoch: 008 Batch: 00028/00094 | Loss: 954.7559 | CE: 0.4461 | KD: 1060.3444\n",
      "Train Epoch: 008 Batch: 00029/00094 | Loss: 954.6866 | CE: 0.3586 | KD: 1060.3645\n",
      "Train Epoch: 008 Batch: 00030/00094 | Loss: 954.5963 | CE: 0.4375 | KD: 1060.1764\n",
      "Train Epoch: 008 Batch: 00031/00094 | Loss: 954.7480 | CE: 0.4982 | KD: 1060.2776\n",
      "Train Epoch: 008 Batch: 00032/00094 | Loss: 955.9504 | CE: 0.6554 | KD: 1061.4390\n",
      "Train Epoch: 008 Batch: 00033/00094 | Loss: 955.4470 | CE: 0.4802 | KD: 1061.0743\n",
      "Train Epoch: 008 Batch: 00034/00094 | Loss: 955.0248 | CE: 0.3716 | KD: 1060.7258\n",
      "Train Epoch: 008 Batch: 00035/00094 | Loss: 955.1232 | CE: 0.4694 | KD: 1060.7264\n",
      "Train Epoch: 008 Batch: 00036/00094 | Loss: 954.7780 | CE: 0.3929 | KD: 1060.4279\n",
      "Train Epoch: 008 Batch: 00037/00094 | Loss: 954.9708 | CE: 0.3936 | KD: 1060.6414\n",
      "Train Epoch: 008 Batch: 00038/00094 | Loss: 955.0015 | CE: 0.4784 | KD: 1060.5813\n",
      "Train Epoch: 008 Batch: 00039/00094 | Loss: 954.9914 | CE: 0.4856 | KD: 1060.5620\n",
      "Train Epoch: 008 Batch: 00040/00094 | Loss: 954.6686 | CE: 0.4757 | KD: 1060.2145\n",
      "Train Epoch: 008 Batch: 00041/00094 | Loss: 954.8491 | CE: 0.5323 | KD: 1060.3521\n",
      "Train Epoch: 008 Batch: 00042/00094 | Loss: 955.0474 | CE: 0.4641 | KD: 1060.6481\n",
      "Train Epoch: 008 Batch: 00043/00094 | Loss: 955.2952 | CE: 0.5115 | KD: 1060.8707\n",
      "Train Epoch: 008 Batch: 00044/00094 | Loss: 954.8900 | CE: 0.4681 | KD: 1060.4689\n",
      "Train Epoch: 008 Batch: 00045/00094 | Loss: 954.8372 | CE: 0.3930 | KD: 1060.4935\n",
      "Train Epoch: 008 Batch: 00046/00094 | Loss: 955.7598 | CE: 0.4716 | KD: 1061.4314\n",
      "Train Epoch: 008 Batch: 00047/00094 | Loss: 954.9801 | CE: 0.4271 | KD: 1060.6145\n",
      "Train Epoch: 008 Batch: 00048/00094 | Loss: 954.9271 | CE: 0.5061 | KD: 1060.4678\n",
      "Train Epoch: 008 Batch: 00049/00094 | Loss: 955.3206 | CE: 0.5461 | KD: 1060.8605\n",
      "Train Epoch: 008 Batch: 00050/00094 | Loss: 954.6624 | CE: 0.4024 | KD: 1060.2889\n",
      "Train Epoch: 008 Batch: 00051/00094 | Loss: 954.8271 | CE: 0.4953 | KD: 1060.3688\n",
      "Train Epoch: 008 Batch: 00052/00094 | Loss: 954.5892 | CE: 0.4664 | KD: 1060.1365\n",
      "Train Epoch: 008 Batch: 00053/00094 | Loss: 954.9138 | CE: 0.5384 | KD: 1060.4171\n",
      "Train Epoch: 008 Batch: 00054/00094 | Loss: 954.6006 | CE: 0.3701 | KD: 1060.2561\n",
      "Train Epoch: 008 Batch: 00055/00094 | Loss: 954.9424 | CE: 0.4214 | KD: 1060.5790\n",
      "Train Epoch: 008 Batch: 00056/00094 | Loss: 955.2523 | CE: 0.4949 | KD: 1060.8416\n",
      "Train Epoch: 008 Batch: 00057/00094 | Loss: 955.1403 | CE: 0.4358 | KD: 1060.7828\n",
      "Train Epoch: 008 Batch: 00058/00094 | Loss: 954.6119 | CE: 0.3529 | KD: 1060.2878\n",
      "Train Epoch: 008 Batch: 00059/00094 | Loss: 954.8945 | CE: 0.4734 | KD: 1060.4679\n",
      "Train Epoch: 008 Batch: 00060/00094 | Loss: 954.7835 | CE: 0.4391 | KD: 1060.3828\n",
      "Train Epoch: 008 Batch: 00061/00094 | Loss: 954.8163 | CE: 0.4110 | KD: 1060.4503\n",
      "Train Epoch: 008 Batch: 00062/00094 | Loss: 955.1541 | CE: 0.4509 | KD: 1060.7814\n",
      "Train Epoch: 008 Batch: 00063/00094 | Loss: 955.1591 | CE: 0.4580 | KD: 1060.7791\n",
      "Train Epoch: 008 Batch: 00064/00094 | Loss: 954.9874 | CE: 0.5836 | KD: 1060.4486\n",
      "Train Epoch: 008 Batch: 00065/00094 | Loss: 955.0630 | CE: 0.4387 | KD: 1060.6937\n",
      "Train Epoch: 008 Batch: 00066/00094 | Loss: 955.0770 | CE: 0.5940 | KD: 1060.5367\n",
      "Train Epoch: 008 Batch: 00067/00094 | Loss: 955.6260 | CE: 0.4733 | KD: 1061.2809\n",
      "Train Epoch: 008 Batch: 00068/00094 | Loss: 954.6673 | CE: 0.3587 | KD: 1060.3429\n",
      "Train Epoch: 008 Batch: 00069/00094 | Loss: 955.1062 | CE: 0.5065 | KD: 1060.6664\n",
      "Train Epoch: 008 Batch: 00070/00094 | Loss: 955.4790 | CE: 0.6075 | KD: 1060.9684\n",
      "Train Epoch: 008 Batch: 00071/00094 | Loss: 954.8887 | CE: 0.4486 | KD: 1060.4890\n",
      "Train Epoch: 008 Batch: 00072/00094 | Loss: 954.8862 | CE: 0.4231 | KD: 1060.5145\n",
      "Train Epoch: 008 Batch: 00073/00094 | Loss: 955.3941 | CE: 0.4708 | KD: 1061.0259\n",
      "Train Epoch: 008 Batch: 00074/00094 | Loss: 954.7620 | CE: 0.4663 | KD: 1060.3286\n",
      "Train Epoch: 008 Batch: 00075/00094 | Loss: 955.3476 | CE: 0.4413 | KD: 1061.0070\n",
      "Train Epoch: 008 Batch: 00076/00094 | Loss: 954.7707 | CE: 0.3825 | KD: 1060.4313\n",
      "Train Epoch: 008 Batch: 00077/00094 | Loss: 954.9507 | CE: 0.5023 | KD: 1060.4983\n",
      "Train Epoch: 008 Batch: 00078/00094 | Loss: 955.0289 | CE: 0.6252 | KD: 1060.4485\n",
      "Train Epoch: 008 Batch: 00079/00094 | Loss: 954.6085 | CE: 0.3625 | KD: 1060.2733\n",
      "Train Epoch: 008 Batch: 00080/00094 | Loss: 954.8755 | CE: 0.5562 | KD: 1060.3549\n",
      "Train Epoch: 008 Batch: 00081/00094 | Loss: 954.9324 | CE: 0.4389 | KD: 1060.5483\n",
      "Train Epoch: 008 Batch: 00082/00094 | Loss: 955.0438 | CE: 0.4687 | KD: 1060.6390\n",
      "Train Epoch: 008 Batch: 00083/00094 | Loss: 954.7581 | CE: 0.4401 | KD: 1060.3533\n",
      "Train Epoch: 008 Batch: 00084/00094 | Loss: 954.7066 | CE: 0.3876 | KD: 1060.3545\n",
      "Train Epoch: 008 Batch: 00085/00094 | Loss: 955.0567 | CE: 0.4896 | KD: 1060.6301\n",
      "Train Epoch: 008 Batch: 00086/00094 | Loss: 955.0556 | CE: 0.4987 | KD: 1060.6188\n",
      "Train Epoch: 008 Batch: 00087/00094 | Loss: 954.9854 | CE: 0.5555 | KD: 1060.4777\n",
      "Train Epoch: 008 Batch: 00088/00094 | Loss: 954.9529 | CE: 0.4936 | KD: 1060.5105\n",
      "Train Epoch: 008 Batch: 00089/00094 | Loss: 955.2123 | CE: 0.5269 | KD: 1060.7616\n",
      "Train Epoch: 008 Batch: 00090/00094 | Loss: 955.0111 | CE: 0.4017 | KD: 1060.6771\n",
      "Train Epoch: 008 Batch: 00091/00094 | Loss: 955.3002 | CE: 0.5878 | KD: 1060.7916\n",
      "Train Epoch: 008 Batch: 00092/00094 | Loss: 954.9841 | CE: 0.4419 | KD: 1060.6025\n",
      "Train Epoch: 008 Batch: 00093/00094 | Loss: 955.0452 | CE: 0.4620 | KD: 1060.6479\n",
      "Train Epoch: 008 Batch: 00094/00094 | Loss: 954.8036 | CE: 0.5271 | KD: 1060.3073\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.5380 | acc:80.5500\n",
      "[VAL Acc] Target: 80.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2622 | acc:49.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9261 | acc:53.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1906 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8690 | acc:51.0580\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8275 | acc:51.3863\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6653 | acc:59.7179\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 59.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8703 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7796 | acc:60.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.05%\n",
      "[VAL Acc] Avg 56.04%\n",
      "Train Epoch: 009 Batch: 00001/00094 | Loss: 954.9249 | CE: 0.4134 | KD: 1060.5684\n",
      "Train Epoch: 009 Batch: 00002/00094 | Loss: 954.6368 | CE: 0.4237 | KD: 1060.2369\n",
      "Train Epoch: 009 Batch: 00003/00094 | Loss: 954.6216 | CE: 0.3659 | KD: 1060.2842\n",
      "Train Epoch: 009 Batch: 00004/00094 | Loss: 954.8985 | CE: 0.4891 | KD: 1060.4550\n",
      "Train Epoch: 009 Batch: 00005/00094 | Loss: 955.5103 | CE: 0.6486 | KD: 1060.9574\n",
      "Train Epoch: 009 Batch: 00006/00094 | Loss: 954.7172 | CE: 0.4612 | KD: 1060.2845\n",
      "Train Epoch: 009 Batch: 00007/00094 | Loss: 954.9578 | CE: 0.5263 | KD: 1060.4795\n",
      "Train Epoch: 009 Batch: 00008/00094 | Loss: 954.5538 | CE: 0.4107 | KD: 1060.1589\n",
      "Train Epoch: 009 Batch: 00009/00094 | Loss: 955.0311 | CE: 0.5003 | KD: 1060.5898\n",
      "Train Epoch: 009 Batch: 00010/00094 | Loss: 954.8989 | CE: 0.5026 | KD: 1060.4404\n",
      "Train Epoch: 009 Batch: 00011/00094 | Loss: 955.0029 | CE: 0.3854 | KD: 1060.6860\n",
      "Train Epoch: 009 Batch: 00012/00094 | Loss: 954.9135 | CE: 0.4520 | KD: 1060.5128\n",
      "Train Epoch: 009 Batch: 00013/00094 | Loss: 955.6950 | CE: 0.4373 | KD: 1061.3975\n",
      "Train Epoch: 009 Batch: 00014/00094 | Loss: 954.8289 | CE: 0.3102 | KD: 1060.5764\n",
      "Train Epoch: 009 Batch: 00015/00094 | Loss: 954.8989 | CE: 0.4728 | KD: 1060.4735\n",
      "Train Epoch: 009 Batch: 00016/00094 | Loss: 954.8138 | CE: 0.5157 | KD: 1060.3312\n",
      "Train Epoch: 009 Batch: 00017/00094 | Loss: 954.9230 | CE: 0.4718 | KD: 1060.5013\n",
      "Train Epoch: 009 Batch: 00018/00094 | Loss: 954.7249 | CE: 0.4614 | KD: 1060.2927\n",
      "Train Epoch: 009 Batch: 00019/00094 | Loss: 954.9147 | CE: 0.3623 | KD: 1060.6138\n",
      "Train Epoch: 009 Batch: 00020/00094 | Loss: 955.0063 | CE: 0.5152 | KD: 1060.5457\n",
      "Train Epoch: 009 Batch: 00021/00094 | Loss: 954.9550 | CE: 0.4584 | KD: 1060.5518\n",
      "Train Epoch: 009 Batch: 00022/00094 | Loss: 954.7535 | CE: 0.4533 | KD: 1060.3336\n",
      "Train Epoch: 009 Batch: 00023/00094 | Loss: 954.8701 | CE: 0.2949 | KD: 1060.6392\n",
      "Train Epoch: 009 Batch: 00024/00094 | Loss: 955.5562 | CE: 0.6379 | KD: 1061.0204\n",
      "Train Epoch: 009 Batch: 00025/00094 | Loss: 954.9113 | CE: 0.3705 | KD: 1060.6010\n",
      "Train Epoch: 009 Batch: 00026/00094 | Loss: 954.9188 | CE: 0.4607 | KD: 1060.5092\n",
      "Train Epoch: 009 Batch: 00027/00094 | Loss: 955.3494 | CE: 0.5223 | KD: 1060.9191\n",
      "Train Epoch: 009 Batch: 00028/00094 | Loss: 954.8986 | CE: 0.4686 | KD: 1060.4778\n",
      "Train Epoch: 009 Batch: 00029/00094 | Loss: 955.0920 | CE: 0.3944 | KD: 1060.7751\n",
      "Train Epoch: 009 Batch: 00030/00094 | Loss: 954.9089 | CE: 0.5183 | KD: 1060.4340\n",
      "Train Epoch: 009 Batch: 00031/00094 | Loss: 955.1762 | CE: 0.4214 | KD: 1060.8386\n",
      "Train Epoch: 009 Batch: 00032/00094 | Loss: 954.9091 | CE: 0.5068 | KD: 1060.4470\n",
      "Train Epoch: 009 Batch: 00033/00094 | Loss: 955.1251 | CE: 0.4082 | KD: 1060.7966\n",
      "Train Epoch: 009 Batch: 00034/00094 | Loss: 954.8240 | CE: 0.3874 | KD: 1060.4851\n",
      "Train Epoch: 009 Batch: 00035/00094 | Loss: 954.6673 | CE: 0.3569 | KD: 1060.3450\n",
      "Train Epoch: 009 Batch: 00036/00094 | Loss: 955.1546 | CE: 0.5649 | KD: 1060.6553\n",
      "Train Epoch: 009 Batch: 00037/00094 | Loss: 954.9785 | CE: 0.4800 | KD: 1060.5540\n",
      "Train Epoch: 009 Batch: 00038/00094 | Loss: 955.2118 | CE: 0.4733 | KD: 1060.8206\n",
      "Train Epoch: 009 Batch: 00039/00094 | Loss: 954.8792 | CE: 0.4471 | KD: 1060.4801\n",
      "Train Epoch: 009 Batch: 00040/00094 | Loss: 954.9043 | CE: 0.5280 | KD: 1060.4181\n",
      "Train Epoch: 009 Batch: 00041/00094 | Loss: 954.7611 | CE: 0.4205 | KD: 1060.3785\n",
      "Train Epoch: 009 Batch: 00042/00094 | Loss: 954.8538 | CE: 0.4142 | KD: 1060.4884\n",
      "Train Epoch: 009 Batch: 00043/00094 | Loss: 954.7626 | CE: 0.4267 | KD: 1060.3733\n",
      "Train Epoch: 009 Batch: 00044/00094 | Loss: 955.4194 | CE: 0.6135 | KD: 1060.8954\n",
      "Train Epoch: 009 Batch: 00045/00094 | Loss: 955.0346 | CE: 0.5308 | KD: 1060.5598\n",
      "Train Epoch: 009 Batch: 00046/00094 | Loss: 954.6171 | CE: 0.3578 | KD: 1060.2882\n",
      "Train Epoch: 009 Batch: 00047/00094 | Loss: 955.1183 | CE: 0.5616 | KD: 1060.6185\n",
      "Train Epoch: 009 Batch: 00048/00094 | Loss: 954.8529 | CE: 0.3132 | KD: 1060.5997\n",
      "Train Epoch: 009 Batch: 00049/00094 | Loss: 954.6228 | CE: 0.4109 | KD: 1060.2355\n",
      "Train Epoch: 009 Batch: 00050/00094 | Loss: 954.5347 | CE: 0.4133 | KD: 1060.1349\n",
      "Train Epoch: 009 Batch: 00051/00094 | Loss: 955.8754 | CE: 0.4916 | KD: 1061.5375\n",
      "Train Epoch: 009 Batch: 00052/00094 | Loss: 955.1959 | CE: 0.4858 | KD: 1060.7889\n",
      "Train Epoch: 009 Batch: 00053/00094 | Loss: 954.5883 | CE: 0.4322 | KD: 1060.1735\n",
      "Train Epoch: 009 Batch: 00054/00094 | Loss: 954.9377 | CE: 0.5294 | KD: 1060.4537\n",
      "Train Epoch: 009 Batch: 00055/00094 | Loss: 955.0786 | CE: 0.5939 | KD: 1060.5386\n",
      "Train Epoch: 009 Batch: 00056/00094 | Loss: 955.0433 | CE: 0.3819 | KD: 1060.7349\n",
      "Train Epoch: 009 Batch: 00057/00094 | Loss: 954.6408 | CE: 0.3786 | KD: 1060.2914\n",
      "Train Epoch: 009 Batch: 00058/00094 | Loss: 955.1608 | CE: 0.5370 | KD: 1060.6931\n",
      "Train Epoch: 009 Batch: 00059/00094 | Loss: 954.9304 | CE: 0.4132 | KD: 1060.5747\n",
      "Train Epoch: 009 Batch: 00060/00094 | Loss: 954.6578 | CE: 0.4238 | KD: 1060.2600\n",
      "Train Epoch: 009 Batch: 00061/00094 | Loss: 955.2451 | CE: 0.5562 | KD: 1060.7654\n",
      "Train Epoch: 009 Batch: 00062/00094 | Loss: 955.2592 | CE: 0.6342 | KD: 1060.6945\n",
      "Train Epoch: 009 Batch: 00063/00094 | Loss: 954.4838 | CE: 0.3310 | KD: 1060.1698\n",
      "Train Epoch: 009 Batch: 00064/00094 | Loss: 954.7892 | CE: 0.4039 | KD: 1060.4281\n",
      "Train Epoch: 009 Batch: 00065/00094 | Loss: 954.9434 | CE: 0.4408 | KD: 1060.5585\n",
      "Train Epoch: 009 Batch: 00066/00094 | Loss: 955.1395 | CE: 0.4943 | KD: 1060.7169\n",
      "Train Epoch: 009 Batch: 00067/00094 | Loss: 955.3323 | CE: 0.4926 | KD: 1060.9331\n",
      "Train Epoch: 009 Batch: 00068/00094 | Loss: 955.1230 | CE: 0.4555 | KD: 1060.7417\n",
      "Train Epoch: 009 Batch: 00069/00094 | Loss: 954.9173 | CE: 0.4737 | KD: 1060.4929\n",
      "Train Epoch: 009 Batch: 00070/00094 | Loss: 954.6328 | CE: 0.4399 | KD: 1060.2144\n",
      "Train Epoch: 009 Batch: 00071/00094 | Loss: 954.8546 | CE: 0.4117 | KD: 1060.4922\n",
      "Train Epoch: 009 Batch: 00072/00094 | Loss: 955.2112 | CE: 0.4816 | KD: 1060.8108\n",
      "Train Epoch: 009 Batch: 00073/00094 | Loss: 955.3467 | CE: 0.7475 | KD: 1060.6659\n",
      "Train Epoch: 009 Batch: 00074/00094 | Loss: 954.8719 | CE: 0.4648 | KD: 1060.4524\n",
      "Train Epoch: 009 Batch: 00075/00094 | Loss: 954.8325 | CE: 0.4547 | KD: 1060.4198\n",
      "Train Epoch: 009 Batch: 00076/00094 | Loss: 955.3951 | CE: 0.6483 | KD: 1060.8298\n",
      "Train Epoch: 009 Batch: 00077/00094 | Loss: 954.7904 | CE: 0.4601 | KD: 1060.3671\n",
      "Train Epoch: 009 Batch: 00078/00094 | Loss: 954.9871 | CE: 0.5136 | KD: 1060.5261\n",
      "Train Epoch: 009 Batch: 00079/00094 | Loss: 954.9583 | CE: 0.4264 | KD: 1060.5909\n",
      "Train Epoch: 009 Batch: 00080/00094 | Loss: 955.1603 | CE: 0.5009 | KD: 1060.7327\n",
      "Train Epoch: 009 Batch: 00081/00094 | Loss: 954.9604 | CE: 0.5230 | KD: 1060.4861\n",
      "Train Epoch: 009 Batch: 00082/00094 | Loss: 954.8521 | CE: 0.3696 | KD: 1060.5361\n",
      "Train Epoch: 009 Batch: 00083/00094 | Loss: 955.0901 | CE: 0.5856 | KD: 1060.5605\n",
      "Train Epoch: 009 Batch: 00084/00094 | Loss: 954.9754 | CE: 0.4139 | KD: 1060.6239\n",
      "Train Epoch: 009 Batch: 00085/00094 | Loss: 954.6180 | CE: 0.4635 | KD: 1060.1718\n",
      "Train Epoch: 009 Batch: 00086/00094 | Loss: 954.4230 | CE: 0.3348 | KD: 1060.0980\n",
      "Train Epoch: 009 Batch: 00087/00094 | Loss: 954.8133 | CE: 0.4988 | KD: 1060.3495\n",
      "Train Epoch: 009 Batch: 00088/00094 | Loss: 954.6427 | CE: 0.3386 | KD: 1060.3379\n",
      "Train Epoch: 009 Batch: 00089/00094 | Loss: 955.3201 | CE: 0.6097 | KD: 1060.7893\n",
      "Train Epoch: 009 Batch: 00090/00094 | Loss: 954.6462 | CE: 0.4003 | KD: 1060.2733\n",
      "Train Epoch: 009 Batch: 00091/00094 | Loss: 954.7627 | CE: 0.4439 | KD: 1060.3542\n",
      "Train Epoch: 009 Batch: 00092/00094 | Loss: 954.8085 | CE: 0.3992 | KD: 1060.4547\n",
      "Train Epoch: 009 Batch: 00093/00094 | Loss: 955.6481 | CE: 0.5070 | KD: 1061.2679\n",
      "Train Epoch: 009 Batch: 00094/00094 | Loss: 954.7978 | CE: 0.4543 | KD: 1060.3817\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.5432 | acc:80.9000\n",
      "[VAL Acc] Target: 80.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2264 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8963 | acc:55.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1978 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8433 | acc:51.0972\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8309 | acc:52.0333\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6475 | acc:60.8934\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 60.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8777 | acc:48.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7169 | acc:63.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 63.05%\n",
      "[VAL Acc] Avg 56.69%\n",
      "Train Epoch: 010 Batch: 00001/00094 | Loss: 859.2443 | CE: 0.4705 | KD: 1060.2145\n",
      "Train Epoch: 010 Batch: 00002/00094 | Loss: 859.7446 | CE: 0.5730 | KD: 1060.7057\n",
      "Train Epoch: 010 Batch: 00003/00094 | Loss: 859.6851 | CE: 0.4960 | KD: 1060.7273\n",
      "Train Epoch: 010 Batch: 00004/00094 | Loss: 859.6690 | CE: 0.5228 | KD: 1060.6743\n",
      "Train Epoch: 010 Batch: 00005/00094 | Loss: 859.7357 | CE: 0.5394 | KD: 1060.7361\n",
      "Train Epoch: 010 Batch: 00006/00094 | Loss: 859.5719 | CE: 0.5279 | KD: 1060.5481\n",
      "Train Epoch: 010 Batch: 00007/00094 | Loss: 859.7341 | CE: 0.4190 | KD: 1060.8828\n",
      "Train Epoch: 010 Batch: 00008/00094 | Loss: 859.6481 | CE: 0.5543 | KD: 1060.6097\n",
      "Train Epoch: 010 Batch: 00009/00094 | Loss: 859.8795 | CE: 0.4865 | KD: 1060.9790\n",
      "Train Epoch: 010 Batch: 00010/00094 | Loss: 859.3770 | CE: 0.3617 | KD: 1060.5127\n",
      "Train Epoch: 010 Batch: 00011/00094 | Loss: 859.2322 | CE: 0.3917 | KD: 1060.2969\n",
      "Train Epoch: 010 Batch: 00012/00094 | Loss: 859.4975 | CE: 0.4936 | KD: 1060.4985\n",
      "Train Epoch: 010 Batch: 00013/00094 | Loss: 859.2538 | CE: 0.4080 | KD: 1060.3036\n",
      "Train Epoch: 010 Batch: 00014/00094 | Loss: 859.4924 | CE: 0.4654 | KD: 1060.5271\n",
      "Train Epoch: 010 Batch: 00015/00094 | Loss: 859.7575 | CE: 0.5629 | KD: 1060.7341\n",
      "Train Epoch: 010 Batch: 00016/00094 | Loss: 860.2479 | CE: 0.6000 | KD: 1061.2937\n",
      "Train Epoch: 010 Batch: 00017/00094 | Loss: 859.3880 | CE: 0.6122 | KD: 1060.2170\n",
      "Train Epoch: 010 Batch: 00018/00094 | Loss: 859.7894 | CE: 0.6283 | KD: 1060.6927\n",
      "Train Epoch: 010 Batch: 00019/00094 | Loss: 859.6002 | CE: 0.5254 | KD: 1060.5862\n",
      "Train Epoch: 010 Batch: 00020/00094 | Loss: 859.5881 | CE: 0.3823 | KD: 1060.7478\n",
      "Train Epoch: 010 Batch: 00021/00094 | Loss: 859.1770 | CE: 0.4609 | KD: 1060.1433\n",
      "Train Epoch: 010 Batch: 00022/00094 | Loss: 859.3915 | CE: 0.4547 | KD: 1060.4158\n",
      "Train Epoch: 010 Batch: 00023/00094 | Loss: 859.5389 | CE: 0.6160 | KD: 1060.3986\n",
      "Train Epoch: 010 Batch: 00024/00094 | Loss: 859.8201 | CE: 0.5346 | KD: 1060.8463\n",
      "Train Epoch: 010 Batch: 00025/00094 | Loss: 859.6359 | CE: 0.5942 | KD: 1060.5453\n",
      "Train Epoch: 010 Batch: 00026/00094 | Loss: 859.7719 | CE: 0.5310 | KD: 1060.7913\n",
      "Train Epoch: 010 Batch: 00027/00094 | Loss: 859.4071 | CE: 0.3861 | KD: 1060.5198\n",
      "Train Epoch: 010 Batch: 00028/00094 | Loss: 859.8868 | CE: 0.5044 | KD: 1060.9658\n",
      "Train Epoch: 010 Batch: 00029/00094 | Loss: 859.2075 | CE: 0.3114 | KD: 1060.3656\n",
      "Train Epoch: 010 Batch: 00030/00094 | Loss: 859.5201 | CE: 0.5392 | KD: 1060.4703\n",
      "Train Epoch: 010 Batch: 00031/00094 | Loss: 859.4756 | CE: 0.5281 | KD: 1060.4291\n",
      "Train Epoch: 010 Batch: 00032/00094 | Loss: 859.6425 | CE: 0.5782 | KD: 1060.5731\n",
      "Train Epoch: 010 Batch: 00033/00094 | Loss: 859.4081 | CE: 0.5315 | KD: 1060.3416\n",
      "Train Epoch: 010 Batch: 00034/00094 | Loss: 859.4131 | CE: 0.5398 | KD: 1060.3373\n",
      "Train Epoch: 010 Batch: 00035/00094 | Loss: 859.0640 | CE: 0.3172 | KD: 1060.1813\n",
      "Train Epoch: 010 Batch: 00036/00094 | Loss: 859.0894 | CE: 0.3631 | KD: 1060.1559\n",
      "Train Epoch: 010 Batch: 00037/00094 | Loss: 859.5042 | CE: 0.5536 | KD: 1060.4327\n",
      "Train Epoch: 010 Batch: 00038/00094 | Loss: 859.5590 | CE: 0.6694 | KD: 1060.3574\n",
      "Train Epoch: 010 Batch: 00039/00094 | Loss: 859.4667 | CE: 0.3452 | KD: 1060.6438\n",
      "Train Epoch: 010 Batch: 00040/00094 | Loss: 859.4941 | CE: 0.4321 | KD: 1060.5703\n",
      "Train Epoch: 010 Batch: 00041/00094 | Loss: 859.3471 | CE: 0.3885 | KD: 1060.4427\n",
      "Train Epoch: 010 Batch: 00042/00094 | Loss: 859.9015 | CE: 0.4130 | KD: 1061.0969\n",
      "Train Epoch: 010 Batch: 00043/00094 | Loss: 859.2969 | CE: 0.4820 | KD: 1060.2654\n",
      "Train Epoch: 010 Batch: 00044/00094 | Loss: 859.2578 | CE: 0.4707 | KD: 1060.2310\n",
      "Train Epoch: 010 Batch: 00045/00094 | Loss: 859.1523 | CE: 0.3963 | KD: 1060.1926\n",
      "Train Epoch: 010 Batch: 00046/00094 | Loss: 859.4739 | CE: 0.4291 | KD: 1060.5492\n",
      "Train Epoch: 010 Batch: 00047/00094 | Loss: 859.6556 | CE: 0.4929 | KD: 1060.6947\n",
      "Train Epoch: 010 Batch: 00048/00094 | Loss: 859.3939 | CE: 0.3718 | KD: 1060.5211\n",
      "Train Epoch: 010 Batch: 00049/00094 | Loss: 859.0969 | CE: 0.4224 | KD: 1060.0920\n",
      "Train Epoch: 010 Batch: 00050/00094 | Loss: 859.6324 | CE: 0.4680 | KD: 1060.6967\n",
      "Train Epoch: 010 Batch: 00051/00094 | Loss: 859.2195 | CE: 0.3700 | KD: 1060.3081\n",
      "Train Epoch: 010 Batch: 00052/00094 | Loss: 860.1703 | CE: 0.7238 | KD: 1061.0450\n",
      "Train Epoch: 010 Batch: 00053/00094 | Loss: 859.3157 | CE: 0.4387 | KD: 1060.3420\n",
      "Train Epoch: 010 Batch: 00054/00094 | Loss: 859.8306 | CE: 0.6483 | KD: 1060.7189\n",
      "Train Epoch: 010 Batch: 00055/00094 | Loss: 859.8170 | CE: 0.4562 | KD: 1060.9393\n",
      "Train Epoch: 010 Batch: 00056/00094 | Loss: 859.6556 | CE: 0.6557 | KD: 1060.4938\n",
      "Train Epoch: 010 Batch: 00057/00094 | Loss: 859.3333 | CE: 0.3866 | KD: 1060.4281\n",
      "Train Epoch: 010 Batch: 00058/00094 | Loss: 859.5541 | CE: 0.4649 | KD: 1060.6040\n",
      "Train Epoch: 010 Batch: 00059/00094 | Loss: 859.1274 | CE: 0.4028 | KD: 1060.1538\n",
      "Train Epoch: 010 Batch: 00060/00094 | Loss: 859.6109 | CE: 0.5022 | KD: 1060.6281\n",
      "Train Epoch: 010 Batch: 00061/00094 | Loss: 859.3495 | CE: 0.4008 | KD: 1060.4305\n",
      "Train Epoch: 010 Batch: 00062/00094 | Loss: 859.6123 | CE: 0.4826 | KD: 1060.6541\n",
      "Train Epoch: 010 Batch: 00063/00094 | Loss: 859.2336 | CE: 0.3369 | KD: 1060.3663\n",
      "Train Epoch: 010 Batch: 00064/00094 | Loss: 859.4185 | CE: 0.4607 | KD: 1060.4417\n",
      "Train Epoch: 010 Batch: 00065/00094 | Loss: 859.5969 | CE: 0.5867 | KD: 1060.5063\n",
      "Train Epoch: 010 Batch: 00066/00094 | Loss: 860.1462 | CE: 0.6546 | KD: 1061.1007\n",
      "Train Epoch: 010 Batch: 00067/00094 | Loss: 859.7806 | CE: 0.5727 | KD: 1060.7505\n",
      "Train Epoch: 010 Batch: 00068/00094 | Loss: 859.8764 | CE: 0.5765 | KD: 1060.8641\n",
      "Train Epoch: 010 Batch: 00069/00094 | Loss: 859.4173 | CE: 0.4879 | KD: 1060.4066\n",
      "Train Epoch: 010 Batch: 00070/00094 | Loss: 859.2446 | CE: 0.3947 | KD: 1060.3085\n",
      "Train Epoch: 010 Batch: 00071/00094 | Loss: 859.4557 | CE: 0.4155 | KD: 1060.5435\n",
      "Train Epoch: 010 Batch: 00072/00094 | Loss: 859.3438 | CE: 0.5474 | KD: 1060.2424\n",
      "Train Epoch: 010 Batch: 00073/00094 | Loss: 859.2373 | CE: 0.4169 | KD: 1060.2721\n",
      "Train Epoch: 010 Batch: 00074/00094 | Loss: 859.2064 | CE: 0.3991 | KD: 1060.2559\n",
      "Train Epoch: 010 Batch: 00075/00094 | Loss: 859.5577 | CE: 0.4240 | KD: 1060.6588\n",
      "Train Epoch: 010 Batch: 00076/00094 | Loss: 859.6408 | CE: 0.5223 | KD: 1060.6401\n",
      "Train Epoch: 010 Batch: 00077/00094 | Loss: 859.5611 | CE: 0.5441 | KD: 1060.5148\n",
      "Train Epoch: 010 Batch: 00078/00094 | Loss: 859.5452 | CE: 0.4417 | KD: 1060.6216\n",
      "Train Epoch: 010 Batch: 00079/00094 | Loss: 859.2652 | CE: 0.4452 | KD: 1060.2716\n",
      "Train Epoch: 010 Batch: 00080/00094 | Loss: 859.6399 | CE: 0.5386 | KD: 1060.6189\n",
      "Train Epoch: 010 Batch: 00081/00094 | Loss: 859.3028 | CE: 0.3942 | KD: 1060.3810\n",
      "Train Epoch: 010 Batch: 00082/00094 | Loss: 859.3996 | CE: 0.3935 | KD: 1060.5013\n",
      "Train Epoch: 010 Batch: 00083/00094 | Loss: 859.3685 | CE: 0.4233 | KD: 1060.4263\n",
      "Train Epoch: 010 Batch: 00084/00094 | Loss: 859.5648 | CE: 0.4085 | KD: 1060.6866\n",
      "Train Epoch: 010 Batch: 00085/00094 | Loss: 859.6230 | CE: 0.4673 | KD: 1060.6860\n",
      "Train Epoch: 010 Batch: 00086/00094 | Loss: 859.4449 | CE: 0.5506 | KD: 1060.3634\n",
      "Train Epoch: 010 Batch: 00087/00094 | Loss: 859.1299 | CE: 0.3135 | KD: 1060.2671\n",
      "Train Epoch: 010 Batch: 00088/00094 | Loss: 859.5613 | CE: 0.6155 | KD: 1060.4269\n",
      "Train Epoch: 010 Batch: 00089/00094 | Loss: 859.3681 | CE: 0.3593 | KD: 1060.5048\n",
      "Train Epoch: 010 Batch: 00090/00094 | Loss: 859.4362 | CE: 0.4442 | KD: 1060.4840\n",
      "Train Epoch: 010 Batch: 00091/00094 | Loss: 859.1831 | CE: 0.3487 | KD: 1060.2894\n",
      "Train Epoch: 010 Batch: 00092/00094 | Loss: 859.8039 | CE: 0.4337 | KD: 1060.9509\n",
      "Train Epoch: 010 Batch: 00093/00094 | Loss: 859.3527 | CE: 0.4076 | KD: 1060.4260\n",
      "Train Epoch: 010 Batch: 00094/00094 | Loss: 859.1700 | CE: 0.3312 | KD: 1060.2948\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4924 | acc:82.0000\n",
      "[VAL Acc] Target: 82.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2256 | acc:49.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8866 | acc:52.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1374 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8610 | acc:50.5094\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8312 | acc:52.0333\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6439 | acc:61.4420\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8669 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7131 | acc:62.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 62.05%\n",
      "[VAL Acc] Avg 56.63%\n",
      "Train Epoch: 011 Batch: 00001/00094 | Loss: 859.4391 | CE: 0.4874 | KD: 1060.4342\n",
      "Train Epoch: 011 Batch: 00002/00094 | Loss: 860.0537 | CE: 0.6394 | KD: 1061.0052\n",
      "Train Epoch: 011 Batch: 00003/00094 | Loss: 859.5882 | CE: 0.5050 | KD: 1060.5966\n",
      "Train Epoch: 011 Batch: 00004/00094 | Loss: 859.1486 | CE: 0.3820 | KD: 1060.2057\n",
      "Train Epoch: 011 Batch: 00005/00094 | Loss: 859.5671 | CE: 0.5312 | KD: 1060.5382\n",
      "Train Epoch: 011 Batch: 00006/00094 | Loss: 859.4536 | CE: 0.4128 | KD: 1060.5442\n",
      "Train Epoch: 011 Batch: 00007/00094 | Loss: 859.7298 | CE: 0.5157 | KD: 1060.7581\n",
      "Train Epoch: 011 Batch: 00008/00094 | Loss: 859.5523 | CE: 0.4904 | KD: 1060.5703\n",
      "Train Epoch: 011 Batch: 00009/00094 | Loss: 859.6599 | CE: 0.3826 | KD: 1060.8361\n",
      "Train Epoch: 011 Batch: 00010/00094 | Loss: 859.6113 | CE: 0.4618 | KD: 1060.6783\n",
      "Train Epoch: 011 Batch: 00011/00094 | Loss: 859.2711 | CE: 0.3932 | KD: 1060.3430\n",
      "Train Epoch: 011 Batch: 00012/00094 | Loss: 859.6710 | CE: 0.5346 | KD: 1060.6622\n",
      "Train Epoch: 011 Batch: 00013/00094 | Loss: 859.2010 | CE: 0.4147 | KD: 1060.2301\n",
      "Train Epoch: 011 Batch: 00014/00094 | Loss: 859.9623 | CE: 0.5047 | KD: 1061.0587\n",
      "Train Epoch: 011 Batch: 00015/00094 | Loss: 859.6140 | CE: 0.5978 | KD: 1060.5138\n",
      "Train Epoch: 011 Batch: 00016/00094 | Loss: 859.9256 | CE: 0.6573 | KD: 1060.8251\n",
      "Train Epoch: 011 Batch: 00017/00094 | Loss: 859.2844 | CE: 0.3661 | KD: 1060.3929\n",
      "Train Epoch: 011 Batch: 00018/00094 | Loss: 859.6268 | CE: 0.5129 | KD: 1060.6344\n",
      "Train Epoch: 011 Batch: 00019/00094 | Loss: 859.5439 | CE: 0.5410 | KD: 1060.4974\n",
      "Train Epoch: 011 Batch: 00020/00094 | Loss: 859.7674 | CE: 0.3753 | KD: 1060.9778\n",
      "Train Epoch: 011 Batch: 00021/00094 | Loss: 859.6238 | CE: 0.5165 | KD: 1060.6263\n",
      "Train Epoch: 011 Batch: 00022/00094 | Loss: 859.5338 | CE: 0.5327 | KD: 1060.4951\n",
      "Train Epoch: 011 Batch: 00023/00094 | Loss: 859.6029 | CE: 0.4469 | KD: 1060.6864\n",
      "Train Epoch: 011 Batch: 00024/00094 | Loss: 859.2554 | CE: 0.4303 | KD: 1060.2780\n",
      "Train Epoch: 011 Batch: 00025/00094 | Loss: 859.4194 | CE: 0.3560 | KD: 1060.5720\n",
      "Train Epoch: 011 Batch: 00026/00094 | Loss: 859.6694 | CE: 0.5311 | KD: 1060.6646\n",
      "Train Epoch: 011 Batch: 00027/00094 | Loss: 859.3298 | CE: 0.4665 | KD: 1060.3251\n",
      "Train Epoch: 011 Batch: 00028/00094 | Loss: 859.4965 | CE: 0.4937 | KD: 1060.4972\n",
      "Train Epoch: 011 Batch: 00029/00094 | Loss: 859.5307 | CE: 0.5446 | KD: 1060.4767\n",
      "Train Epoch: 011 Batch: 00030/00094 | Loss: 859.6780 | CE: 0.5298 | KD: 1060.6768\n",
      "Train Epoch: 011 Batch: 00031/00094 | Loss: 859.9975 | CE: 0.3783 | KD: 1061.2583\n",
      "Train Epoch: 011 Batch: 00032/00094 | Loss: 859.0029 | CE: 0.3175 | KD: 1060.1055\n",
      "Train Epoch: 011 Batch: 00033/00094 | Loss: 859.4448 | CE: 0.5240 | KD: 1060.3960\n",
      "Train Epoch: 011 Batch: 00034/00094 | Loss: 859.2819 | CE: 0.4290 | KD: 1060.3121\n",
      "Train Epoch: 011 Batch: 00035/00094 | Loss: 858.9911 | CE: 0.2899 | KD: 1060.1249\n",
      "Train Epoch: 011 Batch: 00036/00094 | Loss: 859.4357 | CE: 0.4304 | KD: 1060.5005\n",
      "Train Epoch: 011 Batch: 00037/00094 | Loss: 859.3634 | CE: 0.3421 | KD: 1060.5201\n",
      "Train Epoch: 011 Batch: 00038/00094 | Loss: 859.3459 | CE: 0.4643 | KD: 1060.3478\n",
      "Train Epoch: 011 Batch: 00039/00094 | Loss: 859.8494 | CE: 0.4799 | KD: 1060.9500\n",
      "Train Epoch: 011 Batch: 00040/00094 | Loss: 859.5634 | CE: 0.5517 | KD: 1060.5083\n",
      "Train Epoch: 011 Batch: 00041/00094 | Loss: 859.2874 | CE: 0.4433 | KD: 1060.3014\n",
      "Train Epoch: 011 Batch: 00042/00094 | Loss: 859.6203 | CE: 0.4043 | KD: 1060.7605\n",
      "Train Epoch: 011 Batch: 00043/00094 | Loss: 859.6671 | CE: 0.5134 | KD: 1060.6835\n",
      "Train Epoch: 011 Batch: 00044/00094 | Loss: 859.3478 | CE: 0.4033 | KD: 1060.4253\n",
      "Train Epoch: 011 Batch: 00045/00094 | Loss: 859.8900 | CE: 0.6617 | KD: 1060.7756\n",
      "Train Epoch: 011 Batch: 00046/00094 | Loss: 859.6771 | CE: 0.6908 | KD: 1060.4771\n",
      "Train Epoch: 011 Batch: 00047/00094 | Loss: 859.3958 | CE: 0.4866 | KD: 1060.3817\n",
      "Train Epoch: 011 Batch: 00048/00094 | Loss: 859.6445 | CE: 0.4200 | KD: 1060.7710\n",
      "Train Epoch: 011 Batch: 00049/00094 | Loss: 859.4138 | CE: 0.3782 | KD: 1060.5377\n",
      "Train Epoch: 011 Batch: 00050/00094 | Loss: 859.3812 | CE: 0.4265 | KD: 1060.4379\n",
      "Train Epoch: 011 Batch: 00051/00094 | Loss: 859.2178 | CE: 0.3912 | KD: 1060.2798\n",
      "Train Epoch: 011 Batch: 00052/00094 | Loss: 859.1776 | CE: 0.3887 | KD: 1060.2333\n",
      "Train Epoch: 011 Batch: 00053/00094 | Loss: 859.6940 | CE: 0.6058 | KD: 1060.6027\n",
      "Train Epoch: 011 Batch: 00054/00094 | Loss: 859.1693 | CE: 0.3908 | KD: 1060.2203\n",
      "Train Epoch: 011 Batch: 00055/00094 | Loss: 859.8943 | CE: 0.5715 | KD: 1060.8925\n",
      "Train Epoch: 011 Batch: 00056/00094 | Loss: 859.4523 | CE: 0.5301 | KD: 1060.3978\n",
      "Train Epoch: 011 Batch: 00057/00094 | Loss: 859.5874 | CE: 0.3849 | KD: 1060.7438\n",
      "Train Epoch: 011 Batch: 00058/00094 | Loss: 859.3304 | CE: 0.4043 | KD: 1060.4026\n",
      "Train Epoch: 011 Batch: 00059/00094 | Loss: 859.2712 | CE: 0.4319 | KD: 1060.2954\n",
      "Train Epoch: 011 Batch: 00060/00094 | Loss: 859.5448 | CE: 0.6114 | KD: 1060.4116\n",
      "Train Epoch: 011 Batch: 00061/00094 | Loss: 859.3723 | CE: 0.4698 | KD: 1060.3734\n",
      "Train Epoch: 011 Batch: 00062/00094 | Loss: 859.1504 | CE: 0.3782 | KD: 1060.2125\n",
      "Train Epoch: 011 Batch: 00063/00094 | Loss: 859.5372 | CE: 0.5761 | KD: 1060.4458\n",
      "Train Epoch: 011 Batch: 00064/00094 | Loss: 859.6422 | CE: 0.5559 | KD: 1060.6005\n",
      "Train Epoch: 011 Batch: 00065/00094 | Loss: 859.2823 | CE: 0.3646 | KD: 1060.3922\n",
      "Train Epoch: 011 Batch: 00066/00094 | Loss: 859.9537 | CE: 0.6806 | KD: 1060.8311\n",
      "Train Epoch: 011 Batch: 00067/00094 | Loss: 859.2711 | CE: 0.4423 | KD: 1060.2825\n",
      "Train Epoch: 011 Batch: 00068/00094 | Loss: 859.3868 | CE: 0.4216 | KD: 1060.4508\n",
      "Train Epoch: 011 Batch: 00069/00094 | Loss: 859.3842 | CE: 0.3774 | KD: 1060.5022\n",
      "Train Epoch: 011 Batch: 00070/00094 | Loss: 859.2325 | CE: 0.4341 | KD: 1060.2450\n",
      "Train Epoch: 011 Batch: 00071/00094 | Loss: 859.7307 | CE: 0.4239 | KD: 1060.8726\n",
      "Train Epoch: 011 Batch: 00072/00094 | Loss: 859.3740 | CE: 0.4979 | KD: 1060.3408\n",
      "Train Epoch: 011 Batch: 00073/00094 | Loss: 859.2914 | CE: 0.3087 | KD: 1060.4725\n",
      "Train Epoch: 011 Batch: 00074/00094 | Loss: 859.3370 | CE: 0.4335 | KD: 1060.3746\n",
      "Train Epoch: 011 Batch: 00075/00094 | Loss: 859.6570 | CE: 0.5208 | KD: 1060.6621\n",
      "Train Epoch: 011 Batch: 00076/00094 | Loss: 859.5531 | CE: 0.6043 | KD: 1060.4307\n",
      "Train Epoch: 011 Batch: 00077/00094 | Loss: 859.5530 | CE: 0.4460 | KD: 1060.6259\n",
      "Train Epoch: 011 Batch: 00078/00094 | Loss: 859.6588 | CE: 0.5637 | KD: 1060.6112\n",
      "Train Epoch: 011 Batch: 00079/00094 | Loss: 859.4678 | CE: 0.3725 | KD: 1060.6115\n",
      "Train Epoch: 011 Batch: 00080/00094 | Loss: 860.1204 | CE: 0.6770 | KD: 1061.0413\n",
      "Train Epoch: 011 Batch: 00081/00094 | Loss: 859.3632 | CE: 0.4697 | KD: 1060.3623\n",
      "Train Epoch: 011 Batch: 00082/00094 | Loss: 859.1842 | CE: 0.3933 | KD: 1060.2357\n",
      "Train Epoch: 011 Batch: 00083/00094 | Loss: 859.3389 | CE: 0.3864 | KD: 1060.4352\n",
      "Train Epoch: 011 Batch: 00084/00094 | Loss: 859.2480 | CE: 0.2861 | KD: 1060.4468\n",
      "Train Epoch: 011 Batch: 00085/00094 | Loss: 859.3417 | CE: 0.3634 | KD: 1060.4669\n",
      "Train Epoch: 011 Batch: 00086/00094 | Loss: 859.7137 | CE: 0.6715 | KD: 1060.5459\n",
      "Train Epoch: 011 Batch: 00087/00094 | Loss: 859.3128 | CE: 0.3754 | KD: 1060.4165\n",
      "Train Epoch: 011 Batch: 00088/00094 | Loss: 860.0077 | CE: 0.5970 | KD: 1061.0009\n",
      "Train Epoch: 011 Batch: 00089/00094 | Loss: 859.8382 | CE: 0.4292 | KD: 1060.9988\n",
      "Train Epoch: 011 Batch: 00090/00094 | Loss: 859.1304 | CE: 0.4234 | KD: 1060.1321\n",
      "Train Epoch: 011 Batch: 00091/00094 | Loss: 859.2734 | CE: 0.4281 | KD: 1060.3029\n",
      "Train Epoch: 011 Batch: 00092/00094 | Loss: 859.4653 | CE: 0.4346 | KD: 1060.5317\n",
      "Train Epoch: 011 Batch: 00093/00094 | Loss: 859.5318 | CE: 0.5121 | KD: 1060.5181\n",
      "Train Epoch: 011 Batch: 00094/00094 | Loss: 859.3911 | CE: 0.2999 | KD: 1060.6064\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4835 | acc:80.4000\n",
      "[VAL Acc] Target: 80.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2565 | acc:49.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8881 | acc:53.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1540 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8478 | acc:51.2931\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.29%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8275 | acc:51.5712\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6296 | acc:62.1082\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8711 | acc:49.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7168 | acc:62.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 62.05%\n",
      "[VAL Acc] Avg 56.41%\n",
      "Train Epoch: 012 Batch: 00001/00094 | Loss: 859.7606 | CE: 0.5184 | KD: 1060.7928\n",
      "Train Epoch: 012 Batch: 00002/00094 | Loss: 859.2484 | CE: 0.4137 | KD: 1060.2897\n",
      "Train Epoch: 012 Batch: 00003/00094 | Loss: 859.3921 | CE: 0.4565 | KD: 1060.4143\n",
      "Train Epoch: 012 Batch: 00004/00094 | Loss: 859.1712 | CE: 0.3961 | KD: 1060.2162\n",
      "Train Epoch: 012 Batch: 00005/00094 | Loss: 859.3847 | CE: 0.5000 | KD: 1060.3514\n",
      "Train Epoch: 012 Batch: 00006/00094 | Loss: 859.2798 | CE: 0.4016 | KD: 1060.3435\n",
      "Train Epoch: 012 Batch: 00007/00094 | Loss: 859.6303 | CE: 0.4582 | KD: 1060.7063\n",
      "Train Epoch: 012 Batch: 00008/00094 | Loss: 859.4631 | CE: 0.4352 | KD: 1060.5282\n",
      "Train Epoch: 012 Batch: 00009/00094 | Loss: 859.4708 | CE: 0.4848 | KD: 1060.4766\n",
      "Train Epoch: 012 Batch: 00010/00094 | Loss: 859.5298 | CE: 0.5859 | KD: 1060.4247\n",
      "Train Epoch: 012 Batch: 00011/00094 | Loss: 859.4158 | CE: 0.5104 | KD: 1060.3771\n",
      "Train Epoch: 012 Batch: 00012/00094 | Loss: 859.3765 | CE: 0.4834 | KD: 1060.3618\n",
      "Train Epoch: 012 Batch: 00013/00094 | Loss: 859.5753 | CE: 0.4315 | KD: 1060.6713\n",
      "Train Epoch: 012 Batch: 00014/00094 | Loss: 859.4528 | CE: 0.5040 | KD: 1060.4305\n",
      "Train Epoch: 012 Batch: 00015/00094 | Loss: 859.3623 | CE: 0.4886 | KD: 1060.3380\n",
      "Train Epoch: 012 Batch: 00016/00094 | Loss: 859.4590 | CE: 0.4919 | KD: 1060.4532\n",
      "Train Epoch: 012 Batch: 00017/00094 | Loss: 859.3647 | CE: 0.4416 | KD: 1060.3989\n",
      "Train Epoch: 012 Batch: 00018/00094 | Loss: 859.5370 | CE: 0.3781 | KD: 1060.6901\n",
      "Train Epoch: 012 Batch: 00019/00094 | Loss: 859.5165 | CE: 0.4609 | KD: 1060.5625\n",
      "Train Epoch: 012 Batch: 00020/00094 | Loss: 859.4413 | CE: 0.4625 | KD: 1060.4677\n",
      "Train Epoch: 012 Batch: 00021/00094 | Loss: 859.4683 | CE: 0.5188 | KD: 1060.4315\n",
      "Train Epoch: 012 Batch: 00022/00094 | Loss: 859.2179 | CE: 0.3564 | KD: 1060.3228\n",
      "Train Epoch: 012 Batch: 00023/00094 | Loss: 859.2141 | CE: 0.3463 | KD: 1060.3307\n",
      "Train Epoch: 012 Batch: 00024/00094 | Loss: 859.8240 | CE: 0.5985 | KD: 1060.7722\n",
      "Train Epoch: 012 Batch: 00025/00094 | Loss: 859.4829 | CE: 0.5313 | KD: 1060.4341\n",
      "Train Epoch: 012 Batch: 00026/00094 | Loss: 859.2277 | CE: 0.3827 | KD: 1060.3024\n",
      "Train Epoch: 012 Batch: 00027/00094 | Loss: 859.4938 | CE: 0.5161 | KD: 1060.4663\n",
      "Train Epoch: 012 Batch: 00028/00094 | Loss: 859.4041 | CE: 0.3350 | KD: 1060.5791\n",
      "Train Epoch: 012 Batch: 00029/00094 | Loss: 859.5036 | CE: 0.5269 | KD: 1060.4651\n",
      "Train Epoch: 012 Batch: 00030/00094 | Loss: 859.4094 | CE: 0.3739 | KD: 1060.5377\n",
      "Train Epoch: 012 Batch: 00031/00094 | Loss: 859.2098 | CE: 0.3595 | KD: 1060.3091\n",
      "Train Epoch: 012 Batch: 00032/00094 | Loss: 859.6453 | CE: 0.4891 | KD: 1060.6866\n",
      "Train Epoch: 012 Batch: 00033/00094 | Loss: 859.3984 | CE: 0.3623 | KD: 1060.5383\n",
      "Train Epoch: 012 Batch: 00034/00094 | Loss: 859.4232 | CE: 0.5280 | KD: 1060.3644\n",
      "Train Epoch: 012 Batch: 00035/00094 | Loss: 859.7370 | CE: 0.3822 | KD: 1060.9319\n",
      "Train Epoch: 012 Batch: 00036/00094 | Loss: 859.5519 | CE: 0.4221 | KD: 1060.6542\n",
      "Train Epoch: 012 Batch: 00037/00094 | Loss: 859.5796 | CE: 0.5211 | KD: 1060.5660\n",
      "Train Epoch: 012 Batch: 00038/00094 | Loss: 860.0759 | CE: 0.4614 | KD: 1061.2524\n",
      "Train Epoch: 012 Batch: 00039/00094 | Loss: 859.3636 | CE: 0.4293 | KD: 1060.4127\n",
      "Train Epoch: 012 Batch: 00040/00094 | Loss: 860.1386 | CE: 0.5634 | KD: 1061.2040\n",
      "Train Epoch: 012 Batch: 00041/00094 | Loss: 859.5419 | CE: 0.4074 | KD: 1060.6598\n",
      "Train Epoch: 012 Batch: 00042/00094 | Loss: 859.4486 | CE: 0.4531 | KD: 1060.4883\n",
      "Train Epoch: 012 Batch: 00043/00094 | Loss: 859.4623 | CE: 0.4827 | KD: 1060.4686\n",
      "Train Epoch: 012 Batch: 00044/00094 | Loss: 859.4567 | CE: 0.4911 | KD: 1060.4513\n",
      "Train Epoch: 012 Batch: 00045/00094 | Loss: 859.3159 | CE: 0.3953 | KD: 1060.3959\n",
      "Train Epoch: 012 Batch: 00046/00094 | Loss: 859.9518 | CE: 0.4919 | KD: 1061.0615\n",
      "Train Epoch: 012 Batch: 00047/00094 | Loss: 859.2415 | CE: 0.3760 | KD: 1060.3276\n",
      "Train Epoch: 012 Batch: 00048/00094 | Loss: 859.6248 | CE: 0.4443 | KD: 1060.7166\n",
      "Train Epoch: 012 Batch: 00049/00094 | Loss: 859.5888 | CE: 0.5894 | KD: 1060.4932\n",
      "Train Epoch: 012 Batch: 00050/00094 | Loss: 859.2727 | CE: 0.4109 | KD: 1060.3232\n",
      "Train Epoch: 012 Batch: 00051/00094 | Loss: 859.3273 | CE: 0.3680 | KD: 1060.4435\n",
      "Train Epoch: 012 Batch: 00052/00094 | Loss: 859.8536 | CE: 0.6288 | KD: 1060.7712\n",
      "Train Epoch: 012 Batch: 00053/00094 | Loss: 859.4880 | CE: 0.5053 | KD: 1060.4724\n",
      "Train Epoch: 012 Batch: 00054/00094 | Loss: 859.5519 | CE: 0.5015 | KD: 1060.5562\n",
      "Train Epoch: 012 Batch: 00055/00094 | Loss: 859.4243 | CE: 0.4430 | KD: 1060.4706\n",
      "Train Epoch: 012 Batch: 00056/00094 | Loss: 859.5748 | CE: 0.3779 | KD: 1060.7369\n",
      "Train Epoch: 012 Batch: 00057/00094 | Loss: 859.6392 | CE: 0.3955 | KD: 1060.7947\n",
      "Train Epoch: 012 Batch: 00058/00094 | Loss: 859.4928 | CE: 0.4913 | KD: 1060.4957\n",
      "Train Epoch: 012 Batch: 00059/00094 | Loss: 859.5916 | CE: 0.4515 | KD: 1060.6667\n",
      "Train Epoch: 012 Batch: 00060/00094 | Loss: 859.7230 | CE: 0.5151 | KD: 1060.7505\n",
      "Train Epoch: 012 Batch: 00061/00094 | Loss: 859.1293 | CE: 0.3253 | KD: 1060.2518\n",
      "Train Epoch: 012 Batch: 00062/00094 | Loss: 859.3927 | CE: 0.4583 | KD: 1060.4128\n",
      "Train Epoch: 012 Batch: 00063/00094 | Loss: 859.2396 | CE: 0.4600 | KD: 1060.2218\n",
      "Train Epoch: 012 Batch: 00064/00094 | Loss: 859.3087 | CE: 0.3996 | KD: 1060.3817\n",
      "Train Epoch: 012 Batch: 00065/00094 | Loss: 859.6935 | CE: 0.4602 | KD: 1060.7819\n",
      "Train Epoch: 012 Batch: 00066/00094 | Loss: 859.3919 | CE: 0.4251 | KD: 1060.4529\n",
      "Train Epoch: 012 Batch: 00067/00094 | Loss: 859.2220 | CE: 0.4030 | KD: 1060.2705\n",
      "Train Epoch: 012 Batch: 00068/00094 | Loss: 859.2661 | CE: 0.4004 | KD: 1060.3281\n",
      "Train Epoch: 012 Batch: 00069/00094 | Loss: 859.7294 | CE: 0.5848 | KD: 1060.6722\n",
      "Train Epoch: 012 Batch: 00070/00094 | Loss: 859.5980 | CE: 0.5733 | KD: 1060.5243\n",
      "Train Epoch: 012 Batch: 00071/00094 | Loss: 859.5866 | CE: 0.5066 | KD: 1060.5927\n",
      "Train Epoch: 012 Batch: 00072/00094 | Loss: 859.1601 | CE: 0.4157 | KD: 1060.1783\n",
      "Train Epoch: 012 Batch: 00073/00094 | Loss: 859.5240 | CE: 0.4712 | KD: 1060.5591\n",
      "Train Epoch: 012 Batch: 00074/00094 | Loss: 859.8175 | CE: 0.5269 | KD: 1060.8525\n",
      "Train Epoch: 012 Batch: 00075/00094 | Loss: 859.5516 | CE: 0.4495 | KD: 1060.6200\n",
      "Train Epoch: 012 Batch: 00076/00094 | Loss: 859.3756 | CE: 0.4042 | KD: 1060.4585\n",
      "Train Epoch: 012 Batch: 00077/00094 | Loss: 859.4187 | CE: 0.4680 | KD: 1060.4330\n",
      "Train Epoch: 012 Batch: 00078/00094 | Loss: 859.2827 | CE: 0.4436 | KD: 1060.2952\n",
      "Train Epoch: 012 Batch: 00079/00094 | Loss: 859.0804 | CE: 0.4085 | KD: 1060.0889\n",
      "Train Epoch: 012 Batch: 00080/00094 | Loss: 859.4205 | CE: 0.5030 | KD: 1060.3920\n",
      "Train Epoch: 012 Batch: 00081/00094 | Loss: 859.6346 | CE: 0.4878 | KD: 1060.6752\n",
      "Train Epoch: 012 Batch: 00082/00094 | Loss: 859.3177 | CE: 0.3982 | KD: 1060.3944\n",
      "Train Epoch: 012 Batch: 00083/00094 | Loss: 859.8140 | CE: 0.4575 | KD: 1060.9340\n",
      "Train Epoch: 012 Batch: 00084/00094 | Loss: 859.4310 | CE: 0.5206 | KD: 1060.3833\n",
      "Train Epoch: 012 Batch: 00085/00094 | Loss: 859.6939 | CE: 0.6067 | KD: 1060.6014\n",
      "Train Epoch: 012 Batch: 00086/00094 | Loss: 859.5906 | CE: 0.5559 | KD: 1060.5366\n",
      "Train Epoch: 012 Batch: 00087/00094 | Loss: 859.7838 | CE: 0.5748 | KD: 1060.7518\n",
      "Train Epoch: 012 Batch: 00088/00094 | Loss: 859.9359 | CE: 0.4190 | KD: 1061.1321\n",
      "Train Epoch: 012 Batch: 00089/00094 | Loss: 859.3527 | CE: 0.4061 | KD: 1060.4279\n",
      "Train Epoch: 012 Batch: 00090/00094 | Loss: 859.1625 | CE: 0.3502 | KD: 1060.2621\n",
      "Train Epoch: 012 Batch: 00091/00094 | Loss: 859.3062 | CE: 0.4758 | KD: 1060.2844\n",
      "Train Epoch: 012 Batch: 00092/00094 | Loss: 859.4437 | CE: 0.4544 | KD: 1060.4806\n",
      "Train Epoch: 012 Batch: 00093/00094 | Loss: 859.6548 | CE: 0.4581 | KD: 1060.7366\n",
      "Train Epoch: 012 Batch: 00094/00094 | Loss: 859.1201 | CE: 0.3218 | KD: 1060.2448\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4706 | acc:81.9000\n",
      "[VAL Acc] Target: 81.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2209 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8934 | acc:53.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2117 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8320 | acc:51.4107\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8535 | acc:50.6470\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 50.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6306 | acc:62.3824\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8831 | acc:48.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7068 | acc:61.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.60%\n",
      "[VAL Acc] Avg 56.25%\n",
      "Train Epoch: 013 Batch: 00001/00094 | Loss: 859.3951 | CE: 0.3673 | KD: 1060.5282\n",
      "Train Epoch: 013 Batch: 00002/00094 | Loss: 859.2358 | CE: 0.4669 | KD: 1060.2086\n",
      "Train Epoch: 013 Batch: 00003/00094 | Loss: 859.1971 | CE: 0.4149 | KD: 1060.2250\n",
      "Train Epoch: 013 Batch: 00004/00094 | Loss: 859.0819 | CE: 0.3555 | KD: 1060.1560\n",
      "Train Epoch: 013 Batch: 00005/00094 | Loss: 859.6136 | CE: 0.5144 | KD: 1060.6163\n",
      "Train Epoch: 013 Batch: 00006/00094 | Loss: 859.8034 | CE: 0.8010 | KD: 1060.4968\n",
      "Train Epoch: 013 Batch: 00007/00094 | Loss: 859.4422 | CE: 0.5217 | KD: 1060.3956\n",
      "Train Epoch: 013 Batch: 00008/00094 | Loss: 859.2563 | CE: 0.4125 | KD: 1060.3010\n",
      "Train Epoch: 013 Batch: 00009/00094 | Loss: 859.6603 | CE: 0.4551 | KD: 1060.7472\n",
      "Train Epoch: 013 Batch: 00010/00094 | Loss: 859.6466 | CE: 0.5253 | KD: 1060.6436\n",
      "Train Epoch: 013 Batch: 00011/00094 | Loss: 859.4113 | CE: 0.4923 | KD: 1060.3939\n",
      "Train Epoch: 013 Batch: 00012/00094 | Loss: 859.4445 | CE: 0.4662 | KD: 1060.4670\n",
      "Train Epoch: 013 Batch: 00013/00094 | Loss: 859.6974 | CE: 0.4657 | KD: 1060.7798\n",
      "Train Epoch: 013 Batch: 00014/00094 | Loss: 859.1451 | CE: 0.3250 | KD: 1060.2717\n",
      "Train Epoch: 013 Batch: 00015/00094 | Loss: 859.9050 | CE: 0.4424 | KD: 1061.0649\n",
      "Train Epoch: 013 Batch: 00016/00094 | Loss: 859.6538 | CE: 0.5450 | KD: 1060.6282\n",
      "Train Epoch: 013 Batch: 00017/00094 | Loss: 859.4798 | CE: 0.4586 | KD: 1060.5200\n",
      "Train Epoch: 013 Batch: 00018/00094 | Loss: 859.7870 | CE: 0.4735 | KD: 1060.8809\n",
      "Train Epoch: 013 Batch: 00019/00094 | Loss: 859.1962 | CE: 0.3550 | KD: 1060.2977\n",
      "Train Epoch: 013 Batch: 00020/00094 | Loss: 859.1979 | CE: 0.3863 | KD: 1060.2612\n",
      "Train Epoch: 013 Batch: 00021/00094 | Loss: 859.7083 | CE: 0.4304 | KD: 1060.8368\n",
      "Train Epoch: 013 Batch: 00022/00094 | Loss: 859.2679 | CE: 0.3833 | KD: 1060.3514\n",
      "Train Epoch: 013 Batch: 00023/00094 | Loss: 859.6602 | CE: 0.4065 | KD: 1060.8070\n",
      "Train Epoch: 013 Batch: 00024/00094 | Loss: 859.5791 | CE: 0.4218 | KD: 1060.6881\n",
      "Train Epoch: 013 Batch: 00025/00094 | Loss: 860.0154 | CE: 0.5226 | KD: 1061.1023\n",
      "Train Epoch: 013 Batch: 00026/00094 | Loss: 860.0638 | CE: 0.7349 | KD: 1060.8999\n",
      "Train Epoch: 013 Batch: 00027/00094 | Loss: 859.3817 | CE: 0.3783 | KD: 1060.4980\n",
      "Train Epoch: 013 Batch: 00028/00094 | Loss: 859.4149 | CE: 0.4775 | KD: 1060.4165\n",
      "Train Epoch: 013 Batch: 00029/00094 | Loss: 859.3580 | CE: 0.4686 | KD: 1060.3573\n",
      "Train Epoch: 013 Batch: 00030/00094 | Loss: 859.4635 | CE: 0.3518 | KD: 1060.6317\n",
      "Train Epoch: 013 Batch: 00031/00094 | Loss: 860.0257 | CE: 0.6311 | KD: 1060.9811\n",
      "Train Epoch: 013 Batch: 00032/00094 | Loss: 859.6559 | CE: 0.6160 | KD: 1060.5431\n",
      "Train Epoch: 013 Batch: 00033/00094 | Loss: 859.5281 | CE: 0.4692 | KD: 1060.5667\n",
      "Train Epoch: 013 Batch: 00034/00094 | Loss: 859.4694 | CE: 0.4373 | KD: 1060.5334\n",
      "Train Epoch: 013 Batch: 00035/00094 | Loss: 859.3657 | CE: 0.4226 | KD: 1060.4236\n",
      "Train Epoch: 013 Batch: 00036/00094 | Loss: 859.3592 | CE: 0.4400 | KD: 1060.3940\n",
      "Train Epoch: 013 Batch: 00037/00094 | Loss: 859.1068 | CE: 0.3319 | KD: 1060.2158\n",
      "Train Epoch: 013 Batch: 00038/00094 | Loss: 859.4736 | CE: 0.3930 | KD: 1060.5933\n",
      "Train Epoch: 013 Batch: 00039/00094 | Loss: 859.4412 | CE: 0.5397 | KD: 1060.3722\n",
      "Train Epoch: 013 Batch: 00040/00094 | Loss: 859.4537 | CE: 0.5877 | KD: 1060.3282\n",
      "Train Epoch: 013 Batch: 00041/00094 | Loss: 859.3023 | CE: 0.4538 | KD: 1060.3068\n",
      "Train Epoch: 013 Batch: 00042/00094 | Loss: 859.2680 | CE: 0.3566 | KD: 1060.3844\n",
      "Train Epoch: 013 Batch: 00043/00094 | Loss: 859.1411 | CE: 0.3766 | KD: 1060.2030\n",
      "Train Epoch: 013 Batch: 00044/00094 | Loss: 859.5845 | CE: 0.4362 | KD: 1060.6769\n",
      "Train Epoch: 013 Batch: 00045/00094 | Loss: 859.9449 | CE: 0.4349 | KD: 1061.1234\n",
      "Train Epoch: 013 Batch: 00046/00094 | Loss: 859.3481 | CE: 0.4120 | KD: 1060.4149\n",
      "Train Epoch: 013 Batch: 00047/00094 | Loss: 859.1672 | CE: 0.4338 | KD: 1060.1647\n",
      "Train Epoch: 013 Batch: 00048/00094 | Loss: 859.8348 | CE: 0.5865 | KD: 1060.8003\n",
      "Train Epoch: 013 Batch: 00049/00094 | Loss: 859.7269 | CE: 0.4525 | KD: 1060.8326\n",
      "Train Epoch: 013 Batch: 00050/00094 | Loss: 859.5507 | CE: 0.4104 | KD: 1060.6671\n",
      "Train Epoch: 013 Batch: 00051/00094 | Loss: 859.2891 | CE: 0.3325 | KD: 1060.4403\n",
      "Train Epoch: 013 Batch: 00052/00094 | Loss: 859.3801 | CE: 0.3485 | KD: 1060.5328\n",
      "Train Epoch: 013 Batch: 00053/00094 | Loss: 859.2958 | CE: 0.3655 | KD: 1060.4078\n",
      "Train Epoch: 013 Batch: 00054/00094 | Loss: 859.3358 | CE: 0.5291 | KD: 1060.2551\n",
      "Train Epoch: 013 Batch: 00055/00094 | Loss: 859.2989 | CE: 0.4278 | KD: 1060.3347\n",
      "Train Epoch: 013 Batch: 00056/00094 | Loss: 859.7033 | CE: 0.5824 | KD: 1060.6431\n",
      "Train Epoch: 013 Batch: 00057/00094 | Loss: 859.4977 | CE: 0.5737 | KD: 1060.3999\n",
      "Train Epoch: 013 Batch: 00058/00094 | Loss: 859.3573 | CE: 0.2941 | KD: 1060.5719\n",
      "Train Epoch: 013 Batch: 00059/00094 | Loss: 859.6503 | CE: 0.4314 | KD: 1060.7640\n",
      "Train Epoch: 013 Batch: 00060/00094 | Loss: 859.4186 | CE: 0.5970 | KD: 1060.2737\n",
      "Train Epoch: 013 Batch: 00061/00094 | Loss: 859.4348 | CE: 0.4041 | KD: 1060.5316\n",
      "Train Epoch: 013 Batch: 00062/00094 | Loss: 859.3780 | CE: 0.5669 | KD: 1060.2605\n",
      "Train Epoch: 013 Batch: 00063/00094 | Loss: 859.2127 | CE: 0.3697 | KD: 1060.2999\n",
      "Train Epoch: 013 Batch: 00064/00094 | Loss: 859.4877 | CE: 0.4144 | KD: 1060.5844\n",
      "Train Epoch: 013 Batch: 00065/00094 | Loss: 859.8654 | CE: 0.5119 | KD: 1060.9303\n",
      "Train Epoch: 013 Batch: 00066/00094 | Loss: 859.7379 | CE: 0.4200 | KD: 1060.8862\n",
      "Train Epoch: 013 Batch: 00067/00094 | Loss: 859.5352 | CE: 0.4162 | KD: 1060.6407\n",
      "Train Epoch: 013 Batch: 00068/00094 | Loss: 859.3796 | CE: 0.5351 | KD: 1060.3020\n",
      "Train Epoch: 013 Batch: 00069/00094 | Loss: 859.3796 | CE: 0.4536 | KD: 1060.4025\n",
      "Train Epoch: 013 Batch: 00070/00094 | Loss: 859.4153 | CE: 0.3768 | KD: 1060.5413\n",
      "Train Epoch: 013 Batch: 00071/00094 | Loss: 859.4302 | CE: 0.5372 | KD: 1060.3618\n",
      "Train Epoch: 013 Batch: 00072/00094 | Loss: 859.3657 | CE: 0.3174 | KD: 1060.5535\n",
      "Train Epoch: 013 Batch: 00073/00094 | Loss: 859.1810 | CE: 0.3709 | KD: 1060.2594\n",
      "Train Epoch: 013 Batch: 00074/00094 | Loss: 859.1505 | CE: 0.3989 | KD: 1060.1871\n",
      "Train Epoch: 013 Batch: 00075/00094 | Loss: 859.5936 | CE: 0.5356 | KD: 1060.5654\n",
      "Train Epoch: 013 Batch: 00076/00094 | Loss: 859.5173 | CE: 0.3829 | KD: 1060.6598\n",
      "Train Epoch: 013 Batch: 00077/00094 | Loss: 859.3548 | CE: 0.4671 | KD: 1060.3551\n",
      "Train Epoch: 013 Batch: 00078/00094 | Loss: 859.3118 | CE: 0.3846 | KD: 1060.4039\n",
      "Train Epoch: 013 Batch: 00079/00094 | Loss: 859.2253 | CE: 0.3308 | KD: 1060.3636\n",
      "Train Epoch: 013 Batch: 00080/00094 | Loss: 859.3274 | CE: 0.3629 | KD: 1060.4500\n",
      "Train Epoch: 013 Batch: 00081/00094 | Loss: 859.5895 | CE: 0.4294 | KD: 1060.6915\n",
      "Train Epoch: 013 Batch: 00082/00094 | Loss: 860.2396 | CE: 0.4873 | KD: 1061.4226\n",
      "Train Epoch: 013 Batch: 00083/00094 | Loss: 859.7108 | CE: 0.6313 | KD: 1060.5919\n",
      "Train Epoch: 013 Batch: 00084/00094 | Loss: 859.3987 | CE: 0.4001 | KD: 1060.4921\n",
      "Train Epoch: 013 Batch: 00085/00094 | Loss: 859.5812 | CE: 0.5149 | KD: 1060.5758\n",
      "Train Epoch: 013 Batch: 00086/00094 | Loss: 859.1202 | CE: 0.3486 | KD: 1060.2119\n",
      "Train Epoch: 013 Batch: 00087/00094 | Loss: 859.2818 | CE: 0.3538 | KD: 1060.4049\n",
      "Train Epoch: 013 Batch: 00088/00094 | Loss: 859.6984 | CE: 0.5273 | KD: 1060.7051\n",
      "Train Epoch: 013 Batch: 00089/00094 | Loss: 859.3765 | CE: 0.4400 | KD: 1060.4153\n",
      "Train Epoch: 013 Batch: 00090/00094 | Loss: 859.1529 | CE: 0.3812 | KD: 1060.2119\n",
      "Train Epoch: 013 Batch: 00091/00094 | Loss: 859.3604 | CE: 0.4487 | KD: 1060.3848\n",
      "Train Epoch: 013 Batch: 00092/00094 | Loss: 859.4421 | CE: 0.4854 | KD: 1060.4404\n",
      "Train Epoch: 013 Batch: 00093/00094 | Loss: 859.5273 | CE: 0.3586 | KD: 1060.7021\n",
      "Train Epoch: 013 Batch: 00094/00094 | Loss: 859.3058 | CE: 0.3499 | KD: 1060.4393\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4913 | acc:81.2000\n",
      "[VAL Acc] Target: 81.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2430 | acc:49.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8805 | acc:53.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1577 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8335 | acc:51.8417\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8130 | acc:51.9409\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6283 | acc:63.1270\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8743 | acc:48.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7126 | acc:61.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.00%\n",
      "[VAL Acc] Avg 56.60%\n",
      "Train Epoch: 014 Batch: 00001/00094 | Loss: 773.5546 | CE: 0.3671 | KD: 1060.6138\n",
      "Train Epoch: 014 Batch: 00002/00094 | Loss: 773.6221 | CE: 0.3734 | KD: 1060.6979\n",
      "Train Epoch: 014 Batch: 00003/00094 | Loss: 773.3517 | CE: 0.3540 | KD: 1060.3536\n",
      "Train Epoch: 014 Batch: 00004/00094 | Loss: 773.4236 | CE: 0.3568 | KD: 1060.4482\n",
      "Train Epoch: 014 Batch: 00005/00094 | Loss: 773.4355 | CE: 0.4157 | KD: 1060.3839\n",
      "Train Epoch: 014 Batch: 00006/00094 | Loss: 773.5585 | CE: 0.5383 | KD: 1060.3844\n",
      "Train Epoch: 014 Batch: 00007/00094 | Loss: 773.5807 | CE: 0.3475 | KD: 1060.6766\n",
      "Train Epoch: 014 Batch: 00008/00094 | Loss: 773.4557 | CE: 0.3986 | KD: 1060.4351\n",
      "Train Epoch: 014 Batch: 00009/00094 | Loss: 773.8821 | CE: 0.6779 | KD: 1060.6368\n",
      "Train Epoch: 014 Batch: 00010/00094 | Loss: 773.4340 | CE: 0.3930 | KD: 1060.4130\n",
      "Train Epoch: 014 Batch: 00011/00094 | Loss: 773.6083 | CE: 0.5910 | KD: 1060.3805\n",
      "Train Epoch: 014 Batch: 00012/00094 | Loss: 773.4134 | CE: 0.4041 | KD: 1060.3695\n",
      "Train Epoch: 014 Batch: 00013/00094 | Loss: 773.5928 | CE: 0.4862 | KD: 1060.5031\n",
      "Train Epoch: 014 Batch: 00014/00094 | Loss: 773.2454 | CE: 0.3315 | KD: 1060.2385\n",
      "Train Epoch: 014 Batch: 00015/00094 | Loss: 773.4616 | CE: 0.4246 | KD: 1060.4075\n",
      "Train Epoch: 014 Batch: 00016/00094 | Loss: 773.4692 | CE: 0.3773 | KD: 1060.4828\n",
      "Train Epoch: 014 Batch: 00017/00094 | Loss: 773.3842 | CE: 0.5088 | KD: 1060.1858\n",
      "Train Epoch: 014 Batch: 00018/00094 | Loss: 773.6736 | CE: 0.5947 | KD: 1060.4648\n",
      "Train Epoch: 014 Batch: 00019/00094 | Loss: 773.4592 | CE: 0.5023 | KD: 1060.2976\n",
      "Train Epoch: 014 Batch: 00020/00094 | Loss: 773.6411 | CE: 0.3294 | KD: 1060.7843\n",
      "Train Epoch: 014 Batch: 00021/00094 | Loss: 773.5048 | CE: 0.4211 | KD: 1060.4716\n",
      "Train Epoch: 014 Batch: 00022/00094 | Loss: 773.9341 | CE: 0.4810 | KD: 1060.9783\n",
      "Train Epoch: 014 Batch: 00023/00094 | Loss: 773.3018 | CE: 0.2872 | KD: 1060.3766\n",
      "Train Epoch: 014 Batch: 00024/00094 | Loss: 773.3111 | CE: 0.3559 | KD: 1060.2952\n",
      "Train Epoch: 014 Batch: 00025/00094 | Loss: 773.7474 | CE: 0.4150 | KD: 1060.8127\n",
      "Train Epoch: 014 Batch: 00026/00094 | Loss: 773.6987 | CE: 0.5494 | KD: 1060.5615\n",
      "Train Epoch: 014 Batch: 00027/00094 | Loss: 773.3187 | CE: 0.3595 | KD: 1060.3008\n",
      "Train Epoch: 014 Batch: 00028/00094 | Loss: 773.6794 | CE: 0.4527 | KD: 1060.6677\n",
      "Train Epoch: 014 Batch: 00029/00094 | Loss: 773.5428 | CE: 0.3946 | KD: 1060.5601\n",
      "Train Epoch: 014 Batch: 00030/00094 | Loss: 774.0657 | CE: 0.7651 | KD: 1060.7690\n",
      "Train Epoch: 014 Batch: 00031/00094 | Loss: 773.4847 | CE: 0.3916 | KD: 1060.4844\n",
      "Train Epoch: 014 Batch: 00032/00094 | Loss: 774.0897 | CE: 0.7244 | KD: 1060.8578\n",
      "Train Epoch: 014 Batch: 00033/00094 | Loss: 773.6215 | CE: 0.5233 | KD: 1060.4913\n",
      "Train Epoch: 014 Batch: 00034/00094 | Loss: 773.6996 | CE: 0.4815 | KD: 1060.6559\n",
      "Train Epoch: 014 Batch: 00035/00094 | Loss: 773.5885 | CE: 0.4704 | KD: 1060.5187\n",
      "Train Epoch: 014 Batch: 00036/00094 | Loss: 773.9042 | CE: 0.6353 | KD: 1060.7257\n",
      "Train Epoch: 014 Batch: 00037/00094 | Loss: 773.4396 | CE: 0.3449 | KD: 1060.4866\n",
      "Train Epoch: 014 Batch: 00038/00094 | Loss: 773.7198 | CE: 0.5821 | KD: 1060.5457\n",
      "Train Epoch: 014 Batch: 00039/00094 | Loss: 773.6028 | CE: 0.3854 | KD: 1060.6549\n",
      "Train Epoch: 014 Batch: 00040/00094 | Loss: 773.9578 | CE: 0.3992 | KD: 1061.1229\n",
      "Train Epoch: 014 Batch: 00041/00094 | Loss: 773.2642 | CE: 0.3545 | KD: 1060.2327\n",
      "Train Epoch: 014 Batch: 00042/00094 | Loss: 773.2263 | CE: 0.3691 | KD: 1060.1608\n",
      "Train Epoch: 014 Batch: 00043/00094 | Loss: 773.3181 | CE: 0.3840 | KD: 1060.2664\n",
      "Train Epoch: 014 Batch: 00044/00094 | Loss: 773.4459 | CE: 0.4343 | KD: 1060.3727\n",
      "Train Epoch: 014 Batch: 00045/00094 | Loss: 774.2296 | CE: 0.7244 | KD: 1061.0497\n",
      "Train Epoch: 014 Batch: 00046/00094 | Loss: 773.3735 | CE: 0.4337 | KD: 1060.2740\n",
      "Train Epoch: 014 Batch: 00047/00094 | Loss: 773.4016 | CE: 0.3990 | KD: 1060.3602\n",
      "Train Epoch: 014 Batch: 00048/00094 | Loss: 773.6734 | CE: 0.5508 | KD: 1060.5249\n",
      "Train Epoch: 014 Batch: 00049/00094 | Loss: 773.5529 | CE: 0.3632 | KD: 1060.6169\n",
      "Train Epoch: 014 Batch: 00050/00094 | Loss: 774.2519 | CE: 0.6299 | KD: 1061.2100\n",
      "Train Epoch: 014 Batch: 00051/00094 | Loss: 773.6359 | CE: 0.6155 | KD: 1060.3846\n",
      "Train Epoch: 014 Batch: 00052/00094 | Loss: 773.9999 | CE: 0.4504 | KD: 1061.1105\n",
      "Train Epoch: 014 Batch: 00053/00094 | Loss: 773.7463 | CE: 0.5363 | KD: 1060.6448\n",
      "Train Epoch: 014 Batch: 00054/00094 | Loss: 773.8824 | CE: 0.6405 | KD: 1060.6885\n",
      "Train Epoch: 014 Batch: 00055/00094 | Loss: 773.5590 | CE: 0.4369 | KD: 1060.5242\n",
      "Train Epoch: 014 Batch: 00056/00094 | Loss: 773.3382 | CE: 0.3758 | KD: 1060.3051\n",
      "Train Epoch: 014 Batch: 00057/00094 | Loss: 773.3508 | CE: 0.3367 | KD: 1060.3761\n",
      "Train Epoch: 014 Batch: 00058/00094 | Loss: 773.4839 | CE: 0.3473 | KD: 1060.5442\n",
      "Train Epoch: 014 Batch: 00059/00094 | Loss: 773.4634 | CE: 0.3910 | KD: 1060.4561\n",
      "Train Epoch: 014 Batch: 00060/00094 | Loss: 773.5085 | CE: 0.4958 | KD: 1060.3741\n",
      "Train Epoch: 014 Batch: 00061/00094 | Loss: 773.6926 | CE: 0.3735 | KD: 1060.7944\n",
      "Train Epoch: 014 Batch: 00062/00094 | Loss: 773.7303 | CE: 0.5513 | KD: 1060.6023\n",
      "Train Epoch: 014 Batch: 00063/00094 | Loss: 773.2748 | CE: 0.3533 | KD: 1060.2491\n",
      "Train Epoch: 014 Batch: 00064/00094 | Loss: 773.2921 | CE: 0.3374 | KD: 1060.2946\n",
      "Train Epoch: 014 Batch: 00065/00094 | Loss: 773.5629 | CE: 0.5386 | KD: 1060.3901\n",
      "Train Epoch: 014 Batch: 00066/00094 | Loss: 773.4135 | CE: 0.3851 | KD: 1060.3956\n",
      "Train Epoch: 014 Batch: 00067/00094 | Loss: 773.5774 | CE: 0.4082 | KD: 1060.5889\n",
      "Train Epoch: 014 Batch: 00068/00094 | Loss: 773.5346 | CE: 0.3652 | KD: 1060.5891\n",
      "Train Epoch: 014 Batch: 00069/00094 | Loss: 773.8516 | CE: 0.4217 | KD: 1060.9464\n",
      "Train Epoch: 014 Batch: 00070/00094 | Loss: 773.4333 | CE: 0.4751 | KD: 1060.2993\n",
      "Train Epoch: 014 Batch: 00071/00094 | Loss: 773.5942 | CE: 0.4536 | KD: 1060.5496\n",
      "Train Epoch: 014 Batch: 00072/00094 | Loss: 773.5573 | CE: 0.4314 | KD: 1060.5294\n",
      "Train Epoch: 014 Batch: 00073/00094 | Loss: 773.7107 | CE: 0.4479 | KD: 1060.7172\n",
      "Train Epoch: 014 Batch: 00074/00094 | Loss: 773.3609 | CE: 0.3403 | KD: 1060.3850\n",
      "Train Epoch: 014 Batch: 00075/00094 | Loss: 773.8074 | CE: 0.4347 | KD: 1060.8679\n",
      "Train Epoch: 014 Batch: 00076/00094 | Loss: 773.7903 | CE: 0.5221 | KD: 1060.7245\n",
      "Train Epoch: 014 Batch: 00077/00094 | Loss: 774.1151 | CE: 0.3934 | KD: 1061.3466\n",
      "Train Epoch: 014 Batch: 00078/00094 | Loss: 773.8402 | CE: 0.5919 | KD: 1060.6973\n",
      "Train Epoch: 014 Batch: 00079/00094 | Loss: 773.7015 | CE: 0.3906 | KD: 1060.7832\n",
      "Train Epoch: 014 Batch: 00080/00094 | Loss: 773.2399 | CE: 0.3454 | KD: 1060.2120\n",
      "Train Epoch: 014 Batch: 00081/00094 | Loss: 773.6920 | CE: 0.5867 | KD: 1060.5011\n",
      "Train Epoch: 014 Batch: 00082/00094 | Loss: 773.7865 | CE: 0.5500 | KD: 1060.6812\n",
      "Train Epoch: 014 Batch: 00083/00094 | Loss: 773.5510 | CE: 0.5492 | KD: 1060.3591\n",
      "Train Epoch: 014 Batch: 00084/00094 | Loss: 773.4416 | CE: 0.3619 | KD: 1060.4661\n",
      "Train Epoch: 014 Batch: 00085/00094 | Loss: 773.9099 | CE: 0.5192 | KD: 1060.8926\n",
      "Train Epoch: 014 Batch: 00086/00094 | Loss: 773.2720 | CE: 0.3855 | KD: 1060.2009\n",
      "Train Epoch: 014 Batch: 00087/00094 | Loss: 773.5807 | CE: 0.5417 | KD: 1060.4102\n",
      "Train Epoch: 014 Batch: 00088/00094 | Loss: 773.6202 | CE: 0.3876 | KD: 1060.6758\n",
      "Train Epoch: 014 Batch: 00089/00094 | Loss: 773.3438 | CE: 0.3184 | KD: 1060.3916\n",
      "Train Epoch: 014 Batch: 00090/00094 | Loss: 773.4360 | CE: 0.4172 | KD: 1060.3824\n",
      "Train Epoch: 014 Batch: 00091/00094 | Loss: 773.8347 | CE: 0.4610 | KD: 1060.8694\n",
      "Train Epoch: 014 Batch: 00092/00094 | Loss: 773.9203 | CE: 0.4964 | KD: 1060.9384\n",
      "Train Epoch: 014 Batch: 00093/00094 | Loss: 773.5468 | CE: 0.3878 | KD: 1060.5748\n",
      "Train Epoch: 014 Batch: 00094/00094 | Loss: 773.6028 | CE: 0.4338 | KD: 1060.5885\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4744 | acc:82.0500\n",
      "[VAL Acc] Target: 82.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2320 | acc:49.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9002 | acc:54.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1840 | acc:49.0458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8042 | acc:51.5674\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8474 | acc:51.7560\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6317 | acc:62.2649\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.26%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8856 | acc:49.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7319 | acc:60.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.50%\n",
      "[VAL Acc] Avg 56.67%\n",
      "Train Epoch: 015 Batch: 00001/00094 | Loss: 773.9525 | CE: 0.6275 | KD: 1060.8025\n",
      "Train Epoch: 015 Batch: 00002/00094 | Loss: 773.4060 | CE: 0.4630 | KD: 1060.2786\n",
      "Train Epoch: 015 Batch: 00003/00094 | Loss: 773.6658 | CE: 0.4917 | KD: 1060.5955\n",
      "Train Epoch: 015 Batch: 00004/00094 | Loss: 773.3001 | CE: 0.3731 | KD: 1060.2566\n",
      "Train Epoch: 015 Batch: 00005/00094 | Loss: 773.7207 | CE: 0.5635 | KD: 1060.5724\n",
      "Train Epoch: 015 Batch: 00006/00094 | Loss: 773.4083 | CE: 0.3915 | KD: 1060.3798\n",
      "Train Epoch: 015 Batch: 00007/00094 | Loss: 774.2790 | CE: 0.6537 | KD: 1061.2145\n",
      "Train Epoch: 015 Batch: 00008/00094 | Loss: 773.3650 | CE: 0.3368 | KD: 1060.3954\n",
      "Train Epoch: 015 Batch: 00009/00094 | Loss: 773.5232 | CE: 0.3545 | KD: 1060.5881\n",
      "Train Epoch: 015 Batch: 00010/00094 | Loss: 773.5439 | CE: 0.4691 | KD: 1060.4594\n",
      "Train Epoch: 015 Batch: 00011/00094 | Loss: 773.3431 | CE: 0.4422 | KD: 1060.2207\n",
      "Train Epoch: 015 Batch: 00012/00094 | Loss: 773.6204 | CE: 0.4344 | KD: 1060.6117\n",
      "Train Epoch: 015 Batch: 00013/00094 | Loss: 773.2889 | CE: 0.4011 | KD: 1060.2029\n",
      "Train Epoch: 015 Batch: 00014/00094 | Loss: 773.4080 | CE: 0.3872 | KD: 1060.3853\n",
      "Train Epoch: 015 Batch: 00015/00094 | Loss: 773.6948 | CE: 0.4758 | KD: 1060.6570\n",
      "Train Epoch: 015 Batch: 00016/00094 | Loss: 773.4583 | CE: 0.4697 | KD: 1060.3411\n",
      "Train Epoch: 015 Batch: 00017/00094 | Loss: 773.6772 | CE: 0.5811 | KD: 1060.4886\n",
      "Train Epoch: 015 Batch: 00018/00094 | Loss: 773.6520 | CE: 0.4461 | KD: 1060.6390\n",
      "Train Epoch: 015 Batch: 00019/00094 | Loss: 773.3103 | CE: 0.3523 | KD: 1060.2991\n",
      "Train Epoch: 015 Batch: 00020/00094 | Loss: 773.5494 | CE: 0.3545 | KD: 1060.6240\n",
      "Train Epoch: 015 Batch: 00021/00094 | Loss: 773.6077 | CE: 0.4995 | KD: 1060.5051\n",
      "Train Epoch: 015 Batch: 00022/00094 | Loss: 773.6302 | CE: 0.4405 | KD: 1060.6169\n",
      "Train Epoch: 015 Batch: 00023/00094 | Loss: 773.3190 | CE: 0.3378 | KD: 1060.3308\n",
      "Train Epoch: 015 Batch: 00024/00094 | Loss: 773.6177 | CE: 0.4190 | KD: 1060.6293\n",
      "Train Epoch: 015 Batch: 00025/00094 | Loss: 773.6471 | CE: 0.3583 | KD: 1060.7528\n",
      "Train Epoch: 015 Batch: 00026/00094 | Loss: 773.6284 | CE: 0.4140 | KD: 1060.6508\n",
      "Train Epoch: 015 Batch: 00027/00094 | Loss: 773.2832 | CE: 0.3602 | KD: 1060.2511\n",
      "Train Epoch: 015 Batch: 00028/00094 | Loss: 773.8568 | CE: 0.3419 | KD: 1061.0630\n",
      "Train Epoch: 015 Batch: 00029/00094 | Loss: 773.4299 | CE: 0.4649 | KD: 1060.3087\n",
      "Train Epoch: 015 Batch: 00030/00094 | Loss: 773.8843 | CE: 0.6819 | KD: 1060.6344\n",
      "Train Epoch: 015 Batch: 00031/00094 | Loss: 773.4586 | CE: 0.3879 | KD: 1060.4537\n",
      "Train Epoch: 015 Batch: 00032/00094 | Loss: 773.5577 | CE: 0.3714 | KD: 1060.6123\n",
      "Train Epoch: 015 Batch: 00033/00094 | Loss: 773.3012 | CE: 0.3950 | KD: 1060.2280\n",
      "Train Epoch: 015 Batch: 00034/00094 | Loss: 774.1292 | CE: 0.5129 | KD: 1061.2021\n",
      "Train Epoch: 015 Batch: 00035/00094 | Loss: 773.7485 | CE: 0.4737 | KD: 1060.7336\n",
      "Train Epoch: 015 Batch: 00036/00094 | Loss: 773.9681 | CE: 0.4666 | KD: 1061.0446\n",
      "Train Epoch: 015 Batch: 00037/00094 | Loss: 773.6309 | CE: 0.3229 | KD: 1060.7792\n",
      "Train Epoch: 015 Batch: 00038/00094 | Loss: 774.0070 | CE: 0.5169 | KD: 1061.0289\n",
      "Train Epoch: 015 Batch: 00039/00094 | Loss: 773.4584 | CE: 0.4481 | KD: 1060.3708\n",
      "Train Epoch: 015 Batch: 00040/00094 | Loss: 773.6709 | CE: 0.3932 | KD: 1060.7377\n",
      "Train Epoch: 015 Batch: 00041/00094 | Loss: 773.4432 | CE: 0.3813 | KD: 1060.4417\n",
      "Train Epoch: 015 Batch: 00042/00094 | Loss: 773.5633 | CE: 0.4918 | KD: 1060.4548\n",
      "Train Epoch: 015 Batch: 00043/00094 | Loss: 773.5147 | CE: 0.4498 | KD: 1060.4457\n",
      "Train Epoch: 015 Batch: 00044/00094 | Loss: 773.3135 | CE: 0.3625 | KD: 1060.2894\n",
      "Train Epoch: 015 Batch: 00045/00094 | Loss: 773.4192 | CE: 0.2883 | KD: 1060.5363\n",
      "Train Epoch: 015 Batch: 00046/00094 | Loss: 773.5470 | CE: 0.4137 | KD: 1060.5394\n",
      "Train Epoch: 015 Batch: 00047/00094 | Loss: 773.2537 | CE: 0.3543 | KD: 1060.2186\n",
      "Train Epoch: 015 Batch: 00048/00094 | Loss: 773.3651 | CE: 0.4041 | KD: 1060.3031\n",
      "Train Epoch: 015 Batch: 00049/00094 | Loss: 773.4749 | CE: 0.4360 | KD: 1060.4099\n",
      "Train Epoch: 015 Batch: 00050/00094 | Loss: 774.0811 | CE: 0.6474 | KD: 1060.9517\n",
      "Train Epoch: 015 Batch: 00051/00094 | Loss: 773.6133 | CE: 0.4872 | KD: 1060.5298\n",
      "Train Epoch: 015 Batch: 00052/00094 | Loss: 773.3654 | CE: 0.4116 | KD: 1060.2933\n",
      "Train Epoch: 015 Batch: 00053/00094 | Loss: 774.0350 | CE: 0.5284 | KD: 1061.0515\n",
      "Train Epoch: 015 Batch: 00054/00094 | Loss: 773.6479 | CE: 0.4139 | KD: 1060.6776\n",
      "Train Epoch: 015 Batch: 00055/00094 | Loss: 773.3706 | CE: 0.4021 | KD: 1060.3135\n",
      "Train Epoch: 015 Batch: 00056/00094 | Loss: 773.4561 | CE: 0.3844 | KD: 1060.4551\n",
      "Train Epoch: 015 Batch: 00057/00094 | Loss: 773.8383 | CE: 0.6307 | KD: 1060.6415\n",
      "Train Epoch: 015 Batch: 00058/00094 | Loss: 773.6066 | CE: 0.4438 | KD: 1060.5800\n",
      "Train Epoch: 015 Batch: 00059/00094 | Loss: 773.5530 | CE: 0.4298 | KD: 1060.5256\n",
      "Train Epoch: 015 Batch: 00060/00094 | Loss: 773.9942 | CE: 0.5026 | KD: 1061.0310\n",
      "Train Epoch: 015 Batch: 00061/00094 | Loss: 773.4648 | CE: 0.4048 | KD: 1060.4390\n",
      "Train Epoch: 015 Batch: 00062/00094 | Loss: 773.4730 | CE: 0.3925 | KD: 1060.4672\n",
      "Train Epoch: 015 Batch: 00063/00094 | Loss: 773.6985 | CE: 0.5347 | KD: 1060.5813\n",
      "Train Epoch: 015 Batch: 00064/00094 | Loss: 773.6967 | CE: 0.6177 | KD: 1060.4651\n",
      "Train Epoch: 015 Batch: 00065/00094 | Loss: 773.7724 | CE: 0.5459 | KD: 1060.6674\n",
      "Train Epoch: 015 Batch: 00066/00094 | Loss: 773.5527 | CE: 0.3595 | KD: 1060.6217\n",
      "Train Epoch: 015 Batch: 00067/00094 | Loss: 773.9949 | CE: 0.4386 | KD: 1061.1199\n",
      "Train Epoch: 015 Batch: 00068/00094 | Loss: 773.9934 | CE: 0.5719 | KD: 1060.9349\n",
      "Train Epoch: 015 Batch: 00069/00094 | Loss: 773.4547 | CE: 0.4136 | KD: 1060.4131\n",
      "Train Epoch: 015 Batch: 00070/00094 | Loss: 773.9237 | CE: 0.4933 | KD: 1060.9470\n",
      "Train Epoch: 015 Batch: 00071/00094 | Loss: 773.7061 | CE: 0.5483 | KD: 1060.5731\n",
      "Train Epoch: 015 Batch: 00072/00094 | Loss: 773.4006 | CE: 0.4131 | KD: 1060.3396\n",
      "Train Epoch: 015 Batch: 00073/00094 | Loss: 773.6342 | CE: 0.5101 | KD: 1060.5269\n",
      "Train Epoch: 015 Batch: 00074/00094 | Loss: 773.2391 | CE: 0.3452 | KD: 1060.2112\n",
      "Train Epoch: 015 Batch: 00075/00094 | Loss: 773.6258 | CE: 0.3817 | KD: 1060.6915\n",
      "Train Epoch: 015 Batch: 00076/00094 | Loss: 773.1207 | CE: 0.2891 | KD: 1060.1257\n",
      "Train Epoch: 015 Batch: 00077/00094 | Loss: 773.3949 | CE: 0.4474 | KD: 1060.2847\n",
      "Train Epoch: 015 Batch: 00078/00094 | Loss: 773.1097 | CE: 0.2799 | KD: 1060.1232\n",
      "Train Epoch: 015 Batch: 00079/00094 | Loss: 773.4249 | CE: 0.4616 | KD: 1060.3063\n",
      "Train Epoch: 015 Batch: 00080/00094 | Loss: 773.4056 | CE: 0.3782 | KD: 1060.3943\n",
      "Train Epoch: 015 Batch: 00081/00094 | Loss: 773.4514 | CE: 0.4239 | KD: 1060.3943\n",
      "Train Epoch: 015 Batch: 00082/00094 | Loss: 773.4149 | CE: 0.4350 | KD: 1060.3291\n",
      "Train Epoch: 015 Batch: 00083/00094 | Loss: 773.4863 | CE: 0.3204 | KD: 1060.5842\n",
      "Train Epoch: 015 Batch: 00084/00094 | Loss: 773.5467 | CE: 0.3996 | KD: 1060.5585\n",
      "Train Epoch: 015 Batch: 00085/00094 | Loss: 773.3262 | CE: 0.3178 | KD: 1060.3682\n",
      "Train Epoch: 015 Batch: 00086/00094 | Loss: 773.8942 | CE: 0.5908 | KD: 1060.7728\n",
      "Train Epoch: 015 Batch: 00087/00094 | Loss: 773.7858 | CE: 0.4663 | KD: 1060.7949\n",
      "Train Epoch: 015 Batch: 00088/00094 | Loss: 774.2335 | CE: 0.4584 | KD: 1061.4199\n",
      "Train Epoch: 015 Batch: 00089/00094 | Loss: 773.6910 | CE: 0.4117 | KD: 1060.7399\n",
      "Train Epoch: 015 Batch: 00090/00094 | Loss: 774.0521 | CE: 0.4945 | KD: 1061.1216\n",
      "Train Epoch: 015 Batch: 00091/00094 | Loss: 774.1145 | CE: 0.4135 | KD: 1061.3184\n",
      "Train Epoch: 015 Batch: 00092/00094 | Loss: 773.5006 | CE: 0.3857 | KD: 1060.5143\n",
      "Train Epoch: 015 Batch: 00093/00094 | Loss: 773.5684 | CE: 0.5202 | KD: 1060.4229\n",
      "Train Epoch: 015 Batch: 00094/00094 | Loss: 773.3669 | CE: 0.4030 | KD: 1060.3070\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4898 | acc:82.3500\n",
      "[VAL Acc] Target: 82.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2300 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8958 | acc:53.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1445 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8468 | acc:51.6850\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8419 | acc:52.2181\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6457 | acc:63.2053\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.21%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8735 | acc:48.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7414 | acc:60.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.55%\n",
      "[VAL Acc] Avg 56.86%\n",
      "Train Epoch: 016 Batch: 00001/00094 | Loss: 773.3921 | CE: 0.3328 | KD: 1060.4380\n",
      "Train Epoch: 016 Batch: 00002/00094 | Loss: 773.6780 | CE: 0.4126 | KD: 1060.7207\n",
      "Train Epoch: 016 Batch: 00003/00094 | Loss: 773.8871 | CE: 0.5871 | KD: 1060.7682\n",
      "Train Epoch: 016 Batch: 00004/00094 | Loss: 773.6269 | CE: 0.3141 | KD: 1060.7858\n",
      "Train Epoch: 016 Batch: 00005/00094 | Loss: 773.3309 | CE: 0.4339 | KD: 1060.2153\n",
      "Train Epoch: 016 Batch: 00006/00094 | Loss: 773.2006 | CE: 0.2852 | KD: 1060.2406\n",
      "Train Epoch: 016 Batch: 00007/00094 | Loss: 773.2941 | CE: 0.3555 | KD: 1060.2725\n",
      "Train Epoch: 016 Batch: 00008/00094 | Loss: 773.2868 | CE: 0.3698 | KD: 1060.2428\n",
      "Train Epoch: 016 Batch: 00009/00094 | Loss: 773.3143 | CE: 0.3240 | KD: 1060.3434\n",
      "Train Epoch: 016 Batch: 00010/00094 | Loss: 773.5270 | CE: 0.4207 | KD: 1060.5026\n",
      "Train Epoch: 016 Batch: 00011/00094 | Loss: 773.8523 | CE: 0.5931 | KD: 1060.7123\n",
      "Train Epoch: 016 Batch: 00012/00094 | Loss: 773.3809 | CE: 0.3984 | KD: 1060.3326\n",
      "Train Epoch: 016 Batch: 00013/00094 | Loss: 774.0498 | CE: 0.4244 | KD: 1061.2146\n",
      "Train Epoch: 016 Batch: 00014/00094 | Loss: 773.6832 | CE: 0.4632 | KD: 1060.6584\n",
      "Train Epoch: 016 Batch: 00015/00094 | Loss: 773.7434 | CE: 0.5310 | KD: 1060.6481\n",
      "Train Epoch: 016 Batch: 00016/00094 | Loss: 773.5283 | CE: 0.4272 | KD: 1060.4954\n",
      "Train Epoch: 016 Batch: 00017/00094 | Loss: 773.9469 | CE: 0.5456 | KD: 1060.9072\n",
      "Train Epoch: 016 Batch: 00018/00094 | Loss: 774.1341 | CE: 0.4733 | KD: 1061.2631\n",
      "Train Epoch: 016 Batch: 00019/00094 | Loss: 773.3964 | CE: 0.3705 | KD: 1060.3922\n",
      "Train Epoch: 016 Batch: 00020/00094 | Loss: 773.6024 | CE: 0.4390 | KD: 1060.5808\n",
      "Train Epoch: 016 Batch: 00021/00094 | Loss: 773.5703 | CE: 0.4941 | KD: 1060.4612\n",
      "Train Epoch: 016 Batch: 00022/00094 | Loss: 773.5873 | CE: 0.4252 | KD: 1060.5791\n",
      "Train Epoch: 016 Batch: 00023/00094 | Loss: 773.7214 | CE: 0.5285 | KD: 1060.6213\n",
      "Train Epoch: 016 Batch: 00024/00094 | Loss: 773.4589 | CE: 0.4372 | KD: 1060.3864\n",
      "Train Epoch: 016 Batch: 00025/00094 | Loss: 773.4717 | CE: 0.3192 | KD: 1060.5660\n",
      "Train Epoch: 016 Batch: 00026/00094 | Loss: 773.2711 | CE: 0.3178 | KD: 1060.2927\n",
      "Train Epoch: 016 Batch: 00027/00094 | Loss: 773.2857 | CE: 0.3407 | KD: 1060.2812\n",
      "Train Epoch: 016 Batch: 00028/00094 | Loss: 774.0656 | CE: 0.6127 | KD: 1060.9779\n",
      "Train Epoch: 016 Batch: 00029/00094 | Loss: 773.4642 | CE: 0.3486 | KD: 1060.5153\n",
      "Train Epoch: 016 Batch: 00030/00094 | Loss: 773.4302 | CE: 0.4019 | KD: 1060.3955\n",
      "Train Epoch: 016 Batch: 00031/00094 | Loss: 773.4352 | CE: 0.3118 | KD: 1060.5260\n",
      "Train Epoch: 016 Batch: 00032/00094 | Loss: 773.3157 | CE: 0.4113 | KD: 1060.2255\n",
      "Train Epoch: 016 Batch: 00033/00094 | Loss: 773.3202 | CE: 0.4002 | KD: 1060.2469\n",
      "Train Epoch: 016 Batch: 00034/00094 | Loss: 773.3730 | CE: 0.3239 | KD: 1060.4242\n",
      "Train Epoch: 016 Batch: 00035/00094 | Loss: 774.0214 | CE: 0.4799 | KD: 1061.0995\n",
      "Train Epoch: 016 Batch: 00036/00094 | Loss: 773.4324 | CE: 0.4256 | KD: 1060.3661\n",
      "Train Epoch: 016 Batch: 00037/00094 | Loss: 773.3677 | CE: 0.4605 | KD: 1060.2294\n",
      "Train Epoch: 016 Batch: 00038/00094 | Loss: 773.9028 | CE: 0.5152 | KD: 1060.8884\n",
      "Train Epoch: 016 Batch: 00039/00094 | Loss: 773.4737 | CE: 0.4211 | KD: 1060.4288\n",
      "Train Epoch: 016 Batch: 00040/00094 | Loss: 773.5884 | CE: 0.4825 | KD: 1060.5020\n",
      "Train Epoch: 016 Batch: 00041/00094 | Loss: 773.5252 | CE: 0.3838 | KD: 1060.5507\n",
      "Train Epoch: 016 Batch: 00042/00094 | Loss: 773.5427 | CE: 0.4660 | KD: 1060.4620\n",
      "Train Epoch: 016 Batch: 00043/00094 | Loss: 773.3247 | CE: 0.4010 | KD: 1060.2521\n",
      "Train Epoch: 016 Batch: 00044/00094 | Loss: 773.6075 | CE: 0.4471 | KD: 1060.5768\n",
      "Train Epoch: 016 Batch: 00045/00094 | Loss: 773.4583 | CE: 0.4396 | KD: 1060.3822\n",
      "Train Epoch: 016 Batch: 00046/00094 | Loss: 773.1791 | CE: 0.3312 | KD: 1060.1481\n",
      "Train Epoch: 016 Batch: 00047/00094 | Loss: 773.5101 | CE: 0.3968 | KD: 1060.5121\n",
      "Train Epoch: 016 Batch: 00048/00094 | Loss: 774.2529 | CE: 0.6138 | KD: 1061.2333\n",
      "Train Epoch: 016 Batch: 00049/00094 | Loss: 773.5123 | CE: 0.4519 | KD: 1060.4396\n",
      "Train Epoch: 016 Batch: 00050/00094 | Loss: 773.5236 | CE: 0.3811 | KD: 1060.5521\n",
      "Train Epoch: 016 Batch: 00051/00094 | Loss: 773.3719 | CE: 0.4052 | KD: 1060.3110\n",
      "Train Epoch: 016 Batch: 00052/00094 | Loss: 773.9468 | CE: 0.4374 | KD: 1061.0554\n",
      "Train Epoch: 016 Batch: 00053/00094 | Loss: 773.4107 | CE: 0.3731 | KD: 1060.4082\n",
      "Train Epoch: 016 Batch: 00054/00094 | Loss: 773.6618 | CE: 0.4659 | KD: 1060.6255\n",
      "Train Epoch: 016 Batch: 00055/00094 | Loss: 773.6917 | CE: 0.5028 | KD: 1060.6157\n",
      "Train Epoch: 016 Batch: 00056/00094 | Loss: 773.8004 | CE: 0.5383 | KD: 1060.7163\n",
      "Train Epoch: 016 Batch: 00057/00094 | Loss: 773.7953 | CE: 0.4020 | KD: 1060.8961\n",
      "Train Epoch: 016 Batch: 00058/00094 | Loss: 773.8290 | CE: 0.3061 | KD: 1061.0740\n",
      "Train Epoch: 016 Batch: 00059/00094 | Loss: 773.4350 | CE: 0.4000 | KD: 1060.4048\n",
      "Train Epoch: 016 Batch: 00060/00094 | Loss: 773.4030 | CE: 0.3676 | KD: 1060.4053\n",
      "Train Epoch: 016 Batch: 00061/00094 | Loss: 773.3073 | CE: 0.3188 | KD: 1060.3408\n",
      "Train Epoch: 016 Batch: 00062/00094 | Loss: 773.4909 | CE: 0.4418 | KD: 1060.4241\n",
      "Train Epoch: 016 Batch: 00063/00094 | Loss: 773.4020 | CE: 0.3867 | KD: 1060.3777\n",
      "Train Epoch: 016 Batch: 00064/00094 | Loss: 773.4519 | CE: 0.3629 | KD: 1060.4788\n",
      "Train Epoch: 016 Batch: 00065/00094 | Loss: 773.3102 | CE: 0.3976 | KD: 1060.2368\n",
      "Train Epoch: 016 Batch: 00066/00094 | Loss: 773.6085 | CE: 0.3853 | KD: 1060.6628\n",
      "Train Epoch: 016 Batch: 00067/00094 | Loss: 773.8019 | CE: 0.4932 | KD: 1060.7802\n",
      "Train Epoch: 016 Batch: 00068/00094 | Loss: 773.9108 | CE: 0.5569 | KD: 1060.8422\n",
      "Train Epoch: 016 Batch: 00069/00094 | Loss: 773.4824 | CE: 0.5300 | KD: 1060.2915\n",
      "Train Epoch: 016 Batch: 00070/00094 | Loss: 773.5807 | CE: 0.4253 | KD: 1060.5698\n",
      "Train Epoch: 016 Batch: 00071/00094 | Loss: 773.5784 | CE: 0.5147 | KD: 1060.4441\n",
      "Train Epoch: 016 Batch: 00072/00094 | Loss: 773.8253 | CE: 0.5451 | KD: 1060.7411\n",
      "Train Epoch: 016 Batch: 00073/00094 | Loss: 773.5286 | CE: 0.4096 | KD: 1060.5198\n",
      "Train Epoch: 016 Batch: 00074/00094 | Loss: 773.5402 | CE: 0.4153 | KD: 1060.5280\n",
      "Train Epoch: 016 Batch: 00075/00094 | Loss: 773.5403 | CE: 0.5315 | KD: 1060.3688\n",
      "Train Epoch: 016 Batch: 00076/00094 | Loss: 773.5680 | CE: 0.4914 | KD: 1060.4617\n",
      "Train Epoch: 016 Batch: 00077/00094 | Loss: 773.2919 | CE: 0.3443 | KD: 1060.2849\n",
      "Train Epoch: 016 Batch: 00078/00094 | Loss: 773.9970 | CE: 0.5834 | KD: 1060.9241\n",
      "Train Epoch: 016 Batch: 00079/00094 | Loss: 773.4242 | CE: 0.4440 | KD: 1060.3295\n",
      "Train Epoch: 016 Batch: 00080/00094 | Loss: 773.4270 | CE: 0.3733 | KD: 1060.4303\n",
      "Train Epoch: 016 Batch: 00081/00094 | Loss: 773.6060 | CE: 0.3515 | KD: 1060.7058\n",
      "Train Epoch: 016 Batch: 00082/00094 | Loss: 773.4410 | CE: 0.3023 | KD: 1060.5471\n",
      "Train Epoch: 016 Batch: 00083/00094 | Loss: 773.6154 | CE: 0.3999 | KD: 1060.6523\n",
      "Train Epoch: 016 Batch: 00084/00094 | Loss: 774.3047 | CE: 0.5788 | KD: 1061.3524\n",
      "Train Epoch: 016 Batch: 00085/00094 | Loss: 773.4037 | CE: 0.3772 | KD: 1060.3932\n",
      "Train Epoch: 016 Batch: 00086/00094 | Loss: 773.4293 | CE: 0.3762 | KD: 1060.4296\n",
      "Train Epoch: 016 Batch: 00087/00094 | Loss: 773.4263 | CE: 0.4162 | KD: 1060.3705\n",
      "Train Epoch: 016 Batch: 00088/00094 | Loss: 773.7972 | CE: 0.5231 | KD: 1060.7328\n",
      "Train Epoch: 016 Batch: 00089/00094 | Loss: 773.4195 | CE: 0.4116 | KD: 1060.3676\n",
      "Train Epoch: 016 Batch: 00090/00094 | Loss: 773.5036 | CE: 0.3538 | KD: 1060.5621\n",
      "Train Epoch: 016 Batch: 00091/00094 | Loss: 773.5363 | CE: 0.5340 | KD: 1060.3597\n",
      "Train Epoch: 016 Batch: 00092/00094 | Loss: 773.6769 | CE: 0.4162 | KD: 1060.7144\n",
      "Train Epoch: 016 Batch: 00093/00094 | Loss: 773.5874 | CE: 0.3801 | KD: 1060.6410\n",
      "Train Epoch: 016 Batch: 00094/00094 | Loss: 774.1489 | CE: 0.5727 | KD: 1061.1472\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4719 | acc:81.9000\n",
      "[VAL Acc] Target: 81.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3257 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9682 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2516 | acc:48.2824\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9458 | acc:50.9013\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8346 | acc:53.4196\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6626 | acc:63.9107\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.91%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8871 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7680 | acc:59.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 59.60%\n",
      "[VAL Acc] Avg 56.72%\n",
      "Train Epoch: 017 Batch: 00001/00094 | Loss: 773.4026 | CE: 0.3859 | KD: 1060.3795\n",
      "Train Epoch: 017 Batch: 00002/00094 | Loss: 773.1373 | CE: 0.3181 | KD: 1060.1088\n",
      "Train Epoch: 017 Batch: 00003/00094 | Loss: 773.1578 | CE: 0.2724 | KD: 1060.1995\n",
      "Train Epoch: 017 Batch: 00004/00094 | Loss: 774.0710 | CE: 0.5132 | KD: 1061.1218\n",
      "Train Epoch: 017 Batch: 00005/00094 | Loss: 773.4539 | CE: 0.5044 | KD: 1060.2874\n",
      "Train Epoch: 017 Batch: 00006/00094 | Loss: 773.9681 | CE: 0.4958 | KD: 1061.0045\n",
      "Train Epoch: 017 Batch: 00007/00094 | Loss: 773.3075 | CE: 0.4316 | KD: 1060.1864\n",
      "Train Epoch: 017 Batch: 00008/00094 | Loss: 773.6429 | CE: 0.2941 | KD: 1060.8351\n",
      "Train Epoch: 017 Batch: 00009/00094 | Loss: 773.4271 | CE: 0.2962 | KD: 1060.5363\n",
      "Train Epoch: 017 Batch: 00010/00094 | Loss: 773.6553 | CE: 0.4502 | KD: 1060.6381\n",
      "Train Epoch: 017 Batch: 00011/00094 | Loss: 773.4924 | CE: 0.4216 | KD: 1060.4537\n",
      "Train Epoch: 017 Batch: 00012/00094 | Loss: 773.6801 | CE: 0.4679 | KD: 1060.6477\n",
      "Train Epoch: 017 Batch: 00013/00094 | Loss: 773.6837 | CE: 0.3564 | KD: 1060.8057\n",
      "Train Epoch: 017 Batch: 00014/00094 | Loss: 774.0136 | CE: 0.5593 | KD: 1060.9799\n",
      "Train Epoch: 017 Batch: 00015/00094 | Loss: 773.7614 | CE: 0.4285 | KD: 1060.8134\n",
      "Train Epoch: 017 Batch: 00016/00094 | Loss: 773.1661 | CE: 0.3110 | KD: 1060.1580\n",
      "Train Epoch: 017 Batch: 00017/00094 | Loss: 773.6062 | CE: 0.4360 | KD: 1060.5902\n",
      "Train Epoch: 017 Batch: 00018/00094 | Loss: 773.5608 | CE: 0.3479 | KD: 1060.6488\n",
      "Train Epoch: 017 Batch: 00019/00094 | Loss: 773.6393 | CE: 0.4571 | KD: 1060.6067\n",
      "Train Epoch: 017 Batch: 00020/00094 | Loss: 773.7212 | CE: 0.5044 | KD: 1060.6542\n",
      "Train Epoch: 017 Batch: 00021/00094 | Loss: 773.5560 | CE: 0.5001 | KD: 1060.4335\n",
      "Train Epoch: 017 Batch: 00022/00094 | Loss: 773.5749 | CE: 0.3874 | KD: 1060.6139\n",
      "Train Epoch: 017 Batch: 00023/00094 | Loss: 773.6382 | CE: 0.4740 | KD: 1060.5818\n",
      "Train Epoch: 017 Batch: 00024/00094 | Loss: 773.1650 | CE: 0.3446 | KD: 1060.1104\n",
      "Train Epoch: 017 Batch: 00025/00094 | Loss: 773.9334 | CE: 0.6149 | KD: 1060.7936\n",
      "Train Epoch: 017 Batch: 00026/00094 | Loss: 774.0724 | CE: 0.5334 | KD: 1061.0959\n",
      "Train Epoch: 017 Batch: 00027/00094 | Loss: 773.5422 | CE: 0.3445 | KD: 1060.6278\n",
      "Train Epoch: 017 Batch: 00028/00094 | Loss: 773.5499 | CE: 0.3721 | KD: 1060.6006\n",
      "Train Epoch: 017 Batch: 00029/00094 | Loss: 773.5911 | CE: 0.4307 | KD: 1060.5768\n",
      "Train Epoch: 017 Batch: 00030/00094 | Loss: 773.5682 | CE: 0.4343 | KD: 1060.5404\n",
      "Train Epoch: 017 Batch: 00031/00094 | Loss: 773.5285 | CE: 0.4483 | KD: 1060.4668\n",
      "Train Epoch: 017 Batch: 00032/00094 | Loss: 773.9698 | CE: 0.4940 | KD: 1061.0095\n",
      "Train Epoch: 017 Batch: 00033/00094 | Loss: 773.3492 | CE: 0.3986 | KD: 1060.2889\n",
      "Train Epoch: 017 Batch: 00034/00094 | Loss: 773.9360 | CE: 0.6778 | KD: 1060.7109\n",
      "Train Epoch: 017 Batch: 00035/00094 | Loss: 773.7112 | CE: 0.5464 | KD: 1060.5828\n",
      "Train Epoch: 017 Batch: 00036/00094 | Loss: 773.5751 | CE: 0.4660 | KD: 1060.5063\n",
      "Train Epoch: 017 Batch: 00037/00094 | Loss: 773.8130 | CE: 0.5101 | KD: 1060.7722\n",
      "Train Epoch: 017 Batch: 00038/00094 | Loss: 773.2357 | CE: 0.3251 | KD: 1060.2340\n",
      "Train Epoch: 017 Batch: 00039/00094 | Loss: 773.4635 | CE: 0.4054 | KD: 1060.4364\n",
      "Train Epoch: 017 Batch: 00040/00094 | Loss: 773.2504 | CE: 0.3879 | KD: 1060.1681\n",
      "Train Epoch: 017 Batch: 00041/00094 | Loss: 773.6597 | CE: 0.4244 | KD: 1060.6796\n",
      "Train Epoch: 017 Batch: 00042/00094 | Loss: 773.6157 | CE: 0.4205 | KD: 1060.6245\n",
      "Train Epoch: 017 Batch: 00043/00094 | Loss: 773.7836 | CE: 0.4611 | KD: 1060.7991\n",
      "Train Epoch: 017 Batch: 00044/00094 | Loss: 773.5739 | CE: 0.5065 | KD: 1060.4491\n",
      "Train Epoch: 017 Batch: 00045/00094 | Loss: 773.9377 | CE: 0.5334 | KD: 1060.9114\n",
      "Train Epoch: 017 Batch: 00046/00094 | Loss: 773.6406 | CE: 0.5335 | KD: 1060.5035\n",
      "Train Epoch: 017 Batch: 00047/00094 | Loss: 773.3569 | CE: 0.4187 | KD: 1060.2720\n",
      "Train Epoch: 017 Batch: 00048/00094 | Loss: 773.6523 | CE: 0.5562 | KD: 1060.4885\n",
      "Train Epoch: 017 Batch: 00049/00094 | Loss: 773.2575 | CE: 0.3377 | KD: 1060.2467\n",
      "Train Epoch: 017 Batch: 00050/00094 | Loss: 773.5542 | CE: 0.5556 | KD: 1060.3547\n",
      "Train Epoch: 017 Batch: 00051/00094 | Loss: 773.1942 | CE: 0.3357 | KD: 1060.1627\n",
      "Train Epoch: 017 Batch: 00052/00094 | Loss: 773.4055 | CE: 0.3208 | KD: 1060.4730\n",
      "Train Epoch: 017 Batch: 00053/00094 | Loss: 773.5184 | CE: 0.4513 | KD: 1060.4487\n",
      "Train Epoch: 017 Batch: 00054/00094 | Loss: 773.7528 | CE: 0.4889 | KD: 1060.7186\n",
      "Train Epoch: 017 Batch: 00055/00094 | Loss: 773.4742 | CE: 0.4743 | KD: 1060.3566\n",
      "Train Epoch: 017 Batch: 00056/00094 | Loss: 773.6464 | CE: 0.4942 | KD: 1060.5654\n",
      "Train Epoch: 017 Batch: 00057/00094 | Loss: 773.8922 | CE: 0.5286 | KD: 1060.8553\n",
      "Train Epoch: 017 Batch: 00058/00094 | Loss: 773.7098 | CE: 0.4707 | KD: 1060.6846\n",
      "Train Epoch: 017 Batch: 00059/00094 | Loss: 773.5000 | CE: 0.4771 | KD: 1060.3882\n",
      "Train Epoch: 017 Batch: 00060/00094 | Loss: 773.3544 | CE: 0.3903 | KD: 1060.3074\n",
      "Train Epoch: 017 Batch: 00061/00094 | Loss: 773.8992 | CE: 0.5807 | KD: 1060.7936\n",
      "Train Epoch: 017 Batch: 00062/00094 | Loss: 773.4799 | CE: 0.4251 | KD: 1060.4319\n",
      "Train Epoch: 017 Batch: 00063/00094 | Loss: 773.4073 | CE: 0.4117 | KD: 1060.3507\n",
      "Train Epoch: 017 Batch: 00064/00094 | Loss: 773.5799 | CE: 0.4369 | KD: 1060.5527\n",
      "Train Epoch: 017 Batch: 00065/00094 | Loss: 774.2590 | CE: 0.3605 | KD: 1061.5892\n",
      "Train Epoch: 017 Batch: 00066/00094 | Loss: 773.3599 | CE: 0.3821 | KD: 1060.3263\n",
      "Train Epoch: 017 Batch: 00067/00094 | Loss: 773.8283 | CE: 0.5115 | KD: 1060.7913\n",
      "Train Epoch: 017 Batch: 00068/00094 | Loss: 773.3543 | CE: 0.4630 | KD: 1060.2076\n",
      "Train Epoch: 017 Batch: 00069/00094 | Loss: 773.6356 | CE: 0.4104 | KD: 1060.6655\n",
      "Train Epoch: 017 Batch: 00070/00094 | Loss: 773.8649 | CE: 0.5274 | KD: 1060.8196\n",
      "Train Epoch: 017 Batch: 00071/00094 | Loss: 773.4399 | CE: 0.4103 | KD: 1060.3973\n",
      "Train Epoch: 017 Batch: 00072/00094 | Loss: 773.6091 | CE: 0.4016 | KD: 1060.6412\n",
      "Train Epoch: 017 Batch: 00073/00094 | Loss: 773.4835 | CE: 0.4093 | KD: 1060.4585\n",
      "Train Epoch: 017 Batch: 00074/00094 | Loss: 773.9179 | CE: 0.4209 | KD: 1061.0385\n",
      "Train Epoch: 017 Batch: 00075/00094 | Loss: 773.1969 | CE: 0.2946 | KD: 1060.2227\n",
      "Train Epoch: 017 Batch: 00076/00094 | Loss: 773.4108 | CE: 0.4405 | KD: 1060.3159\n",
      "Train Epoch: 017 Batch: 00077/00094 | Loss: 773.6172 | CE: 0.3247 | KD: 1060.7579\n",
      "Train Epoch: 017 Batch: 00078/00094 | Loss: 773.4946 | CE: 0.3475 | KD: 1060.5586\n",
      "Train Epoch: 017 Batch: 00079/00094 | Loss: 773.5279 | CE: 0.4688 | KD: 1060.4377\n",
      "Train Epoch: 017 Batch: 00080/00094 | Loss: 773.6167 | CE: 0.4664 | KD: 1060.5629\n",
      "Train Epoch: 017 Batch: 00081/00094 | Loss: 773.6155 | CE: 0.4623 | KD: 1060.5668\n",
      "Train Epoch: 017 Batch: 00082/00094 | Loss: 773.7271 | CE: 0.5819 | KD: 1060.5559\n",
      "Train Epoch: 017 Batch: 00083/00094 | Loss: 773.6329 | CE: 0.5041 | KD: 1060.5333\n",
      "Train Epoch: 017 Batch: 00084/00094 | Loss: 773.4032 | CE: 0.3297 | KD: 1060.4575\n",
      "Train Epoch: 017 Batch: 00085/00094 | Loss: 773.5024 | CE: 0.3275 | KD: 1060.5966\n",
      "Train Epoch: 017 Batch: 00086/00094 | Loss: 773.5833 | CE: 0.3393 | KD: 1060.6914\n",
      "Train Epoch: 017 Batch: 00087/00094 | Loss: 773.7750 | CE: 0.5196 | KD: 1060.7069\n",
      "Train Epoch: 017 Batch: 00088/00094 | Loss: 773.8632 | CE: 0.4290 | KD: 1060.9524\n",
      "Train Epoch: 017 Batch: 00089/00094 | Loss: 773.3265 | CE: 0.3720 | KD: 1060.2943\n",
      "Train Epoch: 017 Batch: 00090/00094 | Loss: 773.4329 | CE: 0.3759 | KD: 1060.4348\n",
      "Train Epoch: 017 Batch: 00091/00094 | Loss: 773.6256 | CE: 0.3967 | KD: 1060.6707\n",
      "Train Epoch: 017 Batch: 00092/00094 | Loss: 773.5737 | CE: 0.4528 | KD: 1060.5226\n",
      "Train Epoch: 017 Batch: 00093/00094 | Loss: 773.3314 | CE: 0.3576 | KD: 1060.3207\n",
      "Train Epoch: 017 Batch: 00094/00094 | Loss: 773.4836 | CE: 0.5075 | KD: 1060.3240\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4609 | acc:81.8000\n",
      "[VAL Acc] Target: 81.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3260 | acc:49.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9616 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2683 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9448 | acc:51.2147\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.21%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9059 | acc:51.7560\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6439 | acc:63.4404\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9359 | acc:48.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7981 | acc:59.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 59.65%\n",
      "[VAL Acc] Avg 56.43%\n",
      "Train Epoch: 018 Batch: 00001/00094 | Loss: 696.2039 | CE: 0.4499 | KD: 1060.4388\n",
      "Train Epoch: 018 Batch: 00002/00094 | Loss: 696.6772 | CE: 0.5374 | KD: 1061.0271\n",
      "Train Epoch: 018 Batch: 00003/00094 | Loss: 696.1572 | CE: 0.4172 | KD: 1060.4177\n",
      "Train Epoch: 018 Batch: 00004/00094 | Loss: 696.5378 | CE: 0.5568 | KD: 1060.7849\n",
      "Train Epoch: 018 Batch: 00005/00094 | Loss: 695.9841 | CE: 0.4145 | KD: 1060.1578\n",
      "Train Epoch: 018 Batch: 00006/00094 | Loss: 696.1163 | CE: 0.3838 | KD: 1060.4062\n",
      "Train Epoch: 018 Batch: 00007/00094 | Loss: 696.2957 | CE: 0.3354 | KD: 1060.7534\n",
      "Train Epoch: 018 Batch: 00008/00094 | Loss: 695.9578 | CE: 0.3386 | KD: 1060.2335\n",
      "Train Epoch: 018 Batch: 00009/00094 | Loss: 696.4022 | CE: 0.4022 | KD: 1060.8138\n",
      "Train Epoch: 018 Batch: 00010/00094 | Loss: 696.0245 | CE: 0.3313 | KD: 1060.3463\n",
      "Train Epoch: 018 Batch: 00011/00094 | Loss: 696.4816 | CE: 0.5317 | KD: 1060.7375\n",
      "Train Epoch: 018 Batch: 00012/00094 | Loss: 696.3336 | CE: 0.4899 | KD: 1060.5756\n",
      "Train Epoch: 018 Batch: 00013/00094 | Loss: 696.4825 | CE: 0.5399 | KD: 1060.7264\n",
      "Train Epoch: 018 Batch: 00014/00094 | Loss: 696.4910 | CE: 0.3908 | KD: 1060.9667\n",
      "Train Epoch: 018 Batch: 00015/00094 | Loss: 696.0418 | CE: 0.4143 | KD: 1060.2462\n",
      "Train Epoch: 018 Batch: 00016/00094 | Loss: 695.9977 | CE: 0.4111 | KD: 1060.1838\n",
      "Train Epoch: 018 Batch: 00017/00094 | Loss: 696.1960 | CE: 0.4138 | KD: 1060.4819\n",
      "Train Epoch: 018 Batch: 00018/00094 | Loss: 696.0864 | CE: 0.4461 | KD: 1060.2657\n",
      "Train Epoch: 018 Batch: 00019/00094 | Loss: 696.0176 | CE: 0.2974 | KD: 1060.3876\n",
      "Train Epoch: 018 Batch: 00020/00094 | Loss: 696.6580 | CE: 0.5931 | KD: 1060.9127\n",
      "Train Epoch: 018 Batch: 00021/00094 | Loss: 696.3119 | CE: 0.4692 | KD: 1060.5742\n",
      "Train Epoch: 018 Batch: 00022/00094 | Loss: 696.2287 | CE: 0.4186 | KD: 1060.5245\n",
      "Train Epoch: 018 Batch: 00023/00094 | Loss: 696.4305 | CE: 0.3669 | KD: 1060.9109\n",
      "Train Epoch: 018 Batch: 00024/00094 | Loss: 696.1651 | CE: 0.3689 | KD: 1060.5033\n",
      "Train Epoch: 018 Batch: 00025/00094 | Loss: 696.3654 | CE: 0.4296 | KD: 1060.7161\n",
      "Train Epoch: 018 Batch: 00026/00094 | Loss: 696.0422 | CE: 0.3876 | KD: 1060.2875\n",
      "Train Epoch: 018 Batch: 00027/00094 | Loss: 696.1338 | CE: 0.3936 | KD: 1060.4180\n",
      "Train Epoch: 018 Batch: 00028/00094 | Loss: 696.4771 | CE: 0.3873 | KD: 1060.9507\n",
      "Train Epoch: 018 Batch: 00029/00094 | Loss: 696.4626 | CE: 0.3907 | KD: 1060.9236\n",
      "Train Epoch: 018 Batch: 00030/00094 | Loss: 696.3425 | CE: 0.3950 | KD: 1060.7339\n",
      "Train Epoch: 018 Batch: 00031/00094 | Loss: 696.6334 | CE: 0.5263 | KD: 1060.9771\n",
      "Train Epoch: 018 Batch: 00032/00094 | Loss: 696.1956 | CE: 0.4372 | KD: 1060.4457\n",
      "Train Epoch: 018 Batch: 00033/00094 | Loss: 696.1805 | CE: 0.3691 | KD: 1060.5265\n",
      "Train Epoch: 018 Batch: 00034/00094 | Loss: 696.2499 | CE: 0.5094 | KD: 1060.4183\n",
      "Train Epoch: 018 Batch: 00035/00094 | Loss: 696.3729 | CE: 0.4086 | KD: 1060.7595\n",
      "Train Epoch: 018 Batch: 00036/00094 | Loss: 696.1170 | CE: 0.3574 | KD: 1060.4475\n",
      "Train Epoch: 018 Batch: 00037/00094 | Loss: 696.0554 | CE: 0.4249 | KD: 1060.2507\n",
      "Train Epoch: 018 Batch: 00038/00094 | Loss: 696.4361 | CE: 0.5726 | KD: 1060.6060\n",
      "Train Epoch: 018 Batch: 00039/00094 | Loss: 696.9548 | CE: 0.4809 | KD: 1061.5361\n",
      "Train Epoch: 018 Batch: 00040/00094 | Loss: 696.6313 | CE: 0.5067 | KD: 1061.0039\n",
      "Train Epoch: 018 Batch: 00041/00094 | Loss: 696.1336 | CE: 0.3951 | KD: 1060.4154\n",
      "Train Epoch: 018 Batch: 00042/00094 | Loss: 696.6901 | CE: 0.5044 | KD: 1061.0969\n",
      "Train Epoch: 018 Batch: 00043/00094 | Loss: 696.2538 | CE: 0.4836 | KD: 1060.4636\n",
      "Train Epoch: 018 Batch: 00044/00094 | Loss: 696.4072 | CE: 0.4916 | KD: 1060.6853\n",
      "Train Epoch: 018 Batch: 00045/00094 | Loss: 696.0754 | CE: 0.3459 | KD: 1060.4017\n",
      "Train Epoch: 018 Batch: 00046/00094 | Loss: 696.0995 | CE: 0.3821 | KD: 1060.3832\n",
      "Train Epoch: 018 Batch: 00047/00094 | Loss: 696.2274 | CE: 0.4887 | KD: 1060.4158\n",
      "Train Epoch: 018 Batch: 00048/00094 | Loss: 696.1801 | CE: 0.3812 | KD: 1060.5074\n",
      "Train Epoch: 018 Batch: 00049/00094 | Loss: 696.1843 | CE: 0.4891 | KD: 1060.3495\n",
      "Train Epoch: 018 Batch: 00050/00094 | Loss: 696.7171 | CE: 0.5422 | KD: 1061.0804\n",
      "Train Epoch: 018 Batch: 00051/00094 | Loss: 696.5500 | CE: 0.3780 | KD: 1061.0762\n",
      "Train Epoch: 018 Batch: 00052/00094 | Loss: 696.1064 | CE: 0.3070 | KD: 1060.5081\n",
      "Train Epoch: 018 Batch: 00053/00094 | Loss: 696.5121 | CE: 0.4278 | KD: 1060.9425\n",
      "Train Epoch: 018 Batch: 00054/00094 | Loss: 696.2620 | CE: 0.4778 | KD: 1060.4850\n",
      "Train Epoch: 018 Batch: 00055/00094 | Loss: 696.2961 | CE: 0.3643 | KD: 1060.7100\n",
      "Train Epoch: 018 Batch: 00056/00094 | Loss: 696.1912 | CE: 0.3889 | KD: 1060.5126\n",
      "Train Epoch: 018 Batch: 00057/00094 | Loss: 696.4017 | CE: 0.4392 | KD: 1060.7570\n",
      "Train Epoch: 018 Batch: 00058/00094 | Loss: 696.1915 | CE: 0.4165 | KD: 1060.4709\n",
      "Train Epoch: 018 Batch: 00059/00094 | Loss: 696.2759 | CE: 0.3308 | KD: 1060.7302\n",
      "Train Epoch: 018 Batch: 00060/00094 | Loss: 696.5485 | CE: 0.4300 | KD: 1060.9946\n",
      "Train Epoch: 018 Batch: 00061/00094 | Loss: 696.0341 | CE: 0.3589 | KD: 1060.3188\n",
      "Train Epoch: 018 Batch: 00062/00094 | Loss: 696.4228 | CE: 0.4664 | KD: 1060.7474\n",
      "Train Epoch: 018 Batch: 00063/00094 | Loss: 696.3568 | CE: 0.4525 | KD: 1060.6680\n",
      "Train Epoch: 018 Batch: 00064/00094 | Loss: 696.4379 | CE: 0.5154 | KD: 1060.6957\n",
      "Train Epoch: 018 Batch: 00065/00094 | Loss: 696.2291 | CE: 0.5629 | KD: 1060.3053\n",
      "Train Epoch: 018 Batch: 00066/00094 | Loss: 696.3559 | CE: 0.4639 | KD: 1060.6493\n",
      "Train Epoch: 018 Batch: 00067/00094 | Loss: 696.2762 | CE: 0.3630 | KD: 1060.6818\n",
      "Train Epoch: 018 Batch: 00068/00094 | Loss: 696.0129 | CE: 0.4027 | KD: 1060.2198\n",
      "Train Epoch: 018 Batch: 00069/00094 | Loss: 696.6813 | CE: 0.3743 | KD: 1061.2820\n",
      "Train Epoch: 018 Batch: 00070/00094 | Loss: 696.2593 | CE: 0.4405 | KD: 1060.5378\n",
      "Train Epoch: 018 Batch: 00071/00094 | Loss: 696.0214 | CE: 0.3721 | KD: 1060.2793\n",
      "Train Epoch: 018 Batch: 00072/00094 | Loss: 696.4214 | CE: 0.4003 | KD: 1060.8462\n",
      "Train Epoch: 018 Batch: 00073/00094 | Loss: 696.1243 | CE: 0.4158 | KD: 1060.3695\n",
      "Train Epoch: 018 Batch: 00074/00094 | Loss: 696.1837 | CE: 0.3925 | KD: 1060.4957\n",
      "Train Epoch: 018 Batch: 00075/00094 | Loss: 696.4849 | CE: 0.5396 | KD: 1060.7306\n",
      "Train Epoch: 018 Batch: 00076/00094 | Loss: 696.2427 | CE: 0.4065 | KD: 1060.5643\n",
      "Train Epoch: 018 Batch: 00077/00094 | Loss: 696.0895 | CE: 0.3700 | KD: 1060.3865\n",
      "Train Epoch: 018 Batch: 00078/00094 | Loss: 696.6270 | CE: 0.4997 | KD: 1061.0079\n",
      "Train Epoch: 018 Batch: 00079/00094 | Loss: 696.3544 | CE: 0.5427 | KD: 1060.5270\n",
      "Train Epoch: 018 Batch: 00080/00094 | Loss: 696.1268 | CE: 0.3859 | KD: 1060.4191\n",
      "Train Epoch: 018 Batch: 00081/00094 | Loss: 696.1765 | CE: 0.3975 | KD: 1060.4771\n",
      "Train Epoch: 018 Batch: 00082/00094 | Loss: 696.2957 | CE: 0.2955 | KD: 1060.8143\n",
      "Train Epoch: 018 Batch: 00083/00094 | Loss: 696.1854 | CE: 0.4585 | KD: 1060.3977\n",
      "Train Epoch: 018 Batch: 00084/00094 | Loss: 696.6318 | CE: 0.4328 | KD: 1061.1173\n",
      "Train Epoch: 018 Batch: 00085/00094 | Loss: 696.1849 | CE: 0.4286 | KD: 1060.4424\n",
      "Train Epoch: 018 Batch: 00086/00094 | Loss: 695.9677 | CE: 0.3309 | KD: 1060.2604\n",
      "Train Epoch: 018 Batch: 00087/00094 | Loss: 696.4626 | CE: 0.4436 | KD: 1060.8429\n",
      "Train Epoch: 018 Batch: 00088/00094 | Loss: 696.1462 | CE: 0.4123 | KD: 1060.4083\n",
      "Train Epoch: 018 Batch: 00089/00094 | Loss: 695.9548 | CE: 0.3687 | KD: 1060.1831\n",
      "Train Epoch: 018 Batch: 00090/00094 | Loss: 696.2636 | CE: 0.4371 | KD: 1060.5496\n",
      "Train Epoch: 018 Batch: 00091/00094 | Loss: 696.0359 | CE: 0.3298 | KD: 1060.3661\n",
      "Train Epoch: 018 Batch: 00092/00094 | Loss: 696.1616 | CE: 0.3356 | KD: 1060.5487\n",
      "Train Epoch: 018 Batch: 00093/00094 | Loss: 696.1027 | CE: 0.3318 | KD: 1060.4648\n",
      "Train Epoch: 018 Batch: 00094/00094 | Loss: 695.8868 | CE: 0.2978 | KD: 1060.1875\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3979 | acc:83.2000\n",
      "[VAL Acc] Target: 83.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3022 | acc:48.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9366 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2559 | acc:47.3282\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8911 | acc:49.6473\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 49.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8692 | acc:52.0333\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6227 | acc:64.9295\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.93%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9282 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7280 | acc:61.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.70%\n",
      "[VAL Acc] Avg 56.55%\n",
      "Train Epoch: 019 Batch: 00001/00094 | Loss: 696.3483 | CE: 0.5674 | KD: 1060.4800\n",
      "Train Epoch: 019 Batch: 00002/00094 | Loss: 696.3358 | CE: 0.4436 | KD: 1060.6495\n",
      "Train Epoch: 019 Batch: 00003/00094 | Loss: 696.0331 | CE: 0.3766 | KD: 1060.2904\n",
      "Train Epoch: 019 Batch: 00004/00094 | Loss: 696.3583 | CE: 0.3629 | KD: 1060.8070\n",
      "Train Epoch: 019 Batch: 00005/00094 | Loss: 696.3101 | CE: 0.5455 | KD: 1060.4552\n",
      "Train Epoch: 019 Batch: 00006/00094 | Loss: 696.2892 | CE: 0.3971 | KD: 1060.6495\n",
      "Train Epoch: 019 Batch: 00007/00094 | Loss: 696.0782 | CE: 0.4388 | KD: 1060.2644\n",
      "Train Epoch: 019 Batch: 00008/00094 | Loss: 696.1389 | CE: 0.3947 | KD: 1060.4241\n",
      "Train Epoch: 019 Batch: 00009/00094 | Loss: 696.3229 | CE: 0.4291 | KD: 1060.6521\n",
      "Train Epoch: 019 Batch: 00010/00094 | Loss: 696.4379 | CE: 0.5175 | KD: 1060.6927\n",
      "Train Epoch: 019 Batch: 00011/00094 | Loss: 696.5508 | CE: 0.4526 | KD: 1060.9636\n",
      "Train Epoch: 019 Batch: 00012/00094 | Loss: 696.2151 | CE: 0.3697 | KD: 1060.5782\n",
      "Train Epoch: 019 Batch: 00013/00094 | Loss: 695.9932 | CE: 0.3130 | KD: 1060.3265\n",
      "Train Epoch: 019 Batch: 00014/00094 | Loss: 696.2648 | CE: 0.3947 | KD: 1060.6158\n",
      "Train Epoch: 019 Batch: 00015/00094 | Loss: 696.0804 | CE: 0.4043 | KD: 1060.3203\n",
      "Train Epoch: 019 Batch: 00016/00094 | Loss: 696.9250 | CE: 0.4389 | KD: 1061.5548\n",
      "Train Epoch: 019 Batch: 00017/00094 | Loss: 696.1099 | CE: 0.4563 | KD: 1060.2860\n",
      "Train Epoch: 019 Batch: 00018/00094 | Loss: 696.3281 | CE: 0.4698 | KD: 1060.5979\n",
      "Train Epoch: 019 Batch: 00019/00094 | Loss: 696.2050 | CE: 0.4456 | KD: 1060.4471\n",
      "Train Epoch: 019 Batch: 00020/00094 | Loss: 696.3266 | CE: 0.3739 | KD: 1060.7418\n",
      "Train Epoch: 019 Batch: 00021/00094 | Loss: 696.5139 | CE: 0.3075 | KD: 1061.1285\n",
      "Train Epoch: 019 Batch: 00022/00094 | Loss: 695.9562 | CE: 0.3603 | KD: 1060.1981\n",
      "Train Epoch: 019 Batch: 00023/00094 | Loss: 696.2504 | CE: 0.3452 | KD: 1060.6694\n",
      "Train Epoch: 019 Batch: 00024/00094 | Loss: 695.9743 | CE: 0.2709 | KD: 1060.3619\n",
      "Train Epoch: 019 Batch: 00025/00094 | Loss: 696.2553 | CE: 0.4135 | KD: 1060.5729\n",
      "Train Epoch: 019 Batch: 00026/00094 | Loss: 696.5447 | CE: 0.6428 | KD: 1060.6644\n",
      "Train Epoch: 019 Batch: 00027/00094 | Loss: 696.1300 | CE: 0.3910 | KD: 1060.4161\n",
      "Train Epoch: 019 Batch: 00028/00094 | Loss: 696.2899 | CE: 0.4068 | KD: 1060.6357\n",
      "Train Epoch: 019 Batch: 00029/00094 | Loss: 695.9170 | CE: 0.2600 | KD: 1060.2911\n",
      "Train Epoch: 019 Batch: 00030/00094 | Loss: 696.6678 | CE: 0.4323 | KD: 1061.1730\n",
      "Train Epoch: 019 Batch: 00031/00094 | Loss: 696.2339 | CE: 0.4640 | KD: 1060.4633\n",
      "Train Epoch: 019 Batch: 00032/00094 | Loss: 696.1138 | CE: 0.3725 | KD: 1060.4196\n",
      "Train Epoch: 019 Batch: 00033/00094 | Loss: 696.1196 | CE: 0.3667 | KD: 1060.4373\n",
      "Train Epoch: 019 Batch: 00034/00094 | Loss: 696.3173 | CE: 0.3487 | KD: 1060.7661\n",
      "Train Epoch: 019 Batch: 00035/00094 | Loss: 696.0273 | CE: 0.3603 | KD: 1060.3064\n",
      "Train Epoch: 019 Batch: 00036/00094 | Loss: 696.6121 | CE: 0.4413 | KD: 1061.0742\n",
      "Train Epoch: 019 Batch: 00037/00094 | Loss: 696.0222 | CE: 0.3656 | KD: 1060.2905\n",
      "Train Epoch: 019 Batch: 00038/00094 | Loss: 696.1439 | CE: 0.3626 | KD: 1060.4806\n",
      "Train Epoch: 019 Batch: 00039/00094 | Loss: 696.4185 | CE: 0.5275 | KD: 1060.6478\n",
      "Train Epoch: 019 Batch: 00040/00094 | Loss: 696.2784 | CE: 0.4965 | KD: 1060.4814\n",
      "Train Epoch: 019 Batch: 00041/00094 | Loss: 696.3047 | CE: 0.4743 | KD: 1060.5555\n",
      "Train Epoch: 019 Batch: 00042/00094 | Loss: 696.6136 | CE: 0.5445 | KD: 1060.9194\n",
      "Train Epoch: 019 Batch: 00043/00094 | Loss: 696.2598 | CE: 0.4133 | KD: 1060.5800\n",
      "Train Epoch: 019 Batch: 00044/00094 | Loss: 696.2610 | CE: 0.5008 | KD: 1060.4484\n",
      "Train Epoch: 019 Batch: 00045/00094 | Loss: 696.4439 | CE: 0.4132 | KD: 1060.8606\n",
      "Train Epoch: 019 Batch: 00046/00094 | Loss: 696.1340 | CE: 0.3472 | KD: 1060.4890\n",
      "Train Epoch: 019 Batch: 00047/00094 | Loss: 696.2543 | CE: 0.4781 | KD: 1060.4729\n",
      "Train Epoch: 019 Batch: 00048/00094 | Loss: 696.2874 | CE: 0.5263 | KD: 1060.4497\n",
      "Train Epoch: 019 Batch: 00049/00094 | Loss: 696.4222 | CE: 0.4987 | KD: 1060.6973\n",
      "Train Epoch: 019 Batch: 00050/00094 | Loss: 696.5615 | CE: 0.4323 | KD: 1061.0109\n",
      "Train Epoch: 019 Batch: 00051/00094 | Loss: 696.4054 | CE: 0.3665 | KD: 1060.8732\n",
      "Train Epoch: 019 Batch: 00052/00094 | Loss: 696.3772 | CE: 0.4539 | KD: 1060.6971\n",
      "Train Epoch: 019 Batch: 00053/00094 | Loss: 696.2655 | CE: 0.4949 | KD: 1060.4642\n",
      "Train Epoch: 019 Batch: 00054/00094 | Loss: 696.1403 | CE: 0.3623 | KD: 1060.4756\n",
      "Train Epoch: 019 Batch: 00055/00094 | Loss: 696.4045 | CE: 0.4544 | KD: 1060.7380\n",
      "Train Epoch: 019 Batch: 00056/00094 | Loss: 696.1812 | CE: 0.4799 | KD: 1060.3586\n",
      "Train Epoch: 019 Batch: 00057/00094 | Loss: 696.1036 | CE: 0.4436 | KD: 1060.2958\n",
      "Train Epoch: 019 Batch: 00058/00094 | Loss: 696.1840 | CE: 0.4645 | KD: 1060.3865\n",
      "Train Epoch: 019 Batch: 00059/00094 | Loss: 696.3666 | CE: 0.4042 | KD: 1060.7567\n",
      "Train Epoch: 019 Batch: 00060/00094 | Loss: 696.1022 | CE: 0.3854 | KD: 1060.3823\n",
      "Train Epoch: 019 Batch: 00061/00094 | Loss: 696.1438 | CE: 0.5287 | KD: 1060.2272\n",
      "Train Epoch: 019 Batch: 00062/00094 | Loss: 696.2393 | CE: 0.3715 | KD: 1060.6124\n",
      "Train Epoch: 019 Batch: 00063/00094 | Loss: 696.3391 | CE: 0.4999 | KD: 1060.5688\n",
      "Train Epoch: 019 Batch: 00064/00094 | Loss: 696.3252 | CE: 0.4660 | KD: 1060.5994\n",
      "Train Epoch: 019 Batch: 00065/00094 | Loss: 696.4670 | CE: 0.4684 | KD: 1060.8118\n",
      "Train Epoch: 019 Batch: 00066/00094 | Loss: 696.3655 | CE: 0.3809 | KD: 1060.7905\n",
      "Train Epoch: 019 Batch: 00067/00094 | Loss: 696.8101 | CE: 0.4326 | KD: 1061.3893\n",
      "Train Epoch: 019 Batch: 00068/00094 | Loss: 695.8579 | CE: 0.3295 | KD: 1060.0952\n",
      "Train Epoch: 019 Batch: 00069/00094 | Loss: 696.2502 | CE: 0.3625 | KD: 1060.6427\n",
      "Train Epoch: 019 Batch: 00070/00094 | Loss: 696.1599 | CE: 0.4236 | KD: 1060.4119\n",
      "Train Epoch: 019 Batch: 00071/00094 | Loss: 696.0173 | CE: 0.4140 | KD: 1060.2094\n",
      "Train Epoch: 019 Batch: 00072/00094 | Loss: 696.6936 | CE: 0.8742 | KD: 1060.5387\n",
      "Train Epoch: 019 Batch: 00073/00094 | Loss: 696.3784 | CE: 0.4842 | KD: 1060.6527\n",
      "Train Epoch: 019 Batch: 00074/00094 | Loss: 696.4540 | CE: 0.6070 | KD: 1060.5808\n",
      "Train Epoch: 019 Batch: 00075/00094 | Loss: 696.1592 | CE: 0.3493 | KD: 1060.5243\n",
      "Train Epoch: 019 Batch: 00076/00094 | Loss: 696.4267 | CE: 0.3782 | KD: 1060.8879\n",
      "Train Epoch: 019 Batch: 00077/00094 | Loss: 696.1832 | CE: 0.3612 | KD: 1060.5426\n",
      "Train Epoch: 019 Batch: 00078/00094 | Loss: 696.1252 | CE: 0.3269 | KD: 1060.5067\n",
      "Train Epoch: 019 Batch: 00079/00094 | Loss: 696.0923 | CE: 0.3841 | KD: 1060.3693\n",
      "Train Epoch: 019 Batch: 00080/00094 | Loss: 696.2478 | CE: 0.4567 | KD: 1060.4955\n",
      "Train Epoch: 019 Batch: 00081/00094 | Loss: 696.6462 | CE: 0.4628 | KD: 1061.0934\n",
      "Train Epoch: 019 Batch: 00082/00094 | Loss: 696.7500 | CE: 0.5028 | KD: 1061.1907\n",
      "Train Epoch: 019 Batch: 00083/00094 | Loss: 696.3109 | CE: 0.4040 | KD: 1060.6720\n",
      "Train Epoch: 019 Batch: 00084/00094 | Loss: 696.3560 | CE: 0.5927 | KD: 1060.4531\n",
      "Train Epoch: 019 Batch: 00085/00094 | Loss: 696.6719 | CE: 0.5476 | KD: 1061.0035\n",
      "Train Epoch: 019 Batch: 00086/00094 | Loss: 696.6846 | CE: 0.4733 | KD: 1061.1360\n",
      "Train Epoch: 019 Batch: 00087/00094 | Loss: 696.6367 | CE: 0.3452 | KD: 1061.2583\n",
      "Train Epoch: 019 Batch: 00088/00094 | Loss: 695.9789 | CE: 0.3488 | KD: 1060.2502\n",
      "Train Epoch: 019 Batch: 00089/00094 | Loss: 696.1827 | CE: 0.3920 | KD: 1060.4950\n",
      "Train Epoch: 019 Batch: 00090/00094 | Loss: 696.3533 | CE: 0.4256 | KD: 1060.7037\n",
      "Train Epoch: 019 Batch: 00091/00094 | Loss: 696.0326 | CE: 0.3221 | KD: 1060.3727\n",
      "Train Epoch: 019 Batch: 00092/00094 | Loss: 696.5326 | CE: 0.4359 | KD: 1060.9612\n",
      "Train Epoch: 019 Batch: 00093/00094 | Loss: 696.2726 | CE: 0.5244 | KD: 1060.4302\n",
      "Train Epoch: 019 Batch: 00094/00094 | Loss: 696.1192 | CE: 0.3489 | KD: 1060.4637\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.5154 | acc:79.5500\n",
      "[VAL Acc] Target: 79.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.4610 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0284 | acc:53.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.4501 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8906 | acc:50.9013\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8003 | acc:53.5120\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6237 | acc:64.0282\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8676 | acc:48.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.8420 | acc:57.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 57.45%\n",
      "[VAL Acc] Avg 56.08%\n",
      "Train Epoch: 020 Batch: 00001/00094 | Loss: 696.4492 | CE: 0.4257 | KD: 1060.8497\n",
      "Train Epoch: 020 Batch: 00002/00094 | Loss: 696.6047 | CE: 0.4583 | KD: 1061.0370\n",
      "Train Epoch: 020 Batch: 00003/00094 | Loss: 696.2056 | CE: 0.4388 | KD: 1060.4586\n",
      "Train Epoch: 020 Batch: 00004/00094 | Loss: 696.5876 | CE: 0.3421 | KD: 1061.1881\n",
      "Train Epoch: 020 Batch: 00005/00094 | Loss: 696.2755 | CE: 0.5449 | KD: 1060.4033\n",
      "Train Epoch: 020 Batch: 00006/00094 | Loss: 696.3531 | CE: 0.3639 | KD: 1060.7976\n",
      "Train Epoch: 020 Batch: 00007/00094 | Loss: 696.2472 | CE: 0.3613 | KD: 1060.6400\n",
      "Train Epoch: 020 Batch: 00008/00094 | Loss: 696.2848 | CE: 0.3916 | KD: 1060.6511\n",
      "Train Epoch: 020 Batch: 00009/00094 | Loss: 696.0778 | CE: 0.4100 | KD: 1060.3076\n",
      "Train Epoch: 020 Batch: 00010/00094 | Loss: 696.6049 | CE: 0.5269 | KD: 1060.9329\n",
      "Train Epoch: 020 Batch: 00011/00094 | Loss: 696.5911 | CE: 0.4321 | KD: 1061.0564\n",
      "Train Epoch: 020 Batch: 00012/00094 | Loss: 696.3622 | CE: 0.4259 | KD: 1060.7168\n",
      "Train Epoch: 020 Batch: 00013/00094 | Loss: 696.3289 | CE: 0.3903 | KD: 1060.7203\n",
      "Train Epoch: 020 Batch: 00014/00094 | Loss: 696.6186 | CE: 0.4709 | KD: 1061.0391\n",
      "Train Epoch: 020 Batch: 00015/00094 | Loss: 696.5506 | CE: 0.6166 | KD: 1060.7134\n",
      "Train Epoch: 020 Batch: 00016/00094 | Loss: 696.7555 | CE: 0.7106 | KD: 1060.8824\n",
      "Train Epoch: 020 Batch: 00017/00094 | Loss: 696.2936 | CE: 0.4063 | KD: 1060.6422\n",
      "Train Epoch: 020 Batch: 00018/00094 | Loss: 696.4153 | CE: 0.4794 | KD: 1060.7162\n",
      "Train Epoch: 020 Batch: 00019/00094 | Loss: 696.1424 | CE: 0.3444 | KD: 1060.5060\n",
      "Train Epoch: 020 Batch: 00020/00094 | Loss: 696.4828 | CE: 0.4906 | KD: 1060.8021\n",
      "Train Epoch: 020 Batch: 00021/00094 | Loss: 696.5495 | CE: 0.4273 | KD: 1061.0002\n",
      "Train Epoch: 020 Batch: 00022/00094 | Loss: 696.3017 | CE: 0.4739 | KD: 1060.5514\n",
      "Train Epoch: 020 Batch: 00023/00094 | Loss: 696.0617 | CE: 0.3045 | KD: 1060.4440\n",
      "Train Epoch: 020 Batch: 00024/00094 | Loss: 695.8754 | CE: 0.3036 | KD: 1060.1613\n",
      "Train Epoch: 020 Batch: 00025/00094 | Loss: 696.1743 | CE: 0.3817 | KD: 1060.4978\n",
      "Train Epoch: 020 Batch: 00026/00094 | Loss: 696.3579 | CE: 0.4082 | KD: 1060.7373\n",
      "Train Epoch: 020 Batch: 00027/00094 | Loss: 696.4528 | CE: 0.4565 | KD: 1060.8082\n",
      "Train Epoch: 020 Batch: 00028/00094 | Loss: 696.1996 | CE: 0.4124 | KD: 1060.4896\n",
      "Train Epoch: 020 Batch: 00029/00094 | Loss: 696.3209 | CE: 0.3999 | KD: 1060.6936\n",
      "Train Epoch: 020 Batch: 00030/00094 | Loss: 696.0241 | CE: 0.4036 | KD: 1060.2355\n",
      "Train Epoch: 020 Batch: 00031/00094 | Loss: 696.2006 | CE: 0.3378 | KD: 1060.6047\n",
      "Train Epoch: 020 Batch: 00032/00094 | Loss: 696.2685 | CE: 0.4580 | KD: 1060.5251\n",
      "Train Epoch: 020 Batch: 00033/00094 | Loss: 696.4850 | CE: 0.4691 | KD: 1060.8383\n",
      "Train Epoch: 020 Batch: 00034/00094 | Loss: 696.1753 | CE: 0.4152 | KD: 1060.4484\n",
      "Train Epoch: 020 Batch: 00035/00094 | Loss: 696.0257 | CE: 0.3703 | KD: 1060.2886\n",
      "Train Epoch: 020 Batch: 00036/00094 | Loss: 696.1877 | CE: 0.4610 | KD: 1060.3975\n",
      "Train Epoch: 020 Batch: 00037/00094 | Loss: 696.0507 | CE: 0.3007 | KD: 1060.4329\n",
      "Train Epoch: 020 Batch: 00038/00094 | Loss: 696.3000 | CE: 0.4133 | KD: 1060.6412\n",
      "Train Epoch: 020 Batch: 00039/00094 | Loss: 696.1218 | CE: 0.4195 | KD: 1060.3604\n",
      "Train Epoch: 020 Batch: 00040/00094 | Loss: 696.1072 | CE: 0.3393 | KD: 1060.4603\n",
      "Train Epoch: 020 Batch: 00041/00094 | Loss: 696.0213 | CE: 0.3108 | KD: 1060.3727\n",
      "Train Epoch: 020 Batch: 00042/00094 | Loss: 696.1440 | CE: 0.4093 | KD: 1060.4097\n",
      "Train Epoch: 020 Batch: 00043/00094 | Loss: 696.4456 | CE: 0.3373 | KD: 1060.9790\n",
      "Train Epoch: 020 Batch: 00044/00094 | Loss: 696.2115 | CE: 0.4263 | KD: 1060.4866\n",
      "Train Epoch: 020 Batch: 00045/00094 | Loss: 695.9764 | CE: 0.2917 | KD: 1060.3334\n",
      "Train Epoch: 020 Batch: 00046/00094 | Loss: 696.1255 | CE: 0.3243 | KD: 1060.5110\n",
      "Train Epoch: 020 Batch: 00047/00094 | Loss: 696.2704 | CE: 0.4456 | KD: 1060.5469\n",
      "Train Epoch: 020 Batch: 00048/00094 | Loss: 696.1309 | CE: 0.3350 | KD: 1060.5029\n",
      "Train Epoch: 020 Batch: 00049/00094 | Loss: 696.6453 | CE: 0.4115 | KD: 1061.1702\n",
      "Train Epoch: 020 Batch: 00050/00094 | Loss: 696.4067 | CE: 0.5450 | KD: 1060.6033\n",
      "Train Epoch: 020 Batch: 00051/00094 | Loss: 696.5600 | CE: 0.4297 | KD: 1061.0125\n",
      "Train Epoch: 020 Batch: 00052/00094 | Loss: 696.3064 | CE: 0.3721 | KD: 1060.7139\n",
      "Train Epoch: 020 Batch: 00053/00094 | Loss: 696.1901 | CE: 0.4961 | KD: 1060.3475\n",
      "Train Epoch: 020 Batch: 00054/00094 | Loss: 696.3015 | CE: 0.3385 | KD: 1060.7574\n",
      "Train Epoch: 020 Batch: 00055/00094 | Loss: 696.3085 | CE: 0.4484 | KD: 1060.6007\n",
      "Train Epoch: 020 Batch: 00056/00094 | Loss: 696.1426 | CE: 0.4071 | KD: 1060.4109\n",
      "Train Epoch: 020 Batch: 00057/00094 | Loss: 696.2510 | CE: 0.4258 | KD: 1060.5476\n",
      "Train Epoch: 020 Batch: 00058/00094 | Loss: 696.2021 | CE: 0.4885 | KD: 1060.3773\n",
      "Train Epoch: 020 Batch: 00059/00094 | Loss: 696.2971 | CE: 0.4105 | KD: 1060.6410\n",
      "Train Epoch: 020 Batch: 00060/00094 | Loss: 696.3168 | CE: 0.5165 | KD: 1060.5095\n",
      "Train Epoch: 020 Batch: 00061/00094 | Loss: 696.0871 | CE: 0.3048 | KD: 1060.4822\n",
      "Train Epoch: 020 Batch: 00062/00094 | Loss: 696.3381 | CE: 0.4695 | KD: 1060.6136\n",
      "Train Epoch: 020 Batch: 00063/00094 | Loss: 696.4957 | CE: 0.3484 | KD: 1061.0385\n",
      "Train Epoch: 020 Batch: 00064/00094 | Loss: 696.1678 | CE: 0.2990 | KD: 1060.6139\n",
      "Train Epoch: 020 Batch: 00065/00094 | Loss: 696.0113 | CE: 0.3651 | KD: 1060.2748\n",
      "Train Epoch: 020 Batch: 00066/00094 | Loss: 696.4273 | CE: 0.4219 | KD: 1060.8221\n",
      "Train Epoch: 020 Batch: 00067/00094 | Loss: 696.4574 | CE: 0.4090 | KD: 1060.8877\n",
      "Train Epoch: 020 Batch: 00068/00094 | Loss: 696.3197 | CE: 0.3826 | KD: 1060.7180\n",
      "Train Epoch: 020 Batch: 00069/00094 | Loss: 696.2924 | CE: 0.3386 | KD: 1060.7437\n",
      "Train Epoch: 020 Batch: 00070/00094 | Loss: 696.2769 | CE: 0.3860 | KD: 1060.6476\n",
      "Train Epoch: 020 Batch: 00071/00094 | Loss: 696.1365 | CE: 0.3321 | KD: 1060.5159\n",
      "Train Epoch: 020 Batch: 00072/00094 | Loss: 696.1922 | CE: 0.3582 | KD: 1060.5609\n",
      "Train Epoch: 020 Batch: 00073/00094 | Loss: 696.2408 | CE: 0.4146 | KD: 1060.5491\n",
      "Train Epoch: 020 Batch: 00074/00094 | Loss: 696.0041 | CE: 0.4006 | KD: 1060.2096\n",
      "Train Epoch: 020 Batch: 00075/00094 | Loss: 696.6509 | CE: 0.5242 | KD: 1061.0071\n",
      "Train Epoch: 020 Batch: 00076/00094 | Loss: 696.2044 | CE: 0.4804 | KD: 1060.3933\n",
      "Train Epoch: 020 Batch: 00077/00094 | Loss: 696.0571 | CE: 0.3786 | KD: 1060.3241\n",
      "Train Epoch: 020 Batch: 00078/00094 | Loss: 696.0889 | CE: 0.3515 | KD: 1060.4138\n",
      "Train Epoch: 020 Batch: 00079/00094 | Loss: 696.1689 | CE: 0.3594 | KD: 1060.5236\n",
      "Train Epoch: 020 Batch: 00080/00094 | Loss: 696.1752 | CE: 0.3572 | KD: 1060.5365\n",
      "Train Epoch: 020 Batch: 00081/00094 | Loss: 696.1573 | CE: 0.4668 | KD: 1060.3423\n",
      "Train Epoch: 020 Batch: 00082/00094 | Loss: 696.4431 | CE: 0.5118 | KD: 1060.7092\n",
      "Train Epoch: 020 Batch: 00083/00094 | Loss: 696.1504 | CE: 0.4013 | KD: 1060.4315\n",
      "Train Epoch: 020 Batch: 00084/00094 | Loss: 696.0875 | CE: 0.4420 | KD: 1060.2736\n",
      "Train Epoch: 020 Batch: 00085/00094 | Loss: 696.1926 | CE: 0.3792 | KD: 1060.5295\n",
      "Train Epoch: 020 Batch: 00086/00094 | Loss: 696.1291 | CE: 0.3022 | KD: 1060.5500\n",
      "Train Epoch: 020 Batch: 00087/00094 | Loss: 696.0435 | CE: 0.3766 | KD: 1060.3063\n",
      "Train Epoch: 020 Batch: 00088/00094 | Loss: 695.8300 | CE: 0.2869 | KD: 1060.1174\n",
      "Train Epoch: 020 Batch: 00089/00094 | Loss: 696.2173 | CE: 0.3837 | KD: 1060.5603\n",
      "Train Epoch: 020 Batch: 00090/00094 | Loss: 696.1698 | CE: 0.3554 | KD: 1060.5310\n",
      "Train Epoch: 020 Batch: 00091/00094 | Loss: 696.4433 | CE: 0.4358 | KD: 1060.8253\n",
      "Train Epoch: 020 Batch: 00092/00094 | Loss: 696.0599 | CE: 0.3217 | KD: 1060.4150\n",
      "Train Epoch: 020 Batch: 00093/00094 | Loss: 696.2951 | CE: 0.4716 | KD: 1060.5449\n",
      "Train Epoch: 020 Batch: 00094/00094 | Loss: 696.2305 | CE: 0.3724 | KD: 1060.5978\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4669 | acc:81.0500\n",
      "[VAL Acc] Target: 81.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2917 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9805 | acc:53.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2897 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9332 | acc:50.0784\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8217 | acc:53.8817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6665 | acc:62.8918\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8791 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7753 | acc:58.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 58.65%\n",
      "[VAL Acc] Avg 56.38%\n",
      "Train Epoch: 021 Batch: 00001/00094 | Loss: 696.2040 | CE: 0.4273 | KD: 1060.4736\n",
      "Train Epoch: 021 Batch: 00002/00094 | Loss: 696.3539 | CE: 0.5450 | KD: 1060.5228\n",
      "Train Epoch: 021 Batch: 00003/00094 | Loss: 696.1071 | CE: 0.3580 | KD: 1060.4315\n",
      "Train Epoch: 021 Batch: 00004/00094 | Loss: 696.0446 | CE: 0.4093 | KD: 1060.2581\n",
      "Train Epoch: 021 Batch: 00005/00094 | Loss: 696.0929 | CE: 0.3725 | KD: 1060.3877\n",
      "Train Epoch: 021 Batch: 00006/00094 | Loss: 696.1204 | CE: 0.3133 | KD: 1060.5200\n",
      "Train Epoch: 021 Batch: 00007/00094 | Loss: 696.5668 | CE: 0.5535 | KD: 1060.8344\n",
      "Train Epoch: 021 Batch: 00008/00094 | Loss: 696.3444 | CE: 0.4962 | KD: 1060.5826\n",
      "Train Epoch: 021 Batch: 00009/00094 | Loss: 696.2526 | CE: 0.4449 | KD: 1060.5209\n",
      "Train Epoch: 021 Batch: 00010/00094 | Loss: 696.0993 | CE: 0.3613 | KD: 1060.4147\n",
      "Train Epoch: 021 Batch: 00011/00094 | Loss: 696.0913 | CE: 0.3943 | KD: 1060.3522\n",
      "Train Epoch: 021 Batch: 00012/00094 | Loss: 696.5218 | CE: 0.4366 | KD: 1060.9437\n",
      "Train Epoch: 021 Batch: 00013/00094 | Loss: 696.3692 | CE: 0.4265 | KD: 1060.7267\n",
      "Train Epoch: 021 Batch: 00014/00094 | Loss: 696.1030 | CE: 0.3751 | KD: 1060.3992\n",
      "Train Epoch: 021 Batch: 00015/00094 | Loss: 696.2344 | CE: 0.4163 | KD: 1060.5367\n",
      "Train Epoch: 021 Batch: 00016/00094 | Loss: 696.4655 | CE: 0.3459 | KD: 1060.9962\n",
      "Train Epoch: 021 Batch: 00017/00094 | Loss: 696.3782 | CE: 0.3482 | KD: 1060.8596\n",
      "Train Epoch: 021 Batch: 00018/00094 | Loss: 696.3653 | CE: 0.5016 | KD: 1060.6061\n",
      "Train Epoch: 021 Batch: 00019/00094 | Loss: 696.2143 | CE: 0.3924 | KD: 1060.5424\n",
      "Train Epoch: 021 Batch: 00020/00094 | Loss: 696.5991 | CE: 0.6594 | KD: 1060.7220\n",
      "Train Epoch: 021 Batch: 00021/00094 | Loss: 696.4728 | CE: 0.2642 | KD: 1061.1318\n",
      "Train Epoch: 021 Batch: 00022/00094 | Loss: 696.1180 | CE: 0.4052 | KD: 1060.3762\n",
      "Train Epoch: 021 Batch: 00023/00094 | Loss: 696.1760 | CE: 0.4972 | KD: 1060.3245\n",
      "Train Epoch: 021 Batch: 00024/00094 | Loss: 696.4879 | CE: 0.5537 | KD: 1060.7136\n",
      "Train Epoch: 021 Batch: 00025/00094 | Loss: 696.2201 | CE: 0.2969 | KD: 1060.6968\n",
      "Train Epoch: 021 Batch: 00026/00094 | Loss: 696.5168 | CE: 0.4367 | KD: 1060.9359\n",
      "Train Epoch: 021 Batch: 00027/00094 | Loss: 696.0961 | CE: 0.3855 | KD: 1060.3727\n",
      "Train Epoch: 021 Batch: 00028/00094 | Loss: 696.1984 | CE: 0.3513 | KD: 1060.5809\n",
      "Train Epoch: 021 Batch: 00029/00094 | Loss: 696.3676 | CE: 0.3928 | KD: 1060.7755\n",
      "Train Epoch: 021 Batch: 00030/00094 | Loss: 696.0854 | CE: 0.3995 | KD: 1060.3351\n",
      "Train Epoch: 021 Batch: 00031/00094 | Loss: 696.3160 | CE: 0.4669 | KD: 1060.5840\n",
      "Train Epoch: 021 Batch: 00032/00094 | Loss: 696.1002 | CE: 0.3705 | KD: 1060.4019\n",
      "Train Epoch: 021 Batch: 00033/00094 | Loss: 696.0276 | CE: 0.3137 | KD: 1060.3778\n",
      "Train Epoch: 021 Batch: 00034/00094 | Loss: 696.1070 | CE: 0.3458 | KD: 1060.4500\n",
      "Train Epoch: 021 Batch: 00035/00094 | Loss: 696.3168 | CE: 0.4284 | KD: 1060.6439\n",
      "Train Epoch: 021 Batch: 00036/00094 | Loss: 696.3468 | CE: 0.5471 | KD: 1060.5087\n",
      "Train Epoch: 021 Batch: 00037/00094 | Loss: 696.1339 | CE: 0.3534 | KD: 1060.4794\n",
      "Train Epoch: 021 Batch: 00038/00094 | Loss: 696.1558 | CE: 0.3843 | KD: 1060.4656\n",
      "Train Epoch: 021 Batch: 00039/00094 | Loss: 696.0934 | CE: 0.4139 | KD: 1060.3253\n",
      "Train Epoch: 021 Batch: 00040/00094 | Loss: 696.2397 | CE: 0.4021 | KD: 1060.5665\n",
      "Train Epoch: 021 Batch: 00041/00094 | Loss: 695.9434 | CE: 0.2798 | KD: 1060.3011\n",
      "Train Epoch: 021 Batch: 00042/00094 | Loss: 696.2899 | CE: 0.3664 | KD: 1060.6974\n",
      "Train Epoch: 021 Batch: 00043/00094 | Loss: 696.2486 | CE: 0.3371 | KD: 1060.6791\n",
      "Train Epoch: 021 Batch: 00044/00094 | Loss: 696.0330 | CE: 0.3842 | KD: 1060.2786\n",
      "Train Epoch: 021 Batch: 00045/00094 | Loss: 696.3993 | CE: 0.4092 | KD: 1060.7988\n",
      "Train Epoch: 021 Batch: 00046/00094 | Loss: 696.1982 | CE: 0.4623 | KD: 1060.4114\n",
      "Train Epoch: 021 Batch: 00047/00094 | Loss: 696.6435 | CE: 0.3651 | KD: 1061.2383\n",
      "Train Epoch: 021 Batch: 00048/00094 | Loss: 696.3537 | CE: 0.3706 | KD: 1060.7882\n",
      "Train Epoch: 021 Batch: 00049/00094 | Loss: 696.3099 | CE: 0.3817 | KD: 1060.7045\n",
      "Train Epoch: 021 Batch: 00050/00094 | Loss: 696.5590 | CE: 0.4790 | KD: 1060.9358\n",
      "Train Epoch: 021 Batch: 00051/00094 | Loss: 696.4553 | CE: 0.4554 | KD: 1060.8137\n",
      "Train Epoch: 021 Batch: 00052/00094 | Loss: 696.2139 | CE: 0.3598 | KD: 1060.5916\n",
      "Train Epoch: 021 Batch: 00053/00094 | Loss: 696.2990 | CE: 0.4139 | KD: 1060.6388\n",
      "Train Epoch: 021 Batch: 00054/00094 | Loss: 696.1749 | CE: 0.4351 | KD: 1060.4172\n",
      "Train Epoch: 021 Batch: 00055/00094 | Loss: 696.4163 | CE: 0.4470 | KD: 1060.7671\n",
      "Train Epoch: 021 Batch: 00056/00094 | Loss: 696.3417 | CE: 0.4112 | KD: 1060.7080\n",
      "Train Epoch: 021 Batch: 00057/00094 | Loss: 696.3499 | CE: 0.3974 | KD: 1060.7415\n",
      "Train Epoch: 021 Batch: 00058/00094 | Loss: 696.2687 | CE: 0.4017 | KD: 1060.6113\n",
      "Train Epoch: 021 Batch: 00059/00094 | Loss: 696.3553 | CE: 0.4798 | KD: 1060.6244\n",
      "Train Epoch: 021 Batch: 00060/00094 | Loss: 696.2381 | CE: 0.3612 | KD: 1060.6263\n",
      "Train Epoch: 021 Batch: 00061/00094 | Loss: 696.2358 | CE: 0.3529 | KD: 1060.6354\n",
      "Train Epoch: 021 Batch: 00062/00094 | Loss: 696.0785 | CE: 0.4024 | KD: 1060.3203\n",
      "Train Epoch: 021 Batch: 00063/00094 | Loss: 696.1408 | CE: 0.4552 | KD: 1060.3347\n",
      "Train Epoch: 021 Batch: 00064/00094 | Loss: 696.2459 | CE: 0.4637 | KD: 1060.4821\n",
      "Train Epoch: 021 Batch: 00065/00094 | Loss: 696.5609 | CE: 0.4979 | KD: 1060.9100\n",
      "Train Epoch: 021 Batch: 00066/00094 | Loss: 696.0743 | CE: 0.3388 | KD: 1060.4108\n",
      "Train Epoch: 021 Batch: 00067/00094 | Loss: 696.2251 | CE: 0.4071 | KD: 1060.5365\n",
      "Train Epoch: 021 Batch: 00068/00094 | Loss: 696.2758 | CE: 0.3442 | KD: 1060.7097\n",
      "Train Epoch: 021 Batch: 00069/00094 | Loss: 696.4281 | CE: 0.4109 | KD: 1060.8402\n",
      "Train Epoch: 021 Batch: 00070/00094 | Loss: 696.5060 | CE: 0.5030 | KD: 1060.8186\n",
      "Train Epoch: 021 Batch: 00071/00094 | Loss: 696.3545 | CE: 0.3081 | KD: 1060.8848\n",
      "Train Epoch: 021 Batch: 00072/00094 | Loss: 696.6260 | CE: 0.5985 | KD: 1060.8558\n",
      "Train Epoch: 021 Batch: 00073/00094 | Loss: 696.2161 | CE: 0.3464 | KD: 1060.6154\n",
      "Train Epoch: 021 Batch: 00074/00094 | Loss: 696.2980 | CE: 0.4387 | KD: 1060.5995\n",
      "Train Epoch: 021 Batch: 00075/00094 | Loss: 696.3988 | CE: 0.4176 | KD: 1060.7853\n",
      "Train Epoch: 021 Batch: 00076/00094 | Loss: 696.2802 | CE: 0.3779 | KD: 1060.6650\n",
      "Train Epoch: 021 Batch: 00077/00094 | Loss: 696.1223 | CE: 0.2951 | KD: 1060.5505\n",
      "Train Epoch: 021 Batch: 00078/00094 | Loss: 696.0927 | CE: 0.3593 | KD: 1060.4075\n",
      "Train Epoch: 021 Batch: 00079/00094 | Loss: 696.0279 | CE: 0.3022 | KD: 1060.3958\n",
      "Train Epoch: 021 Batch: 00080/00094 | Loss: 696.2924 | CE: 0.4043 | KD: 1060.6433\n",
      "Train Epoch: 021 Batch: 00081/00094 | Loss: 696.4690 | CE: 0.4464 | KD: 1060.8484\n",
      "Train Epoch: 021 Batch: 00082/00094 | Loss: 696.0350 | CE: 0.3282 | KD: 1060.3669\n",
      "Train Epoch: 021 Batch: 00083/00094 | Loss: 695.8892 | CE: 0.3252 | KD: 1060.1493\n",
      "Train Epoch: 021 Batch: 00084/00094 | Loss: 696.2092 | CE: 0.4502 | KD: 1060.4465\n",
      "Train Epoch: 021 Batch: 00085/00094 | Loss: 696.5182 | CE: 0.5538 | KD: 1060.7596\n",
      "Train Epoch: 021 Batch: 00086/00094 | Loss: 696.3122 | CE: 0.3967 | KD: 1060.6852\n",
      "Train Epoch: 021 Batch: 00087/00094 | Loss: 696.3509 | CE: 0.4428 | KD: 1060.6738\n",
      "Train Epoch: 021 Batch: 00088/00094 | Loss: 696.6967 | CE: 0.5843 | KD: 1060.9852\n",
      "Train Epoch: 021 Batch: 00089/00094 | Loss: 696.1966 | CE: 0.4335 | KD: 1060.4529\n",
      "Train Epoch: 021 Batch: 00090/00094 | Loss: 696.2250 | CE: 0.3995 | KD: 1060.5481\n",
      "Train Epoch: 021 Batch: 00091/00094 | Loss: 696.2314 | CE: 0.4729 | KD: 1060.4459\n",
      "Train Epoch: 021 Batch: 00092/00094 | Loss: 696.1633 | CE: 0.3240 | KD: 1060.5690\n",
      "Train Epoch: 021 Batch: 00093/00094 | Loss: 696.2285 | CE: 0.3493 | KD: 1060.6299\n",
      "Train Epoch: 021 Batch: 00094/00094 | Loss: 696.2557 | CE: 0.4668 | KD: 1060.4921\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3967 | acc:85.7000\n",
      "[VAL Acc] Target: 85.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2161 | acc:49.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8993 | acc:53.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1372 | acc:48.0916\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9486 | acc:49.0987\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 49.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8943 | acc:51.7560\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6660 | acc:64.1850\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9105 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7417 | acc:59.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 59.70%\n",
      "[VAL Acc] Avg 56.85%\n",
      "Train Epoch: 022 Batch: 00001/00094 | Loss: 626.5286 | CE: 0.3913 | KD: 1060.3690\n",
      "Train Epoch: 022 Batch: 00002/00094 | Loss: 626.5984 | CE: 0.3733 | KD: 1060.5179\n",
      "Train Epoch: 022 Batch: 00003/00094 | Loss: 626.4723 | CE: 0.3818 | KD: 1060.2898\n",
      "Train Epoch: 022 Batch: 00004/00094 | Loss: 626.4940 | CE: 0.3339 | KD: 1060.4076\n",
      "Train Epoch: 022 Batch: 00005/00094 | Loss: 627.0778 | CE: 0.4093 | KD: 1061.2684\n",
      "Train Epoch: 022 Batch: 00006/00094 | Loss: 626.4277 | CE: 0.3627 | KD: 1060.2465\n",
      "Train Epoch: 022 Batch: 00007/00094 | Loss: 626.6821 | CE: 0.4560 | KD: 1060.5194\n",
      "Train Epoch: 022 Batch: 00008/00094 | Loss: 626.9550 | CE: 0.3579 | KD: 1061.1477\n",
      "Train Epoch: 022 Batch: 00009/00094 | Loss: 626.4189 | CE: 0.2935 | KD: 1060.3489\n",
      "Train Epoch: 022 Batch: 00010/00094 | Loss: 626.5156 | CE: 0.4058 | KD: 1060.3225\n",
      "Train Epoch: 022 Batch: 00011/00094 | Loss: 627.1888 | CE: 0.4031 | KD: 1061.4672\n",
      "Train Epoch: 022 Batch: 00012/00094 | Loss: 626.5185 | CE: 0.3591 | KD: 1060.4065\n",
      "Train Epoch: 022 Batch: 00013/00094 | Loss: 626.5283 | CE: 0.4385 | KD: 1060.2885\n",
      "Train Epoch: 022 Batch: 00014/00094 | Loss: 626.9307 | CE: 0.4043 | KD: 1061.0278\n",
      "Train Epoch: 022 Batch: 00015/00094 | Loss: 626.6276 | CE: 0.4473 | KD: 1060.4419\n",
      "Train Epoch: 022 Batch: 00016/00094 | Loss: 626.7672 | CE: 0.3834 | KD: 1060.7865\n",
      "Train Epoch: 022 Batch: 00017/00094 | Loss: 626.6044 | CE: 0.4249 | KD: 1060.4406\n",
      "Train Epoch: 022 Batch: 00018/00094 | Loss: 626.5260 | CE: 0.3963 | KD: 1060.3563\n",
      "Train Epoch: 022 Batch: 00019/00094 | Loss: 626.7573 | CE: 0.4151 | KD: 1060.7159\n",
      "Train Epoch: 022 Batch: 00020/00094 | Loss: 626.7229 | CE: 0.3745 | KD: 1060.7266\n",
      "Train Epoch: 022 Batch: 00021/00094 | Loss: 626.6092 | CE: 0.3345 | KD: 1060.6018\n",
      "Train Epoch: 022 Batch: 00022/00094 | Loss: 626.3889 | CE: 0.3314 | KD: 1060.2338\n",
      "Train Epoch: 022 Batch: 00023/00094 | Loss: 626.5569 | CE: 0.4313 | KD: 1060.3492\n",
      "Train Epoch: 022 Batch: 00024/00094 | Loss: 626.9279 | CE: 0.4809 | KD: 1060.8936\n",
      "Train Epoch: 022 Batch: 00025/00094 | Loss: 626.6879 | CE: 0.3955 | KD: 1060.6317\n",
      "Train Epoch: 022 Batch: 00026/00094 | Loss: 626.7654 | CE: 0.3359 | KD: 1060.8640\n",
      "Train Epoch: 022 Batch: 00027/00094 | Loss: 626.7542 | CE: 0.3296 | KD: 1060.8555\n",
      "Train Epoch: 022 Batch: 00028/00094 | Loss: 626.6422 | CE: 0.4536 | KD: 1060.4559\n",
      "Train Epoch: 022 Batch: 00029/00094 | Loss: 626.9248 | CE: 0.3431 | KD: 1061.1217\n",
      "Train Epoch: 022 Batch: 00030/00094 | Loss: 626.4241 | CE: 0.2964 | KD: 1060.3528\n",
      "Train Epoch: 022 Batch: 00031/00094 | Loss: 626.5070 | CE: 0.3342 | KD: 1060.4291\n",
      "Train Epoch: 022 Batch: 00032/00094 | Loss: 626.5718 | CE: 0.3565 | KD: 1060.5012\n",
      "Train Epoch: 022 Batch: 00033/00094 | Loss: 626.4521 | CE: 0.3391 | KD: 1060.3280\n",
      "Train Epoch: 022 Batch: 00034/00094 | Loss: 626.3630 | CE: 0.3107 | KD: 1060.2252\n",
      "Train Epoch: 022 Batch: 00035/00094 | Loss: 626.6293 | CE: 0.3523 | KD: 1060.6056\n",
      "Train Epoch: 022 Batch: 00036/00094 | Loss: 626.7294 | CE: 0.4083 | KD: 1060.6803\n",
      "Train Epoch: 022 Batch: 00037/00094 | Loss: 626.9919 | CE: 0.4883 | KD: 1060.9895\n",
      "Train Epoch: 022 Batch: 00038/00094 | Loss: 626.5582 | CE: 0.2703 | KD: 1060.6241\n",
      "Train Epoch: 022 Batch: 00039/00094 | Loss: 626.4731 | CE: 0.3098 | KD: 1060.4132\n",
      "Train Epoch: 022 Batch: 00040/00094 | Loss: 626.5739 | CE: 0.2616 | KD: 1060.6654\n",
      "Train Epoch: 022 Batch: 00041/00094 | Loss: 626.4797 | CE: 0.3090 | KD: 1060.4255\n",
      "Train Epoch: 022 Batch: 00042/00094 | Loss: 626.3726 | CE: 0.2746 | KD: 1060.3025\n",
      "Train Epoch: 022 Batch: 00043/00094 | Loss: 626.6581 | CE: 0.2964 | KD: 1060.7490\n",
      "Train Epoch: 022 Batch: 00044/00094 | Loss: 626.6343 | CE: 0.4098 | KD: 1060.5168\n",
      "Train Epoch: 022 Batch: 00045/00094 | Loss: 626.5859 | CE: 0.4113 | KD: 1060.4321\n",
      "Train Epoch: 022 Batch: 00046/00094 | Loss: 626.4274 | CE: 0.3586 | KD: 1060.2531\n",
      "Train Epoch: 022 Batch: 00047/00094 | Loss: 626.5082 | CE: 0.3843 | KD: 1060.3464\n",
      "Train Epoch: 022 Batch: 00048/00094 | Loss: 626.7237 | CE: 0.3938 | KD: 1060.6952\n",
      "Train Epoch: 022 Batch: 00049/00094 | Loss: 626.5159 | CE: 0.2779 | KD: 1060.5396\n",
      "Train Epoch: 022 Batch: 00050/00094 | Loss: 626.6904 | CE: 0.4954 | KD: 1060.4669\n",
      "Train Epoch: 022 Batch: 00051/00094 | Loss: 626.6050 | CE: 0.5202 | KD: 1060.2800\n",
      "Train Epoch: 022 Batch: 00052/00094 | Loss: 626.6379 | CE: 0.3743 | KD: 1060.5829\n",
      "Train Epoch: 022 Batch: 00053/00094 | Loss: 626.8744 | CE: 0.5477 | KD: 1060.6898\n",
      "Train Epoch: 022 Batch: 00054/00094 | Loss: 626.5695 | CE: 0.4122 | KD: 1060.4030\n",
      "Train Epoch: 022 Batch: 00055/00094 | Loss: 626.7532 | CE: 0.3878 | KD: 1060.7552\n",
      "Train Epoch: 022 Batch: 00056/00094 | Loss: 626.7389 | CE: 0.3465 | KD: 1060.8009\n",
      "Train Epoch: 022 Batch: 00057/00094 | Loss: 626.4346 | CE: 0.3606 | KD: 1060.2618\n",
      "Train Epoch: 022 Batch: 00058/00094 | Loss: 626.9571 | CE: 0.5505 | KD: 1060.8251\n",
      "Train Epoch: 022 Batch: 00059/00094 | Loss: 626.5247 | CE: 0.4027 | KD: 1060.3430\n",
      "Train Epoch: 022 Batch: 00060/00094 | Loss: 627.1382 | CE: 0.5180 | KD: 1061.1870\n",
      "Train Epoch: 022 Batch: 00061/00094 | Loss: 627.1268 | CE: 0.3895 | KD: 1061.3851\n",
      "Train Epoch: 022 Batch: 00062/00094 | Loss: 626.4470 | CE: 0.3422 | KD: 1060.3141\n",
      "Train Epoch: 022 Batch: 00063/00094 | Loss: 626.7582 | CE: 0.4523 | KD: 1060.6547\n",
      "Train Epoch: 022 Batch: 00064/00094 | Loss: 626.5544 | CE: 0.3302 | KD: 1060.5164\n",
      "Train Epoch: 022 Batch: 00065/00094 | Loss: 626.5296 | CE: 0.3165 | KD: 1060.4974\n",
      "Train Epoch: 022 Batch: 00066/00094 | Loss: 626.7231 | CE: 0.3318 | KD: 1060.7992\n",
      "Train Epoch: 022 Batch: 00067/00094 | Loss: 626.4484 | CE: 0.3841 | KD: 1060.2455\n",
      "Train Epoch: 022 Batch: 00068/00094 | Loss: 626.6271 | CE: 0.3727 | KD: 1060.5674\n",
      "Train Epoch: 022 Batch: 00069/00094 | Loss: 626.6706 | CE: 0.3174 | KD: 1060.7347\n",
      "Train Epoch: 022 Batch: 00070/00094 | Loss: 626.5004 | CE: 0.3108 | KD: 1060.4576\n",
      "Train Epoch: 022 Batch: 00071/00094 | Loss: 626.6793 | CE: 0.3926 | KD: 1060.6221\n",
      "Train Epoch: 022 Batch: 00072/00094 | Loss: 626.6288 | CE: 0.3943 | KD: 1060.5337\n",
      "Train Epoch: 022 Batch: 00073/00094 | Loss: 626.6694 | CE: 0.3559 | KD: 1060.6675\n",
      "Train Epoch: 022 Batch: 00074/00094 | Loss: 626.4722 | CE: 0.3828 | KD: 1060.2881\n",
      "Train Epoch: 022 Batch: 00075/00094 | Loss: 627.0858 | CE: 0.3875 | KD: 1061.3190\n",
      "Train Epoch: 022 Batch: 00076/00094 | Loss: 626.6585 | CE: 0.3777 | KD: 1060.6121\n",
      "Train Epoch: 022 Batch: 00077/00094 | Loss: 626.8964 | CE: 0.4904 | KD: 1060.8241\n",
      "Train Epoch: 022 Batch: 00078/00094 | Loss: 626.8643 | CE: 0.4825 | KD: 1060.7830\n",
      "Train Epoch: 022 Batch: 00079/00094 | Loss: 626.4471 | CE: 0.3940 | KD: 1060.2263\n",
      "Train Epoch: 022 Batch: 00080/00094 | Loss: 626.7898 | CE: 0.3321 | KD: 1060.9116\n",
      "Train Epoch: 022 Batch: 00081/00094 | Loss: 626.6061 | CE: 0.3698 | KD: 1060.5366\n",
      "Train Epoch: 022 Batch: 00082/00094 | Loss: 627.0422 | CE: 0.5683 | KD: 1060.9392\n",
      "Train Epoch: 022 Batch: 00083/00094 | Loss: 626.9117 | CE: 0.3700 | KD: 1061.0538\n",
      "Train Epoch: 022 Batch: 00084/00094 | Loss: 626.8690 | CE: 0.5515 | KD: 1060.6741\n",
      "Train Epoch: 022 Batch: 00085/00094 | Loss: 626.5574 | CE: 0.4182 | KD: 1060.3722\n",
      "Train Epoch: 022 Batch: 00086/00094 | Loss: 626.6265 | CE: 0.4070 | KD: 1060.5082\n",
      "Train Epoch: 022 Batch: 00087/00094 | Loss: 627.0164 | CE: 0.6200 | KD: 1060.8076\n",
      "Train Epoch: 022 Batch: 00088/00094 | Loss: 626.5395 | CE: 0.3729 | KD: 1060.4186\n",
      "Train Epoch: 022 Batch: 00089/00094 | Loss: 626.6569 | CE: 0.3016 | KD: 1060.7382\n",
      "Train Epoch: 022 Batch: 00090/00094 | Loss: 626.7720 | CE: 0.3740 | KD: 1060.8105\n",
      "Train Epoch: 022 Batch: 00091/00094 | Loss: 626.5492 | CE: 0.3601 | KD: 1060.4567\n",
      "Train Epoch: 022 Batch: 00092/00094 | Loss: 626.5447 | CE: 0.3684 | KD: 1060.4352\n",
      "Train Epoch: 022 Batch: 00093/00094 | Loss: 626.5318 | CE: 0.3771 | KD: 1060.3986\n",
      "Train Epoch: 022 Batch: 00094/00094 | Loss: 626.7679 | CE: 0.5121 | KD: 1060.5698\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3984 | acc:84.3000\n",
      "[VAL Acc] Target: 84.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1962 | acc:48.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8416 | acc:56.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1544 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8729 | acc:47.0219\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 47.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8915 | acc:51.7560\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6302 | acc:66.3793\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9101 | acc:50.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 50.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7034 | acc:62.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 62.40%\n",
      "[VAL Acc] Avg 57.02%\n",
      "Train Epoch: 023 Batch: 00001/00094 | Loss: 626.5057 | CE: 0.3232 | KD: 1060.4457\n",
      "Train Epoch: 023 Batch: 00002/00094 | Loss: 626.3448 | CE: 0.3263 | KD: 1060.1680\n",
      "Train Epoch: 023 Batch: 00003/00094 | Loss: 626.8514 | CE: 0.3716 | KD: 1060.9491\n",
      "Train Epoch: 023 Batch: 00004/00094 | Loss: 626.7363 | CE: 0.3675 | KD: 1060.7611\n",
      "Train Epoch: 023 Batch: 00005/00094 | Loss: 626.5154 | CE: 0.3552 | KD: 1060.4077\n",
      "Train Epoch: 023 Batch: 00006/00094 | Loss: 626.3475 | CE: 0.3172 | KD: 1060.1879\n",
      "Train Epoch: 023 Batch: 00007/00094 | Loss: 626.7141 | CE: 0.4040 | KD: 1060.6617\n",
      "Train Epoch: 023 Batch: 00008/00094 | Loss: 626.5338 | CE: 0.3763 | KD: 1060.4032\n",
      "Train Epoch: 023 Batch: 00009/00094 | Loss: 626.6210 | CE: 0.2631 | KD: 1060.7427\n",
      "Train Epoch: 023 Batch: 00010/00094 | Loss: 626.7398 | CE: 0.3191 | KD: 1060.8489\n",
      "Train Epoch: 023 Batch: 00011/00094 | Loss: 626.8577 | CE: 0.2671 | KD: 1061.1366\n",
      "Train Epoch: 023 Batch: 00012/00094 | Loss: 626.6099 | CE: 0.3771 | KD: 1060.5308\n",
      "Train Epoch: 023 Batch: 00013/00094 | Loss: 626.7238 | CE: 0.3171 | KD: 1060.8253\n",
      "Train Epoch: 023 Batch: 00014/00094 | Loss: 626.4703 | CE: 0.3139 | KD: 1060.4014\n",
      "Train Epoch: 023 Batch: 00015/00094 | Loss: 626.6482 | CE: 0.4342 | KD: 1060.4988\n",
      "Train Epoch: 023 Batch: 00016/00094 | Loss: 626.6504 | CE: 0.4215 | KD: 1060.5242\n",
      "Train Epoch: 023 Batch: 00017/00094 | Loss: 626.6635 | CE: 0.3936 | KD: 1060.5935\n",
      "Train Epoch: 023 Batch: 00018/00094 | Loss: 626.6980 | CE: 0.3136 | KD: 1060.7876\n",
      "Train Epoch: 023 Batch: 00019/00094 | Loss: 626.6882 | CE: 0.4846 | KD: 1060.4813\n",
      "Train Epoch: 023 Batch: 00020/00094 | Loss: 626.4274 | CE: 0.3136 | KD: 1060.3292\n",
      "Train Epoch: 023 Batch: 00021/00094 | Loss: 626.5518 | CE: 0.3143 | KD: 1060.5387\n",
      "Train Epoch: 023 Batch: 00022/00094 | Loss: 626.5784 | CE: 0.3103 | KD: 1060.5907\n",
      "Train Epoch: 023 Batch: 00023/00094 | Loss: 627.0539 | CE: 0.4160 | KD: 1061.2168\n",
      "Train Epoch: 023 Batch: 00024/00094 | Loss: 626.8027 | CE: 0.4901 | KD: 1060.6660\n",
      "Train Epoch: 023 Batch: 00025/00094 | Loss: 626.4458 | CE: 0.2849 | KD: 1060.4091\n",
      "Train Epoch: 023 Batch: 00026/00094 | Loss: 626.7231 | CE: 0.3427 | KD: 1060.7808\n",
      "Train Epoch: 023 Batch: 00027/00094 | Loss: 626.6323 | CE: 0.2953 | KD: 1060.7072\n",
      "Train Epoch: 023 Batch: 00028/00094 | Loss: 626.5096 | CE: 0.4043 | KD: 1060.3151\n",
      "Train Epoch: 023 Batch: 00029/00094 | Loss: 626.6537 | CE: 0.4539 | KD: 1060.4750\n",
      "Train Epoch: 023 Batch: 00030/00094 | Loss: 626.9151 | CE: 0.4824 | KD: 1060.8693\n",
      "Train Epoch: 023 Batch: 00031/00094 | Loss: 626.7255 | CE: 0.3731 | KD: 1060.7334\n",
      "Train Epoch: 023 Batch: 00032/00094 | Loss: 626.9683 | CE: 0.5055 | KD: 1060.9203\n",
      "Train Epoch: 023 Batch: 00033/00094 | Loss: 626.6088 | CE: 0.3319 | KD: 1060.6053\n",
      "Train Epoch: 023 Batch: 00034/00094 | Loss: 626.8031 | CE: 0.3430 | KD: 1060.9155\n",
      "Train Epoch: 023 Batch: 00035/00094 | Loss: 626.5316 | CE: 0.3411 | KD: 1060.4592\n",
      "Train Epoch: 023 Batch: 00036/00094 | Loss: 626.5368 | CE: 0.3411 | KD: 1060.4679\n",
      "Train Epoch: 023 Batch: 00037/00094 | Loss: 626.5229 | CE: 0.4027 | KD: 1060.3401\n",
      "Train Epoch: 023 Batch: 00038/00094 | Loss: 626.7747 | CE: 0.4087 | KD: 1060.7563\n",
      "Train Epoch: 023 Batch: 00039/00094 | Loss: 626.7494 | CE: 0.5356 | KD: 1060.4987\n",
      "Train Epoch: 023 Batch: 00040/00094 | Loss: 626.6654 | CE: 0.4518 | KD: 1060.4982\n",
      "Train Epoch: 023 Batch: 00041/00094 | Loss: 626.5988 | CE: 0.4239 | KD: 1060.4326\n",
      "Train Epoch: 023 Batch: 00042/00094 | Loss: 626.6803 | CE: 0.4003 | KD: 1060.6107\n",
      "Train Epoch: 023 Batch: 00043/00094 | Loss: 626.6633 | CE: 0.3988 | KD: 1060.5846\n",
      "Train Epoch: 023 Batch: 00044/00094 | Loss: 626.3729 | CE: 0.3868 | KD: 1060.1129\n",
      "Train Epoch: 023 Batch: 00045/00094 | Loss: 626.6970 | CE: 0.3870 | KD: 1060.6615\n",
      "Train Epoch: 023 Batch: 00046/00094 | Loss: 626.6599 | CE: 0.3236 | KD: 1060.7061\n",
      "Train Epoch: 023 Batch: 00047/00094 | Loss: 626.5218 | CE: 0.3351 | KD: 1060.4526\n",
      "Train Epoch: 023 Batch: 00048/00094 | Loss: 626.5475 | CE: 0.3784 | KD: 1060.4229\n",
      "Train Epoch: 023 Batch: 00049/00094 | Loss: 626.5704 | CE: 0.4570 | KD: 1060.3285\n",
      "Train Epoch: 023 Batch: 00050/00094 | Loss: 627.0570 | CE: 0.4951 | KD: 1061.0881\n",
      "Train Epoch: 023 Batch: 00051/00094 | Loss: 626.6357 | CE: 0.4216 | KD: 1060.4991\n",
      "Train Epoch: 023 Batch: 00052/00094 | Loss: 626.6180 | CE: 0.3684 | KD: 1060.5592\n",
      "Train Epoch: 023 Batch: 00053/00094 | Loss: 626.8940 | CE: 0.4722 | KD: 1060.8510\n",
      "Train Epoch: 023 Batch: 00054/00094 | Loss: 626.5962 | CE: 0.3901 | KD: 1060.4855\n",
      "Train Epoch: 023 Batch: 00055/00094 | Loss: 626.6740 | CE: 0.3781 | KD: 1060.6376\n",
      "Train Epoch: 023 Batch: 00056/00094 | Loss: 626.7488 | CE: 0.3885 | KD: 1060.7467\n",
      "Train Epoch: 023 Batch: 00057/00094 | Loss: 626.6122 | CE: 0.3055 | KD: 1060.6559\n",
      "Train Epoch: 023 Batch: 00058/00094 | Loss: 626.6445 | CE: 0.4327 | KD: 1060.4951\n",
      "Train Epoch: 023 Batch: 00059/00094 | Loss: 626.7141 | CE: 0.3284 | KD: 1060.7897\n",
      "Train Epoch: 023 Batch: 00060/00094 | Loss: 626.8594 | CE: 0.4412 | KD: 1060.8448\n",
      "Train Epoch: 023 Batch: 00061/00094 | Loss: 626.6531 | CE: 0.3809 | KD: 1060.5977\n",
      "Train Epoch: 023 Batch: 00062/00094 | Loss: 626.4988 | CE: 0.3784 | KD: 1060.3403\n",
      "Train Epoch: 023 Batch: 00063/00094 | Loss: 626.3809 | CE: 0.3235 | KD: 1060.2336\n",
      "Train Epoch: 023 Batch: 00064/00094 | Loss: 626.9311 | CE: 0.3601 | KD: 1061.1036\n",
      "Train Epoch: 023 Batch: 00065/00094 | Loss: 626.7103 | CE: 0.3498 | KD: 1060.7469\n",
      "Train Epoch: 023 Batch: 00066/00094 | Loss: 626.7776 | CE: 0.4275 | KD: 1060.7294\n",
      "Train Epoch: 023 Batch: 00067/00094 | Loss: 626.5416 | CE: 0.3314 | KD: 1060.4926\n",
      "Train Epoch: 023 Batch: 00068/00094 | Loss: 627.1224 | CE: 0.6149 | KD: 1060.9960\n",
      "Train Epoch: 023 Batch: 00069/00094 | Loss: 626.7432 | CE: 0.4076 | KD: 1060.7048\n",
      "Train Epoch: 023 Batch: 00070/00094 | Loss: 626.7004 | CE: 0.3322 | KD: 1060.7603\n",
      "Train Epoch: 023 Batch: 00071/00094 | Loss: 626.6588 | CE: 0.3419 | KD: 1060.6732\n",
      "Train Epoch: 023 Batch: 00072/00094 | Loss: 626.6235 | CE: 0.4478 | KD: 1060.4341\n",
      "Train Epoch: 023 Batch: 00073/00094 | Loss: 626.6925 | CE: 0.3849 | KD: 1060.6575\n",
      "Train Epoch: 023 Batch: 00074/00094 | Loss: 626.8265 | CE: 0.4589 | KD: 1060.7590\n",
      "Train Epoch: 023 Batch: 00075/00094 | Loss: 626.7053 | CE: 0.3288 | KD: 1060.7742\n",
      "Train Epoch: 023 Batch: 00076/00094 | Loss: 626.8229 | CE: 0.3789 | KD: 1060.8884\n",
      "Train Epoch: 023 Batch: 00077/00094 | Loss: 626.7318 | CE: 0.3916 | KD: 1060.7128\n",
      "Train Epoch: 023 Batch: 00078/00094 | Loss: 626.4041 | CE: 0.3609 | KD: 1060.2097\n",
      "Train Epoch: 023 Batch: 00079/00094 | Loss: 626.6375 | CE: 0.3457 | KD: 1060.6307\n",
      "Train Epoch: 023 Batch: 00080/00094 | Loss: 626.6891 | CE: 0.3309 | KD: 1060.7433\n",
      "Train Epoch: 023 Batch: 00081/00094 | Loss: 626.7788 | CE: 0.4641 | KD: 1060.6696\n",
      "Train Epoch: 023 Batch: 00082/00094 | Loss: 626.6712 | CE: 0.4368 | KD: 1060.5336\n",
      "Train Epoch: 023 Batch: 00083/00094 | Loss: 626.8120 | CE: 0.4394 | KD: 1060.7675\n",
      "Train Epoch: 023 Batch: 00084/00094 | Loss: 627.0999 | CE: 0.4462 | KD: 1061.2434\n",
      "Train Epoch: 023 Batch: 00085/00094 | Loss: 626.8079 | CE: 0.3838 | KD: 1060.8547\n",
      "Train Epoch: 023 Batch: 00086/00094 | Loss: 626.6859 | CE: 0.3831 | KD: 1060.6493\n",
      "Train Epoch: 023 Batch: 00087/00094 | Loss: 626.8876 | CE: 0.4656 | KD: 1060.8513\n",
      "Train Epoch: 023 Batch: 00088/00094 | Loss: 626.7622 | CE: 0.4068 | KD: 1060.7385\n",
      "Train Epoch: 023 Batch: 00089/00094 | Loss: 626.5130 | CE: 0.3276 | KD: 1060.4504\n",
      "Train Epoch: 023 Batch: 00090/00094 | Loss: 626.6291 | CE: 0.4492 | KD: 1060.4412\n",
      "Train Epoch: 023 Batch: 00091/00094 | Loss: 626.6673 | CE: 0.4395 | KD: 1060.5223\n",
      "Train Epoch: 023 Batch: 00092/00094 | Loss: 626.6974 | CE: 0.4471 | KD: 1060.5605\n",
      "Train Epoch: 023 Batch: 00093/00094 | Loss: 626.7117 | CE: 0.4021 | KD: 1060.6608\n",
      "Train Epoch: 023 Batch: 00094/00094 | Loss: 626.6482 | CE: 0.4701 | KD: 1060.4380\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4526 | acc:81.7000\n",
      "[VAL Acc] Target: 81.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2764 | acc:49.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9763 | acc:51.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2965 | acc:47.3282\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9383 | acc:48.9812\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 48.98%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7996 | acc:55.5453\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 55.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6632 | acc:63.0878\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8416 | acc:51.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 51.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.8066 | acc:57.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 57.85%\n",
      "[VAL Acc] Avg 56.31%\n",
      "Train Epoch: 024 Batch: 00001/00094 | Loss: 626.5466 | CE: 0.3404 | KD: 1060.4856\n",
      "Train Epoch: 024 Batch: 00002/00094 | Loss: 626.9142 | CE: 0.3385 | KD: 1061.1116\n",
      "Train Epoch: 024 Batch: 00003/00094 | Loss: 626.9255 | CE: 0.3952 | KD: 1061.0347\n",
      "Train Epoch: 024 Batch: 00004/00094 | Loss: 626.5593 | CE: 0.4738 | KD: 1060.2814\n",
      "Train Epoch: 024 Batch: 00005/00094 | Loss: 626.6157 | CE: 0.3638 | KD: 1060.5630\n",
      "Train Epoch: 024 Batch: 00006/00094 | Loss: 626.6127 | CE: 0.3629 | KD: 1060.5594\n",
      "Train Epoch: 024 Batch: 00007/00094 | Loss: 627.1182 | CE: 0.4368 | KD: 1061.2905\n",
      "Train Epoch: 024 Batch: 00008/00094 | Loss: 626.7103 | CE: 0.5603 | KD: 1060.3907\n",
      "Train Epoch: 024 Batch: 00009/00094 | Loss: 626.7918 | CE: 0.4268 | KD: 1060.7548\n",
      "Train Epoch: 024 Batch: 00010/00094 | Loss: 626.8110 | CE: 0.3548 | KD: 1060.9091\n",
      "Train Epoch: 024 Batch: 00011/00094 | Loss: 626.5154 | CE: 0.3225 | KD: 1060.4633\n",
      "Train Epoch: 024 Batch: 00012/00094 | Loss: 626.7675 | CE: 0.4133 | KD: 1060.7363\n",
      "Train Epoch: 024 Batch: 00013/00094 | Loss: 627.0753 | CE: 0.5993 | KD: 1060.9425\n",
      "Train Epoch: 024 Batch: 00014/00094 | Loss: 626.8979 | CE: 0.4089 | KD: 1060.9647\n",
      "Train Epoch: 024 Batch: 00015/00094 | Loss: 626.7792 | CE: 0.3768 | KD: 1060.8180\n",
      "Train Epoch: 024 Batch: 00016/00094 | Loss: 626.3087 | CE: 0.2964 | KD: 1060.1572\n",
      "Train Epoch: 024 Batch: 00017/00094 | Loss: 626.3898 | CE: 0.3188 | KD: 1060.2567\n",
      "Train Epoch: 024 Batch: 00018/00094 | Loss: 626.4265 | CE: 0.3074 | KD: 1060.3383\n",
      "Train Epoch: 024 Batch: 00019/00094 | Loss: 626.3359 | CE: 0.2772 | KD: 1060.2358\n",
      "Train Epoch: 024 Batch: 00020/00094 | Loss: 626.5157 | CE: 0.2955 | KD: 1060.5095\n",
      "Train Epoch: 024 Batch: 00021/00094 | Loss: 626.5515 | CE: 0.3124 | KD: 1060.5414\n",
      "Train Epoch: 024 Batch: 00022/00094 | Loss: 626.7996 | CE: 0.3774 | KD: 1060.8514\n",
      "Train Epoch: 024 Batch: 00023/00094 | Loss: 626.8446 | CE: 0.4764 | KD: 1060.7601\n",
      "Train Epoch: 024 Batch: 00024/00094 | Loss: 626.8135 | CE: 0.4693 | KD: 1060.7195\n",
      "Train Epoch: 024 Batch: 00025/00094 | Loss: 626.6064 | CE: 0.3311 | KD: 1060.6028\n",
      "Train Epoch: 024 Batch: 00026/00094 | Loss: 626.7123 | CE: 0.3334 | KD: 1060.7781\n",
      "Train Epoch: 024 Batch: 00027/00094 | Loss: 626.4028 | CE: 0.2939 | KD: 1060.3209\n",
      "Train Epoch: 024 Batch: 00028/00094 | Loss: 627.1892 | CE: 0.6378 | KD: 1061.0704\n",
      "Train Epoch: 024 Batch: 00029/00094 | Loss: 626.6174 | CE: 0.3576 | KD: 1060.5764\n",
      "Train Epoch: 024 Batch: 00030/00094 | Loss: 626.5966 | CE: 0.4495 | KD: 1060.3856\n",
      "Train Epoch: 024 Batch: 00031/00094 | Loss: 626.4114 | CE: 0.3560 | KD: 1060.2305\n",
      "Train Epoch: 024 Batch: 00032/00094 | Loss: 626.6027 | CE: 0.4118 | KD: 1060.4598\n",
      "Train Epoch: 024 Batch: 00033/00094 | Loss: 626.8005 | CE: 0.5099 | KD: 1060.6288\n",
      "Train Epoch: 024 Batch: 00034/00094 | Loss: 626.5877 | CE: 0.3931 | KD: 1060.4661\n",
      "Train Epoch: 024 Batch: 00035/00094 | Loss: 626.6445 | CE: 0.4409 | KD: 1060.4812\n",
      "Train Epoch: 024 Batch: 00036/00094 | Loss: 626.9504 | CE: 0.5023 | KD: 1060.8954\n",
      "Train Epoch: 024 Batch: 00037/00094 | Loss: 626.6663 | CE: 0.3530 | KD: 1060.6670\n",
      "Train Epoch: 024 Batch: 00038/00094 | Loss: 626.3965 | CE: 0.3195 | KD: 1060.2670\n",
      "Train Epoch: 024 Batch: 00039/00094 | Loss: 626.6584 | CE: 0.2458 | KD: 1060.8353\n",
      "Train Epoch: 024 Batch: 00040/00094 | Loss: 626.6165 | CE: 0.3344 | KD: 1060.6143\n",
      "Train Epoch: 024 Batch: 00041/00094 | Loss: 626.4269 | CE: 0.3426 | KD: 1060.2794\n",
      "Train Epoch: 024 Batch: 00042/00094 | Loss: 626.5378 | CE: 0.3399 | KD: 1060.4717\n",
      "Train Epoch: 024 Batch: 00043/00094 | Loss: 626.5814 | CE: 0.3572 | KD: 1060.5162\n",
      "Train Epoch: 024 Batch: 00044/00094 | Loss: 626.6501 | CE: 0.4088 | KD: 1060.5452\n",
      "Train Epoch: 024 Batch: 00045/00094 | Loss: 626.5369 | CE: 0.4392 | KD: 1060.3019\n",
      "Train Epoch: 024 Batch: 00046/00094 | Loss: 626.8819 | CE: 0.3752 | KD: 1060.9946\n",
      "Train Epoch: 024 Batch: 00047/00094 | Loss: 626.6991 | CE: 0.3938 | KD: 1060.6536\n",
      "Train Epoch: 024 Batch: 00048/00094 | Loss: 626.5786 | CE: 0.3309 | KD: 1060.5560\n",
      "Train Epoch: 024 Batch: 00049/00094 | Loss: 626.8602 | CE: 0.4192 | KD: 1060.8834\n",
      "Train Epoch: 024 Batch: 00050/00094 | Loss: 626.6218 | CE: 0.4547 | KD: 1060.4196\n",
      "Train Epoch: 024 Batch: 00051/00094 | Loss: 626.6557 | CE: 0.4777 | KD: 1060.4380\n",
      "Train Epoch: 024 Batch: 00052/00094 | Loss: 626.4625 | CE: 0.3552 | KD: 1060.3182\n",
      "Train Epoch: 024 Batch: 00053/00094 | Loss: 626.6905 | CE: 0.2968 | KD: 1060.8032\n",
      "Train Epoch: 024 Batch: 00054/00094 | Loss: 626.4253 | CE: 0.3423 | KD: 1060.2771\n",
      "Train Epoch: 024 Batch: 00055/00094 | Loss: 626.7523 | CE: 0.3629 | KD: 1060.7959\n",
      "Train Epoch: 024 Batch: 00056/00094 | Loss: 626.7563 | CE: 0.3175 | KD: 1060.8796\n",
      "Train Epoch: 024 Batch: 00057/00094 | Loss: 626.8353 | CE: 0.3850 | KD: 1060.8990\n",
      "Train Epoch: 024 Batch: 00058/00094 | Loss: 626.4517 | CE: 0.3473 | KD: 1060.3132\n",
      "Train Epoch: 024 Batch: 00059/00094 | Loss: 626.6325 | CE: 0.3889 | KD: 1060.5492\n",
      "Train Epoch: 024 Batch: 00060/00094 | Loss: 626.6819 | CE: 0.3423 | KD: 1060.7117\n",
      "Train Epoch: 024 Batch: 00061/00094 | Loss: 626.6395 | CE: 0.4432 | KD: 1060.4690\n",
      "Train Epoch: 024 Batch: 00062/00094 | Loss: 626.7979 | CE: 0.5014 | KD: 1060.6385\n",
      "Train Epoch: 024 Batch: 00063/00094 | Loss: 626.5884 | CE: 0.4329 | KD: 1060.3998\n",
      "Train Epoch: 024 Batch: 00064/00094 | Loss: 626.7530 | CE: 0.3786 | KD: 1060.7705\n",
      "Train Epoch: 024 Batch: 00065/00094 | Loss: 626.4177 | CE: 0.3053 | KD: 1060.3269\n",
      "Train Epoch: 024 Batch: 00066/00094 | Loss: 626.8778 | CE: 0.4159 | KD: 1060.9187\n",
      "Train Epoch: 024 Batch: 00067/00094 | Loss: 626.7460 | CE: 0.4355 | KD: 1060.6622\n",
      "Train Epoch: 024 Batch: 00068/00094 | Loss: 626.4716 | CE: 0.3129 | KD: 1060.4053\n",
      "Train Epoch: 024 Batch: 00069/00094 | Loss: 626.6740 | CE: 0.3487 | KD: 1060.6875\n",
      "Train Epoch: 024 Batch: 00070/00094 | Loss: 626.7506 | CE: 0.4726 | KD: 1060.6073\n",
      "Train Epoch: 024 Batch: 00071/00094 | Loss: 626.5303 | CE: 0.3426 | KD: 1060.4543\n",
      "Train Epoch: 024 Batch: 00072/00094 | Loss: 626.8161 | CE: 0.3300 | KD: 1060.9597\n",
      "Train Epoch: 024 Batch: 00073/00094 | Loss: 626.5615 | CE: 0.3885 | KD: 1060.4293\n",
      "Train Epoch: 024 Batch: 00074/00094 | Loss: 626.7352 | CE: 0.3713 | KD: 1060.7528\n",
      "Train Epoch: 024 Batch: 00075/00094 | Loss: 626.7271 | CE: 0.3547 | KD: 1060.7672\n",
      "Train Epoch: 024 Batch: 00076/00094 | Loss: 626.8326 | CE: 0.5122 | KD: 1060.6792\n",
      "Train Epoch: 024 Batch: 00077/00094 | Loss: 626.5559 | CE: 0.3465 | KD: 1060.4912\n",
      "Train Epoch: 024 Batch: 00078/00094 | Loss: 626.8602 | CE: 0.4305 | KD: 1060.8641\n",
      "Train Epoch: 024 Batch: 00079/00094 | Loss: 626.6848 | CE: 0.4031 | KD: 1060.6135\n",
      "Train Epoch: 024 Batch: 00080/00094 | Loss: 626.5212 | CE: 0.3586 | KD: 1060.4119\n",
      "Train Epoch: 024 Batch: 00081/00094 | Loss: 626.4269 | CE: 0.3515 | KD: 1060.2644\n",
      "Train Epoch: 024 Batch: 00082/00094 | Loss: 627.1224 | CE: 0.3902 | KD: 1061.3765\n",
      "Train Epoch: 024 Batch: 00083/00094 | Loss: 626.4031 | CE: 0.3622 | KD: 1060.2058\n",
      "Train Epoch: 024 Batch: 00084/00094 | Loss: 626.7202 | CE: 0.3957 | KD: 1060.6862\n",
      "Train Epoch: 024 Batch: 00085/00094 | Loss: 626.7239 | CE: 0.3258 | KD: 1060.8107\n",
      "Train Epoch: 024 Batch: 00086/00094 | Loss: 626.7137 | CE: 0.4187 | KD: 1060.6361\n",
      "Train Epoch: 024 Batch: 00087/00094 | Loss: 626.7279 | CE: 0.4113 | KD: 1060.6727\n",
      "Train Epoch: 024 Batch: 00088/00094 | Loss: 626.6160 | CE: 0.3962 | KD: 1060.5088\n",
      "Train Epoch: 024 Batch: 00089/00094 | Loss: 627.0261 | CE: 0.4803 | KD: 1061.0607\n",
      "Train Epoch: 024 Batch: 00090/00094 | Loss: 626.8150 | CE: 0.3118 | KD: 1060.9886\n",
      "Train Epoch: 024 Batch: 00091/00094 | Loss: 626.4586 | CE: 0.3075 | KD: 1060.3922\n",
      "Train Epoch: 024 Batch: 00092/00094 | Loss: 626.6875 | CE: 0.4568 | KD: 1060.5273\n",
      "Train Epoch: 024 Batch: 00093/00094 | Loss: 626.4478 | CE: 0.3565 | KD: 1060.2911\n",
      "Train Epoch: 024 Batch: 00094/00094 | Loss: 626.9185 | CE: 0.2883 | KD: 1061.2037\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4209 | acc:84.8000\n",
      "[VAL Acc] Target: 84.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2433 | acc:49.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9574 | acc:53.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2458 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8155 | acc:50.5878\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.59%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9184 | acc:51.6636\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6175 | acc:65.9483\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9198 | acc:50.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 50.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7392 | acc:63.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 63.40%\n",
      "[VAL Acc] Avg 57.57%\n",
      "Train Epoch: 025 Batch: 00001/00094 | Loss: 626.3622 | CE: 0.2353 | KD: 1060.3514\n",
      "Train Epoch: 025 Batch: 00002/00094 | Loss: 626.9308 | CE: 0.4049 | KD: 1061.0272\n",
      "Train Epoch: 025 Batch: 00003/00094 | Loss: 626.6671 | CE: 0.4005 | KD: 1060.5879\n",
      "Train Epoch: 025 Batch: 00004/00094 | Loss: 626.6423 | CE: 0.3430 | KD: 1060.6434\n",
      "Train Epoch: 025 Batch: 00005/00094 | Loss: 626.4691 | CE: 0.3495 | KD: 1060.3389\n",
      "Train Epoch: 025 Batch: 00006/00094 | Loss: 626.7755 | CE: 0.3498 | KD: 1060.8573\n",
      "Train Epoch: 025 Batch: 00007/00094 | Loss: 626.5235 | CE: 0.2644 | KD: 1060.5754\n",
      "Train Epoch: 025 Batch: 00008/00094 | Loss: 626.7250 | CE: 0.3078 | KD: 1060.8430\n",
      "Train Epoch: 025 Batch: 00009/00094 | Loss: 626.8319 | CE: 0.4943 | KD: 1060.7084\n",
      "Train Epoch: 025 Batch: 00010/00094 | Loss: 626.6708 | CE: 0.3005 | KD: 1060.7635\n",
      "Train Epoch: 025 Batch: 00011/00094 | Loss: 626.5126 | CE: 0.3038 | KD: 1060.4902\n",
      "Train Epoch: 025 Batch: 00012/00094 | Loss: 626.6579 | CE: 0.3196 | KD: 1060.7094\n",
      "Train Epoch: 025 Batch: 00013/00094 | Loss: 626.9877 | CE: 0.6208 | KD: 1060.7578\n",
      "Train Epoch: 025 Batch: 00014/00094 | Loss: 626.6158 | CE: 0.3757 | KD: 1060.5431\n",
      "Train Epoch: 025 Batch: 00015/00094 | Loss: 626.6890 | CE: 0.4116 | KD: 1060.6064\n",
      "Train Epoch: 025 Batch: 00016/00094 | Loss: 626.4545 | CE: 0.3179 | KD: 1060.3678\n",
      "Train Epoch: 025 Batch: 00017/00094 | Loss: 626.4741 | CE: 0.3647 | KD: 1060.3218\n",
      "Train Epoch: 025 Batch: 00018/00094 | Loss: 626.6105 | CE: 0.4125 | KD: 1060.4719\n",
      "Train Epoch: 025 Batch: 00019/00094 | Loss: 626.6318 | CE: 0.2993 | KD: 1060.6995\n",
      "Train Epoch: 025 Batch: 00020/00094 | Loss: 626.5935 | CE: 0.3420 | KD: 1060.5624\n",
      "Train Epoch: 025 Batch: 00021/00094 | Loss: 626.6257 | CE: 0.3733 | KD: 1060.5638\n",
      "Train Epoch: 025 Batch: 00022/00094 | Loss: 626.5126 | CE: 0.3376 | KD: 1060.4330\n",
      "Train Epoch: 025 Batch: 00023/00094 | Loss: 626.6987 | CE: 0.3643 | KD: 1060.7029\n",
      "Train Epoch: 025 Batch: 00024/00094 | Loss: 626.6378 | CE: 0.4103 | KD: 1060.5217\n",
      "Train Epoch: 025 Batch: 00025/00094 | Loss: 626.6789 | CE: 0.2861 | KD: 1060.8016\n",
      "Train Epoch: 025 Batch: 00026/00094 | Loss: 626.6423 | CE: 0.4192 | KD: 1060.5145\n",
      "Train Epoch: 025 Batch: 00027/00094 | Loss: 626.8849 | CE: 0.3378 | KD: 1061.0631\n",
      "Train Epoch: 025 Batch: 00028/00094 | Loss: 626.6133 | CE: 0.2891 | KD: 1060.6855\n",
      "Train Epoch: 025 Batch: 00029/00094 | Loss: 626.5737 | CE: 0.4135 | KD: 1060.4077\n",
      "Train Epoch: 025 Batch: 00030/00094 | Loss: 626.6943 | CE: 0.3601 | KD: 1060.7025\n",
      "Train Epoch: 025 Batch: 00031/00094 | Loss: 626.4686 | CE: 0.2987 | KD: 1060.4242\n",
      "Train Epoch: 025 Batch: 00032/00094 | Loss: 626.6903 | CE: 0.4993 | KD: 1060.4601\n",
      "Train Epoch: 025 Batch: 00033/00094 | Loss: 626.4791 | CE: 0.2900 | KD: 1060.4568\n",
      "Train Epoch: 025 Batch: 00034/00094 | Loss: 626.5804 | CE: 0.3834 | KD: 1060.4702\n",
      "Train Epoch: 025 Batch: 00035/00094 | Loss: 626.8220 | CE: 0.4957 | KD: 1060.6891\n",
      "Train Epoch: 025 Batch: 00036/00094 | Loss: 626.8711 | CE: 0.4608 | KD: 1060.8313\n",
      "Train Epoch: 025 Batch: 00037/00094 | Loss: 627.0529 | CE: 0.3005 | KD: 1061.4108\n",
      "Train Epoch: 025 Batch: 00038/00094 | Loss: 626.7667 | CE: 0.3881 | KD: 1060.7776\n",
      "Train Epoch: 025 Batch: 00039/00094 | Loss: 626.9916 | CE: 0.4504 | KD: 1061.0531\n",
      "Train Epoch: 025 Batch: 00040/00094 | Loss: 626.7445 | CE: 0.3967 | KD: 1060.7256\n",
      "Train Epoch: 025 Batch: 00041/00094 | Loss: 626.5089 | CE: 0.3424 | KD: 1060.4186\n",
      "Train Epoch: 025 Batch: 00042/00094 | Loss: 626.5895 | CE: 0.3825 | KD: 1060.4872\n",
      "Train Epoch: 025 Batch: 00043/00094 | Loss: 626.4969 | CE: 0.2971 | KD: 1060.4750\n",
      "Train Epoch: 025 Batch: 00044/00094 | Loss: 626.6002 | CE: 0.4004 | KD: 1060.4749\n",
      "Train Epoch: 025 Batch: 00045/00094 | Loss: 626.6105 | CE: 0.3615 | KD: 1060.5582\n",
      "Train Epoch: 025 Batch: 00046/00094 | Loss: 626.6504 | CE: 0.3870 | KD: 1060.5826\n",
      "Train Epoch: 025 Batch: 00047/00094 | Loss: 626.5885 | CE: 0.3873 | KD: 1060.4773\n",
      "Train Epoch: 025 Batch: 00048/00094 | Loss: 626.6473 | CE: 0.3382 | KD: 1060.6600\n",
      "Train Epoch: 025 Batch: 00049/00094 | Loss: 626.6406 | CE: 0.4573 | KD: 1060.4468\n",
      "Train Epoch: 025 Batch: 00050/00094 | Loss: 626.3971 | CE: 0.3228 | KD: 1060.2623\n",
      "Train Epoch: 025 Batch: 00051/00094 | Loss: 626.8151 | CE: 0.3751 | KD: 1060.8817\n",
      "Train Epoch: 025 Batch: 00052/00094 | Loss: 626.5413 | CE: 0.3348 | KD: 1060.4862\n",
      "Train Epoch: 025 Batch: 00053/00094 | Loss: 626.4956 | CE: 0.3453 | KD: 1060.3910\n",
      "Train Epoch: 025 Batch: 00054/00094 | Loss: 627.2150 | CE: 0.7164 | KD: 1060.9810\n",
      "Train Epoch: 025 Batch: 00055/00094 | Loss: 626.7599 | CE: 0.3578 | KD: 1060.8175\n",
      "Train Epoch: 025 Batch: 00056/00094 | Loss: 626.4855 | CE: 0.3224 | KD: 1060.4127\n",
      "Train Epoch: 025 Batch: 00057/00094 | Loss: 626.4900 | CE: 0.3643 | KD: 1060.3494\n",
      "Train Epoch: 025 Batch: 00058/00094 | Loss: 626.6520 | CE: 0.4132 | KD: 1060.5410\n",
      "Train Epoch: 025 Batch: 00059/00094 | Loss: 626.4728 | CE: 0.3441 | KD: 1060.3546\n",
      "Train Epoch: 025 Batch: 00060/00094 | Loss: 626.7152 | CE: 0.4602 | KD: 1060.5685\n",
      "Train Epoch: 025 Batch: 00061/00094 | Loss: 626.5797 | CE: 0.4608 | KD: 1060.3378\n",
      "Train Epoch: 025 Batch: 00062/00094 | Loss: 626.6791 | CE: 0.4554 | KD: 1060.5154\n",
      "Train Epoch: 025 Batch: 00063/00094 | Loss: 626.5227 | CE: 0.3825 | KD: 1060.3739\n",
      "Train Epoch: 025 Batch: 00064/00094 | Loss: 626.6501 | CE: 0.3326 | KD: 1060.6743\n",
      "Train Epoch: 025 Batch: 00065/00094 | Loss: 626.8599 | CE: 0.3169 | KD: 1061.0559\n",
      "Train Epoch: 025 Batch: 00066/00094 | Loss: 627.1997 | CE: 0.4660 | KD: 1061.3790\n",
      "Train Epoch: 025 Batch: 00067/00094 | Loss: 626.8066 | CE: 0.4253 | KD: 1060.7822\n",
      "Train Epoch: 025 Batch: 00068/00094 | Loss: 626.4396 | CE: 0.3287 | KD: 1060.3245\n",
      "Train Epoch: 025 Batch: 00069/00094 | Loss: 626.4205 | CE: 0.3147 | KD: 1060.3158\n",
      "Train Epoch: 025 Batch: 00070/00094 | Loss: 627.0587 | CE: 0.3835 | KD: 1061.2799\n",
      "Train Epoch: 025 Batch: 00071/00094 | Loss: 626.7905 | CE: 0.4925 | KD: 1060.6412\n",
      "Train Epoch: 025 Batch: 00072/00094 | Loss: 626.3997 | CE: 0.2855 | KD: 1060.3298\n",
      "Train Epoch: 025 Batch: 00073/00094 | Loss: 626.9827 | CE: 0.4923 | KD: 1060.9670\n",
      "Train Epoch: 025 Batch: 00074/00094 | Loss: 627.1401 | CE: 0.4114 | KD: 1061.3706\n",
      "Train Epoch: 025 Batch: 00075/00094 | Loss: 626.5898 | CE: 0.3164 | KD: 1060.5997\n",
      "Train Epoch: 025 Batch: 00076/00094 | Loss: 626.6942 | CE: 0.3691 | KD: 1060.6871\n",
      "Train Epoch: 025 Batch: 00077/00094 | Loss: 626.5276 | CE: 0.3553 | KD: 1060.4285\n",
      "Train Epoch: 025 Batch: 00078/00094 | Loss: 626.5815 | CE: 0.3326 | KD: 1060.5580\n",
      "Train Epoch: 025 Batch: 00079/00094 | Loss: 626.5763 | CE: 0.3974 | KD: 1060.4395\n",
      "Train Epoch: 025 Batch: 00080/00094 | Loss: 626.3894 | CE: 0.3005 | KD: 1060.2870\n",
      "Train Epoch: 025 Batch: 00081/00094 | Loss: 626.3443 | CE: 0.3232 | KD: 1060.1722\n",
      "Train Epoch: 025 Batch: 00082/00094 | Loss: 626.4922 | CE: 0.4141 | KD: 1060.2688\n",
      "Train Epoch: 025 Batch: 00083/00094 | Loss: 626.5734 | CE: 0.3446 | KD: 1060.5239\n",
      "Train Epoch: 025 Batch: 00084/00094 | Loss: 626.7521 | CE: 0.4386 | KD: 1060.6674\n",
      "Train Epoch: 025 Batch: 00085/00094 | Loss: 626.8191 | CE: 0.3715 | KD: 1060.8945\n",
      "Train Epoch: 025 Batch: 00086/00094 | Loss: 626.8055 | CE: 0.3445 | KD: 1060.9171\n",
      "Train Epoch: 025 Batch: 00087/00094 | Loss: 626.7212 | CE: 0.3351 | KD: 1060.7904\n",
      "Train Epoch: 025 Batch: 00088/00094 | Loss: 627.0916 | CE: 0.4354 | KD: 1061.2479\n",
      "Train Epoch: 025 Batch: 00089/00094 | Loss: 626.6494 | CE: 0.4770 | KD: 1060.4285\n",
      "Train Epoch: 025 Batch: 00090/00094 | Loss: 626.6121 | CE: 0.3533 | KD: 1060.5748\n",
      "Train Epoch: 025 Batch: 00091/00094 | Loss: 626.6260 | CE: 0.3526 | KD: 1060.5995\n",
      "Train Epoch: 025 Batch: 00092/00094 | Loss: 626.4246 | CE: 0.3006 | KD: 1060.3464\n",
      "Train Epoch: 025 Batch: 00093/00094 | Loss: 626.3882 | CE: 0.3069 | KD: 1060.2743\n",
      "Train Epoch: 025 Batch: 00094/00094 | Loss: 626.3781 | CE: 0.3054 | KD: 1060.2596\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4554 | acc:83.7000\n",
      "[VAL Acc] Target: 83.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3074 | acc:49.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9701 | acc:52.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3071 | acc:47.1374\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8648 | acc:49.8041\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9269 | acc:52.3105\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6451 | acc:63.0878\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9596 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7886 | acc:60.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.50%\n",
      "[VAL Acc] Avg 56.52%\n",
      "Train Epoch: 026 Batch: 00001/00094 | Loss: 564.2626 | CE: 0.5689 | KD: 1060.6891\n",
      "Train Epoch: 026 Batch: 00002/00094 | Loss: 563.9262 | CE: 0.3725 | KD: 1060.4258\n",
      "Train Epoch: 026 Batch: 00003/00094 | Loss: 563.8145 | CE: 0.2661 | KD: 1060.4158\n",
      "Train Epoch: 026 Batch: 00004/00094 | Loss: 563.8703 | CE: 0.3774 | KD: 1060.3113\n",
      "Train Epoch: 026 Batch: 00005/00094 | Loss: 563.8403 | CE: 0.2286 | KD: 1060.5349\n",
      "Train Epoch: 026 Batch: 00006/00094 | Loss: 564.1155 | CE: 0.3420 | KD: 1060.8392\n",
      "Train Epoch: 026 Batch: 00007/00094 | Loss: 563.7958 | CE: 0.2809 | KD: 1060.3527\n",
      "Train Epoch: 026 Batch: 00008/00094 | Loss: 563.9464 | CE: 0.3640 | KD: 1060.4796\n",
      "Train Epoch: 026 Batch: 00009/00094 | Loss: 563.9374 | CE: 0.3142 | KD: 1060.5565\n",
      "Train Epoch: 026 Batch: 00010/00094 | Loss: 564.2294 | CE: 0.4127 | KD: 1060.9207\n",
      "Train Epoch: 026 Batch: 00011/00094 | Loss: 563.9807 | CE: 0.4024 | KD: 1060.4722\n",
      "Train Epoch: 026 Batch: 00012/00094 | Loss: 563.9935 | CE: 0.2777 | KD: 1060.7306\n",
      "Train Epoch: 026 Batch: 00013/00094 | Loss: 563.7897 | CE: 0.2993 | KD: 1060.3066\n",
      "Train Epoch: 026 Batch: 00014/00094 | Loss: 564.3952 | CE: 0.4842 | KD: 1061.0980\n",
      "Train Epoch: 026 Batch: 00015/00094 | Loss: 564.1235 | CE: 0.4024 | KD: 1060.7408\n",
      "Train Epoch: 026 Batch: 00016/00094 | Loss: 564.2033 | CE: 0.2814 | KD: 1061.1185\n",
      "Train Epoch: 026 Batch: 00017/00094 | Loss: 563.7521 | CE: 0.3038 | KD: 1060.2275\n",
      "Train Epoch: 026 Batch: 00018/00094 | Loss: 563.7903 | CE: 0.2749 | KD: 1060.3538\n",
      "Train Epoch: 026 Batch: 00019/00094 | Loss: 563.8318 | CE: 0.3375 | KD: 1060.3141\n",
      "Train Epoch: 026 Batch: 00020/00094 | Loss: 563.9037 | CE: 0.3367 | KD: 1060.4508\n",
      "Train Epoch: 026 Batch: 00021/00094 | Loss: 563.8619 | CE: 0.2171 | KD: 1060.5972\n",
      "Train Epoch: 026 Batch: 00022/00094 | Loss: 564.0175 | CE: 0.4373 | KD: 1060.4757\n",
      "Train Epoch: 026 Batch: 00023/00094 | Loss: 563.9025 | CE: 0.2861 | KD: 1060.5437\n",
      "Train Epoch: 026 Batch: 00024/00094 | Loss: 564.1020 | CE: 0.4649 | KD: 1060.5828\n",
      "Train Epoch: 026 Batch: 00025/00094 | Loss: 563.9415 | CE: 0.3139 | KD: 1060.5649\n",
      "Train Epoch: 026 Batch: 00026/00094 | Loss: 563.9435 | CE: 0.2533 | KD: 1060.6827\n",
      "Train Epoch: 026 Batch: 00027/00094 | Loss: 563.8857 | CE: 0.3385 | KD: 1060.4136\n",
      "Train Epoch: 026 Batch: 00028/00094 | Loss: 564.1453 | CE: 0.4123 | KD: 1060.7631\n",
      "Train Epoch: 026 Batch: 00029/00094 | Loss: 563.7892 | CE: 0.2950 | KD: 1060.3140\n",
      "Train Epoch: 026 Batch: 00030/00094 | Loss: 564.1619 | CE: 0.3556 | KD: 1060.9010\n",
      "Train Epoch: 026 Batch: 00031/00094 | Loss: 563.9196 | CE: 0.2822 | KD: 1060.5834\n",
      "Train Epoch: 026 Batch: 00032/00094 | Loss: 564.1145 | CE: 0.5032 | KD: 1060.5342\n",
      "Train Epoch: 026 Batch: 00033/00094 | Loss: 563.8077 | CE: 0.3290 | KD: 1060.2847\n",
      "Train Epoch: 026 Batch: 00034/00094 | Loss: 564.0702 | CE: 0.3683 | KD: 1060.7045\n",
      "Train Epoch: 026 Batch: 00035/00094 | Loss: 563.8250 | CE: 0.3030 | KD: 1060.3662\n",
      "Train Epoch: 026 Batch: 00036/00094 | Loss: 564.3337 | CE: 0.5460 | KD: 1060.8662\n",
      "Train Epoch: 026 Batch: 00037/00094 | Loss: 564.0676 | CE: 0.3230 | KD: 1060.7849\n",
      "Train Epoch: 026 Batch: 00038/00094 | Loss: 564.2203 | CE: 0.4682 | KD: 1060.7992\n",
      "Train Epoch: 026 Batch: 00039/00094 | Loss: 563.8152 | CE: 0.3427 | KD: 1060.2729\n",
      "Train Epoch: 026 Batch: 00040/00094 | Loss: 563.9454 | CE: 0.3069 | KD: 1060.5853\n",
      "Train Epoch: 026 Batch: 00041/00094 | Loss: 564.0828 | CE: 0.4484 | KD: 1060.5776\n",
      "Train Epoch: 026 Batch: 00042/00094 | Loss: 563.8157 | CE: 0.3638 | KD: 1060.2344\n",
      "Train Epoch: 026 Batch: 00043/00094 | Loss: 563.9368 | CE: 0.4282 | KD: 1060.3408\n",
      "Train Epoch: 026 Batch: 00044/00094 | Loss: 564.0626 | CE: 0.5066 | KD: 1060.4301\n",
      "Train Epoch: 026 Batch: 00045/00094 | Loss: 564.2646 | CE: 0.5926 | KD: 1060.6484\n",
      "Train Epoch: 026 Batch: 00046/00094 | Loss: 564.0056 | CE: 0.3582 | KD: 1060.6019\n",
      "Train Epoch: 026 Batch: 00047/00094 | Loss: 564.2279 | CE: 0.3549 | KD: 1061.0266\n",
      "Train Epoch: 026 Batch: 00048/00094 | Loss: 563.9962 | CE: 0.3779 | KD: 1060.5474\n",
      "Train Epoch: 026 Batch: 00049/00094 | Loss: 563.8273 | CE: 0.2810 | KD: 1060.4120\n",
      "Train Epoch: 026 Batch: 00050/00094 | Loss: 563.7119 | CE: 0.2427 | KD: 1060.2666\n",
      "Train Epoch: 026 Batch: 00051/00094 | Loss: 564.5217 | CE: 0.3677 | KD: 1061.5553\n",
      "Train Epoch: 026 Batch: 00052/00094 | Loss: 563.8784 | CE: 0.3085 | KD: 1060.4563\n",
      "Train Epoch: 026 Batch: 00053/00094 | Loss: 564.1044 | CE: 0.4861 | KD: 1060.5472\n",
      "Train Epoch: 026 Batch: 00054/00094 | Loss: 563.8952 | CE: 0.2546 | KD: 1060.5894\n",
      "Train Epoch: 026 Batch: 00055/00094 | Loss: 564.0626 | CE: 0.3913 | KD: 1060.6470\n",
      "Train Epoch: 026 Batch: 00056/00094 | Loss: 564.1389 | CE: 0.3829 | KD: 1060.8065\n",
      "Train Epoch: 026 Batch: 00057/00094 | Loss: 563.8445 | CE: 0.2777 | KD: 1060.4502\n",
      "Train Epoch: 026 Batch: 00058/00094 | Loss: 563.9850 | CE: 0.3255 | KD: 1060.6249\n",
      "Train Epoch: 026 Batch: 00059/00094 | Loss: 564.0380 | CE: 0.3030 | KD: 1060.7667\n",
      "Train Epoch: 026 Batch: 00060/00094 | Loss: 564.3714 | CE: 0.5913 | KD: 1060.8518\n",
      "Train Epoch: 026 Batch: 00061/00094 | Loss: 564.2601 | CE: 0.4415 | KD: 1060.9243\n",
      "Train Epoch: 026 Batch: 00062/00094 | Loss: 563.8416 | CE: 0.3453 | KD: 1060.3179\n",
      "Train Epoch: 026 Batch: 00063/00094 | Loss: 563.9471 | CE: 0.3703 | KD: 1060.4692\n",
      "Train Epoch: 026 Batch: 00064/00094 | Loss: 564.3356 | CE: 0.3964 | KD: 1061.1511\n",
      "Train Epoch: 026 Batch: 00065/00094 | Loss: 563.8120 | CE: 0.3263 | KD: 1060.2980\n",
      "Train Epoch: 026 Batch: 00066/00094 | Loss: 564.3018 | CE: 0.4233 | KD: 1061.0369\n",
      "Train Epoch: 026 Batch: 00067/00094 | Loss: 564.0351 | CE: 0.3668 | KD: 1060.6414\n",
      "Train Epoch: 026 Batch: 00068/00094 | Loss: 564.3441 | CE: 0.5032 | KD: 1060.9662\n",
      "Train Epoch: 026 Batch: 00069/00094 | Loss: 563.9130 | CE: 0.4510 | KD: 1060.2532\n",
      "Train Epoch: 026 Batch: 00070/00094 | Loss: 563.9144 | CE: 0.3229 | KD: 1060.4969\n",
      "Train Epoch: 026 Batch: 00071/00094 | Loss: 563.9282 | CE: 0.3015 | KD: 1060.5631\n",
      "Train Epoch: 026 Batch: 00072/00094 | Loss: 563.9679 | CE: 0.3286 | KD: 1060.5869\n",
      "Train Epoch: 026 Batch: 00073/00094 | Loss: 563.9551 | CE: 0.4754 | KD: 1060.2866\n",
      "Train Epoch: 026 Batch: 00074/00094 | Loss: 564.1831 | CE: 0.5153 | KD: 1060.6405\n",
      "Train Epoch: 026 Batch: 00075/00094 | Loss: 563.9569 | CE: 0.3013 | KD: 1060.6176\n",
      "Train Epoch: 026 Batch: 00076/00094 | Loss: 564.1304 | CE: 0.3273 | KD: 1060.8949\n",
      "Train Epoch: 026 Batch: 00077/00094 | Loss: 564.1859 | CE: 0.4365 | KD: 1060.7939\n",
      "Train Epoch: 026 Batch: 00078/00094 | Loss: 564.1212 | CE: 0.3646 | KD: 1060.8076\n",
      "Train Epoch: 026 Batch: 00079/00094 | Loss: 564.1994 | CE: 0.3154 | KD: 1061.0474\n",
      "Train Epoch: 026 Batch: 00080/00094 | Loss: 564.1017 | CE: 0.3416 | KD: 1060.8141\n",
      "Train Epoch: 026 Batch: 00081/00094 | Loss: 563.7854 | CE: 0.3561 | KD: 1060.1918\n",
      "Train Epoch: 026 Batch: 00082/00094 | Loss: 563.7967 | CE: 0.3005 | KD: 1060.3176\n",
      "Train Epoch: 026 Batch: 00083/00094 | Loss: 563.8995 | CE: 0.2937 | KD: 1060.5238\n",
      "Train Epoch: 026 Batch: 00084/00094 | Loss: 563.7501 | CE: 0.2525 | KD: 1060.3201\n",
      "Train Epoch: 026 Batch: 00085/00094 | Loss: 564.1454 | CE: 0.3179 | KD: 1060.9412\n",
      "Train Epoch: 026 Batch: 00086/00094 | Loss: 564.2309 | CE: 0.3928 | KD: 1060.9609\n",
      "Train Epoch: 026 Batch: 00087/00094 | Loss: 563.8742 | CE: 0.3500 | KD: 1060.3704\n",
      "Train Epoch: 026 Batch: 00088/00094 | Loss: 563.8358 | CE: 0.3178 | KD: 1060.3586\n",
      "Train Epoch: 026 Batch: 00089/00094 | Loss: 563.9557 | CE: 0.3696 | KD: 1060.4867\n",
      "Train Epoch: 026 Batch: 00090/00094 | Loss: 564.0278 | CE: 0.2846 | KD: 1060.7825\n",
      "Train Epoch: 026 Batch: 00091/00094 | Loss: 564.3560 | CE: 0.4786 | KD: 1061.0347\n",
      "Train Epoch: 026 Batch: 00092/00094 | Loss: 563.9888 | CE: 0.4233 | KD: 1060.4480\n",
      "Train Epoch: 026 Batch: 00093/00094 | Loss: 563.9648 | CE: 0.3276 | KD: 1060.5830\n",
      "Train Epoch: 026 Batch: 00094/00094 | Loss: 564.1396 | CE: 0.3083 | KD: 1060.9481\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3750 | acc:86.6000\n",
      "[VAL Acc] Target: 86.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.1823 | acc:49.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8684 | acc:54.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1813 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.7862 | acc:50.6661\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.67%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8668 | acc:52.6802\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6239 | acc:64.7335\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.73%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8984 | acc:48.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.6990 | acc:62.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 62.60%\n",
      "[VAL Acc] Avg 57.66%\n",
      "Train Epoch: 027 Batch: 00001/00094 | Loss: 563.9837 | CE: 0.3119 | KD: 1060.6481\n",
      "Train Epoch: 027 Batch: 00002/00094 | Loss: 563.9926 | CE: 0.2884 | KD: 1060.7089\n",
      "Train Epoch: 027 Batch: 00003/00094 | Loss: 563.8504 | CE: 0.3184 | KD: 1060.3850\n",
      "Train Epoch: 027 Batch: 00004/00094 | Loss: 564.1703 | CE: 0.3571 | KD: 1060.9141\n",
      "Train Epoch: 027 Batch: 00005/00094 | Loss: 564.0756 | CE: 0.3461 | KD: 1060.7566\n",
      "Train Epoch: 027 Batch: 00006/00094 | Loss: 564.1584 | CE: 0.4404 | KD: 1060.7349\n",
      "Train Epoch: 027 Batch: 00007/00094 | Loss: 564.0046 | CE: 0.3368 | KD: 1060.6404\n",
      "Train Epoch: 027 Batch: 00008/00094 | Loss: 564.5300 | CE: 0.4598 | KD: 1061.3975\n",
      "Train Epoch: 027 Batch: 00009/00094 | Loss: 563.8892 | CE: 0.2702 | KD: 1060.5487\n",
      "Train Epoch: 027 Batch: 00010/00094 | Loss: 563.9921 | CE: 0.2745 | KD: 1060.7340\n",
      "Train Epoch: 027 Batch: 00011/00094 | Loss: 563.9938 | CE: 0.3898 | KD: 1060.5204\n",
      "Train Epoch: 027 Batch: 00012/00094 | Loss: 563.9496 | CE: 0.3104 | KD: 1060.5868\n",
      "Train Epoch: 027 Batch: 00013/00094 | Loss: 564.1504 | CE: 0.3122 | KD: 1060.9611\n",
      "Train Epoch: 027 Batch: 00014/00094 | Loss: 563.9500 | CE: 0.3527 | KD: 1060.5077\n",
      "Train Epoch: 027 Batch: 00015/00094 | Loss: 564.0464 | CE: 0.4175 | KD: 1060.5674\n",
      "Train Epoch: 027 Batch: 00016/00094 | Loss: 563.7361 | CE: 0.3036 | KD: 1060.1979\n",
      "Train Epoch: 027 Batch: 00017/00094 | Loss: 564.0035 | CE: 0.3139 | KD: 1060.6814\n",
      "Train Epoch: 027 Batch: 00018/00094 | Loss: 564.0616 | CE: 0.3536 | KD: 1060.7161\n",
      "Train Epoch: 027 Batch: 00019/00094 | Loss: 564.1300 | CE: 0.3166 | KD: 1060.9144\n",
      "Train Epoch: 027 Batch: 00020/00094 | Loss: 564.2338 | CE: 0.3202 | KD: 1061.1029\n",
      "Train Epoch: 027 Batch: 00021/00094 | Loss: 564.1258 | CE: 0.3184 | KD: 1060.9031\n",
      "Train Epoch: 027 Batch: 00022/00094 | Loss: 564.0818 | CE: 0.4390 | KD: 1060.5936\n",
      "Train Epoch: 027 Batch: 00023/00094 | Loss: 563.9522 | CE: 0.3775 | KD: 1060.4653\n",
      "Train Epoch: 027 Batch: 00024/00094 | Loss: 563.9550 | CE: 0.2919 | KD: 1060.6316\n",
      "Train Epoch: 027 Batch: 00025/00094 | Loss: 563.9333 | CE: 0.3220 | KD: 1060.5343\n",
      "Train Epoch: 027 Batch: 00026/00094 | Loss: 563.8678 | CE: 0.3105 | KD: 1060.4325\n",
      "Train Epoch: 027 Batch: 00027/00094 | Loss: 563.9003 | CE: 0.3134 | KD: 1060.4883\n",
      "Train Epoch: 027 Batch: 00028/00094 | Loss: 564.0297 | CE: 0.4144 | KD: 1060.5416\n",
      "Train Epoch: 027 Batch: 00029/00094 | Loss: 564.1054 | CE: 0.5176 | KD: 1060.4900\n",
      "Train Epoch: 027 Batch: 00030/00094 | Loss: 564.1343 | CE: 0.3907 | KD: 1060.7831\n",
      "Train Epoch: 027 Batch: 00031/00094 | Loss: 563.9648 | CE: 0.3806 | KD: 1060.4832\n",
      "Train Epoch: 027 Batch: 00032/00094 | Loss: 563.8561 | CE: 0.3086 | KD: 1060.4141\n",
      "Train Epoch: 027 Batch: 00033/00094 | Loss: 564.1740 | CE: 0.3281 | KD: 1060.9756\n",
      "Train Epoch: 027 Batch: 00034/00094 | Loss: 563.9064 | CE: 0.3197 | KD: 1060.4879\n",
      "Train Epoch: 027 Batch: 00035/00094 | Loss: 563.9028 | CE: 0.3614 | KD: 1060.4026\n",
      "Train Epoch: 027 Batch: 00036/00094 | Loss: 564.0900 | CE: 0.4979 | KD: 1060.4982\n",
      "Train Epoch: 027 Batch: 00037/00094 | Loss: 563.8864 | CE: 0.3034 | KD: 1060.4811\n",
      "Train Epoch: 027 Batch: 00038/00094 | Loss: 564.1955 | CE: 0.4578 | KD: 1060.7721\n",
      "Train Epoch: 027 Batch: 00039/00094 | Loss: 563.9079 | CE: 0.2724 | KD: 1060.5797\n",
      "Train Epoch: 027 Batch: 00040/00094 | Loss: 564.3555 | CE: 0.4486 | KD: 1061.0903\n",
      "Train Epoch: 027 Batch: 00041/00094 | Loss: 563.8975 | CE: 0.3427 | KD: 1060.4277\n",
      "Train Epoch: 027 Batch: 00042/00094 | Loss: 564.0016 | CE: 0.3214 | KD: 1060.6637\n",
      "Train Epoch: 027 Batch: 00043/00094 | Loss: 564.1058 | CE: 0.3850 | KD: 1060.7404\n",
      "Train Epoch: 027 Batch: 00044/00094 | Loss: 563.8601 | CE: 0.3123 | KD: 1060.4147\n",
      "Train Epoch: 027 Batch: 00045/00094 | Loss: 563.8530 | CE: 0.3105 | KD: 1060.4047\n",
      "Train Epoch: 027 Batch: 00046/00094 | Loss: 564.0182 | CE: 0.3547 | KD: 1060.6324\n",
      "Train Epoch: 027 Batch: 00047/00094 | Loss: 563.7884 | CE: 0.3294 | KD: 1060.2477\n",
      "Train Epoch: 027 Batch: 00048/00094 | Loss: 563.8326 | CE: 0.2416 | KD: 1060.4958\n",
      "Train Epoch: 027 Batch: 00049/00094 | Loss: 563.8508 | CE: 0.3047 | KD: 1060.4114\n",
      "Train Epoch: 027 Batch: 00050/00094 | Loss: 564.1432 | CE: 0.3514 | KD: 1060.8739\n",
      "Train Epoch: 027 Batch: 00051/00094 | Loss: 563.7585 | CE: 0.2554 | KD: 1060.3304\n",
      "Train Epoch: 027 Batch: 00052/00094 | Loss: 563.8371 | CE: 0.3332 | KD: 1060.3320\n",
      "Train Epoch: 027 Batch: 00053/00094 | Loss: 564.2702 | CE: 0.4871 | KD: 1060.8575\n",
      "Train Epoch: 027 Batch: 00054/00094 | Loss: 564.0511 | CE: 0.3232 | KD: 1060.7535\n",
      "Train Epoch: 027 Batch: 00055/00094 | Loss: 563.8992 | CE: 0.2929 | KD: 1060.5247\n",
      "Train Epoch: 027 Batch: 00056/00094 | Loss: 564.0964 | CE: 0.3537 | KD: 1060.7814\n",
      "Train Epoch: 027 Batch: 00057/00094 | Loss: 563.9364 | CE: 0.3341 | KD: 1060.5173\n",
      "Train Epoch: 027 Batch: 00058/00094 | Loss: 563.9611 | CE: 0.3141 | KD: 1060.6013\n",
      "Train Epoch: 027 Batch: 00059/00094 | Loss: 563.9529 | CE: 0.3979 | KD: 1060.4282\n",
      "Train Epoch: 027 Batch: 00060/00094 | Loss: 563.8600 | CE: 0.3337 | KD: 1060.3743\n",
      "Train Epoch: 027 Batch: 00061/00094 | Loss: 564.0032 | CE: 0.3184 | KD: 1060.6725\n",
      "Train Epoch: 027 Batch: 00062/00094 | Loss: 564.0620 | CE: 0.4945 | KD: 1060.4518\n",
      "Train Epoch: 027 Batch: 00063/00094 | Loss: 563.9388 | CE: 0.3891 | KD: 1060.4182\n",
      "Train Epoch: 027 Batch: 00064/00094 | Loss: 563.9418 | CE: 0.2972 | KD: 1060.5968\n",
      "Train Epoch: 027 Batch: 00065/00094 | Loss: 563.8171 | CE: 0.3180 | KD: 1060.3231\n",
      "Train Epoch: 027 Batch: 00066/00094 | Loss: 563.8116 | CE: 0.3412 | KD: 1060.2690\n",
      "Train Epoch: 027 Batch: 00067/00094 | Loss: 564.0179 | CE: 0.3833 | KD: 1060.5781\n",
      "Train Epoch: 027 Batch: 00068/00094 | Loss: 563.8862 | CE: 0.3271 | KD: 1060.4360\n",
      "Train Epoch: 027 Batch: 00069/00094 | Loss: 564.0150 | CE: 0.2947 | KD: 1060.7393\n",
      "Train Epoch: 027 Batch: 00070/00094 | Loss: 564.0835 | CE: 0.3951 | KD: 1060.6792\n",
      "Train Epoch: 027 Batch: 00071/00094 | Loss: 564.0275 | CE: 0.4260 | KD: 1060.5157\n",
      "Train Epoch: 027 Batch: 00072/00094 | Loss: 564.4152 | CE: 0.3671 | KD: 1061.3560\n",
      "Train Epoch: 027 Batch: 00073/00094 | Loss: 564.2350 | CE: 0.4615 | KD: 1060.8394\n",
      "Train Epoch: 027 Batch: 00074/00094 | Loss: 563.9518 | CE: 0.3414 | KD: 1060.5325\n",
      "Train Epoch: 027 Batch: 00075/00094 | Loss: 563.8817 | CE: 0.2462 | KD: 1060.5797\n",
      "Train Epoch: 027 Batch: 00076/00094 | Loss: 563.9991 | CE: 0.3684 | KD: 1060.5707\n",
      "Train Epoch: 027 Batch: 00077/00094 | Loss: 564.0416 | CE: 0.4191 | KD: 1060.5552\n",
      "Train Epoch: 027 Batch: 00078/00094 | Loss: 563.9579 | CE: 0.3935 | KD: 1060.4460\n",
      "Train Epoch: 027 Batch: 00079/00094 | Loss: 564.3514 | CE: 0.3671 | KD: 1061.2361\n",
      "Train Epoch: 027 Batch: 00080/00094 | Loss: 563.9200 | CE: 0.3423 | KD: 1060.4711\n",
      "Train Epoch: 027 Batch: 00081/00094 | Loss: 564.1799 | CE: 0.4128 | KD: 1060.8273\n",
      "Train Epoch: 027 Batch: 00082/00094 | Loss: 563.9322 | CE: 0.3925 | KD: 1060.3994\n",
      "Train Epoch: 027 Batch: 00083/00094 | Loss: 564.0127 | CE: 0.4404 | KD: 1060.4607\n",
      "Train Epoch: 027 Batch: 00084/00094 | Loss: 564.2361 | CE: 0.3851 | KD: 1060.9852\n",
      "Train Epoch: 027 Batch: 00085/00094 | Loss: 563.8457 | CE: 0.2778 | KD: 1060.4525\n",
      "Train Epoch: 027 Batch: 00086/00094 | Loss: 564.0687 | CE: 0.2779 | KD: 1060.8719\n",
      "Train Epoch: 027 Batch: 00087/00094 | Loss: 563.7666 | CE: 0.3123 | KD: 1060.2386\n",
      "Train Epoch: 027 Batch: 00088/00094 | Loss: 563.8948 | CE: 0.3465 | KD: 1060.4155\n",
      "Train Epoch: 027 Batch: 00089/00094 | Loss: 564.2974 | CE: 0.4028 | KD: 1061.0671\n",
      "Train Epoch: 027 Batch: 00090/00094 | Loss: 564.0918 | CE: 0.3973 | KD: 1060.6907\n",
      "Train Epoch: 027 Batch: 00091/00094 | Loss: 563.9969 | CE: 0.2875 | KD: 1060.7188\n",
      "Train Epoch: 027 Batch: 00092/00094 | Loss: 564.1467 | CE: 0.4741 | KD: 1060.6494\n",
      "Train Epoch: 027 Batch: 00093/00094 | Loss: 563.7764 | CE: 0.3289 | KD: 1060.2260\n",
      "Train Epoch: 027 Batch: 00094/00094 | Loss: 564.3013 | CE: 0.3561 | KD: 1061.1625\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3988 | acc:84.6000\n",
      "[VAL Acc] Target: 84.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3188 | acc:49.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9603 | acc:52.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2177 | acc:47.9008\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8845 | acc:49.5690\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 49.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8919 | acc:52.6802\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6433 | acc:64.4201\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9123 | acc:49.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7507 | acc:61.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.30%\n",
      "[VAL Acc] Avg 56.83%\n",
      "Train Epoch: 028 Batch: 00001/00094 | Loss: 564.2718 | CE: 0.3712 | KD: 1061.0786\n",
      "Train Epoch: 028 Batch: 00002/00094 | Loss: 563.9760 | CE: 0.4555 | KD: 1060.3630\n",
      "Train Epoch: 028 Batch: 00003/00094 | Loss: 563.7332 | CE: 0.2423 | KD: 1060.3075\n",
      "Train Epoch: 028 Batch: 00004/00094 | Loss: 563.8792 | CE: 0.3207 | KD: 1060.4346\n",
      "Train Epoch: 028 Batch: 00005/00094 | Loss: 563.9200 | CE: 0.3823 | KD: 1060.3958\n",
      "Train Epoch: 028 Batch: 00006/00094 | Loss: 564.0756 | CE: 0.4308 | KD: 1060.5973\n",
      "Train Epoch: 028 Batch: 00007/00094 | Loss: 564.2040 | CE: 0.3514 | KD: 1060.9882\n",
      "Train Epoch: 028 Batch: 00008/00094 | Loss: 563.9315 | CE: 0.4160 | KD: 1060.3539\n",
      "Train Epoch: 028 Batch: 00009/00094 | Loss: 564.0110 | CE: 0.3791 | KD: 1060.5731\n",
      "Train Epoch: 028 Batch: 00010/00094 | Loss: 564.0136 | CE: 0.3116 | KD: 1060.7047\n",
      "Train Epoch: 028 Batch: 00011/00094 | Loss: 563.8663 | CE: 0.3501 | KD: 1060.3552\n",
      "Train Epoch: 028 Batch: 00012/00094 | Loss: 564.0279 | CE: 0.3734 | KD: 1060.6155\n",
      "Train Epoch: 028 Batch: 00013/00094 | Loss: 564.2816 | CE: 0.4170 | KD: 1061.0107\n",
      "Train Epoch: 028 Batch: 00014/00094 | Loss: 564.0058 | CE: 0.4340 | KD: 1060.4598\n",
      "Train Epoch: 028 Batch: 00015/00094 | Loss: 563.7169 | CE: 0.2639 | KD: 1060.2363\n",
      "Train Epoch: 028 Batch: 00016/00094 | Loss: 563.6146 | CE: 0.2478 | KD: 1060.0741\n",
      "Train Epoch: 028 Batch: 00017/00094 | Loss: 563.9749 | CE: 0.3667 | KD: 1060.5281\n",
      "Train Epoch: 028 Batch: 00018/00094 | Loss: 564.0264 | CE: 0.4217 | KD: 1060.5219\n",
      "Train Epoch: 028 Batch: 00019/00094 | Loss: 564.1437 | CE: 0.3825 | KD: 1060.8162\n",
      "Train Epoch: 028 Batch: 00020/00094 | Loss: 563.9653 | CE: 0.4068 | KD: 1060.4348\n",
      "Train Epoch: 028 Batch: 00021/00094 | Loss: 564.0438 | CE: 0.3662 | KD: 1060.6588\n",
      "Train Epoch: 028 Batch: 00022/00094 | Loss: 563.9199 | CE: 0.2951 | KD: 1060.5594\n",
      "Train Epoch: 028 Batch: 00023/00094 | Loss: 563.8882 | CE: 0.4211 | KD: 1060.2628\n",
      "Train Epoch: 028 Batch: 00024/00094 | Loss: 563.7531 | CE: 0.2978 | KD: 1060.2405\n",
      "Train Epoch: 028 Batch: 00025/00094 | Loss: 563.8989 | CE: 0.3309 | KD: 1060.4525\n",
      "Train Epoch: 028 Batch: 00026/00094 | Loss: 564.0424 | CE: 0.3229 | KD: 1060.7378\n",
      "Train Epoch: 028 Batch: 00027/00094 | Loss: 564.1683 | CE: 0.3332 | KD: 1060.9552\n",
      "Train Epoch: 028 Batch: 00028/00094 | Loss: 563.9364 | CE: 0.3547 | KD: 1060.4784\n",
      "Train Epoch: 028 Batch: 00029/00094 | Loss: 564.0002 | CE: 0.3113 | KD: 1060.6803\n",
      "Train Epoch: 028 Batch: 00030/00094 | Loss: 564.4300 | CE: 0.3534 | KD: 1061.4097\n",
      "Train Epoch: 028 Batch: 00031/00094 | Loss: 563.7386 | CE: 0.2794 | KD: 1060.2480\n",
      "Train Epoch: 028 Batch: 00032/00094 | Loss: 563.6710 | CE: 0.2268 | KD: 1060.2196\n",
      "Train Epoch: 028 Batch: 00033/00094 | Loss: 564.1212 | CE: 0.4078 | KD: 1060.7262\n",
      "Train Epoch: 028 Batch: 00034/00094 | Loss: 564.1015 | CE: 0.3848 | KD: 1060.7324\n",
      "Train Epoch: 028 Batch: 00035/00094 | Loss: 563.9684 | CE: 0.2910 | KD: 1060.6584\n",
      "Train Epoch: 028 Batch: 00036/00094 | Loss: 563.9663 | CE: 0.3551 | KD: 1060.5339\n",
      "Train Epoch: 028 Batch: 00037/00094 | Loss: 563.9990 | CE: 0.3099 | KD: 1060.6804\n",
      "Train Epoch: 028 Batch: 00038/00094 | Loss: 563.7849 | CE: 0.3124 | KD: 1060.2729\n",
      "Train Epoch: 028 Batch: 00039/00094 | Loss: 564.1781 | CE: 0.4172 | KD: 1060.8156\n",
      "Train Epoch: 028 Batch: 00040/00094 | Loss: 564.2610 | CE: 0.3578 | KD: 1061.0834\n",
      "Train Epoch: 028 Batch: 00041/00094 | Loss: 563.9827 | CE: 0.3444 | KD: 1060.5848\n",
      "Train Epoch: 028 Batch: 00042/00094 | Loss: 564.0042 | CE: 0.3285 | KD: 1060.6553\n",
      "Train Epoch: 028 Batch: 00043/00094 | Loss: 563.7957 | CE: 0.3380 | KD: 1060.2450\n",
      "Train Epoch: 028 Batch: 00044/00094 | Loss: 564.0713 | CE: 0.3880 | KD: 1060.6696\n",
      "Train Epoch: 028 Batch: 00045/00094 | Loss: 563.8895 | CE: 0.3527 | KD: 1060.3939\n",
      "Train Epoch: 028 Batch: 00046/00094 | Loss: 564.0098 | CE: 0.2895 | KD: 1060.7393\n",
      "Train Epoch: 028 Batch: 00047/00094 | Loss: 564.0541 | CE: 0.4482 | KD: 1060.5239\n",
      "Train Epoch: 028 Batch: 00048/00094 | Loss: 563.9251 | CE: 0.3553 | KD: 1060.4561\n",
      "Train Epoch: 028 Batch: 00049/00094 | Loss: 564.3885 | CE: 0.3548 | KD: 1061.3290\n",
      "Train Epoch: 028 Batch: 00050/00094 | Loss: 563.9638 | CE: 0.3789 | KD: 1060.4845\n",
      "Train Epoch: 028 Batch: 00051/00094 | Loss: 563.9303 | CE: 0.3280 | KD: 1060.5172\n",
      "Train Epoch: 028 Batch: 00052/00094 | Loss: 564.0066 | CE: 0.3133 | KD: 1060.6884\n",
      "Train Epoch: 028 Batch: 00053/00094 | Loss: 564.0687 | CE: 0.2695 | KD: 1060.8876\n",
      "Train Epoch: 028 Batch: 00054/00094 | Loss: 564.0742 | CE: 0.4400 | KD: 1060.5771\n",
      "Train Epoch: 028 Batch: 00055/00094 | Loss: 564.0222 | CE: 0.4202 | KD: 1060.5167\n",
      "Train Epoch: 028 Batch: 00056/00094 | Loss: 563.9701 | CE: 0.3474 | KD: 1060.5555\n",
      "Train Epoch: 028 Batch: 00057/00094 | Loss: 563.9528 | CE: 0.3472 | KD: 1060.5236\n",
      "Train Epoch: 028 Batch: 00058/00094 | Loss: 564.0869 | CE: 0.3070 | KD: 1060.8514\n",
      "Train Epoch: 028 Batch: 00059/00094 | Loss: 564.0151 | CE: 0.3307 | KD: 1060.6716\n",
      "Train Epoch: 028 Batch: 00060/00094 | Loss: 564.1185 | CE: 0.4243 | KD: 1060.6902\n",
      "Train Epoch: 028 Batch: 00061/00094 | Loss: 564.1764 | CE: 0.4036 | KD: 1060.8380\n",
      "Train Epoch: 028 Batch: 00062/00094 | Loss: 563.7305 | CE: 0.2346 | KD: 1060.3170\n",
      "Train Epoch: 028 Batch: 00063/00094 | Loss: 563.9987 | CE: 0.3425 | KD: 1060.6185\n",
      "Train Epoch: 028 Batch: 00064/00094 | Loss: 563.7829 | CE: 0.2974 | KD: 1060.2974\n",
      "Train Epoch: 028 Batch: 00065/00094 | Loss: 564.0169 | CE: 0.3350 | KD: 1060.6670\n",
      "Train Epoch: 028 Batch: 00066/00094 | Loss: 564.0914 | CE: 0.3493 | KD: 1060.7802\n",
      "Train Epoch: 028 Batch: 00067/00094 | Loss: 564.0592 | CE: 0.2765 | KD: 1060.8567\n",
      "Train Epoch: 028 Batch: 00068/00094 | Loss: 563.8963 | CE: 0.4070 | KD: 1060.3046\n",
      "Train Epoch: 028 Batch: 00069/00094 | Loss: 564.0965 | CE: 0.4609 | KD: 1060.5798\n",
      "Train Epoch: 028 Batch: 00070/00094 | Loss: 563.8166 | CE: 0.2867 | KD: 1060.3809\n",
      "Train Epoch: 028 Batch: 00071/00094 | Loss: 564.0657 | CE: 0.4132 | KD: 1060.6117\n",
      "Train Epoch: 028 Batch: 00072/00094 | Loss: 564.1296 | CE: 0.3722 | KD: 1060.8092\n",
      "Train Epoch: 028 Batch: 00073/00094 | Loss: 564.3362 | CE: 0.2783 | KD: 1061.3744\n",
      "Train Epoch: 028 Batch: 00074/00094 | Loss: 563.8865 | CE: 0.3672 | KD: 1060.3610\n",
      "Train Epoch: 028 Batch: 00075/00094 | Loss: 564.0109 | CE: 0.3747 | KD: 1060.5809\n",
      "Train Epoch: 028 Batch: 00076/00094 | Loss: 563.8729 | CE: 0.3643 | KD: 1060.3408\n",
      "Train Epoch: 028 Batch: 00077/00094 | Loss: 564.1160 | CE: 0.3817 | KD: 1060.7656\n",
      "Train Epoch: 028 Batch: 00078/00094 | Loss: 564.0176 | CE: 0.4341 | KD: 1060.4817\n",
      "Train Epoch: 028 Batch: 00079/00094 | Loss: 564.2677 | CE: 0.4247 | KD: 1060.9702\n",
      "Train Epoch: 028 Batch: 00080/00094 | Loss: 564.1646 | CE: 0.3718 | KD: 1060.8756\n",
      "Train Epoch: 028 Batch: 00081/00094 | Loss: 564.1196 | CE: 0.3946 | KD: 1060.7480\n",
      "Train Epoch: 028 Batch: 00082/00094 | Loss: 563.6979 | CE: 0.2849 | KD: 1060.1609\n",
      "Train Epoch: 028 Batch: 00083/00094 | Loss: 563.8056 | CE: 0.2944 | KD: 1060.3457\n",
      "Train Epoch: 028 Batch: 00084/00094 | Loss: 564.3158 | CE: 0.4691 | KD: 1060.9771\n",
      "Train Epoch: 028 Batch: 00085/00094 | Loss: 564.0112 | CE: 0.4174 | KD: 1060.5013\n",
      "Train Epoch: 028 Batch: 00086/00094 | Loss: 564.0039 | CE: 0.3347 | KD: 1060.6431\n",
      "Train Epoch: 028 Batch: 00087/00094 | Loss: 564.0327 | CE: 0.4469 | KD: 1060.4863\n",
      "Train Epoch: 028 Batch: 00088/00094 | Loss: 563.8700 | CE: 0.2602 | KD: 1060.5314\n",
      "Train Epoch: 028 Batch: 00089/00094 | Loss: 563.9894 | CE: 0.2705 | KD: 1060.7365\n",
      "Train Epoch: 028 Batch: 00090/00094 | Loss: 564.4158 | CE: 0.5514 | KD: 1061.0104\n",
      "Train Epoch: 028 Batch: 00091/00094 | Loss: 564.0049 | CE: 0.3302 | KD: 1060.6537\n",
      "Train Epoch: 028 Batch: 00092/00094 | Loss: 563.9202 | CE: 0.3733 | KD: 1060.4130\n",
      "Train Epoch: 028 Batch: 00093/00094 | Loss: 563.8587 | CE: 0.2688 | KD: 1060.4939\n",
      "Train Epoch: 028 Batch: 00094/00094 | Loss: 563.9905 | CE: 0.3142 | KD: 1060.6565\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4237 | acc:84.2000\n",
      "[VAL Acc] Target: 84.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2919 | acc:49.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9790 | acc:51.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3183 | acc:47.1374\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8548 | acc:50.2743\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8903 | acc:53.1423\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.14%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6376 | acc:65.9483\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9227 | acc:48.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7643 | acc:61.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.00%\n",
      "[VAL Acc] Avg 56.82%\n",
      "Train Epoch: 029 Batch: 00001/00094 | Loss: 564.1386 | CE: 0.4238 | KD: 1060.7290\n",
      "Train Epoch: 029 Batch: 00002/00094 | Loss: 563.9152 | CE: 0.3127 | KD: 1060.5176\n",
      "Train Epoch: 029 Batch: 00003/00094 | Loss: 564.0685 | CE: 0.4017 | KD: 1060.6385\n",
      "Train Epoch: 029 Batch: 00004/00094 | Loss: 564.0110 | CE: 0.4311 | KD: 1060.4751\n",
      "Train Epoch: 029 Batch: 00005/00094 | Loss: 564.1397 | CE: 0.4207 | KD: 1060.7368\n",
      "Train Epoch: 029 Batch: 00006/00094 | Loss: 563.7908 | CE: 0.2920 | KD: 1060.3223\n",
      "Train Epoch: 029 Batch: 00007/00094 | Loss: 563.8823 | CE: 0.3171 | KD: 1060.4473\n",
      "Train Epoch: 029 Batch: 00008/00094 | Loss: 563.8415 | CE: 0.3009 | KD: 1060.4011\n",
      "Train Epoch: 029 Batch: 00009/00094 | Loss: 563.9083 | CE: 0.3340 | KD: 1060.4646\n",
      "Train Epoch: 029 Batch: 00010/00094 | Loss: 564.0403 | CE: 0.4661 | KD: 1060.4644\n",
      "Train Epoch: 029 Batch: 00011/00094 | Loss: 564.5311 | CE: 0.4512 | KD: 1061.4159\n",
      "Train Epoch: 029 Batch: 00012/00094 | Loss: 563.8301 | CE: 0.3492 | KD: 1060.2888\n",
      "Train Epoch: 029 Batch: 00013/00094 | Loss: 564.2437 | CE: 0.3536 | KD: 1061.0588\n",
      "Train Epoch: 029 Batch: 00014/00094 | Loss: 564.1168 | CE: 0.5163 | KD: 1060.5138\n",
      "Train Epoch: 029 Batch: 00015/00094 | Loss: 563.8641 | CE: 0.3158 | KD: 1060.4156\n",
      "Train Epoch: 029 Batch: 00016/00094 | Loss: 564.0659 | CE: 0.3923 | KD: 1060.6515\n",
      "Train Epoch: 029 Batch: 00017/00094 | Loss: 563.8186 | CE: 0.3135 | KD: 1060.3344\n",
      "Train Epoch: 029 Batch: 00018/00094 | Loss: 564.1075 | CE: 0.3912 | KD: 1060.7317\n",
      "Train Epoch: 029 Batch: 00019/00094 | Loss: 564.0475 | CE: 0.4168 | KD: 1060.5707\n",
      "Train Epoch: 029 Batch: 00020/00094 | Loss: 563.9778 | CE: 0.3750 | KD: 1060.5183\n",
      "Train Epoch: 029 Batch: 00021/00094 | Loss: 564.2960 | CE: 0.3270 | KD: 1061.2072\n",
      "Train Epoch: 029 Batch: 00022/00094 | Loss: 563.9285 | CE: 0.3833 | KD: 1060.4097\n",
      "Train Epoch: 029 Batch: 00023/00094 | Loss: 563.9197 | CE: 0.3670 | KD: 1060.4237\n",
      "Train Epoch: 029 Batch: 00024/00094 | Loss: 563.7222 | CE: 0.2797 | KD: 1060.2166\n",
      "Train Epoch: 029 Batch: 00025/00094 | Loss: 564.0981 | CE: 0.4333 | KD: 1060.6348\n",
      "Train Epoch: 029 Batch: 00026/00094 | Loss: 563.8398 | CE: 0.3686 | KD: 1060.2704\n",
      "Train Epoch: 029 Batch: 00027/00094 | Loss: 563.9141 | CE: 0.3969 | KD: 1060.3572\n",
      "Train Epoch: 029 Batch: 00028/00094 | Loss: 564.0152 | CE: 0.4864 | KD: 1060.3788\n",
      "Train Epoch: 029 Batch: 00029/00094 | Loss: 563.9268 | CE: 0.3165 | KD: 1060.5321\n",
      "Train Epoch: 029 Batch: 00030/00094 | Loss: 564.2251 | CE: 0.4454 | KD: 1060.8511\n",
      "Train Epoch: 029 Batch: 00031/00094 | Loss: 563.9567 | CE: 0.2968 | KD: 1060.6256\n",
      "Train Epoch: 029 Batch: 00032/00094 | Loss: 564.1532 | CE: 0.3781 | KD: 1060.8424\n",
      "Train Epoch: 029 Batch: 00033/00094 | Loss: 563.8583 | CE: 0.3072 | KD: 1060.4209\n",
      "Train Epoch: 029 Batch: 00034/00094 | Loss: 564.3068 | CE: 0.3664 | KD: 1061.1534\n",
      "Train Epoch: 029 Batch: 00035/00094 | Loss: 563.7603 | CE: 0.2736 | KD: 1060.2997\n",
      "Train Epoch: 029 Batch: 00036/00094 | Loss: 564.0250 | CE: 0.4102 | KD: 1060.5408\n",
      "Train Epoch: 029 Batch: 00037/00094 | Loss: 564.1702 | CE: 0.3305 | KD: 1060.9640\n",
      "Train Epoch: 029 Batch: 00038/00094 | Loss: 563.6477 | CE: 0.2494 | KD: 1060.1335\n",
      "Train Epoch: 029 Batch: 00039/00094 | Loss: 563.8005 | CE: 0.2877 | KD: 1060.3488\n",
      "Train Epoch: 029 Batch: 00040/00094 | Loss: 563.7858 | CE: 0.3391 | KD: 1060.2246\n",
      "Train Epoch: 029 Batch: 00041/00094 | Loss: 564.3559 | CE: 0.3910 | KD: 1061.1995\n",
      "Train Epoch: 029 Batch: 00042/00094 | Loss: 563.9056 | CE: 0.3564 | KD: 1060.4172\n",
      "Train Epoch: 029 Batch: 00043/00094 | Loss: 563.8560 | CE: 0.3214 | KD: 1060.3899\n",
      "Train Epoch: 029 Batch: 00044/00094 | Loss: 563.7290 | CE: 0.2247 | KD: 1060.3329\n",
      "Train Epoch: 029 Batch: 00045/00094 | Loss: 564.0518 | CE: 0.2623 | KD: 1060.8694\n",
      "Train Epoch: 029 Batch: 00046/00094 | Loss: 563.9831 | CE: 0.4098 | KD: 1060.4626\n",
      "Train Epoch: 029 Batch: 00047/00094 | Loss: 564.1207 | CE: 0.2772 | KD: 1060.9711\n",
      "Train Epoch: 029 Batch: 00048/00094 | Loss: 563.7422 | CE: 0.3069 | KD: 1060.2031\n",
      "Train Epoch: 029 Batch: 00049/00094 | Loss: 563.8538 | CE: 0.3713 | KD: 1060.2919\n",
      "Train Epoch: 029 Batch: 00050/00094 | Loss: 564.3469 | CE: 0.4756 | KD: 1061.0234\n",
      "Train Epoch: 029 Batch: 00051/00094 | Loss: 563.9220 | CE: 0.2879 | KD: 1060.5771\n",
      "Train Epoch: 029 Batch: 00052/00094 | Loss: 563.7098 | CE: 0.2394 | KD: 1060.2692\n",
      "Train Epoch: 029 Batch: 00053/00094 | Loss: 563.7673 | CE: 0.2903 | KD: 1060.2815\n",
      "Train Epoch: 029 Batch: 00054/00094 | Loss: 563.8745 | CE: 0.3921 | KD: 1060.2915\n",
      "Train Epoch: 029 Batch: 00055/00094 | Loss: 564.0856 | CE: 0.3029 | KD: 1060.8566\n",
      "Train Epoch: 029 Batch: 00056/00094 | Loss: 563.7853 | CE: 0.3675 | KD: 1060.1700\n",
      "Train Epoch: 029 Batch: 00057/00094 | Loss: 564.1628 | CE: 0.5337 | KD: 1060.5676\n",
      "Train Epoch: 029 Batch: 00058/00094 | Loss: 563.7625 | CE: 0.3380 | KD: 1060.1825\n",
      "Train Epoch: 029 Batch: 00059/00094 | Loss: 563.8778 | CE: 0.3528 | KD: 1060.3718\n",
      "Train Epoch: 029 Batch: 00060/00094 | Loss: 564.1049 | CE: 0.4273 | KD: 1060.6589\n",
      "Train Epoch: 029 Batch: 00061/00094 | Loss: 564.3689 | CE: 0.4369 | KD: 1061.1376\n",
      "Train Epoch: 029 Batch: 00062/00094 | Loss: 564.2241 | CE: 0.4474 | KD: 1060.8453\n",
      "Train Epoch: 029 Batch: 00063/00094 | Loss: 564.2053 | CE: 0.4321 | KD: 1060.8387\n",
      "Train Epoch: 029 Batch: 00064/00094 | Loss: 564.0143 | CE: 0.3438 | KD: 1060.6455\n",
      "Train Epoch: 029 Batch: 00065/00094 | Loss: 564.3691 | CE: 0.4499 | KD: 1061.1136\n",
      "Train Epoch: 029 Batch: 00066/00094 | Loss: 564.2288 | CE: 0.3962 | KD: 1060.9506\n",
      "Train Epoch: 029 Batch: 00067/00094 | Loss: 563.8535 | CE: 0.3349 | KD: 1060.3597\n",
      "Train Epoch: 029 Batch: 00068/00094 | Loss: 564.0347 | CE: 0.3687 | KD: 1060.6372\n",
      "Train Epoch: 029 Batch: 00069/00094 | Loss: 564.1121 | CE: 0.4098 | KD: 1060.7053\n",
      "Train Epoch: 029 Batch: 00070/00094 | Loss: 563.9401 | CE: 0.3354 | KD: 1060.5217\n",
      "Train Epoch: 029 Batch: 00071/00094 | Loss: 564.2335 | CE: 0.3577 | KD: 1061.0319\n",
      "Train Epoch: 029 Batch: 00072/00094 | Loss: 564.0355 | CE: 0.3680 | KD: 1060.6398\n",
      "Train Epoch: 029 Batch: 00073/00094 | Loss: 564.1088 | CE: 0.4257 | KD: 1060.6693\n",
      "Train Epoch: 029 Batch: 00074/00094 | Loss: 564.0959 | CE: 0.4232 | KD: 1060.6498\n",
      "Train Epoch: 029 Batch: 00075/00094 | Loss: 564.0463 | CE: 0.3823 | KD: 1060.6333\n",
      "Train Epoch: 029 Batch: 00076/00094 | Loss: 564.2854 | CE: 0.4431 | KD: 1060.9689\n",
      "Train Epoch: 029 Batch: 00077/00094 | Loss: 564.0109 | CE: 0.2924 | KD: 1060.7358\n",
      "Train Epoch: 029 Batch: 00078/00094 | Loss: 563.9073 | CE: 0.3327 | KD: 1060.4650\n",
      "Train Epoch: 029 Batch: 00079/00094 | Loss: 563.9745 | CE: 0.3283 | KD: 1060.5999\n",
      "Train Epoch: 029 Batch: 00080/00094 | Loss: 563.8766 | CE: 0.2999 | KD: 1060.4692\n",
      "Train Epoch: 029 Batch: 00081/00094 | Loss: 564.1379 | CE: 0.4572 | KD: 1060.6647\n",
      "Train Epoch: 029 Batch: 00082/00094 | Loss: 563.8278 | CE: 0.2968 | KD: 1060.3829\n",
      "Train Epoch: 029 Batch: 00083/00094 | Loss: 564.1853 | CE: 0.3268 | KD: 1060.9994\n",
      "Train Epoch: 029 Batch: 00084/00094 | Loss: 564.1552 | CE: 0.3118 | KD: 1060.9708\n",
      "Train Epoch: 029 Batch: 00085/00094 | Loss: 563.8477 | CE: 0.2948 | KD: 1060.4244\n",
      "Train Epoch: 029 Batch: 00086/00094 | Loss: 563.7357 | CE: 0.2797 | KD: 1060.2419\n",
      "Train Epoch: 029 Batch: 00087/00094 | Loss: 563.8062 | CE: 0.2772 | KD: 1060.3793\n",
      "Train Epoch: 029 Batch: 00088/00094 | Loss: 564.0600 | CE: 0.3900 | KD: 1060.6447\n",
      "Train Epoch: 029 Batch: 00089/00094 | Loss: 563.9137 | CE: 0.3104 | KD: 1060.5190\n",
      "Train Epoch: 029 Batch: 00090/00094 | Loss: 563.9510 | CE: 0.4039 | KD: 1060.4133\n",
      "Train Epoch: 029 Batch: 00091/00094 | Loss: 564.2148 | CE: 0.3935 | KD: 1060.9294\n",
      "Train Epoch: 029 Batch: 00092/00094 | Loss: 564.1988 | CE: 0.4053 | KD: 1060.8770\n",
      "Train Epoch: 029 Batch: 00093/00094 | Loss: 563.8546 | CE: 0.2501 | KD: 1060.5215\n",
      "Train Epoch: 029 Batch: 00094/00094 | Loss: 563.8738 | CE: 0.2269 | KD: 1060.6012\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3934 | acc:85.1000\n",
      "[VAL Acc] Target: 85.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2669 | acc:48.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9377 | acc:52.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2165 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8522 | acc:49.0987\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 49.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8884 | acc:52.9575\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6314 | acc:65.0078\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.01%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9130 | acc:49.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7130 | acc:62.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 62.05%\n",
      "[VAL Acc] Avg 57.01%\n",
      "Train Epoch: 030 Batch: 00001/00094 | Loss: 507.5272 | CE: 0.3292 | KD: 1060.4249\n",
      "Train Epoch: 030 Batch: 00002/00094 | Loss: 507.6359 | CE: 0.3169 | KD: 1060.6780\n",
      "Train Epoch: 030 Batch: 00003/00094 | Loss: 507.6925 | CE: 0.3537 | KD: 1060.7195\n",
      "Train Epoch: 030 Batch: 00004/00094 | Loss: 507.4396 | CE: 0.2976 | KD: 1060.3080\n",
      "Train Epoch: 030 Batch: 00005/00094 | Loss: 507.4233 | CE: 0.2806 | KD: 1060.3094\n",
      "Train Epoch: 030 Batch: 00006/00094 | Loss: 507.4395 | CE: 0.3477 | KD: 1060.2029\n",
      "Train Epoch: 030 Batch: 00007/00094 | Loss: 507.5679 | CE: 0.4307 | KD: 1060.2979\n",
      "Train Epoch: 030 Batch: 00008/00094 | Loss: 507.8253 | CE: 0.2887 | KD: 1061.1331\n",
      "Train Epoch: 030 Batch: 00009/00094 | Loss: 507.4862 | CE: 0.3152 | KD: 1060.3685\n",
      "Train Epoch: 030 Batch: 00010/00094 | Loss: 507.5180 | CE: 0.3692 | KD: 1060.3223\n",
      "Train Epoch: 030 Batch: 00011/00094 | Loss: 507.5410 | CE: 0.2960 | KD: 1060.5232\n",
      "Train Epoch: 030 Batch: 00012/00094 | Loss: 507.4955 | CE: 0.3196 | KD: 1060.3787\n",
      "Train Epoch: 030 Batch: 00013/00094 | Loss: 507.5440 | CE: 0.3301 | KD: 1060.4583\n",
      "Train Epoch: 030 Batch: 00014/00094 | Loss: 507.7300 | CE: 0.3737 | KD: 1060.7560\n",
      "Train Epoch: 030 Batch: 00015/00094 | Loss: 507.6400 | CE: 0.4064 | KD: 1060.4994\n",
      "Train Epoch: 030 Batch: 00016/00094 | Loss: 507.6128 | CE: 0.3581 | KD: 1060.5436\n",
      "Train Epoch: 030 Batch: 00017/00094 | Loss: 507.4947 | CE: 0.2870 | KD: 1060.4454\n",
      "Train Epoch: 030 Batch: 00018/00094 | Loss: 507.8623 | CE: 0.3456 | KD: 1061.0913\n",
      "Train Epoch: 030 Batch: 00019/00094 | Loss: 507.9596 | CE: 0.4763 | KD: 1061.0215\n",
      "Train Epoch: 030 Batch: 00020/00094 | Loss: 507.4000 | CE: 0.3162 | KD: 1060.1864\n",
      "Train Epoch: 030 Batch: 00021/00094 | Loss: 507.5663 | CE: 0.3189 | KD: 1060.5283\n",
      "Train Epoch: 030 Batch: 00022/00094 | Loss: 507.5110 | CE: 0.3334 | KD: 1060.3823\n",
      "Train Epoch: 030 Batch: 00023/00094 | Loss: 507.7464 | CE: 0.3980 | KD: 1060.7395\n",
      "Train Epoch: 030 Batch: 00024/00094 | Loss: 507.7360 | CE: 0.4167 | KD: 1060.6786\n",
      "Train Epoch: 030 Batch: 00025/00094 | Loss: 507.6634 | CE: 0.3358 | KD: 1060.6960\n",
      "Train Epoch: 030 Batch: 00026/00094 | Loss: 507.8058 | CE: 0.3947 | KD: 1060.8704\n",
      "Train Epoch: 030 Batch: 00027/00094 | Loss: 507.5055 | CE: 0.3051 | KD: 1060.4301\n",
      "Train Epoch: 030 Batch: 00028/00094 | Loss: 507.6679 | CE: 0.4260 | KD: 1060.5167\n",
      "Train Epoch: 030 Batch: 00029/00094 | Loss: 508.0356 | CE: 0.5028 | KD: 1061.1251\n",
      "Train Epoch: 030 Batch: 00030/00094 | Loss: 507.6002 | CE: 0.3238 | KD: 1060.5889\n",
      "Train Epoch: 030 Batch: 00031/00094 | Loss: 507.7147 | CE: 0.3936 | KD: 1060.6824\n",
      "Train Epoch: 030 Batch: 00032/00094 | Loss: 507.9412 | CE: 0.4450 | KD: 1061.0485\n",
      "Train Epoch: 030 Batch: 00033/00094 | Loss: 507.5985 | CE: 0.3164 | KD: 1060.6010\n",
      "Train Epoch: 030 Batch: 00034/00094 | Loss: 507.9908 | CE: 0.4930 | KD: 1061.0518\n",
      "Train Epoch: 030 Batch: 00035/00094 | Loss: 507.7424 | CE: 0.3350 | KD: 1060.8628\n",
      "Train Epoch: 030 Batch: 00036/00094 | Loss: 507.6467 | CE: 0.3236 | KD: 1060.6865\n",
      "Train Epoch: 030 Batch: 00037/00094 | Loss: 507.6114 | CE: 0.3096 | KD: 1060.6421\n",
      "Train Epoch: 030 Batch: 00038/00094 | Loss: 507.7987 | CE: 0.5176 | KD: 1060.5986\n",
      "Train Epoch: 030 Batch: 00039/00094 | Loss: 507.5275 | CE: 0.2331 | KD: 1060.6265\n",
      "Train Epoch: 030 Batch: 00040/00094 | Loss: 507.4336 | CE: 0.3172 | KD: 1060.2543\n",
      "Train Epoch: 030 Batch: 00041/00094 | Loss: 507.6709 | CE: 0.3236 | KD: 1060.7372\n",
      "Train Epoch: 030 Batch: 00042/00094 | Loss: 507.6078 | CE: 0.3219 | KD: 1060.6089\n",
      "Train Epoch: 030 Batch: 00043/00094 | Loss: 507.5690 | CE: 0.2879 | KD: 1060.5989\n",
      "Train Epoch: 030 Batch: 00044/00094 | Loss: 507.5679 | CE: 0.3546 | KD: 1060.4570\n",
      "Train Epoch: 030 Batch: 00045/00094 | Loss: 507.6800 | CE: 0.3338 | KD: 1060.7350\n",
      "Train Epoch: 030 Batch: 00046/00094 | Loss: 507.9244 | CE: 0.5451 | KD: 1060.8042\n",
      "Train Epoch: 030 Batch: 00047/00094 | Loss: 507.6276 | CE: 0.4234 | KD: 1060.4379\n",
      "Train Epoch: 030 Batch: 00048/00094 | Loss: 507.7109 | CE: 0.3370 | KD: 1060.7927\n",
      "Train Epoch: 030 Batch: 00049/00094 | Loss: 507.5759 | CE: 0.3104 | KD: 1060.5662\n",
      "Train Epoch: 030 Batch: 00050/00094 | Loss: 507.8469 | CE: 0.4302 | KD: 1060.8823\n",
      "Train Epoch: 030 Batch: 00051/00094 | Loss: 507.5831 | CE: 0.3383 | KD: 1060.5229\n",
      "Train Epoch: 030 Batch: 00052/00094 | Loss: 507.6736 | CE: 0.4564 | KD: 1060.4652\n",
      "Train Epoch: 030 Batch: 00053/00094 | Loss: 507.5714 | CE: 0.3421 | KD: 1060.4905\n",
      "Train Epoch: 030 Batch: 00054/00094 | Loss: 507.4175 | CE: 0.2996 | KD: 1060.2574\n",
      "Train Epoch: 030 Batch: 00055/00094 | Loss: 507.7979 | CE: 0.3529 | KD: 1060.9414\n",
      "Train Epoch: 030 Batch: 00056/00094 | Loss: 507.4669 | CE: 0.3187 | KD: 1060.3209\n",
      "Train Epoch: 030 Batch: 00057/00094 | Loss: 507.5075 | CE: 0.3134 | KD: 1060.4170\n",
      "Train Epoch: 030 Batch: 00058/00094 | Loss: 507.4307 | CE: 0.3323 | KD: 1060.2168\n",
      "Train Epoch: 030 Batch: 00059/00094 | Loss: 507.5673 | CE: 0.3061 | KD: 1060.5571\n",
      "Train Epoch: 030 Batch: 00060/00094 | Loss: 507.5985 | CE: 0.3193 | KD: 1060.5948\n",
      "Train Epoch: 030 Batch: 00061/00094 | Loss: 507.6306 | CE: 0.3283 | KD: 1060.6431\n",
      "Train Epoch: 030 Batch: 00062/00094 | Loss: 507.7014 | CE: 0.3126 | KD: 1060.8240\n",
      "Train Epoch: 030 Batch: 00063/00094 | Loss: 507.5957 | CE: 0.3361 | KD: 1060.5540\n",
      "Train Epoch: 030 Batch: 00064/00094 | Loss: 507.7893 | CE: 0.2925 | KD: 1061.0497\n",
      "Train Epoch: 030 Batch: 00065/00094 | Loss: 507.5508 | CE: 0.4405 | KD: 1060.2417\n",
      "Train Epoch: 030 Batch: 00066/00094 | Loss: 507.6004 | CE: 0.3235 | KD: 1060.5900\n",
      "Train Epoch: 030 Batch: 00067/00094 | Loss: 507.6306 | CE: 0.3467 | KD: 1060.6047\n",
      "Train Epoch: 030 Batch: 00068/00094 | Loss: 507.7546 | CE: 0.3354 | KD: 1060.8875\n",
      "Train Epoch: 030 Batch: 00069/00094 | Loss: 507.5255 | CE: 0.3025 | KD: 1060.4772\n",
      "Train Epoch: 030 Batch: 00070/00094 | Loss: 507.7899 | CE: 0.3716 | KD: 1060.8857\n",
      "Train Epoch: 030 Batch: 00071/00094 | Loss: 507.5761 | CE: 0.2812 | KD: 1060.6274\n",
      "Train Epoch: 030 Batch: 00072/00094 | Loss: 507.5971 | CE: 0.2752 | KD: 1060.6841\n",
      "Train Epoch: 030 Batch: 00073/00094 | Loss: 507.7174 | CE: 0.3764 | KD: 1060.7241\n",
      "Train Epoch: 030 Batch: 00074/00094 | Loss: 507.8257 | CE: 0.3832 | KD: 1060.9363\n",
      "Train Epoch: 030 Batch: 00075/00094 | Loss: 507.5872 | CE: 0.3728 | KD: 1060.4595\n",
      "Train Epoch: 030 Batch: 00076/00094 | Loss: 507.4090 | CE: 0.2443 | KD: 1060.3553\n",
      "Train Epoch: 030 Batch: 00077/00094 | Loss: 507.4630 | CE: 0.2602 | KD: 1060.4351\n",
      "Train Epoch: 030 Batch: 00078/00094 | Loss: 507.5866 | CE: 0.3401 | KD: 1060.5265\n",
      "Train Epoch: 030 Batch: 00079/00094 | Loss: 507.4699 | CE: 0.3083 | KD: 1060.3490\n",
      "Train Epoch: 030 Batch: 00080/00094 | Loss: 507.6046 | CE: 0.3056 | KD: 1060.6362\n",
      "Train Epoch: 030 Batch: 00081/00094 | Loss: 507.5313 | CE: 0.3256 | KD: 1060.4412\n",
      "Train Epoch: 030 Batch: 00082/00094 | Loss: 507.8909 | CE: 0.4595 | KD: 1060.9130\n",
      "Train Epoch: 030 Batch: 00083/00094 | Loss: 507.4995 | CE: 0.2631 | KD: 1060.5054\n",
      "Train Epoch: 030 Batch: 00084/00094 | Loss: 507.7065 | CE: 0.3234 | KD: 1060.8120\n",
      "Train Epoch: 030 Batch: 00085/00094 | Loss: 507.4975 | CE: 0.2794 | KD: 1060.4670\n",
      "Train Epoch: 030 Batch: 00086/00094 | Loss: 507.6758 | CE: 0.3694 | KD: 1060.6515\n",
      "Train Epoch: 030 Batch: 00087/00094 | Loss: 507.6618 | CE: 0.4652 | KD: 1060.4221\n",
      "Train Epoch: 030 Batch: 00088/00094 | Loss: 507.7672 | CE: 0.2793 | KD: 1061.0311\n",
      "Train Epoch: 030 Batch: 00089/00094 | Loss: 507.7236 | CE: 0.4508 | KD: 1060.5815\n",
      "Train Epoch: 030 Batch: 00090/00094 | Loss: 507.8242 | CE: 0.4310 | KD: 1060.8331\n",
      "Train Epoch: 030 Batch: 00091/00094 | Loss: 507.4620 | CE: 0.3129 | KD: 1060.3228\n",
      "Train Epoch: 030 Batch: 00092/00094 | Loss: 507.6001 | CE: 0.3219 | KD: 1060.5928\n",
      "Train Epoch: 030 Batch: 00093/00094 | Loss: 507.6299 | CE: 0.2683 | KD: 1060.7671\n",
      "Train Epoch: 030 Batch: 00094/00094 | Loss: 507.5442 | CE: 0.3916 | KD: 1060.3301\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4012 | acc:85.8500\n",
      "[VAL Acc] Target: 85.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2817 | acc:48.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9418 | acc:53.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2505 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8461 | acc:50.5486\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9217 | acc:52.7726\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.77%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6330 | acc:66.0266\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9355 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7191 | acc:61.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.90%\n",
      "[VAL Acc] Avg 57.43%\n",
      "Train Epoch: 031 Batch: 00001/00094 | Loss: 507.7155 | CE: 0.4130 | KD: 1060.6436\n",
      "Train Epoch: 031 Batch: 00002/00094 | Loss: 507.8078 | CE: 0.3797 | KD: 1060.9060\n",
      "Train Epoch: 031 Batch: 00003/00094 | Loss: 507.9910 | CE: 0.4072 | KD: 1061.2314\n",
      "Train Epoch: 031 Batch: 00004/00094 | Loss: 507.9551 | CE: 0.5585 | KD: 1060.8403\n",
      "Train Epoch: 031 Batch: 00005/00094 | Loss: 507.7255 | CE: 0.3534 | KD: 1060.7888\n",
      "Train Epoch: 031 Batch: 00006/00094 | Loss: 507.6693 | CE: 0.4367 | KD: 1060.4974\n",
      "Train Epoch: 031 Batch: 00007/00094 | Loss: 507.5966 | CE: 0.3057 | KD: 1060.6193\n",
      "Train Epoch: 031 Batch: 00008/00094 | Loss: 507.5351 | CE: 0.3232 | KD: 1060.4541\n",
      "Train Epoch: 031 Batch: 00009/00094 | Loss: 507.6364 | CE: 0.3279 | KD: 1060.6559\n",
      "Train Epoch: 031 Batch: 00010/00094 | Loss: 507.6268 | CE: 0.2455 | KD: 1060.8083\n",
      "Train Epoch: 031 Batch: 00011/00094 | Loss: 507.4823 | CE: 0.3397 | KD: 1060.3092\n",
      "Train Epoch: 031 Batch: 00012/00094 | Loss: 507.6230 | CE: 0.3274 | KD: 1060.6290\n",
      "Train Epoch: 031 Batch: 00013/00094 | Loss: 507.6578 | CE: 0.3826 | KD: 1060.5863\n",
      "Train Epoch: 031 Batch: 00014/00094 | Loss: 507.7987 | CE: 0.3418 | KD: 1060.9664\n",
      "Train Epoch: 031 Batch: 00015/00094 | Loss: 507.7566 | CE: 0.3353 | KD: 1060.8918\n",
      "Train Epoch: 031 Batch: 00016/00094 | Loss: 507.6562 | CE: 0.3077 | KD: 1060.7396\n",
      "Train Epoch: 031 Batch: 00017/00094 | Loss: 507.8281 | CE: 0.4202 | KD: 1060.8639\n",
      "Train Epoch: 031 Batch: 00018/00094 | Loss: 507.6039 | CE: 0.3636 | KD: 1060.5133\n",
      "Train Epoch: 031 Batch: 00019/00094 | Loss: 507.4319 | CE: 0.2332 | KD: 1060.4264\n",
      "Train Epoch: 031 Batch: 00020/00094 | Loss: 507.4719 | CE: 0.2928 | KD: 1060.3854\n",
      "Train Epoch: 031 Batch: 00021/00094 | Loss: 507.8715 | CE: 0.4017 | KD: 1060.9932\n",
      "Train Epoch: 031 Batch: 00022/00094 | Loss: 507.5998 | CE: 0.2873 | KD: 1060.6646\n",
      "Train Epoch: 031 Batch: 00023/00094 | Loss: 507.9530 | CE: 0.4575 | KD: 1061.0471\n",
      "Train Epoch: 031 Batch: 00024/00094 | Loss: 507.7448 | CE: 0.3194 | KD: 1060.9006\n",
      "Train Epoch: 031 Batch: 00025/00094 | Loss: 507.9052 | CE: 0.4162 | KD: 1061.0336\n",
      "Train Epoch: 031 Batch: 00026/00094 | Loss: 507.6319 | CE: 0.5365 | KD: 1060.2104\n",
      "Train Epoch: 031 Batch: 00027/00094 | Loss: 507.8086 | CE: 0.3725 | KD: 1060.9229\n",
      "Train Epoch: 031 Batch: 00028/00094 | Loss: 507.9180 | CE: 0.4907 | KD: 1060.9044\n",
      "Train Epoch: 031 Batch: 00029/00094 | Loss: 507.5838 | CE: 0.3486 | KD: 1060.5027\n",
      "Train Epoch: 031 Batch: 00030/00094 | Loss: 507.5197 | CE: 0.2792 | KD: 1060.5139\n",
      "Train Epoch: 031 Batch: 00031/00094 | Loss: 507.7186 | CE: 0.4758 | KD: 1060.5187\n",
      "Train Epoch: 031 Batch: 00032/00094 | Loss: 507.4610 | CE: 0.3260 | KD: 1060.2933\n",
      "Train Epoch: 031 Batch: 00033/00094 | Loss: 507.3973 | CE: 0.2774 | KD: 1060.2617\n",
      "Train Epoch: 031 Batch: 00034/00094 | Loss: 507.6541 | CE: 0.3102 | KD: 1060.7301\n",
      "Train Epoch: 031 Batch: 00035/00094 | Loss: 507.4519 | CE: 0.3250 | KD: 1060.2762\n",
      "Train Epoch: 031 Batch: 00036/00094 | Loss: 507.8271 | CE: 0.4104 | KD: 1060.8824\n",
      "Train Epoch: 031 Batch: 00037/00094 | Loss: 507.6026 | CE: 0.2733 | KD: 1060.6995\n",
      "Train Epoch: 031 Batch: 00038/00094 | Loss: 507.6357 | CE: 0.3175 | KD: 1060.6763\n",
      "Train Epoch: 031 Batch: 00039/00094 | Loss: 507.4762 | CE: 0.3392 | KD: 1060.2974\n",
      "Train Epoch: 031 Batch: 00040/00094 | Loss: 507.5281 | CE: 0.3264 | KD: 1060.4327\n",
      "Train Epoch: 031 Batch: 00041/00094 | Loss: 507.7826 | CE: 0.3316 | KD: 1060.9541\n",
      "Train Epoch: 031 Batch: 00042/00094 | Loss: 507.7362 | CE: 0.5401 | KD: 1060.4211\n",
      "Train Epoch: 031 Batch: 00043/00094 | Loss: 507.5306 | CE: 0.3730 | KD: 1060.3406\n",
      "Train Epoch: 031 Batch: 00044/00094 | Loss: 507.6573 | CE: 0.2877 | KD: 1060.7839\n",
      "Train Epoch: 031 Batch: 00045/00094 | Loss: 507.5409 | CE: 0.2938 | KD: 1060.5276\n",
      "Train Epoch: 031 Batch: 00046/00094 | Loss: 507.5423 | CE: 0.3569 | KD: 1060.3988\n",
      "Train Epoch: 031 Batch: 00047/00094 | Loss: 507.3787 | CE: 0.2754 | KD: 1060.2271\n",
      "Train Epoch: 031 Batch: 00048/00094 | Loss: 507.4818 | CE: 0.3040 | KD: 1060.3828\n",
      "Train Epoch: 031 Batch: 00049/00094 | Loss: 507.5705 | CE: 0.2911 | KD: 1060.5952\n",
      "Train Epoch: 031 Batch: 00050/00094 | Loss: 507.5472 | CE: 0.3936 | KD: 1060.3323\n",
      "Train Epoch: 031 Batch: 00051/00094 | Loss: 507.4378 | CE: 0.2757 | KD: 1060.3500\n",
      "Train Epoch: 031 Batch: 00052/00094 | Loss: 507.5612 | CE: 0.2571 | KD: 1060.6469\n",
      "Train Epoch: 031 Batch: 00053/00094 | Loss: 507.8930 | CE: 0.5207 | KD: 1060.7894\n",
      "Train Epoch: 031 Batch: 00054/00094 | Loss: 507.5635 | CE: 0.2989 | KD: 1060.5642\n",
      "Train Epoch: 031 Batch: 00055/00094 | Loss: 507.8581 | CE: 0.3643 | KD: 1061.0433\n",
      "Train Epoch: 031 Batch: 00056/00094 | Loss: 507.5939 | CE: 0.3360 | KD: 1060.5503\n",
      "Train Epoch: 031 Batch: 00057/00094 | Loss: 507.8905 | CE: 0.4139 | KD: 1061.0077\n",
      "Train Epoch: 031 Batch: 00058/00094 | Loss: 507.9720 | CE: 0.5334 | KD: 1060.9279\n",
      "Train Epoch: 031 Batch: 00059/00094 | Loss: 507.7243 | CE: 0.4403 | KD: 1060.6047\n",
      "Train Epoch: 031 Batch: 00060/00094 | Loss: 507.7908 | CE: 0.3439 | KD: 1060.9454\n",
      "Train Epoch: 031 Batch: 00061/00094 | Loss: 507.8086 | CE: 0.4013 | KD: 1060.8625\n",
      "Train Epoch: 031 Batch: 00062/00094 | Loss: 507.4336 | CE: 0.3524 | KD: 1060.1808\n",
      "Train Epoch: 031 Batch: 00063/00094 | Loss: 507.8081 | CE: 0.3374 | KD: 1060.9951\n",
      "Train Epoch: 031 Batch: 00064/00094 | Loss: 507.6538 | CE: 0.4221 | KD: 1060.4955\n",
      "Train Epoch: 031 Batch: 00065/00094 | Loss: 507.7165 | CE: 0.3333 | KD: 1060.8123\n",
      "Train Epoch: 031 Batch: 00066/00094 | Loss: 507.5658 | CE: 0.2756 | KD: 1060.6177\n",
      "Train Epoch: 031 Batch: 00067/00094 | Loss: 507.5529 | CE: 0.2389 | KD: 1060.6675\n",
      "Train Epoch: 031 Batch: 00068/00094 | Loss: 507.5819 | CE: 0.2574 | KD: 1060.6896\n",
      "Train Epoch: 031 Batch: 00069/00094 | Loss: 507.4845 | CE: 0.3428 | KD: 1060.3073\n",
      "Train Epoch: 031 Batch: 00070/00094 | Loss: 507.8650 | CE: 0.4098 | KD: 1060.9626\n",
      "Train Epoch: 031 Batch: 00071/00094 | Loss: 507.5139 | CE: 0.2797 | KD: 1060.5006\n",
      "Train Epoch: 031 Batch: 00072/00094 | Loss: 507.6621 | CE: 0.4256 | KD: 1060.5054\n",
      "Train Epoch: 031 Batch: 00073/00094 | Loss: 507.6412 | CE: 0.3309 | KD: 1060.6599\n",
      "Train Epoch: 031 Batch: 00074/00094 | Loss: 507.2916 | CE: 0.2295 | KD: 1060.1409\n",
      "Train Epoch: 031 Batch: 00075/00094 | Loss: 507.6031 | CE: 0.2769 | KD: 1060.6931\n",
      "Train Epoch: 031 Batch: 00076/00094 | Loss: 507.8314 | CE: 0.3390 | KD: 1061.0405\n",
      "Train Epoch: 031 Batch: 00077/00094 | Loss: 507.6438 | CE: 0.3526 | KD: 1060.6198\n",
      "Train Epoch: 031 Batch: 00078/00094 | Loss: 507.7069 | CE: 0.3477 | KD: 1060.7620\n",
      "Train Epoch: 031 Batch: 00079/00094 | Loss: 507.5790 | CE: 0.3339 | KD: 1060.5236\n",
      "Train Epoch: 031 Batch: 00080/00094 | Loss: 507.6336 | CE: 0.3551 | KD: 1060.5935\n",
      "Train Epoch: 031 Batch: 00081/00094 | Loss: 507.4466 | CE: 0.3228 | KD: 1060.2699\n",
      "Train Epoch: 031 Batch: 00082/00094 | Loss: 507.6362 | CE: 0.3474 | KD: 1060.6149\n",
      "Train Epoch: 031 Batch: 00083/00094 | Loss: 507.7852 | CE: 0.4412 | KD: 1060.7303\n",
      "Train Epoch: 031 Batch: 00084/00094 | Loss: 507.4360 | CE: 0.2569 | KD: 1060.3855\n",
      "Train Epoch: 031 Batch: 00085/00094 | Loss: 507.5482 | CE: 0.3364 | KD: 1060.4539\n",
      "Train Epoch: 031 Batch: 00086/00094 | Loss: 507.4216 | CE: 0.2746 | KD: 1060.3184\n",
      "Train Epoch: 031 Batch: 00087/00094 | Loss: 507.6086 | CE: 0.3663 | KD: 1060.5176\n",
      "Train Epoch: 031 Batch: 00088/00094 | Loss: 507.5006 | CE: 0.2839 | KD: 1060.4640\n",
      "Train Epoch: 031 Batch: 00089/00094 | Loss: 508.2033 | CE: 0.4108 | KD: 1061.6680\n",
      "Train Epoch: 031 Batch: 00090/00094 | Loss: 507.9987 | CE: 0.3473 | KD: 1061.3728\n",
      "Train Epoch: 031 Batch: 00091/00094 | Loss: 507.7017 | CE: 0.3994 | KD: 1060.6431\n",
      "Train Epoch: 031 Batch: 00092/00094 | Loss: 507.6838 | CE: 0.3187 | KD: 1060.7745\n",
      "Train Epoch: 031 Batch: 00093/00094 | Loss: 508.0615 | CE: 0.4074 | KD: 1061.3785\n",
      "Train Epoch: 031 Batch: 00094/00094 | Loss: 507.4845 | CE: 0.2760 | KD: 1060.4470\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3696 | acc:86.1000\n",
      "[VAL Acc] Target: 86.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2494 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9132 | acc:52.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2262 | acc:48.4733\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8320 | acc:50.0784\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9125 | acc:52.2181\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6316 | acc:64.4592\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.46%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9350 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.6876 | acc:63.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 63.85%\n",
      "[VAL Acc] Avg 57.33%\n",
      "Train Epoch: 032 Batch: 00001/00094 | Loss: 507.5770 | CE: 0.3317 | KD: 1060.5239\n",
      "Train Epoch: 032 Batch: 00002/00094 | Loss: 507.4868 | CE: 0.2747 | KD: 1060.4545\n",
      "Train Epoch: 032 Batch: 00003/00094 | Loss: 507.4935 | CE: 0.3351 | KD: 1060.3420\n",
      "Train Epoch: 032 Batch: 00004/00094 | Loss: 507.9368 | CE: 0.5422 | KD: 1060.8361\n",
      "Train Epoch: 032 Batch: 00005/00094 | Loss: 507.6625 | CE: 0.3703 | KD: 1060.6219\n",
      "Train Epoch: 032 Batch: 00006/00094 | Loss: 507.5059 | CE: 0.3110 | KD: 1060.4187\n",
      "Train Epoch: 032 Batch: 00007/00094 | Loss: 507.6210 | CE: 0.3105 | KD: 1060.6602\n",
      "Train Epoch: 032 Batch: 00008/00094 | Loss: 507.4752 | CE: 0.3209 | KD: 1060.3336\n",
      "Train Epoch: 032 Batch: 00009/00094 | Loss: 507.5936 | CE: 0.3724 | KD: 1060.4736\n",
      "Train Epoch: 032 Batch: 00010/00094 | Loss: 507.8369 | CE: 0.3193 | KD: 1061.0933\n",
      "Train Epoch: 032 Batch: 00011/00094 | Loss: 507.8840 | CE: 0.4316 | KD: 1060.9568\n",
      "Train Epoch: 032 Batch: 00012/00094 | Loss: 507.5483 | CE: 0.2853 | KD: 1060.5610\n",
      "Train Epoch: 032 Batch: 00013/00094 | Loss: 507.7153 | CE: 0.4584 | KD: 1060.5481\n",
      "Train Epoch: 032 Batch: 00014/00094 | Loss: 507.6863 | CE: 0.2949 | KD: 1060.8293\n",
      "Train Epoch: 032 Batch: 00015/00094 | Loss: 507.8947 | CE: 0.3707 | KD: 1061.1067\n",
      "Train Epoch: 032 Batch: 00016/00094 | Loss: 507.4802 | CE: 0.3498 | KD: 1060.2836\n",
      "Train Epoch: 032 Batch: 00017/00094 | Loss: 507.5613 | CE: 0.3298 | KD: 1060.4950\n",
      "Train Epoch: 032 Batch: 00018/00094 | Loss: 507.7582 | CE: 0.3089 | KD: 1060.9504\n",
      "Train Epoch: 032 Batch: 00019/00094 | Loss: 507.5257 | CE: 0.3687 | KD: 1060.3392\n",
      "Train Epoch: 032 Batch: 00020/00094 | Loss: 507.6974 | CE: 0.3525 | KD: 1060.7322\n",
      "Train Epoch: 032 Batch: 00021/00094 | Loss: 507.6290 | CE: 0.3647 | KD: 1060.5636\n",
      "Train Epoch: 032 Batch: 00022/00094 | Loss: 507.7412 | CE: 0.4039 | KD: 1060.7163\n",
      "Train Epoch: 032 Batch: 00023/00094 | Loss: 507.2733 | CE: 0.2389 | KD: 1060.0829\n",
      "Train Epoch: 032 Batch: 00024/00094 | Loss: 507.4114 | CE: 0.3188 | KD: 1060.2046\n",
      "Train Epoch: 032 Batch: 00025/00094 | Loss: 507.8059 | CE: 0.4514 | KD: 1060.7522\n",
      "Train Epoch: 032 Batch: 00026/00094 | Loss: 507.7894 | CE: 0.2982 | KD: 1061.0380\n",
      "Train Epoch: 032 Batch: 00027/00094 | Loss: 507.6037 | CE: 0.3316 | KD: 1060.5798\n",
      "Train Epoch: 032 Batch: 00028/00094 | Loss: 507.4733 | CE: 0.2869 | KD: 1060.4008\n",
      "Train Epoch: 032 Batch: 00029/00094 | Loss: 507.4082 | CE: 0.3048 | KD: 1060.2272\n",
      "Train Epoch: 032 Batch: 00030/00094 | Loss: 507.4978 | CE: 0.2521 | KD: 1060.5248\n",
      "Train Epoch: 032 Batch: 00031/00094 | Loss: 507.6444 | CE: 0.3387 | KD: 1060.6500\n",
      "Train Epoch: 032 Batch: 00032/00094 | Loss: 507.8168 | CE: 0.3927 | KD: 1060.8977\n",
      "Train Epoch: 032 Batch: 00033/00094 | Loss: 507.7084 | CE: 0.4855 | KD: 1060.4771\n",
      "Train Epoch: 032 Batch: 00034/00094 | Loss: 507.4927 | CE: 0.2865 | KD: 1060.4423\n",
      "Train Epoch: 032 Batch: 00035/00094 | Loss: 507.5403 | CE: 0.3489 | KD: 1060.4113\n",
      "Train Epoch: 032 Batch: 00036/00094 | Loss: 507.7754 | CE: 0.4206 | KD: 1060.7528\n",
      "Train Epoch: 032 Batch: 00037/00094 | Loss: 507.3210 | CE: 0.2496 | KD: 1060.1603\n",
      "Train Epoch: 032 Batch: 00038/00094 | Loss: 507.9660 | CE: 0.4759 | KD: 1061.0359\n",
      "Train Epoch: 032 Batch: 00039/00094 | Loss: 507.6119 | CE: 0.4235 | KD: 1060.4049\n",
      "Train Epoch: 032 Batch: 00040/00094 | Loss: 507.7670 | CE: 0.4017 | KD: 1060.7748\n",
      "Train Epoch: 032 Batch: 00041/00094 | Loss: 507.7169 | CE: 0.4963 | KD: 1060.4723\n",
      "Train Epoch: 032 Batch: 00042/00094 | Loss: 507.4662 | CE: 0.2822 | KD: 1060.3956\n",
      "Train Epoch: 032 Batch: 00043/00094 | Loss: 507.7633 | CE: 0.4513 | KD: 1060.6633\n",
      "Train Epoch: 032 Batch: 00044/00094 | Loss: 507.5001 | CE: 0.3855 | KD: 1060.2505\n",
      "Train Epoch: 032 Batch: 00045/00094 | Loss: 507.8254 | CE: 0.4857 | KD: 1060.7212\n",
      "Train Epoch: 032 Batch: 00046/00094 | Loss: 507.7278 | CE: 0.3361 | KD: 1060.8300\n",
      "Train Epoch: 032 Batch: 00047/00094 | Loss: 507.7393 | CE: 0.3610 | KD: 1060.8019\n",
      "Train Epoch: 032 Batch: 00048/00094 | Loss: 507.8699 | CE: 0.4669 | KD: 1060.8538\n",
      "Train Epoch: 032 Batch: 00049/00094 | Loss: 507.5038 | CE: 0.3744 | KD: 1060.2816\n",
      "Train Epoch: 032 Batch: 00050/00094 | Loss: 507.5324 | CE: 0.2791 | KD: 1060.5408\n",
      "Train Epoch: 032 Batch: 00051/00094 | Loss: 507.7965 | CE: 0.3004 | KD: 1061.0483\n",
      "Train Epoch: 032 Batch: 00052/00094 | Loss: 507.6700 | CE: 0.3028 | KD: 1060.7787\n",
      "Train Epoch: 032 Batch: 00053/00094 | Loss: 507.4648 | CE: 0.3042 | KD: 1060.3468\n",
      "Train Epoch: 032 Batch: 00054/00094 | Loss: 507.5924 | CE: 0.3716 | KD: 1060.4727\n",
      "Train Epoch: 032 Batch: 00055/00094 | Loss: 507.5429 | CE: 0.2741 | KD: 1060.5729\n",
      "Train Epoch: 032 Batch: 00056/00094 | Loss: 507.6833 | CE: 0.3452 | KD: 1060.7178\n",
      "Train Epoch: 032 Batch: 00057/00094 | Loss: 507.5187 | CE: 0.3555 | KD: 1060.3524\n",
      "Train Epoch: 032 Batch: 00058/00094 | Loss: 507.5620 | CE: 0.3099 | KD: 1060.5382\n",
      "Train Epoch: 032 Batch: 00059/00094 | Loss: 507.5323 | CE: 0.3283 | KD: 1060.4376\n",
      "Train Epoch: 032 Batch: 00060/00094 | Loss: 507.6081 | CE: 0.3577 | KD: 1060.5344\n",
      "Train Epoch: 032 Batch: 00061/00094 | Loss: 507.5930 | CE: 0.3520 | KD: 1060.5149\n",
      "Train Epoch: 032 Batch: 00062/00094 | Loss: 507.7855 | CE: 0.4475 | KD: 1060.7177\n",
      "Train Epoch: 032 Batch: 00063/00094 | Loss: 507.5434 | CE: 0.3170 | KD: 1060.4843\n",
      "Train Epoch: 032 Batch: 00064/00094 | Loss: 507.4708 | CE: 0.3269 | KD: 1060.3119\n",
      "Train Epoch: 032 Batch: 00065/00094 | Loss: 507.6051 | CE: 0.4487 | KD: 1060.3379\n",
      "Train Epoch: 032 Batch: 00066/00094 | Loss: 507.6912 | CE: 0.3900 | KD: 1060.6407\n",
      "Train Epoch: 032 Batch: 00067/00094 | Loss: 507.5067 | CE: 0.2530 | KD: 1060.5415\n",
      "Train Epoch: 032 Batch: 00068/00094 | Loss: 507.6327 | CE: 0.2970 | KD: 1060.7128\n",
      "Train Epoch: 032 Batch: 00069/00094 | Loss: 507.6147 | CE: 0.3108 | KD: 1060.6464\n",
      "Train Epoch: 032 Batch: 00070/00094 | Loss: 507.4855 | CE: 0.2442 | KD: 1060.5155\n",
      "Train Epoch: 032 Batch: 00071/00094 | Loss: 507.3808 | CE: 0.2734 | KD: 1060.2356\n",
      "Train Epoch: 032 Batch: 00072/00094 | Loss: 507.7887 | CE: 0.3229 | KD: 1060.9850\n",
      "Train Epoch: 032 Batch: 00073/00094 | Loss: 507.5806 | CE: 0.3720 | KD: 1060.4471\n",
      "Train Epoch: 032 Batch: 00074/00094 | Loss: 507.6574 | CE: 0.3363 | KD: 1060.6824\n",
      "Train Epoch: 032 Batch: 00075/00094 | Loss: 507.6487 | CE: 0.3338 | KD: 1060.6693\n",
      "Train Epoch: 032 Batch: 00076/00094 | Loss: 507.9431 | CE: 0.5578 | KD: 1060.8165\n",
      "Train Epoch: 032 Batch: 00077/00094 | Loss: 507.5272 | CE: 0.3043 | KD: 1060.4771\n",
      "Train Epoch: 032 Batch: 00078/00094 | Loss: 507.4468 | CE: 0.3356 | KD: 1060.2435\n",
      "Train Epoch: 032 Batch: 00079/00094 | Loss: 507.4334 | CE: 0.2657 | KD: 1060.3616\n",
      "Train Epoch: 032 Batch: 00080/00094 | Loss: 507.5378 | CE: 0.3514 | KD: 1060.4008\n",
      "Train Epoch: 032 Batch: 00081/00094 | Loss: 507.5312 | CE: 0.2692 | KD: 1060.5588\n",
      "Train Epoch: 032 Batch: 00082/00094 | Loss: 507.7032 | CE: 0.2946 | KD: 1060.8651\n",
      "Train Epoch: 032 Batch: 00083/00094 | Loss: 507.5173 | CE: 0.2995 | KD: 1060.4664\n",
      "Train Epoch: 032 Batch: 00084/00094 | Loss: 507.5289 | CE: 0.2716 | KD: 1060.5490\n",
      "Train Epoch: 032 Batch: 00085/00094 | Loss: 507.7710 | CE: 0.4181 | KD: 1060.7488\n",
      "Train Epoch: 032 Batch: 00086/00094 | Loss: 507.5107 | CE: 0.2560 | KD: 1060.5437\n",
      "Train Epoch: 032 Batch: 00087/00094 | Loss: 507.6764 | CE: 0.3619 | KD: 1060.6687\n",
      "Train Epoch: 032 Batch: 00088/00094 | Loss: 507.5191 | CE: 0.3454 | KD: 1060.3741\n",
      "Train Epoch: 032 Batch: 00089/00094 | Loss: 507.5781 | CE: 0.3319 | KD: 1060.5258\n",
      "Train Epoch: 032 Batch: 00090/00094 | Loss: 507.7022 | CE: 0.3531 | KD: 1060.7408\n",
      "Train Epoch: 032 Batch: 00091/00094 | Loss: 507.8887 | CE: 0.5036 | KD: 1060.8163\n",
      "Train Epoch: 032 Batch: 00092/00094 | Loss: 507.8254 | CE: 0.3631 | KD: 1060.9777\n",
      "Train Epoch: 032 Batch: 00093/00094 | Loss: 507.6661 | CE: 0.3042 | KD: 1060.7678\n",
      "Train Epoch: 032 Batch: 00094/00094 | Loss: 507.6036 | CE: 0.2987 | KD: 1060.6484\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4028 | acc:85.3500\n",
      "[VAL Acc] Target: 85.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2731 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9339 | acc:55.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2638 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8813 | acc:49.1771\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 49.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9575 | acc:51.7560\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 51.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6354 | acc:65.1254\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.13%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9507 | acc:49.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7326 | acc:62.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 62.40%\n",
      "[VAL Acc] Avg 57.46%\n",
      "Train Epoch: 033 Batch: 00001/00094 | Loss: 507.8871 | CE: 0.3792 | KD: 1061.0729\n",
      "Train Epoch: 033 Batch: 00002/00094 | Loss: 507.6114 | CE: 0.2734 | KD: 1060.7177\n",
      "Train Epoch: 033 Batch: 00003/00094 | Loss: 507.6826 | CE: 0.3361 | KD: 1060.7355\n",
      "Train Epoch: 033 Batch: 00004/00094 | Loss: 507.8691 | CE: 0.5258 | KD: 1060.7288\n",
      "Train Epoch: 033 Batch: 00005/00094 | Loss: 507.7054 | CE: 0.2703 | KD: 1060.9208\n",
      "Train Epoch: 033 Batch: 00006/00094 | Loss: 507.6158 | CE: 0.3804 | KD: 1060.5032\n",
      "Train Epoch: 033 Batch: 00007/00094 | Loss: 507.6873 | CE: 0.3630 | KD: 1060.6891\n",
      "Train Epoch: 033 Batch: 00008/00094 | Loss: 507.7589 | CE: 0.4335 | KD: 1060.6913\n",
      "Train Epoch: 033 Batch: 00009/00094 | Loss: 507.6405 | CE: 0.3591 | KD: 1060.5992\n",
      "Train Epoch: 033 Batch: 00010/00094 | Loss: 507.6178 | CE: 0.3369 | KD: 1060.5984\n",
      "Train Epoch: 033 Batch: 00011/00094 | Loss: 507.7197 | CE: 0.2627 | KD: 1060.9666\n",
      "Train Epoch: 033 Batch: 00012/00094 | Loss: 507.6620 | CE: 0.3721 | KD: 1060.6171\n",
      "Train Epoch: 033 Batch: 00013/00094 | Loss: 507.6443 | CE: 0.3113 | KD: 1060.7073\n",
      "Train Epoch: 033 Batch: 00014/00094 | Loss: 507.5498 | CE: 0.2924 | KD: 1060.5493\n",
      "Train Epoch: 033 Batch: 00015/00094 | Loss: 507.5394 | CE: 0.3107 | KD: 1060.4893\n",
      "Train Epoch: 033 Batch: 00016/00094 | Loss: 507.5566 | CE: 0.3293 | KD: 1060.4862\n",
      "Train Epoch: 033 Batch: 00017/00094 | Loss: 507.8833 | CE: 0.4071 | KD: 1061.0066\n",
      "Train Epoch: 033 Batch: 00018/00094 | Loss: 507.4708 | CE: 0.3373 | KD: 1060.2900\n",
      "Train Epoch: 033 Batch: 00019/00094 | Loss: 507.5940 | CE: 0.3174 | KD: 1060.5892\n",
      "Train Epoch: 033 Batch: 00020/00094 | Loss: 507.4980 | CE: 0.2950 | KD: 1060.4355\n",
      "Train Epoch: 033 Batch: 00021/00094 | Loss: 507.6295 | CE: 0.3316 | KD: 1060.6338\n",
      "Train Epoch: 033 Batch: 00022/00094 | Loss: 507.8047 | CE: 0.4626 | KD: 1060.7264\n",
      "Train Epoch: 033 Batch: 00023/00094 | Loss: 507.5725 | CE: 0.3339 | KD: 1060.5099\n",
      "Train Epoch: 033 Batch: 00024/00094 | Loss: 507.6242 | CE: 0.3597 | KD: 1060.5641\n",
      "Train Epoch: 033 Batch: 00025/00094 | Loss: 507.7267 | CE: 0.4471 | KD: 1060.5956\n",
      "Train Epoch: 033 Batch: 00026/00094 | Loss: 507.6681 | CE: 0.2904 | KD: 1060.8008\n",
      "Train Epoch: 033 Batch: 00027/00094 | Loss: 507.4435 | CE: 0.2965 | KD: 1060.3184\n",
      "Train Epoch: 033 Batch: 00028/00094 | Loss: 507.9308 | CE: 0.4325 | KD: 1061.0529\n",
      "Train Epoch: 033 Batch: 00029/00094 | Loss: 507.7085 | CE: 0.4139 | KD: 1060.6270\n",
      "Train Epoch: 033 Batch: 00030/00094 | Loss: 507.7244 | CE: 0.4331 | KD: 1060.6201\n",
      "Train Epoch: 033 Batch: 00031/00094 | Loss: 507.7201 | CE: 0.3987 | KD: 1060.6830\n",
      "Train Epoch: 033 Batch: 00032/00094 | Loss: 507.7265 | CE: 0.2723 | KD: 1060.9606\n",
      "Train Epoch: 033 Batch: 00033/00094 | Loss: 507.8323 | CE: 0.4646 | KD: 1060.7797\n",
      "Train Epoch: 033 Batch: 00034/00094 | Loss: 507.9723 | CE: 0.3594 | KD: 1061.2925\n",
      "Train Epoch: 033 Batch: 00035/00094 | Loss: 507.3853 | CE: 0.2826 | KD: 1060.2257\n",
      "Train Epoch: 033 Batch: 00036/00094 | Loss: 507.5667 | CE: 0.3120 | KD: 1060.5435\n",
      "Train Epoch: 033 Batch: 00037/00094 | Loss: 507.6058 | CE: 0.3668 | KD: 1060.5107\n",
      "Train Epoch: 033 Batch: 00038/00094 | Loss: 507.6208 | CE: 0.4144 | KD: 1060.4425\n",
      "Train Epoch: 033 Batch: 00039/00094 | Loss: 507.5842 | CE: 0.3649 | KD: 1060.4695\n",
      "Train Epoch: 033 Batch: 00040/00094 | Loss: 507.5698 | CE: 0.3911 | KD: 1060.3845\n",
      "Train Epoch: 033 Batch: 00041/00094 | Loss: 507.8047 | CE: 0.4352 | KD: 1060.7837\n",
      "Train Epoch: 033 Batch: 00042/00094 | Loss: 507.7886 | CE: 0.3494 | KD: 1060.9293\n",
      "Train Epoch: 033 Batch: 00043/00094 | Loss: 507.6320 | CE: 0.3991 | KD: 1060.4979\n",
      "Train Epoch: 033 Batch: 00044/00094 | Loss: 507.7916 | CE: 0.3655 | KD: 1060.9020\n",
      "Train Epoch: 033 Batch: 00045/00094 | Loss: 507.3997 | CE: 0.2572 | KD: 1060.3090\n",
      "Train Epoch: 033 Batch: 00046/00094 | Loss: 507.3848 | CE: 0.2489 | KD: 1060.2953\n",
      "Train Epoch: 033 Batch: 00047/00094 | Loss: 507.6653 | CE: 0.4164 | KD: 1060.5312\n",
      "Train Epoch: 033 Batch: 00048/00094 | Loss: 507.7124 | CE: 0.4081 | KD: 1060.6473\n",
      "Train Epoch: 033 Batch: 00049/00094 | Loss: 507.6974 | CE: 0.3420 | KD: 1060.7540\n",
      "Train Epoch: 033 Batch: 00050/00094 | Loss: 507.9585 | CE: 0.3411 | KD: 1061.3019\n",
      "Train Epoch: 033 Batch: 00051/00094 | Loss: 507.6552 | CE: 0.2256 | KD: 1060.9092\n",
      "Train Epoch: 033 Batch: 00052/00094 | Loss: 507.6266 | CE: 0.2904 | KD: 1060.7139\n",
      "Train Epoch: 033 Batch: 00053/00094 | Loss: 507.8062 | CE: 0.3322 | KD: 1061.0021\n",
      "Train Epoch: 033 Batch: 00054/00094 | Loss: 508.0194 | CE: 0.3659 | KD: 1061.3774\n",
      "Train Epoch: 033 Batch: 00055/00094 | Loss: 507.5325 | CE: 0.2874 | KD: 1060.5236\n",
      "Train Epoch: 033 Batch: 00056/00094 | Loss: 507.8790 | CE: 0.4215 | KD: 1060.9675\n",
      "Train Epoch: 033 Batch: 00057/00094 | Loss: 507.9007 | CE: 0.4855 | KD: 1060.8790\n",
      "Train Epoch: 033 Batch: 00058/00094 | Loss: 507.7589 | CE: 0.2961 | KD: 1060.9786\n",
      "Train Epoch: 033 Batch: 00059/00094 | Loss: 507.4034 | CE: 0.3100 | KD: 1060.2063\n",
      "Train Epoch: 033 Batch: 00060/00094 | Loss: 507.7132 | CE: 0.4787 | KD: 1060.5013\n",
      "Train Epoch: 033 Batch: 00061/00094 | Loss: 507.5817 | CE: 0.3826 | KD: 1060.4274\n",
      "Train Epoch: 033 Batch: 00062/00094 | Loss: 507.3660 | CE: 0.3086 | KD: 1060.1310\n",
      "Train Epoch: 033 Batch: 00063/00094 | Loss: 507.6181 | CE: 0.3416 | KD: 1060.5890\n",
      "Train Epoch: 033 Batch: 00064/00094 | Loss: 507.6943 | CE: 0.3298 | KD: 1060.7731\n",
      "Train Epoch: 033 Batch: 00065/00094 | Loss: 507.6446 | CE: 0.3218 | KD: 1060.6858\n",
      "Train Epoch: 033 Batch: 00066/00094 | Loss: 507.6508 | CE: 0.3853 | KD: 1060.5659\n",
      "Train Epoch: 033 Batch: 00067/00094 | Loss: 507.5457 | CE: 0.3375 | KD: 1060.4463\n",
      "Train Epoch: 033 Batch: 00068/00094 | Loss: 507.4737 | CE: 0.3098 | KD: 1060.3536\n",
      "Train Epoch: 033 Batch: 00069/00094 | Loss: 507.5207 | CE: 0.3153 | KD: 1060.4404\n",
      "Train Epoch: 033 Batch: 00070/00094 | Loss: 507.4347 | CE: 0.2748 | KD: 1060.3452\n",
      "Train Epoch: 033 Batch: 00071/00094 | Loss: 507.4999 | CE: 0.3581 | KD: 1060.3075\n",
      "Train Epoch: 033 Batch: 00072/00094 | Loss: 507.7018 | CE: 0.4001 | KD: 1060.6416\n",
      "Train Epoch: 033 Batch: 00073/00094 | Loss: 507.9891 | CE: 0.2917 | KD: 1061.4692\n",
      "Train Epoch: 033 Batch: 00074/00094 | Loss: 507.7301 | CE: 0.3780 | KD: 1060.7473\n",
      "Train Epoch: 033 Batch: 00075/00094 | Loss: 507.7982 | CE: 0.3809 | KD: 1060.8835\n",
      "Train Epoch: 033 Batch: 00076/00094 | Loss: 507.7132 | CE: 0.4347 | KD: 1060.5933\n",
      "Train Epoch: 033 Batch: 00077/00094 | Loss: 507.3807 | CE: 0.2603 | KD: 1060.2627\n",
      "Train Epoch: 033 Batch: 00078/00094 | Loss: 507.5120 | CE: 0.3104 | KD: 1060.4326\n",
      "Train Epoch: 033 Batch: 00079/00094 | Loss: 507.6434 | CE: 0.4023 | KD: 1060.5151\n",
      "Train Epoch: 033 Batch: 00080/00094 | Loss: 507.7462 | CE: 0.3929 | KD: 1060.7496\n",
      "Train Epoch: 033 Batch: 00081/00094 | Loss: 507.6260 | CE: 0.3382 | KD: 1060.6127\n",
      "Train Epoch: 033 Batch: 00082/00094 | Loss: 507.7605 | CE: 0.3360 | KD: 1060.8986\n",
      "Train Epoch: 033 Batch: 00083/00094 | Loss: 507.4975 | CE: 0.3186 | KD: 1060.3851\n",
      "Train Epoch: 033 Batch: 00084/00094 | Loss: 507.6070 | CE: 0.4010 | KD: 1060.4418\n",
      "Train Epoch: 033 Batch: 00085/00094 | Loss: 507.4817 | CE: 0.3098 | KD: 1060.3702\n",
      "Train Epoch: 033 Batch: 00086/00094 | Loss: 507.4848 | CE: 0.2888 | KD: 1060.4208\n",
      "Train Epoch: 033 Batch: 00087/00094 | Loss: 507.5990 | CE: 0.3269 | KD: 1060.5800\n",
      "Train Epoch: 033 Batch: 00088/00094 | Loss: 507.3656 | CE: 0.2902 | KD: 1060.1688\n",
      "Train Epoch: 033 Batch: 00089/00094 | Loss: 507.5959 | CE: 0.3018 | KD: 1060.6260\n",
      "Train Epoch: 033 Batch: 00090/00094 | Loss: 507.6245 | CE: 0.3776 | KD: 1060.5273\n",
      "Train Epoch: 033 Batch: 00091/00094 | Loss: 507.5779 | CE: 0.3859 | KD: 1060.4125\n",
      "Train Epoch: 033 Batch: 00092/00094 | Loss: 507.6326 | CE: 0.3519 | KD: 1060.5979\n",
      "Train Epoch: 033 Batch: 00093/00094 | Loss: 507.5109 | CE: 0.3213 | KD: 1060.4075\n",
      "Train Epoch: 033 Batch: 00094/00094 | Loss: 507.6552 | CE: 0.3628 | KD: 1060.6226\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4203 | acc:84.7000\n",
      "[VAL Acc] Target: 84.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2911 | acc:48.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9337 | acc:54.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1956 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8240 | acc:50.4702\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9054 | acc:53.2348\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6297 | acc:66.6144\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9155 | acc:48.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7574 | acc:61.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.80%\n",
      "[VAL Acc] Avg 57.66%\n",
      "Train Epoch: 034 Batch: 00001/00094 | Loss: 456.9690 | CE: 0.4871 | KD: 1060.4337\n",
      "Train Epoch: 034 Batch: 00002/00094 | Loss: 456.7932 | CE: 0.3095 | KD: 1060.4377\n",
      "Train Epoch: 034 Batch: 00003/00094 | Loss: 456.7656 | CE: 0.2734 | KD: 1060.4575\n",
      "Train Epoch: 034 Batch: 00004/00094 | Loss: 456.7812 | CE: 0.3198 | KD: 1060.3860\n",
      "Train Epoch: 034 Batch: 00005/00094 | Loss: 457.1693 | CE: 0.4427 | KD: 1061.0021\n",
      "Train Epoch: 034 Batch: 00006/00094 | Loss: 457.0323 | CE: 0.2376 | KD: 1061.1603\n",
      "Train Epoch: 034 Batch: 00007/00094 | Loss: 456.8752 | CE: 0.3739 | KD: 1060.4786\n",
      "Train Epoch: 034 Batch: 00008/00094 | Loss: 457.0453 | CE: 0.4619 | KD: 1060.6692\n",
      "Train Epoch: 034 Batch: 00009/00094 | Loss: 457.1201 | CE: 0.3901 | KD: 1061.0101\n",
      "Train Epoch: 034 Batch: 00010/00094 | Loss: 456.9895 | CE: 0.3067 | KD: 1060.9001\n",
      "Train Epoch: 034 Batch: 00011/00094 | Loss: 456.9267 | CE: 0.3542 | KD: 1060.6440\n",
      "Train Epoch: 034 Batch: 00012/00094 | Loss: 456.7838 | CE: 0.3191 | KD: 1060.3938\n",
      "Train Epoch: 034 Batch: 00013/00094 | Loss: 456.6457 | CE: 0.2589 | KD: 1060.2125\n",
      "Train Epoch: 034 Batch: 00014/00094 | Loss: 456.7812 | CE: 0.3156 | KD: 1060.3955\n",
      "Train Epoch: 034 Batch: 00015/00094 | Loss: 457.1860 | CE: 0.5811 | KD: 1060.7195\n",
      "Train Epoch: 034 Batch: 00016/00094 | Loss: 457.0135 | CE: 0.3342 | KD: 1060.8922\n",
      "Train Epoch: 034 Batch: 00017/00094 | Loss: 456.8612 | CE: 0.3072 | KD: 1060.6011\n",
      "Train Epoch: 034 Batch: 00018/00094 | Loss: 456.7112 | CE: 0.2677 | KD: 1060.3442\n",
      "Train Epoch: 034 Batch: 00019/00094 | Loss: 456.9851 | CE: 0.4166 | KD: 1060.6348\n",
      "Train Epoch: 034 Batch: 00020/00094 | Loss: 457.2521 | CE: 0.4028 | KD: 1061.2871\n",
      "Train Epoch: 034 Batch: 00021/00094 | Loss: 456.8809 | CE: 0.3097 | KD: 1060.6411\n",
      "Train Epoch: 034 Batch: 00022/00094 | Loss: 457.2193 | CE: 0.4454 | KD: 1061.1118\n",
      "Train Epoch: 034 Batch: 00023/00094 | Loss: 456.8509 | CE: 0.3382 | KD: 1060.5050\n",
      "Train Epoch: 034 Batch: 00024/00094 | Loss: 456.9664 | CE: 0.2602 | KD: 1060.9547\n",
      "Train Epoch: 034 Batch: 00025/00094 | Loss: 457.1375 | CE: 0.3813 | KD: 1061.0708\n",
      "Train Epoch: 034 Batch: 00026/00094 | Loss: 457.0720 | CE: 0.3580 | KD: 1060.9727\n",
      "Train Epoch: 034 Batch: 00027/00094 | Loss: 456.8394 | CE: 0.2825 | KD: 1060.6079\n",
      "Train Epoch: 034 Batch: 00028/00094 | Loss: 456.8994 | CE: 0.3711 | KD: 1060.5414\n",
      "Train Epoch: 034 Batch: 00029/00094 | Loss: 456.8660 | CE: 0.3532 | KD: 1060.5052\n",
      "Train Epoch: 034 Batch: 00030/00094 | Loss: 456.8114 | CE: 0.3176 | KD: 1060.4613\n",
      "Train Epoch: 034 Batch: 00031/00094 | Loss: 456.7243 | CE: 0.2451 | KD: 1060.4272\n",
      "Train Epoch: 034 Batch: 00032/00094 | Loss: 456.8053 | CE: 0.3538 | KD: 1060.3629\n",
      "Train Epoch: 034 Batch: 00033/00094 | Loss: 457.0168 | CE: 0.3034 | KD: 1060.9713\n",
      "Train Epoch: 034 Batch: 00034/00094 | Loss: 457.0485 | CE: 0.3130 | KD: 1061.0228\n",
      "Train Epoch: 034 Batch: 00035/00094 | Loss: 456.8877 | CE: 0.3582 | KD: 1060.5441\n",
      "Train Epoch: 034 Batch: 00036/00094 | Loss: 456.8643 | CE: 0.3686 | KD: 1060.4658\n",
      "Train Epoch: 034 Batch: 00037/00094 | Loss: 457.0736 | CE: 0.4178 | KD: 1060.8375\n",
      "Train Epoch: 034 Batch: 00038/00094 | Loss: 456.8524 | CE: 0.3601 | KD: 1060.4578\n",
      "Train Epoch: 034 Batch: 00039/00094 | Loss: 456.7901 | CE: 0.2703 | KD: 1060.5217\n",
      "Train Epoch: 034 Batch: 00040/00094 | Loss: 456.8776 | CE: 0.3417 | KD: 1060.5588\n",
      "Train Epoch: 034 Batch: 00041/00094 | Loss: 456.7040 | CE: 0.2518 | KD: 1060.3646\n",
      "Train Epoch: 034 Batch: 00042/00094 | Loss: 457.1280 | CE: 0.3851 | KD: 1061.0399\n",
      "Train Epoch: 034 Batch: 00043/00094 | Loss: 456.7759 | CE: 0.3777 | KD: 1060.2390\n",
      "Train Epoch: 034 Batch: 00044/00094 | Loss: 456.9192 | CE: 0.3122 | KD: 1060.7241\n",
      "Train Epoch: 034 Batch: 00045/00094 | Loss: 457.1742 | CE: 0.3671 | KD: 1061.1890\n",
      "Train Epoch: 034 Batch: 00046/00094 | Loss: 456.9189 | CE: 0.4315 | KD: 1060.4464\n",
      "Train Epoch: 034 Batch: 00047/00094 | Loss: 456.8781 | CE: 0.3620 | KD: 1060.5129\n",
      "Train Epoch: 034 Batch: 00048/00094 | Loss: 456.8599 | CE: 0.2521 | KD: 1060.7261\n",
      "Train Epoch: 034 Batch: 00049/00094 | Loss: 457.1666 | CE: 0.4875 | KD: 1060.8917\n",
      "Train Epoch: 034 Batch: 00050/00094 | Loss: 457.0877 | CE: 0.6084 | KD: 1060.4275\n",
      "Train Epoch: 034 Batch: 00051/00094 | Loss: 457.0944 | CE: 0.4030 | KD: 1060.9203\n",
      "Train Epoch: 034 Batch: 00052/00094 | Loss: 456.8193 | CE: 0.2950 | KD: 1060.5320\n",
      "Train Epoch: 034 Batch: 00053/00094 | Loss: 456.9656 | CE: 0.4157 | KD: 1060.5916\n",
      "Train Epoch: 034 Batch: 00054/00094 | Loss: 456.9570 | CE: 0.3124 | KD: 1060.8115\n",
      "Train Epoch: 034 Batch: 00055/00094 | Loss: 456.9351 | CE: 0.3914 | KD: 1060.5770\n",
      "Train Epoch: 034 Batch: 00056/00094 | Loss: 456.8912 | CE: 0.2972 | KD: 1060.6940\n",
      "Train Epoch: 034 Batch: 00057/00094 | Loss: 456.9836 | CE: 0.3153 | KD: 1060.8666\n",
      "Train Epoch: 034 Batch: 00058/00094 | Loss: 457.4512 | CE: 0.4043 | KD: 1061.7461\n",
      "Train Epoch: 034 Batch: 00059/00094 | Loss: 456.8073 | CE: 0.2716 | KD: 1060.5586\n",
      "Train Epoch: 034 Batch: 00060/00094 | Loss: 456.7816 | CE: 0.2913 | KD: 1060.4532\n",
      "Train Epoch: 034 Batch: 00061/00094 | Loss: 457.1086 | CE: 0.3977 | KD: 1060.9656\n",
      "Train Epoch: 034 Batch: 00062/00094 | Loss: 456.8447 | CE: 0.2974 | KD: 1060.5857\n",
      "Train Epoch: 034 Batch: 00063/00094 | Loss: 457.2559 | CE: 0.5387 | KD: 1060.9801\n",
      "Train Epoch: 034 Batch: 00064/00094 | Loss: 457.0298 | CE: 0.3613 | KD: 1060.8672\n",
      "Train Epoch: 034 Batch: 00065/00094 | Loss: 456.7937 | CE: 0.3325 | KD: 1060.3855\n",
      "Train Epoch: 034 Batch: 00066/00094 | Loss: 456.8702 | CE: 0.3414 | KD: 1060.5426\n",
      "Train Epoch: 034 Batch: 00067/00094 | Loss: 456.8882 | CE: 0.3268 | KD: 1060.6182\n",
      "Train Epoch: 034 Batch: 00068/00094 | Loss: 456.9594 | CE: 0.3785 | KD: 1060.6636\n",
      "Train Epoch: 034 Batch: 00069/00094 | Loss: 456.7187 | CE: 0.2686 | KD: 1060.3597\n",
      "Train Epoch: 034 Batch: 00070/00094 | Loss: 456.8477 | CE: 0.3572 | KD: 1060.4536\n",
      "Train Epoch: 034 Batch: 00071/00094 | Loss: 456.8167 | CE: 0.3445 | KD: 1060.4111\n",
      "Train Epoch: 034 Batch: 00072/00094 | Loss: 456.8036 | CE: 0.3317 | KD: 1060.4103\n",
      "Train Epoch: 034 Batch: 00073/00094 | Loss: 457.1332 | CE: 0.4282 | KD: 1060.9519\n",
      "Train Epoch: 034 Batch: 00074/00094 | Loss: 456.7921 | CE: 0.3142 | KD: 1060.4242\n",
      "Train Epoch: 034 Batch: 00075/00094 | Loss: 456.9747 | CE: 0.4083 | KD: 1060.6300\n",
      "Train Epoch: 034 Batch: 00076/00094 | Loss: 457.1674 | CE: 0.3610 | KD: 1061.1874\n",
      "Train Epoch: 034 Batch: 00077/00094 | Loss: 457.0636 | CE: 0.3290 | KD: 1061.0205\n",
      "Train Epoch: 034 Batch: 00078/00094 | Loss: 456.6561 | CE: 0.2456 | KD: 1060.2678\n",
      "Train Epoch: 034 Batch: 00079/00094 | Loss: 456.8297 | CE: 0.3442 | KD: 1060.4420\n",
      "Train Epoch: 034 Batch: 00080/00094 | Loss: 456.8468 | CE: 0.2891 | KD: 1060.6099\n",
      "Train Epoch: 034 Batch: 00081/00094 | Loss: 457.0582 | CE: 0.3203 | KD: 1061.0282\n",
      "Train Epoch: 034 Batch: 00082/00094 | Loss: 456.8812 | CE: 0.3048 | KD: 1060.6531\n",
      "Train Epoch: 034 Batch: 00083/00094 | Loss: 457.0612 | CE: 0.4109 | KD: 1060.8248\n",
      "Train Epoch: 034 Batch: 00084/00094 | Loss: 457.0481 | CE: 0.3552 | KD: 1060.9238\n",
      "Train Epoch: 034 Batch: 00085/00094 | Loss: 456.8786 | CE: 0.3622 | KD: 1060.5137\n",
      "Train Epoch: 034 Batch: 00086/00094 | Loss: 456.9348 | CE: 0.4575 | KD: 1060.4227\n",
      "Train Epoch: 034 Batch: 00087/00094 | Loss: 456.8456 | CE: 0.3471 | KD: 1060.4720\n",
      "Train Epoch: 034 Batch: 00088/00094 | Loss: 456.7689 | CE: 0.3915 | KD: 1060.1908\n",
      "Train Epoch: 034 Batch: 00089/00094 | Loss: 456.9294 | CE: 0.3212 | KD: 1060.7271\n",
      "Train Epoch: 034 Batch: 00090/00094 | Loss: 456.8524 | CE: 0.3955 | KD: 1060.3755\n",
      "Train Epoch: 034 Batch: 00091/00094 | Loss: 457.1408 | CE: 0.3569 | KD: 1061.1353\n",
      "Train Epoch: 034 Batch: 00092/00094 | Loss: 456.8835 | CE: 0.3569 | KD: 1060.5376\n",
      "Train Epoch: 034 Batch: 00093/00094 | Loss: 457.1987 | CE: 0.4666 | KD: 1061.0148\n",
      "Train Epoch: 034 Batch: 00094/00094 | Loss: 456.6951 | CE: 0.2842 | KD: 1060.2686\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3974 | acc:85.6000\n",
      "[VAL Acc] Target: 85.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3000 | acc:49.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9350 | acc:53.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2345 | acc:47.7099\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8064 | acc:51.6458\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 51.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9418 | acc:52.3105\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 52.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6268 | acc:65.0862\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9393 | acc:50.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 50.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7247 | acc:61.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.65%\n",
      "[VAL Acc] Avg 57.47%\n",
      "Train Epoch: 035 Batch: 00001/00094 | Loss: 456.8256 | CE: 0.2797 | KD: 1060.5824\n",
      "Train Epoch: 035 Batch: 00002/00094 | Loss: 457.0422 | CE: 0.4450 | KD: 1060.7014\n",
      "Train Epoch: 035 Batch: 00003/00094 | Loss: 456.8287 | CE: 0.3976 | KD: 1060.3156\n",
      "Train Epoch: 035 Batch: 00004/00094 | Loss: 456.7933 | CE: 0.2795 | KD: 1060.5077\n",
      "Train Epoch: 035 Batch: 00005/00094 | Loss: 456.8367 | CE: 0.3074 | KD: 1060.5437\n",
      "Train Epoch: 035 Batch: 00006/00094 | Loss: 456.8022 | CE: 0.2662 | KD: 1060.5593\n",
      "Train Epoch: 035 Batch: 00007/00094 | Loss: 456.8839 | CE: 0.3320 | KD: 1060.5962\n",
      "Train Epoch: 035 Batch: 00008/00094 | Loss: 456.9033 | CE: 0.3223 | KD: 1060.6637\n",
      "Train Epoch: 035 Batch: 00009/00094 | Loss: 456.9261 | CE: 0.3378 | KD: 1060.6807\n",
      "Train Epoch: 035 Batch: 00010/00094 | Loss: 457.2273 | CE: 0.3719 | KD: 1061.3013\n",
      "Train Epoch: 035 Batch: 00011/00094 | Loss: 456.8896 | CE: 0.2694 | KD: 1060.7548\n",
      "Train Epoch: 035 Batch: 00012/00094 | Loss: 456.8241 | CE: 0.2698 | KD: 1060.6019\n",
      "Train Epoch: 035 Batch: 00013/00094 | Loss: 456.9580 | CE: 0.3411 | KD: 1060.7473\n",
      "Train Epoch: 035 Batch: 00014/00094 | Loss: 457.1170 | CE: 0.4866 | KD: 1060.7787\n",
      "Train Epoch: 035 Batch: 00015/00094 | Loss: 456.8901 | CE: 0.3265 | KD: 1060.6234\n",
      "Train Epoch: 035 Batch: 00016/00094 | Loss: 456.9011 | CE: 0.2825 | KD: 1060.7511\n",
      "Train Epoch: 035 Batch: 00017/00094 | Loss: 456.8700 | CE: 0.3225 | KD: 1060.5859\n",
      "Train Epoch: 035 Batch: 00018/00094 | Loss: 457.0093 | CE: 0.4035 | KD: 1060.7213\n",
      "Train Epoch: 035 Batch: 00019/00094 | Loss: 457.0720 | CE: 0.4363 | KD: 1060.7908\n",
      "Train Epoch: 035 Batch: 00020/00094 | Loss: 456.7507 | CE: 0.2868 | KD: 1060.3917\n",
      "Train Epoch: 035 Batch: 00021/00094 | Loss: 456.9725 | CE: 0.4228 | KD: 1060.5911\n",
      "Train Epoch: 035 Batch: 00022/00094 | Loss: 456.9810 | CE: 0.3409 | KD: 1060.8011\n",
      "Train Epoch: 035 Batch: 00023/00094 | Loss: 456.9616 | CE: 0.3950 | KD: 1060.6304\n",
      "Train Epoch: 035 Batch: 00024/00094 | Loss: 456.8924 | CE: 0.3522 | KD: 1060.5690\n",
      "Train Epoch: 035 Batch: 00025/00094 | Loss: 456.7848 | CE: 0.3494 | KD: 1060.3256\n",
      "Train Epoch: 035 Batch: 00026/00094 | Loss: 456.9060 | CE: 0.3912 | KD: 1060.5101\n",
      "Train Epoch: 035 Batch: 00027/00094 | Loss: 456.9574 | CE: 0.2876 | KD: 1060.8700\n",
      "Train Epoch: 035 Batch: 00028/00094 | Loss: 456.9925 | CE: 0.3207 | KD: 1060.8748\n",
      "Train Epoch: 035 Batch: 00029/00094 | Loss: 456.8938 | CE: 0.3214 | KD: 1060.6439\n",
      "Train Epoch: 035 Batch: 00030/00094 | Loss: 457.0181 | CE: 0.3071 | KD: 1060.9658\n",
      "Train Epoch: 035 Batch: 00031/00094 | Loss: 456.8085 | CE: 0.3272 | KD: 1060.4323\n",
      "Train Epoch: 035 Batch: 00032/00094 | Loss: 456.7363 | CE: 0.3248 | KD: 1060.2701\n",
      "Train Epoch: 035 Batch: 00033/00094 | Loss: 456.8350 | CE: 0.2727 | KD: 1060.6205\n",
      "Train Epoch: 035 Batch: 00034/00094 | Loss: 457.2038 | CE: 0.4252 | KD: 1061.1229\n",
      "Train Epoch: 035 Batch: 00035/00094 | Loss: 456.6798 | CE: 0.2780 | KD: 1060.2477\n",
      "Train Epoch: 035 Batch: 00036/00094 | Loss: 456.9496 | CE: 0.4107 | KD: 1060.5659\n",
      "Train Epoch: 035 Batch: 00037/00094 | Loss: 456.7854 | CE: 0.3220 | KD: 1060.3905\n",
      "Train Epoch: 035 Batch: 00038/00094 | Loss: 456.9182 | CE: 0.3778 | KD: 1060.5693\n",
      "Train Epoch: 035 Batch: 00039/00094 | Loss: 456.7443 | CE: 0.3386 | KD: 1060.2566\n",
      "Train Epoch: 035 Batch: 00040/00094 | Loss: 456.8055 | CE: 0.3109 | KD: 1060.4633\n",
      "Train Epoch: 035 Batch: 00041/00094 | Loss: 457.1023 | CE: 0.3817 | KD: 1060.9880\n",
      "Train Epoch: 035 Batch: 00042/00094 | Loss: 456.9041 | CE: 0.3588 | KD: 1060.5809\n",
      "Train Epoch: 035 Batch: 00043/00094 | Loss: 456.8180 | CE: 0.2025 | KD: 1060.7440\n",
      "Train Epoch: 035 Batch: 00044/00094 | Loss: 456.8337 | CE: 0.2721 | KD: 1060.6188\n",
      "Train Epoch: 035 Batch: 00045/00094 | Loss: 456.7893 | CE: 0.3266 | KD: 1060.3892\n",
      "Train Epoch: 035 Batch: 00046/00094 | Loss: 456.7230 | CE: 0.3115 | KD: 1060.2700\n",
      "Train Epoch: 035 Batch: 00047/00094 | Loss: 456.8469 | CE: 0.3158 | KD: 1060.5479\n",
      "Train Epoch: 035 Batch: 00048/00094 | Loss: 456.6354 | CE: 0.2158 | KD: 1060.2887\n",
      "Train Epoch: 035 Batch: 00049/00094 | Loss: 456.9357 | CE: 0.4172 | KD: 1060.5187\n",
      "Train Epoch: 035 Batch: 00050/00094 | Loss: 456.7932 | CE: 0.2572 | KD: 1060.5593\n",
      "Train Epoch: 035 Batch: 00051/00094 | Loss: 456.8029 | CE: 0.3416 | KD: 1060.3859\n",
      "Train Epoch: 035 Batch: 00052/00094 | Loss: 456.7177 | CE: 0.2543 | KD: 1060.3906\n",
      "Train Epoch: 035 Batch: 00053/00094 | Loss: 456.7093 | CE: 0.2615 | KD: 1060.3542\n",
      "Train Epoch: 035 Batch: 00054/00094 | Loss: 456.8028 | CE: 0.3064 | KD: 1060.4672\n",
      "Train Epoch: 035 Batch: 00055/00094 | Loss: 456.8457 | CE: 0.2946 | KD: 1060.5944\n",
      "Train Epoch: 035 Batch: 00056/00094 | Loss: 457.2457 | CE: 0.4191 | KD: 1061.2345\n",
      "Train Epoch: 035 Batch: 00057/00094 | Loss: 456.8110 | CE: 0.3531 | KD: 1060.3777\n",
      "Train Epoch: 035 Batch: 00058/00094 | Loss: 456.8748 | CE: 0.3919 | KD: 1060.4359\n",
      "Train Epoch: 035 Batch: 00059/00094 | Loss: 456.8974 | CE: 0.3158 | KD: 1060.6653\n",
      "Train Epoch: 035 Batch: 00060/00094 | Loss: 456.6842 | CE: 0.2407 | KD: 1060.3445\n",
      "Train Epoch: 035 Batch: 00061/00094 | Loss: 457.1061 | CE: 0.3077 | KD: 1061.1689\n",
      "Train Epoch: 035 Batch: 00062/00094 | Loss: 456.7121 | CE: 0.3191 | KD: 1060.2269\n",
      "Train Epoch: 035 Batch: 00063/00094 | Loss: 456.8562 | CE: 0.4113 | KD: 1060.3475\n",
      "Train Epoch: 035 Batch: 00064/00094 | Loss: 457.1058 | CE: 0.4684 | KD: 1060.7949\n",
      "Train Epoch: 035 Batch: 00065/00094 | Loss: 456.8646 | CE: 0.3628 | KD: 1060.4800\n",
      "Train Epoch: 035 Batch: 00066/00094 | Loss: 457.0339 | CE: 0.3236 | KD: 1060.9642\n",
      "Train Epoch: 035 Batch: 00067/00094 | Loss: 456.8264 | CE: 0.3208 | KD: 1060.4885\n",
      "Train Epoch: 035 Batch: 00068/00094 | Loss: 456.8118 | CE: 0.2609 | KD: 1060.5940\n",
      "Train Epoch: 035 Batch: 00069/00094 | Loss: 456.7083 | CE: 0.3202 | KD: 1060.2156\n",
      "Train Epoch: 035 Batch: 00070/00094 | Loss: 457.0149 | CE: 0.3364 | KD: 1060.8903\n",
      "Train Epoch: 035 Batch: 00071/00094 | Loss: 457.0066 | CE: 0.2738 | KD: 1061.0165\n",
      "Train Epoch: 035 Batch: 00072/00094 | Loss: 457.0411 | CE: 0.4306 | KD: 1060.7323\n",
      "Train Epoch: 035 Batch: 00073/00094 | Loss: 456.9449 | CE: 0.3954 | KD: 1060.5906\n",
      "Train Epoch: 035 Batch: 00074/00094 | Loss: 457.0622 | CE: 0.3644 | KD: 1060.9351\n",
      "Train Epoch: 035 Batch: 00075/00094 | Loss: 456.8859 | CE: 0.2995 | KD: 1060.6764\n",
      "Train Epoch: 035 Batch: 00076/00094 | Loss: 457.0405 | CE: 0.4708 | KD: 1060.6376\n",
      "Train Epoch: 035 Batch: 00077/00094 | Loss: 456.7299 | CE: 0.2570 | KD: 1060.4126\n",
      "Train Epoch: 035 Batch: 00078/00094 | Loss: 456.9666 | CE: 0.3475 | KD: 1060.7522\n",
      "Train Epoch: 035 Batch: 00079/00094 | Loss: 456.8411 | CE: 0.2803 | KD: 1060.6169\n",
      "Train Epoch: 035 Batch: 00080/00094 | Loss: 456.9555 | CE: 0.2397 | KD: 1060.9771\n",
      "Train Epoch: 035 Batch: 00081/00094 | Loss: 456.8624 | CE: 0.3137 | KD: 1060.5887\n",
      "Train Epoch: 035 Batch: 00082/00094 | Loss: 456.8251 | CE: 0.3468 | KD: 1060.4252\n",
      "Train Epoch: 035 Batch: 00083/00094 | Loss: 456.9278 | CE: 0.2569 | KD: 1060.8727\n",
      "Train Epoch: 035 Batch: 00084/00094 | Loss: 456.8793 | CE: 0.2917 | KD: 1060.6793\n",
      "Train Epoch: 035 Batch: 00085/00094 | Loss: 457.3635 | CE: 0.3631 | KD: 1061.6382\n",
      "Train Epoch: 035 Batch: 00086/00094 | Loss: 456.7905 | CE: 0.3684 | KD: 1060.2946\n",
      "Train Epoch: 035 Batch: 00087/00094 | Loss: 456.7354 | CE: 0.3651 | KD: 1060.1743\n",
      "Train Epoch: 035 Batch: 00088/00094 | Loss: 457.0185 | CE: 0.4379 | KD: 1060.6628\n",
      "Train Epoch: 035 Batch: 00089/00094 | Loss: 457.0531 | CE: 0.3162 | KD: 1061.0261\n",
      "Train Epoch: 035 Batch: 00090/00094 | Loss: 456.8608 | CE: 0.3490 | KD: 1060.5031\n",
      "Train Epoch: 035 Batch: 00091/00094 | Loss: 456.9442 | CE: 0.2943 | KD: 1060.8240\n",
      "Train Epoch: 035 Batch: 00092/00094 | Loss: 457.0936 | CE: 0.3550 | KD: 1061.0300\n",
      "Train Epoch: 035 Batch: 00093/00094 | Loss: 456.8513 | CE: 0.3031 | KD: 1060.5876\n",
      "Train Epoch: 035 Batch: 00094/00094 | Loss: 457.1117 | CE: 0.4363 | KD: 1060.8831\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.3558 | acc:87.4000\n",
      "[VAL Acc] Target: 87.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2787 | acc:49.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9149 | acc:55.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 55.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2573 | acc:48.8550\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8567 | acc:49.8041\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 49.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8916 | acc:54.1590\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 54.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6348 | acc:66.8887\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9317 | acc:49.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7300 | acc:61.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 61.20%\n",
      "[VAL Acc] Avg 58.09%\n",
      "Train Epoch: 036 Batch: 00001/00094 | Loss: 456.8498 | CE: 0.3117 | KD: 1060.5642\n",
      "Train Epoch: 036 Batch: 00002/00094 | Loss: 457.0835 | CE: 0.3170 | KD: 1061.0946\n",
      "Train Epoch: 036 Batch: 00003/00094 | Loss: 457.0923 | CE: 0.3408 | KD: 1061.0599\n",
      "Train Epoch: 036 Batch: 00004/00094 | Loss: 456.8848 | CE: 0.2753 | KD: 1060.7300\n",
      "Train Epoch: 036 Batch: 00005/00094 | Loss: 456.8149 | CE: 0.2643 | KD: 1060.5931\n",
      "Train Epoch: 036 Batch: 00006/00094 | Loss: 456.8921 | CE: 0.2386 | KD: 1060.8322\n",
      "Train Epoch: 036 Batch: 00007/00094 | Loss: 456.7859 | CE: 0.3048 | KD: 1060.4318\n",
      "Train Epoch: 036 Batch: 00008/00094 | Loss: 456.7988 | CE: 0.2888 | KD: 1060.4989\n",
      "Train Epoch: 036 Batch: 00009/00094 | Loss: 457.0201 | CE: 0.3592 | KD: 1060.8495\n",
      "Train Epoch: 036 Batch: 00010/00094 | Loss: 456.9328 | CE: 0.3972 | KD: 1060.5585\n",
      "Train Epoch: 036 Batch: 00011/00094 | Loss: 456.9089 | CE: 0.2670 | KD: 1060.8051\n",
      "Train Epoch: 036 Batch: 00012/00094 | Loss: 456.9168 | CE: 0.3517 | KD: 1060.6268\n",
      "Train Epoch: 036 Batch: 00013/00094 | Loss: 456.8234 | CE: 0.3671 | KD: 1060.3741\n",
      "Train Epoch: 036 Batch: 00014/00094 | Loss: 456.8563 | CE: 0.4110 | KD: 1060.3486\n",
      "Train Epoch: 036 Batch: 00015/00094 | Loss: 457.0203 | CE: 0.3671 | KD: 1060.8314\n",
      "Train Epoch: 036 Batch: 00016/00094 | Loss: 457.1089 | CE: 0.3626 | KD: 1061.0477\n",
      "Train Epoch: 036 Batch: 00017/00094 | Loss: 456.8378 | CE: 0.3041 | KD: 1060.5538\n",
      "Train Epoch: 036 Batch: 00018/00094 | Loss: 456.6673 | CE: 0.2541 | KD: 1060.2740\n",
      "Train Epoch: 036 Batch: 00019/00094 | Loss: 456.9501 | CE: 0.4130 | KD: 1060.5618\n",
      "Train Epoch: 036 Batch: 00020/00094 | Loss: 456.8207 | CE: 0.3433 | KD: 1060.4231\n",
      "Train Epoch: 036 Batch: 00021/00094 | Loss: 456.8316 | CE: 0.2217 | KD: 1060.7310\n",
      "Train Epoch: 036 Batch: 00022/00094 | Loss: 457.2605 | CE: 0.4338 | KD: 1061.2345\n",
      "Train Epoch: 036 Batch: 00023/00094 | Loss: 456.9478 | CE: 0.3543 | KD: 1060.6929\n",
      "Train Epoch: 036 Batch: 00024/00094 | Loss: 456.7245 | CE: 0.2282 | KD: 1060.4669\n",
      "Train Epoch: 036 Batch: 00025/00094 | Loss: 456.7179 | CE: 0.2799 | KD: 1060.3315\n",
      "Train Epoch: 036 Batch: 00026/00094 | Loss: 457.0435 | CE: 0.3526 | KD: 1060.9191\n",
      "Train Epoch: 036 Batch: 00027/00094 | Loss: 457.2197 | CE: 0.4213 | KD: 1061.1688\n",
      "Train Epoch: 036 Batch: 00028/00094 | Loss: 457.2148 | CE: 0.2997 | KD: 1061.4397\n",
      "Train Epoch: 036 Batch: 00029/00094 | Loss: 456.9649 | CE: 0.3549 | KD: 1060.7312\n",
      "Train Epoch: 036 Batch: 00030/00094 | Loss: 456.9540 | CE: 0.3568 | KD: 1060.7015\n",
      "Train Epoch: 036 Batch: 00031/00094 | Loss: 456.9128 | CE: 0.3695 | KD: 1060.5763\n",
      "Train Epoch: 036 Batch: 00032/00094 | Loss: 456.8816 | CE: 0.2754 | KD: 1060.7224\n",
      "Train Epoch: 036 Batch: 00033/00094 | Loss: 456.9335 | CE: 0.3370 | KD: 1060.7000\n",
      "Train Epoch: 036 Batch: 00034/00094 | Loss: 456.7801 | CE: 0.3044 | KD: 1060.4193\n",
      "Train Epoch: 036 Batch: 00035/00094 | Loss: 456.7502 | CE: 0.2472 | KD: 1060.4825\n",
      "Train Epoch: 036 Batch: 00036/00094 | Loss: 456.8052 | CE: 0.3263 | KD: 1060.4268\n",
      "Train Epoch: 036 Batch: 00037/00094 | Loss: 456.9200 | CE: 0.3718 | KD: 1060.5878\n",
      "Train Epoch: 036 Batch: 00038/00094 | Loss: 456.8486 | CE: 0.3028 | KD: 1060.5820\n",
      "Train Epoch: 036 Batch: 00039/00094 | Loss: 456.8699 | CE: 0.3156 | KD: 1060.6019\n",
      "Train Epoch: 036 Batch: 00040/00094 | Loss: 456.7232 | CE: 0.2614 | KD: 1060.3868\n",
      "Train Epoch: 036 Batch: 00041/00094 | Loss: 456.7762 | CE: 0.2658 | KD: 1060.4998\n",
      "Train Epoch: 036 Batch: 00042/00094 | Loss: 456.9979 | CE: 0.3720 | KD: 1060.7681\n",
      "Train Epoch: 036 Batch: 00043/00094 | Loss: 456.8542 | CE: 0.2909 | KD: 1060.6228\n",
      "Train Epoch: 036 Batch: 00044/00094 | Loss: 456.8087 | CE: 0.2624 | KD: 1060.5831\n",
      "Train Epoch: 036 Batch: 00045/00094 | Loss: 456.8116 | CE: 0.3319 | KD: 1060.4285\n",
      "Train Epoch: 036 Batch: 00046/00094 | Loss: 456.7562 | CE: 0.2945 | KD: 1060.3865\n",
      "Train Epoch: 036 Batch: 00047/00094 | Loss: 456.7146 | CE: 0.2798 | KD: 1060.3241\n",
      "Train Epoch: 036 Batch: 00048/00094 | Loss: 457.0823 | CE: 0.4646 | KD: 1060.7490\n",
      "Train Epoch: 036 Batch: 00049/00094 | Loss: 456.8406 | CE: 0.2821 | KD: 1060.6117\n",
      "Train Epoch: 036 Batch: 00050/00094 | Loss: 457.2045 | CE: 0.3770 | KD: 1061.2366\n",
      "Train Epoch: 036 Batch: 00051/00094 | Loss: 456.6165 | CE: 0.2044 | KD: 1060.2714\n",
      "Train Epoch: 036 Batch: 00052/00094 | Loss: 456.8127 | CE: 0.3523 | KD: 1060.3837\n",
      "Train Epoch: 036 Batch: 00053/00094 | Loss: 457.1176 | CE: 0.3193 | KD: 1061.1687\n",
      "Train Epoch: 036 Batch: 00054/00094 | Loss: 456.9431 | CE: 0.3445 | KD: 1060.7047\n",
      "Train Epoch: 036 Batch: 00055/00094 | Loss: 456.6277 | CE: 0.2295 | KD: 1060.2390\n",
      "Train Epoch: 036 Batch: 00056/00094 | Loss: 456.6918 | CE: 0.2235 | KD: 1060.4020\n",
      "Train Epoch: 036 Batch: 00057/00094 | Loss: 457.1299 | CE: 0.3333 | KD: 1061.1647\n",
      "Train Epoch: 036 Batch: 00058/00094 | Loss: 456.7792 | CE: 0.3357 | KD: 1060.3442\n",
      "Train Epoch: 036 Batch: 00059/00094 | Loss: 456.6268 | CE: 0.2546 | KD: 1060.1787\n",
      "Train Epoch: 036 Batch: 00060/00094 | Loss: 456.8767 | CE: 0.3127 | KD: 1060.6243\n",
      "Train Epoch: 036 Batch: 00061/00094 | Loss: 456.9247 | CE: 0.3283 | KD: 1060.6996\n",
      "Train Epoch: 036 Batch: 00062/00094 | Loss: 456.9790 | CE: 0.4093 | KD: 1060.6376\n",
      "Train Epoch: 036 Batch: 00063/00094 | Loss: 456.9047 | CE: 0.2632 | KD: 1060.8043\n",
      "Train Epoch: 036 Batch: 00064/00094 | Loss: 456.8790 | CE: 0.3497 | KD: 1060.5437\n",
      "Train Epoch: 036 Batch: 00065/00094 | Loss: 456.9408 | CE: 0.3158 | KD: 1060.7660\n",
      "Train Epoch: 036 Batch: 00066/00094 | Loss: 456.7695 | CE: 0.3105 | KD: 1060.3805\n",
      "Train Epoch: 036 Batch: 00067/00094 | Loss: 457.0679 | CE: 0.4118 | KD: 1060.8383\n",
      "Train Epoch: 036 Batch: 00068/00094 | Loss: 456.8743 | CE: 0.3917 | KD: 1060.4353\n",
      "Train Epoch: 036 Batch: 00069/00094 | Loss: 456.8804 | CE: 0.3303 | KD: 1060.5920\n",
      "Train Epoch: 036 Batch: 00070/00094 | Loss: 456.9549 | CE: 0.3340 | KD: 1060.7563\n",
      "Train Epoch: 036 Batch: 00071/00094 | Loss: 456.7162 | CE: 0.2659 | KD: 1060.3601\n",
      "Train Epoch: 036 Batch: 00072/00094 | Loss: 456.8914 | CE: 0.3027 | KD: 1060.6816\n",
      "Train Epoch: 036 Batch: 00073/00094 | Loss: 456.9935 | CE: 0.4243 | KD: 1060.6365\n",
      "Train Epoch: 036 Batch: 00074/00094 | Loss: 457.2181 | CE: 0.3625 | KD: 1061.3018\n",
      "Train Epoch: 036 Batch: 00075/00094 | Loss: 457.2228 | CE: 0.3154 | KD: 1061.4221\n",
      "Train Epoch: 036 Batch: 00076/00094 | Loss: 456.7780 | CE: 0.3179 | KD: 1060.3829\n",
      "Train Epoch: 036 Batch: 00077/00094 | Loss: 457.1195 | CE: 0.3915 | KD: 1061.0051\n",
      "Train Epoch: 036 Batch: 00078/00094 | Loss: 456.9699 | CE: 0.4139 | KD: 1060.6058\n",
      "Train Epoch: 036 Batch: 00079/00094 | Loss: 456.9058 | CE: 0.3874 | KD: 1060.5186\n",
      "Train Epoch: 036 Batch: 00080/00094 | Loss: 456.7513 | CE: 0.3253 | KD: 1060.3037\n",
      "Train Epoch: 036 Batch: 00081/00094 | Loss: 456.9529 | CE: 0.3873 | KD: 1060.6282\n",
      "Train Epoch: 036 Batch: 00082/00094 | Loss: 456.9205 | CE: 0.2941 | KD: 1060.7692\n",
      "Train Epoch: 036 Batch: 00083/00094 | Loss: 456.9409 | CE: 0.3074 | KD: 1060.7858\n",
      "Train Epoch: 036 Batch: 00084/00094 | Loss: 456.9747 | CE: 0.4030 | KD: 1060.6422\n",
      "Train Epoch: 036 Batch: 00085/00094 | Loss: 456.8965 | CE: 0.2852 | KD: 1060.7341\n",
      "Train Epoch: 036 Batch: 00086/00094 | Loss: 457.0361 | CE: 0.4355 | KD: 1060.7095\n",
      "Train Epoch: 036 Batch: 00087/00094 | Loss: 456.5772 | CE: 0.2205 | KD: 1060.1427\n",
      "Train Epoch: 036 Batch: 00088/00094 | Loss: 456.8024 | CE: 0.3002 | KD: 1060.4808\n",
      "Train Epoch: 036 Batch: 00089/00094 | Loss: 456.7741 | CE: 0.2485 | KD: 1060.5352\n",
      "Train Epoch: 036 Batch: 00090/00094 | Loss: 456.8989 | CE: 0.3618 | KD: 1060.5618\n",
      "Train Epoch: 036 Batch: 00091/00094 | Loss: 456.7967 | CE: 0.2909 | KD: 1060.4890\n",
      "Train Epoch: 036 Batch: 00092/00094 | Loss: 456.9348 | CE: 0.3509 | KD: 1060.6705\n",
      "Train Epoch: 036 Batch: 00093/00094 | Loss: 456.9972 | CE: 0.3637 | KD: 1060.7856\n",
      "Train Epoch: 036 Batch: 00094/00094 | Loss: 456.8355 | CE: 0.2597 | KD: 1060.6520\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.4155 | acc:85.6000\n",
      "[VAL Acc] Target: 85.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.3522 | acc:49.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9849 | acc:53.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2586 | acc:48.2824\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8914 | acc:50.8229\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 50.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9025 | acc:53.0499\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 53.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6474 | acc:65.5564\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8977 | acc:49.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 49.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/diffusionshort\n",
      "\n",
      "Test results | Loss:0.7850 | acc:60.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/diffusionshort: 60.10%\n",
      "[VAL Acc] Avg 57.38%\n",
      "Early stopping ...\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : txc_diff_elsa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/0cd1a97b7b0b4523bf338e802aae1142\u001b[0m\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/biggan_val_acc [36]         : (51.5, 59.5)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/crn_val_acc [36]            : (54.46708463949843, 69.5141065830721)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/cyclegan_val_acc [36]       : (45.99236641221374, 51.526717557251914)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/diffusionshort_val_acc [36] : (56.2, 63.849999999999994)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/faceforensics_val_acc [36]  : (50.64695009242144, 55.5452865064695)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/gaugan_val_acc [36]         : (48.449999999999996, 50.849999999999994)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/imle_val_acc [36]           : (47.02194357366771, 57.24921630094044)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/wild_val_acc [36]           : (47.625, 51.1875)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/target_val_acc [36]                                              : (74.65, 87.4)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/val_acc [36]                                                     : (54.11755699763338, 58.77859660684414)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [866]                                                           : (456.627685546875, 1062.3897705078125)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss [3384]                                                   : (456.57720947265625, 1062.3897705078125)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_kd [3384]                                                : (1060.0740966796875, 1061.74609375)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_main [3384]                                              : (0.2024543732404709, 1.1597375869750977)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start_acc                                                            : 49.75\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : txc_diff_elsa\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size           : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config_file          : model_config.conf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decay_factor         : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stop           : 35\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs               : 250\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flip                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kd_alpha             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr                   : 0.005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_schedule          : cosine\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_gpu              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     output_model_version : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resolution           : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (411.58 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1 (7.41 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\", \"biggan\", \"cyclegan\", \"imle\", \"faceforensics\", \"crn\", \"wild\", \"diffusionshort\"] \n",
    "TARGET_DATASET = \"elsa\"\n",
    "NETWORK = \"Xception\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/KD_Xception/diff\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/KD_Xception/diff\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "COMET_NAME = \"txc_diff_elsa\"\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='ewc_model_config.conf', network='MobileNet2', input_model='../KD-AIGC-Detection/checkpoints/EWC_mn/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild', output_dir='../KD-AIGC-Detection/checkpoints/EWC_mn/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild', target_dataset='../KD-AIGC-Detection/datasets/custom/diffusionshort', use_comet=True, comet_name='tewcmn_diff', num_gpu='0', epochs='250', early_stop='35', batch_size='64', resolution='128', lr_schedule='cosine', lr='0.005', importance='10', sample_batch='4', flip='False')\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/a3ca79664a1a41159054f5243e8208d8\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/imle\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/crn\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/wild\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/train\n",
      "Dataset available in target_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan / ../KD-AIGC-Detection/datasets/custom/biggan / ../KD-AIGC-Detection/datasets/custom/cyclegan / ../KD-AIGC-Detection/datasets/custom/imle / ../KD-AIGC-Detection/datasets/custom/faceforensics / ../KD-AIGC-Detection/datasets/custom/crn / ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Loading MobileNet2 from ../KD-AIGC-Detection/checkpoints/EWC_mn/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild/model_best_accuracy.pth\n",
      "Apply Cosine learning rate schedule\n",
      "/home/fra/miniconda3/envs/paper/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.8918 | acc:58.5000\n",
      "Start Target Validation ACC: 58.50%\n",
      "Start training in 250 epochs\n",
      "\n",
      "\n",
      "---------- Starting epoch 0 ----------\n",
      "Train Epoch: 001 Batch: 00001/00094 | Loss: 0.8586 | EWC Loss: 0.0000 | CE Loss: 0.8586\n",
      "Train Epoch: 001 Batch: 00002/00094 | Loss: 0.9229 | EWC Loss: 0.0014 | CE Loss: 0.9215\n",
      "Train Epoch: 001 Batch: 00003/00094 | Loss: 0.8696 | EWC Loss: 0.0013 | CE Loss: 0.8683\n",
      "Train Epoch: 001 Batch: 00004/00094 | Loss: 0.8966 | EWC Loss: 0.0019 | CE Loss: 0.8947\n",
      "Train Epoch: 001 Batch: 00005/00094 | Loss: 0.6149 | EWC Loss: 0.0040 | CE Loss: 0.6109\n",
      "Train Epoch: 001 Batch: 00006/00094 | Loss: 0.7458 | EWC Loss: 0.0041 | CE Loss: 0.7418\n",
      "Train Epoch: 001 Batch: 00007/00094 | Loss: 0.7099 | EWC Loss: 0.0038 | CE Loss: 0.7061\n",
      "Train Epoch: 001 Batch: 00008/00094 | Loss: 0.8122 | EWC Loss: 0.0036 | CE Loss: 0.8086\n",
      "Train Epoch: 001 Batch: 00009/00094 | Loss: 0.7547 | EWC Loss: 0.0041 | CE Loss: 0.7506\n",
      "Train Epoch: 001 Batch: 00010/00094 | Loss: 0.5643 | EWC Loss: 0.0042 | CE Loss: 0.5601\n",
      "Train Epoch: 001 Batch: 00011/00094 | Loss: 0.6391 | EWC Loss: 0.0046 | CE Loss: 0.6345\n",
      "Train Epoch: 001 Batch: 00012/00094 | Loss: 0.6323 | EWC Loss: 0.0055 | CE Loss: 0.6268\n",
      "Train Epoch: 001 Batch: 00013/00094 | Loss: 0.6792 | EWC Loss: 0.0062 | CE Loss: 0.6730\n",
      "Train Epoch: 001 Batch: 00014/00094 | Loss: 0.4906 | EWC Loss: 0.0068 | CE Loss: 0.4838\n",
      "Train Epoch: 001 Batch: 00015/00094 | Loss: 0.5184 | EWC Loss: 0.0065 | CE Loss: 0.5119\n",
      "Train Epoch: 001 Batch: 00016/00094 | Loss: 0.5883 | EWC Loss: 0.0065 | CE Loss: 0.5818\n",
      "Train Epoch: 001 Batch: 00017/00094 | Loss: 0.5709 | EWC Loss: 0.0067 | CE Loss: 0.5642\n",
      "Train Epoch: 001 Batch: 00018/00094 | Loss: 0.4987 | EWC Loss: 0.0069 | CE Loss: 0.4918\n",
      "Train Epoch: 001 Batch: 00019/00094 | Loss: 0.6117 | EWC Loss: 0.0072 | CE Loss: 0.6045\n",
      "Train Epoch: 001 Batch: 00020/00094 | Loss: 0.5915 | EWC Loss: 0.0070 | CE Loss: 0.5845\n",
      "Train Epoch: 001 Batch: 00021/00094 | Loss: 0.4622 | EWC Loss: 0.0070 | CE Loss: 0.4552\n",
      "Train Epoch: 001 Batch: 00022/00094 | Loss: 0.5194 | EWC Loss: 0.0073 | CE Loss: 0.5122\n",
      "Train Epoch: 001 Batch: 00023/00094 | Loss: 0.4676 | EWC Loss: 0.0078 | CE Loss: 0.4598\n",
      "Train Epoch: 001 Batch: 00024/00094 | Loss: 0.4966 | EWC Loss: 0.0078 | CE Loss: 0.4888\n",
      "Train Epoch: 001 Batch: 00025/00094 | Loss: 0.6978 | EWC Loss: 0.0079 | CE Loss: 0.6898\n",
      "Train Epoch: 001 Batch: 00026/00094 | Loss: 0.5828 | EWC Loss: 0.0080 | CE Loss: 0.5748\n",
      "Train Epoch: 001 Batch: 00027/00094 | Loss: 0.5288 | EWC Loss: 0.0082 | CE Loss: 0.5206\n",
      "Train Epoch: 001 Batch: 00028/00094 | Loss: 0.4589 | EWC Loss: 0.0083 | CE Loss: 0.4505\n",
      "Train Epoch: 001 Batch: 00029/00094 | Loss: 0.3737 | EWC Loss: 0.0086 | CE Loss: 0.3651\n",
      "Train Epoch: 001 Batch: 00030/00094 | Loss: 0.4865 | EWC Loss: 0.0086 | CE Loss: 0.4778\n",
      "Train Epoch: 001 Batch: 00031/00094 | Loss: 0.5150 | EWC Loss: 0.0090 | CE Loss: 0.5060\n",
      "Train Epoch: 001 Batch: 00032/00094 | Loss: 0.3841 | EWC Loss: 0.0094 | CE Loss: 0.3747\n",
      "Train Epoch: 001 Batch: 00033/00094 | Loss: 0.3776 | EWC Loss: 0.0100 | CE Loss: 0.3676\n",
      "Train Epoch: 001 Batch: 00034/00094 | Loss: 0.4458 | EWC Loss: 0.0104 | CE Loss: 0.4354\n",
      "Train Epoch: 001 Batch: 00035/00094 | Loss: 0.5137 | EWC Loss: 0.0106 | CE Loss: 0.5031\n",
      "Train Epoch: 001 Batch: 00036/00094 | Loss: 0.5179 | EWC Loss: 0.0108 | CE Loss: 0.5071\n",
      "Train Epoch: 001 Batch: 00037/00094 | Loss: 0.4224 | EWC Loss: 0.0110 | CE Loss: 0.4114\n",
      "Train Epoch: 001 Batch: 00038/00094 | Loss: 0.4370 | EWC Loss: 0.0112 | CE Loss: 0.4258\n",
      "Train Epoch: 001 Batch: 00039/00094 | Loss: 0.3938 | EWC Loss: 0.0113 | CE Loss: 0.3825\n",
      "Train Epoch: 001 Batch: 00040/00094 | Loss: 0.5520 | EWC Loss: 0.0115 | CE Loss: 0.5405\n",
      "Train Epoch: 001 Batch: 00041/00094 | Loss: 0.4426 | EWC Loss: 0.0118 | CE Loss: 0.4308\n",
      "Train Epoch: 001 Batch: 00042/00094 | Loss: 0.4272 | EWC Loss: 0.0122 | CE Loss: 0.4151\n",
      "Train Epoch: 001 Batch: 00043/00094 | Loss: 0.3598 | EWC Loss: 0.0124 | CE Loss: 0.3474\n",
      "Train Epoch: 001 Batch: 00044/00094 | Loss: 0.3901 | EWC Loss: 0.0126 | CE Loss: 0.3775\n",
      "Train Epoch: 001 Batch: 00045/00094 | Loss: 0.4112 | EWC Loss: 0.0127 | CE Loss: 0.3985\n",
      "Train Epoch: 001 Batch: 00046/00094 | Loss: 0.4276 | EWC Loss: 0.0126 | CE Loss: 0.4149\n",
      "Train Epoch: 001 Batch: 00047/00094 | Loss: 0.2579 | EWC Loss: 0.0124 | CE Loss: 0.2456\n",
      "Train Epoch: 001 Batch: 00048/00094 | Loss: 0.3649 | EWC Loss: 0.0125 | CE Loss: 0.3524\n",
      "Train Epoch: 001 Batch: 00049/00094 | Loss: 0.3299 | EWC Loss: 0.0126 | CE Loss: 0.3173\n",
      "Train Epoch: 001 Batch: 00050/00094 | Loss: 0.4050 | EWC Loss: 0.0126 | CE Loss: 0.3924\n",
      "Train Epoch: 001 Batch: 00051/00094 | Loss: 0.4488 | EWC Loss: 0.0125 | CE Loss: 0.4363\n",
      "Train Epoch: 001 Batch: 00052/00094 | Loss: 0.4592 | EWC Loss: 0.0123 | CE Loss: 0.4469\n",
      "Train Epoch: 001 Batch: 00053/00094 | Loss: 0.2951 | EWC Loss: 0.0123 | CE Loss: 0.2828\n",
      "Train Epoch: 001 Batch: 00054/00094 | Loss: 0.4145 | EWC Loss: 0.0122 | CE Loss: 0.4023\n",
      "Train Epoch: 001 Batch: 00055/00094 | Loss: 0.3643 | EWC Loss: 0.0125 | CE Loss: 0.3518\n",
      "Train Epoch: 001 Batch: 00056/00094 | Loss: 0.4047 | EWC Loss: 0.0125 | CE Loss: 0.3922\n",
      "Train Epoch: 001 Batch: 00057/00094 | Loss: 0.3831 | EWC Loss: 0.0124 | CE Loss: 0.3707\n",
      "Train Epoch: 001 Batch: 00058/00094 | Loss: 0.3887 | EWC Loss: 0.0127 | CE Loss: 0.3761\n",
      "Train Epoch: 001 Batch: 00059/00094 | Loss: 0.3558 | EWC Loss: 0.0125 | CE Loss: 0.3433\n",
      "Train Epoch: 001 Batch: 00060/00094 | Loss: 0.3382 | EWC Loss: 0.0125 | CE Loss: 0.3257\n",
      "Train Epoch: 001 Batch: 00061/00094 | Loss: 0.3428 | EWC Loss: 0.0125 | CE Loss: 0.3302\n",
      "Train Epoch: 001 Batch: 00062/00094 | Loss: 0.4344 | EWC Loss: 0.0129 | CE Loss: 0.4215\n",
      "Train Epoch: 001 Batch: 00063/00094 | Loss: 0.3240 | EWC Loss: 0.0128 | CE Loss: 0.3112\n",
      "Train Epoch: 001 Batch: 00064/00094 | Loss: 0.2817 | EWC Loss: 0.0129 | CE Loss: 0.2688\n",
      "Train Epoch: 001 Batch: 00065/00094 | Loss: 0.3630 | EWC Loss: 0.0129 | CE Loss: 0.3501\n",
      "Train Epoch: 001 Batch: 00066/00094 | Loss: 0.4836 | EWC Loss: 0.0131 | CE Loss: 0.4705\n",
      "Train Epoch: 001 Batch: 00067/00094 | Loss: 0.3025 | EWC Loss: 0.0132 | CE Loss: 0.2893\n",
      "Train Epoch: 001 Batch: 00068/00094 | Loss: 0.3487 | EWC Loss: 0.0133 | CE Loss: 0.3354\n",
      "Train Epoch: 001 Batch: 00069/00094 | Loss: 0.4395 | EWC Loss: 0.0133 | CE Loss: 0.4262\n",
      "Train Epoch: 001 Batch: 00070/00094 | Loss: 0.2297 | EWC Loss: 0.0138 | CE Loss: 0.2159\n",
      "Train Epoch: 001 Batch: 00071/00094 | Loss: 0.3296 | EWC Loss: 0.0136 | CE Loss: 0.3160\n",
      "Train Epoch: 001 Batch: 00072/00094 | Loss: 0.3055 | EWC Loss: 0.0140 | CE Loss: 0.2916\n",
      "Train Epoch: 001 Batch: 00073/00094 | Loss: 0.2636 | EWC Loss: 0.0142 | CE Loss: 0.2494\n",
      "Train Epoch: 001 Batch: 00074/00094 | Loss: 0.3074 | EWC Loss: 0.0142 | CE Loss: 0.2932\n",
      "Train Epoch: 001 Batch: 00075/00094 | Loss: 0.4308 | EWC Loss: 0.0142 | CE Loss: 0.4166\n",
      "Train Epoch: 001 Batch: 00076/00094 | Loss: 0.3107 | EWC Loss: 0.0146 | CE Loss: 0.2961\n",
      "Train Epoch: 001 Batch: 00077/00094 | Loss: 0.3257 | EWC Loss: 0.0146 | CE Loss: 0.3112\n",
      "Train Epoch: 001 Batch: 00078/00094 | Loss: 0.2823 | EWC Loss: 0.0142 | CE Loss: 0.2681\n",
      "Train Epoch: 001 Batch: 00079/00094 | Loss: 0.3616 | EWC Loss: 0.0141 | CE Loss: 0.3475\n",
      "Train Epoch: 001 Batch: 00080/00094 | Loss: 0.2559 | EWC Loss: 0.0140 | CE Loss: 0.2419\n",
      "Train Epoch: 001 Batch: 00081/00094 | Loss: 0.3681 | EWC Loss: 0.0141 | CE Loss: 0.3540\n",
      "Train Epoch: 001 Batch: 00082/00094 | Loss: 0.4184 | EWC Loss: 0.0142 | CE Loss: 0.4042\n",
      "Train Epoch: 001 Batch: 00083/00094 | Loss: 0.3781 | EWC Loss: 0.0140 | CE Loss: 0.3640\n",
      "Train Epoch: 001 Batch: 00084/00094 | Loss: 0.3086 | EWC Loss: 0.0138 | CE Loss: 0.2948\n",
      "Train Epoch: 001 Batch: 00085/00094 | Loss: 0.2623 | EWC Loss: 0.0140 | CE Loss: 0.2483\n",
      "Train Epoch: 001 Batch: 00086/00094 | Loss: 0.3338 | EWC Loss: 0.0145 | CE Loss: 0.3193\n",
      "Train Epoch: 001 Batch: 00087/00094 | Loss: 0.2427 | EWC Loss: 0.0151 | CE Loss: 0.2277\n",
      "Train Epoch: 001 Batch: 00088/00094 | Loss: 0.3327 | EWC Loss: 0.0149 | CE Loss: 0.3178\n",
      "Train Epoch: 001 Batch: 00089/00094 | Loss: 0.2940 | EWC Loss: 0.0152 | CE Loss: 0.2788\n",
      "Train Epoch: 001 Batch: 00090/00094 | Loss: 0.2590 | EWC Loss: 0.0153 | CE Loss: 0.2437\n",
      "Train Epoch: 001 Batch: 00091/00094 | Loss: 0.3942 | EWC Loss: 0.0154 | CE Loss: 0.3787\n",
      "Train Epoch: 001 Batch: 00092/00094 | Loss: 0.3070 | EWC Loss: 0.0151 | CE Loss: 0.2919\n",
      "Train Epoch: 001 Batch: 00093/00094 | Loss: 0.2935 | EWC Loss: 0.0155 | CE Loss: 0.2780\n",
      "Train Epoch: 001 Batch: 00094/00094 | Loss: 0.2082 | EWC Loss: 0.0171 | CE Loss: 0.1912\n",
      "Train Epoch: 000 |Acc 79.56667 | Loss: 0.44677 | Task Loss 0.43606 | EWC Loss: 0.01071\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2912 | acc:88.6000\n",
      "[VAL Acc] Target: 88.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.2841 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8062 | acc:64.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 64.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.7848 | acc:64.3130\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 64.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6643 | acc:63.4013\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 63.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6958 | acc:61.8299\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 61.83%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4245 | acc:80.6034\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 80.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8076 | acc:59.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.38%\n",
      "[VAL Acc] Avg 67.12%\n",
      "VAL Acc improve from 0.00% to 67.12%\n",
      "Save best model\n",
      "\n",
      "\n",
      "---------- Starting epoch 1 ----------\n",
      "Train Epoch: 002 Batch: 00001/00094 | Loss: 0.3187 | EWC Loss: 0.0161 | CE Loss: 0.3026\n",
      "Train Epoch: 002 Batch: 00002/00094 | Loss: 0.2905 | EWC Loss: 0.0159 | CE Loss: 0.2746\n",
      "Train Epoch: 002 Batch: 00003/00094 | Loss: 0.3081 | EWC Loss: 0.0157 | CE Loss: 0.2924\n",
      "Train Epoch: 002 Batch: 00004/00094 | Loss: 0.2558 | EWC Loss: 0.0157 | CE Loss: 0.2401\n",
      "Train Epoch: 002 Batch: 00005/00094 | Loss: 0.2713 | EWC Loss: 0.0160 | CE Loss: 0.2552\n",
      "Train Epoch: 002 Batch: 00006/00094 | Loss: 0.2992 | EWC Loss: 0.0159 | CE Loss: 0.2833\n",
      "Train Epoch: 002 Batch: 00007/00094 | Loss: 0.2663 | EWC Loss: 0.0162 | CE Loss: 0.2501\n",
      "Train Epoch: 002 Batch: 00008/00094 | Loss: 0.1772 | EWC Loss: 0.0158 | CE Loss: 0.1614\n",
      "Train Epoch: 002 Batch: 00009/00094 | Loss: 0.3566 | EWC Loss: 0.0156 | CE Loss: 0.3410\n",
      "Train Epoch: 002 Batch: 00010/00094 | Loss: 0.2859 | EWC Loss: 0.0154 | CE Loss: 0.2705\n",
      "Train Epoch: 002 Batch: 00011/00094 | Loss: 0.3155 | EWC Loss: 0.0156 | CE Loss: 0.2999\n",
      "Train Epoch: 002 Batch: 00012/00094 | Loss: 0.2442 | EWC Loss: 0.0164 | CE Loss: 0.2278\n",
      "Train Epoch: 002 Batch: 00013/00094 | Loss: 0.1853 | EWC Loss: 0.0162 | CE Loss: 0.1691\n",
      "Train Epoch: 002 Batch: 00014/00094 | Loss: 0.4449 | EWC Loss: 0.0159 | CE Loss: 0.4290\n",
      "Train Epoch: 002 Batch: 00015/00094 | Loss: 0.3509 | EWC Loss: 0.0161 | CE Loss: 0.3348\n",
      "Train Epoch: 002 Batch: 00016/00094 | Loss: 0.2289 | EWC Loss: 0.0164 | CE Loss: 0.2125\n",
      "Train Epoch: 002 Batch: 00017/00094 | Loss: 0.2752 | EWC Loss: 0.0162 | CE Loss: 0.2590\n",
      "Train Epoch: 002 Batch: 00018/00094 | Loss: 0.3575 | EWC Loss: 0.0167 | CE Loss: 0.3409\n",
      "Train Epoch: 002 Batch: 00019/00094 | Loss: 0.3657 | EWC Loss: 0.0168 | CE Loss: 0.3489\n",
      "Train Epoch: 002 Batch: 00020/00094 | Loss: 0.3241 | EWC Loss: 0.0170 | CE Loss: 0.3071\n",
      "Train Epoch: 002 Batch: 00021/00094 | Loss: 0.2650 | EWC Loss: 0.0171 | CE Loss: 0.2479\n",
      "Train Epoch: 002 Batch: 00022/00094 | Loss: 0.2495 | EWC Loss: 0.0172 | CE Loss: 0.2323\n",
      "Train Epoch: 002 Batch: 00023/00094 | Loss: 0.4555 | EWC Loss: 0.0173 | CE Loss: 0.4382\n",
      "Train Epoch: 002 Batch: 00024/00094 | Loss: 0.2636 | EWC Loss: 0.0174 | CE Loss: 0.2462\n",
      "Train Epoch: 002 Batch: 00025/00094 | Loss: 0.2269 | EWC Loss: 0.0174 | CE Loss: 0.2095\n",
      "Train Epoch: 002 Batch: 00026/00094 | Loss: 0.3068 | EWC Loss: 0.0174 | CE Loss: 0.2894\n",
      "Train Epoch: 002 Batch: 00027/00094 | Loss: 0.2315 | EWC Loss: 0.0173 | CE Loss: 0.2143\n",
      "Train Epoch: 002 Batch: 00028/00094 | Loss: 0.3257 | EWC Loss: 0.0174 | CE Loss: 0.3083\n",
      "Train Epoch: 002 Batch: 00029/00094 | Loss: 0.2576 | EWC Loss: 0.0172 | CE Loss: 0.2405\n",
      "Train Epoch: 002 Batch: 00030/00094 | Loss: 0.3871 | EWC Loss: 0.0169 | CE Loss: 0.3702\n",
      "Train Epoch: 002 Batch: 00031/00094 | Loss: 0.2801 | EWC Loss: 0.0174 | CE Loss: 0.2627\n",
      "Train Epoch: 002 Batch: 00032/00094 | Loss: 0.3046 | EWC Loss: 0.0175 | CE Loss: 0.2871\n",
      "Train Epoch: 002 Batch: 00033/00094 | Loss: 0.2312 | EWC Loss: 0.0175 | CE Loss: 0.2136\n",
      "Train Epoch: 002 Batch: 00034/00094 | Loss: 0.2307 | EWC Loss: 0.0176 | CE Loss: 0.2130\n",
      "Train Epoch: 002 Batch: 00035/00094 | Loss: 0.3599 | EWC Loss: 0.0174 | CE Loss: 0.3426\n",
      "Train Epoch: 002 Batch: 00036/00094 | Loss: 0.2839 | EWC Loss: 0.0174 | CE Loss: 0.2665\n",
      "Train Epoch: 002 Batch: 00037/00094 | Loss: 0.2411 | EWC Loss: 0.0172 | CE Loss: 0.2239\n",
      "Train Epoch: 002 Batch: 00038/00094 | Loss: 0.1990 | EWC Loss: 0.0172 | CE Loss: 0.1818\n",
      "Train Epoch: 002 Batch: 00039/00094 | Loss: 0.2721 | EWC Loss: 0.0171 | CE Loss: 0.2550\n",
      "Train Epoch: 002 Batch: 00040/00094 | Loss: 0.2919 | EWC Loss: 0.0173 | CE Loss: 0.2746\n",
      "Train Epoch: 002 Batch: 00041/00094 | Loss: 0.2584 | EWC Loss: 0.0171 | CE Loss: 0.2413\n",
      "Train Epoch: 002 Batch: 00042/00094 | Loss: 0.2120 | EWC Loss: 0.0170 | CE Loss: 0.1950\n",
      "Train Epoch: 002 Batch: 00043/00094 | Loss: 0.3142 | EWC Loss: 0.0172 | CE Loss: 0.2970\n",
      "Train Epoch: 002 Batch: 00044/00094 | Loss: 0.2297 | EWC Loss: 0.0175 | CE Loss: 0.2122\n",
      "Train Epoch: 002 Batch: 00045/00094 | Loss: 0.1891 | EWC Loss: 0.0172 | CE Loss: 0.1719\n",
      "Train Epoch: 002 Batch: 00046/00094 | Loss: 0.2590 | EWC Loss: 0.0172 | CE Loss: 0.2418\n",
      "Train Epoch: 002 Batch: 00047/00094 | Loss: 0.2728 | EWC Loss: 0.0171 | CE Loss: 0.2557\n",
      "Train Epoch: 002 Batch: 00048/00094 | Loss: 0.3371 | EWC Loss: 0.0174 | CE Loss: 0.3197\n",
      "Train Epoch: 002 Batch: 00049/00094 | Loss: 0.2195 | EWC Loss: 0.0172 | CE Loss: 0.2022\n",
      "Train Epoch: 002 Batch: 00050/00094 | Loss: 0.2161 | EWC Loss: 0.0169 | CE Loss: 0.1993\n",
      "Train Epoch: 002 Batch: 00051/00094 | Loss: 0.2595 | EWC Loss: 0.0170 | CE Loss: 0.2425\n",
      "Train Epoch: 002 Batch: 00052/00094 | Loss: 0.2338 | EWC Loss: 0.0170 | CE Loss: 0.2169\n",
      "Train Epoch: 002 Batch: 00053/00094 | Loss: 0.2972 | EWC Loss: 0.0173 | CE Loss: 0.2798\n",
      "Train Epoch: 002 Batch: 00054/00094 | Loss: 0.3263 | EWC Loss: 0.0178 | CE Loss: 0.3085\n",
      "Train Epoch: 002 Batch: 00055/00094 | Loss: 0.2606 | EWC Loss: 0.0177 | CE Loss: 0.2429\n",
      "Train Epoch: 002 Batch: 00056/00094 | Loss: 0.2474 | EWC Loss: 0.0177 | CE Loss: 0.2297\n",
      "Train Epoch: 002 Batch: 00057/00094 | Loss: 0.2696 | EWC Loss: 0.0179 | CE Loss: 0.2517\n",
      "Train Epoch: 002 Batch: 00058/00094 | Loss: 0.3303 | EWC Loss: 0.0178 | CE Loss: 0.3125\n",
      "Train Epoch: 002 Batch: 00059/00094 | Loss: 0.2136 | EWC Loss: 0.0178 | CE Loss: 0.1958\n",
      "Train Epoch: 002 Batch: 00060/00094 | Loss: 0.2298 | EWC Loss: 0.0178 | CE Loss: 0.2120\n",
      "Train Epoch: 002 Batch: 00061/00094 | Loss: 0.3655 | EWC Loss: 0.0175 | CE Loss: 0.3480\n",
      "Train Epoch: 002 Batch: 00062/00094 | Loss: 0.3745 | EWC Loss: 0.0175 | CE Loss: 0.3570\n",
      "Train Epoch: 002 Batch: 00063/00094 | Loss: 0.3736 | EWC Loss: 0.0175 | CE Loss: 0.3561\n",
      "Train Epoch: 002 Batch: 00064/00094 | Loss: 0.1582 | EWC Loss: 0.0174 | CE Loss: 0.1408\n",
      "Train Epoch: 002 Batch: 00065/00094 | Loss: 0.3590 | EWC Loss: 0.0175 | CE Loss: 0.3414\n",
      "Train Epoch: 002 Batch: 00066/00094 | Loss: 0.2647 | EWC Loss: 0.0177 | CE Loss: 0.2471\n",
      "Train Epoch: 002 Batch: 00067/00094 | Loss: 0.2843 | EWC Loss: 0.0183 | CE Loss: 0.2660\n",
      "Train Epoch: 002 Batch: 00068/00094 | Loss: 0.2722 | EWC Loss: 0.0182 | CE Loss: 0.2540\n",
      "Train Epoch: 002 Batch: 00069/00094 | Loss: 0.2071 | EWC Loss: 0.0179 | CE Loss: 0.1892\n",
      "Train Epoch: 002 Batch: 00070/00094 | Loss: 0.2534 | EWC Loss: 0.0182 | CE Loss: 0.2352\n",
      "Train Epoch: 002 Batch: 00071/00094 | Loss: 0.3117 | EWC Loss: 0.0182 | CE Loss: 0.2935\n",
      "Train Epoch: 002 Batch: 00072/00094 | Loss: 0.2002 | EWC Loss: 0.0180 | CE Loss: 0.1822\n",
      "Train Epoch: 002 Batch: 00073/00094 | Loss: 0.2881 | EWC Loss: 0.0181 | CE Loss: 0.2700\n",
      "Train Epoch: 002 Batch: 00074/00094 | Loss: 0.3425 | EWC Loss: 0.0183 | CE Loss: 0.3241\n",
      "Train Epoch: 002 Batch: 00075/00094 | Loss: 0.1787 | EWC Loss: 0.0179 | CE Loss: 0.1608\n",
      "Train Epoch: 002 Batch: 00076/00094 | Loss: 0.2499 | EWC Loss: 0.0177 | CE Loss: 0.2322\n",
      "Train Epoch: 002 Batch: 00077/00094 | Loss: 0.1543 | EWC Loss: 0.0175 | CE Loss: 0.1368\n",
      "Train Epoch: 002 Batch: 00078/00094 | Loss: 0.2674 | EWC Loss: 0.0174 | CE Loss: 0.2500\n",
      "Train Epoch: 002 Batch: 00079/00094 | Loss: 0.2442 | EWC Loss: 0.0174 | CE Loss: 0.2268\n",
      "Train Epoch: 002 Batch: 00080/00094 | Loss: 0.2929 | EWC Loss: 0.0171 | CE Loss: 0.2757\n",
      "Train Epoch: 002 Batch: 00081/00094 | Loss: 0.2519 | EWC Loss: 0.0173 | CE Loss: 0.2346\n",
      "Train Epoch: 002 Batch: 00082/00094 | Loss: 0.2215 | EWC Loss: 0.0173 | CE Loss: 0.2041\n",
      "Train Epoch: 002 Batch: 00083/00094 | Loss: 0.2182 | EWC Loss: 0.0172 | CE Loss: 0.2010\n",
      "Train Epoch: 002 Batch: 00084/00094 | Loss: 0.2363 | EWC Loss: 0.0174 | CE Loss: 0.2189\n",
      "Train Epoch: 002 Batch: 00085/00094 | Loss: 0.2932 | EWC Loss: 0.0178 | CE Loss: 0.2754\n",
      "Train Epoch: 002 Batch: 00086/00094 | Loss: 0.2729 | EWC Loss: 0.0181 | CE Loss: 0.2548\n",
      "Train Epoch: 002 Batch: 00087/00094 | Loss: 0.3047 | EWC Loss: 0.0181 | CE Loss: 0.2866\n",
      "Train Epoch: 002 Batch: 00088/00094 | Loss: 0.4044 | EWC Loss: 0.0181 | CE Loss: 0.3863\n",
      "Train Epoch: 002 Batch: 00089/00094 | Loss: 0.1771 | EWC Loss: 0.0186 | CE Loss: 0.1585\n",
      "Train Epoch: 002 Batch: 00090/00094 | Loss: 0.2435 | EWC Loss: 0.0187 | CE Loss: 0.2248\n",
      "Train Epoch: 002 Batch: 00091/00094 | Loss: 0.1641 | EWC Loss: 0.0187 | CE Loss: 0.1454\n",
      "Train Epoch: 002 Batch: 00092/00094 | Loss: 0.1288 | EWC Loss: 0.0188 | CE Loss: 0.1100\n",
      "Train Epoch: 002 Batch: 00093/00094 | Loss: 0.2571 | EWC Loss: 0.0187 | CE Loss: 0.2384\n",
      "Train Epoch: 002 Batch: 00094/00094 | Loss: 0.2666 | EWC Loss: 0.0186 | CE Loss: 0.2480\n",
      "Train Epoch: 001 |Acc 89.50000 | Loss: 0.27281 | Task Loss 0.25553 | EWC Loss: 0.01728\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2265 | acc:90.9500\n",
      "[VAL Acc] Target: 90.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.6328 | acc:52.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 52.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9412 | acc:60.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.8513 | acc:59.1603\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 59.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9298 | acc:56.7790\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5950 | acc:68.8540\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 68.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5476 | acc:74.1771\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 74.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8035 | acc:59.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.44%\n",
      "[VAL Acc] Avg 65.28%\n",
      "\n",
      "\n",
      "---------- Starting epoch 2 ----------\n",
      "Train Epoch: 003 Batch: 00001/00094 | Loss: 0.1612 | EWC Loss: 0.0185 | CE Loss: 0.1428\n",
      "Train Epoch: 003 Batch: 00002/00094 | Loss: 0.1558 | EWC Loss: 0.0185 | CE Loss: 0.1373\n",
      "Train Epoch: 003 Batch: 00003/00094 | Loss: 0.2577 | EWC Loss: 0.0184 | CE Loss: 0.2394\n",
      "Train Epoch: 003 Batch: 00004/00094 | Loss: 0.2234 | EWC Loss: 0.0183 | CE Loss: 0.2050\n",
      "Train Epoch: 003 Batch: 00005/00094 | Loss: 0.3338 | EWC Loss: 0.0183 | CE Loss: 0.3154\n",
      "Train Epoch: 003 Batch: 00006/00094 | Loss: 0.2641 | EWC Loss: 0.0182 | CE Loss: 0.2459\n",
      "Train Epoch: 003 Batch: 00007/00094 | Loss: 0.2086 | EWC Loss: 0.0183 | CE Loss: 0.1903\n",
      "Train Epoch: 003 Batch: 00008/00094 | Loss: 0.2879 | EWC Loss: 0.0180 | CE Loss: 0.2699\n",
      "Train Epoch: 003 Batch: 00009/00094 | Loss: 0.1681 | EWC Loss: 0.0181 | CE Loss: 0.1500\n",
      "Train Epoch: 003 Batch: 00010/00094 | Loss: 0.2106 | EWC Loss: 0.0181 | CE Loss: 0.1925\n",
      "Train Epoch: 003 Batch: 00011/00094 | Loss: 0.2061 | EWC Loss: 0.0180 | CE Loss: 0.1881\n",
      "Train Epoch: 003 Batch: 00012/00094 | Loss: 0.2358 | EWC Loss: 0.0180 | CE Loss: 0.2177\n",
      "Train Epoch: 003 Batch: 00013/00094 | Loss: 0.2281 | EWC Loss: 0.0180 | CE Loss: 0.2101\n",
      "Train Epoch: 003 Batch: 00014/00094 | Loss: 0.2325 | EWC Loss: 0.0180 | CE Loss: 0.2145\n",
      "Train Epoch: 003 Batch: 00015/00094 | Loss: 0.4276 | EWC Loss: 0.0178 | CE Loss: 0.4098\n",
      "Train Epoch: 003 Batch: 00016/00094 | Loss: 0.2601 | EWC Loss: 0.0177 | CE Loss: 0.2424\n",
      "Train Epoch: 003 Batch: 00017/00094 | Loss: 0.1256 | EWC Loss: 0.0178 | CE Loss: 0.1077\n",
      "Train Epoch: 003 Batch: 00018/00094 | Loss: 0.2443 | EWC Loss: 0.0179 | CE Loss: 0.2264\n",
      "Train Epoch: 003 Batch: 00019/00094 | Loss: 0.2635 | EWC Loss: 0.0178 | CE Loss: 0.2457\n",
      "Train Epoch: 003 Batch: 00020/00094 | Loss: 0.2349 | EWC Loss: 0.0180 | CE Loss: 0.2169\n",
      "Train Epoch: 003 Batch: 00021/00094 | Loss: 0.1989 | EWC Loss: 0.0182 | CE Loss: 0.1807\n",
      "Train Epoch: 003 Batch: 00022/00094 | Loss: 0.1650 | EWC Loss: 0.0181 | CE Loss: 0.1469\n",
      "Train Epoch: 003 Batch: 00023/00094 | Loss: 0.2312 | EWC Loss: 0.0179 | CE Loss: 0.2133\n",
      "Train Epoch: 003 Batch: 00024/00094 | Loss: 0.3005 | EWC Loss: 0.0177 | CE Loss: 0.2828\n",
      "Train Epoch: 003 Batch: 00025/00094 | Loss: 0.2351 | EWC Loss: 0.0178 | CE Loss: 0.2173\n",
      "Train Epoch: 003 Batch: 00026/00094 | Loss: 0.2044 | EWC Loss: 0.0177 | CE Loss: 0.1866\n",
      "Train Epoch: 003 Batch: 00027/00094 | Loss: 0.2544 | EWC Loss: 0.0178 | CE Loss: 0.2365\n",
      "Train Epoch: 003 Batch: 00028/00094 | Loss: 0.2212 | EWC Loss: 0.0179 | CE Loss: 0.2033\n",
      "Train Epoch: 003 Batch: 00029/00094 | Loss: 0.2373 | EWC Loss: 0.0179 | CE Loss: 0.2194\n",
      "Train Epoch: 003 Batch: 00030/00094 | Loss: 0.1523 | EWC Loss: 0.0180 | CE Loss: 0.1343\n",
      "Train Epoch: 003 Batch: 00031/00094 | Loss: 0.2349 | EWC Loss: 0.0182 | CE Loss: 0.2167\n",
      "Train Epoch: 003 Batch: 00032/00094 | Loss: 0.2332 | EWC Loss: 0.0184 | CE Loss: 0.2148\n",
      "Train Epoch: 003 Batch: 00033/00094 | Loss: 0.2910 | EWC Loss: 0.0187 | CE Loss: 0.2723\n",
      "Train Epoch: 003 Batch: 00034/00094 | Loss: 0.1893 | EWC Loss: 0.0186 | CE Loss: 0.1707\n",
      "Train Epoch: 003 Batch: 00035/00094 | Loss: 0.2177 | EWC Loss: 0.0187 | CE Loss: 0.1989\n",
      "Train Epoch: 003 Batch: 00036/00094 | Loss: 0.2933 | EWC Loss: 0.0186 | CE Loss: 0.2748\n",
      "Train Epoch: 003 Batch: 00037/00094 | Loss: 0.2476 | EWC Loss: 0.0184 | CE Loss: 0.2291\n",
      "Train Epoch: 003 Batch: 00038/00094 | Loss: 0.2111 | EWC Loss: 0.0182 | CE Loss: 0.1929\n",
      "Train Epoch: 003 Batch: 00039/00094 | Loss: 0.2047 | EWC Loss: 0.0183 | CE Loss: 0.1865\n",
      "Train Epoch: 003 Batch: 00040/00094 | Loss: 0.2152 | EWC Loss: 0.0184 | CE Loss: 0.1968\n",
      "Train Epoch: 003 Batch: 00041/00094 | Loss: 0.2264 | EWC Loss: 0.0184 | CE Loss: 0.2080\n",
      "Train Epoch: 003 Batch: 00042/00094 | Loss: 0.2408 | EWC Loss: 0.0184 | CE Loss: 0.2223\n",
      "Train Epoch: 003 Batch: 00043/00094 | Loss: 0.1953 | EWC Loss: 0.0183 | CE Loss: 0.1770\n",
      "Train Epoch: 003 Batch: 00044/00094 | Loss: 0.2224 | EWC Loss: 0.0184 | CE Loss: 0.2040\n",
      "Train Epoch: 003 Batch: 00045/00094 | Loss: 0.2584 | EWC Loss: 0.0182 | CE Loss: 0.2402\n",
      "Train Epoch: 003 Batch: 00046/00094 | Loss: 0.1985 | EWC Loss: 0.0183 | CE Loss: 0.1802\n",
      "Train Epoch: 003 Batch: 00047/00094 | Loss: 0.2429 | EWC Loss: 0.0184 | CE Loss: 0.2245\n",
      "Train Epoch: 003 Batch: 00048/00094 | Loss: 0.2089 | EWC Loss: 0.0182 | CE Loss: 0.1906\n",
      "Train Epoch: 003 Batch: 00049/00094 | Loss: 0.2368 | EWC Loss: 0.0180 | CE Loss: 0.2188\n",
      "Train Epoch: 003 Batch: 00050/00094 | Loss: 0.1555 | EWC Loss: 0.0181 | CE Loss: 0.1374\n",
      "Train Epoch: 003 Batch: 00051/00094 | Loss: 0.2163 | EWC Loss: 0.0180 | CE Loss: 0.1983\n",
      "Train Epoch: 003 Batch: 00052/00094 | Loss: 0.2168 | EWC Loss: 0.0182 | CE Loss: 0.1986\n",
      "Train Epoch: 003 Batch: 00053/00094 | Loss: 0.2741 | EWC Loss: 0.0182 | CE Loss: 0.2560\n",
      "Train Epoch: 003 Batch: 00054/00094 | Loss: 0.2005 | EWC Loss: 0.0181 | CE Loss: 0.1824\n",
      "Train Epoch: 003 Batch: 00055/00094 | Loss: 0.2133 | EWC Loss: 0.0181 | CE Loss: 0.1953\n",
      "Train Epoch: 003 Batch: 00056/00094 | Loss: 0.2144 | EWC Loss: 0.0184 | CE Loss: 0.1960\n",
      "Train Epoch: 003 Batch: 00057/00094 | Loss: 0.2867 | EWC Loss: 0.0186 | CE Loss: 0.2681\n",
      "Train Epoch: 003 Batch: 00058/00094 | Loss: 0.1886 | EWC Loss: 0.0188 | CE Loss: 0.1698\n",
      "Train Epoch: 003 Batch: 00059/00094 | Loss: 0.2743 | EWC Loss: 0.0189 | CE Loss: 0.2555\n",
      "Train Epoch: 003 Batch: 00060/00094 | Loss: 0.1503 | EWC Loss: 0.0188 | CE Loss: 0.1315\n",
      "Train Epoch: 003 Batch: 00061/00094 | Loss: 0.2694 | EWC Loss: 0.0187 | CE Loss: 0.2507\n",
      "Train Epoch: 003 Batch: 00062/00094 | Loss: 0.1925 | EWC Loss: 0.0186 | CE Loss: 0.1739\n",
      "Train Epoch: 003 Batch: 00063/00094 | Loss: 0.2945 | EWC Loss: 0.0186 | CE Loss: 0.2758\n",
      "Train Epoch: 003 Batch: 00064/00094 | Loss: 0.1703 | EWC Loss: 0.0187 | CE Loss: 0.1516\n",
      "Train Epoch: 003 Batch: 00065/00094 | Loss: 0.2070 | EWC Loss: 0.0187 | CE Loss: 0.1883\n",
      "Train Epoch: 003 Batch: 00066/00094 | Loss: 0.2576 | EWC Loss: 0.0187 | CE Loss: 0.2389\n",
      "Train Epoch: 003 Batch: 00067/00094 | Loss: 0.1832 | EWC Loss: 0.0188 | CE Loss: 0.1645\n",
      "Train Epoch: 003 Batch: 00068/00094 | Loss: 0.2311 | EWC Loss: 0.0187 | CE Loss: 0.2124\n",
      "Train Epoch: 003 Batch: 00069/00094 | Loss: 0.4636 | EWC Loss: 0.0191 | CE Loss: 0.4445\n",
      "Train Epoch: 003 Batch: 00070/00094 | Loss: 0.2468 | EWC Loss: 0.0194 | CE Loss: 0.2275\n",
      "Train Epoch: 003 Batch: 00071/00094 | Loss: 0.2322 | EWC Loss: 0.0198 | CE Loss: 0.2125\n",
      "Train Epoch: 003 Batch: 00072/00094 | Loss: 0.2307 | EWC Loss: 0.0199 | CE Loss: 0.2108\n",
      "Train Epoch: 003 Batch: 00073/00094 | Loss: 0.1647 | EWC Loss: 0.0197 | CE Loss: 0.1449\n",
      "Train Epoch: 003 Batch: 00074/00094 | Loss: 0.1608 | EWC Loss: 0.0197 | CE Loss: 0.1410\n",
      "Train Epoch: 003 Batch: 00075/00094 | Loss: 0.2269 | EWC Loss: 0.0196 | CE Loss: 0.2073\n",
      "Train Epoch: 003 Batch: 00076/00094 | Loss: 0.3123 | EWC Loss: 0.0194 | CE Loss: 0.2929\n",
      "Train Epoch: 003 Batch: 00077/00094 | Loss: 0.2738 | EWC Loss: 0.0198 | CE Loss: 0.2540\n",
      "Train Epoch: 003 Batch: 00078/00094 | Loss: 0.2795 | EWC Loss: 0.0202 | CE Loss: 0.2593\n",
      "Train Epoch: 003 Batch: 00079/00094 | Loss: 0.1692 | EWC Loss: 0.0204 | CE Loss: 0.1488\n",
      "Train Epoch: 003 Batch: 00080/00094 | Loss: 0.2047 | EWC Loss: 0.0204 | CE Loss: 0.1842\n",
      "Train Epoch: 003 Batch: 00081/00094 | Loss: 0.1643 | EWC Loss: 0.0203 | CE Loss: 0.1440\n",
      "Train Epoch: 003 Batch: 00082/00094 | Loss: 0.1920 | EWC Loss: 0.0199 | CE Loss: 0.1721\n",
      "Train Epoch: 003 Batch: 00083/00094 | Loss: 0.1541 | EWC Loss: 0.0197 | CE Loss: 0.1344\n",
      "Train Epoch: 003 Batch: 00084/00094 | Loss: 0.1817 | EWC Loss: 0.0199 | CE Loss: 0.1619\n",
      "Train Epoch: 003 Batch: 00085/00094 | Loss: 0.3178 | EWC Loss: 0.0198 | CE Loss: 0.2980\n",
      "Train Epoch: 003 Batch: 00086/00094 | Loss: 0.2317 | EWC Loss: 0.0198 | CE Loss: 0.2119\n",
      "Train Epoch: 003 Batch: 00087/00094 | Loss: 0.2460 | EWC Loss: 0.0198 | CE Loss: 0.2262\n",
      "Train Epoch: 003 Batch: 00088/00094 | Loss: 0.2090 | EWC Loss: 0.0205 | CE Loss: 0.1885\n",
      "Train Epoch: 003 Batch: 00089/00094 | Loss: 0.1356 | EWC Loss: 0.0204 | CE Loss: 0.1151\n",
      "Train Epoch: 003 Batch: 00090/00094 | Loss: 0.2317 | EWC Loss: 0.0203 | CE Loss: 0.2114\n",
      "Train Epoch: 003 Batch: 00091/00094 | Loss: 0.1811 | EWC Loss: 0.0204 | CE Loss: 0.1607\n",
      "Train Epoch: 003 Batch: 00092/00094 | Loss: 0.2640 | EWC Loss: 0.0203 | CE Loss: 0.2437\n",
      "Train Epoch: 003 Batch: 00093/00094 | Loss: 0.1834 | EWC Loss: 0.0207 | CE Loss: 0.1627\n",
      "Train Epoch: 003 Batch: 00094/00094 | Loss: 0.2002 | EWC Loss: 0.0209 | CE Loss: 0.1793\n",
      "Train Epoch: 002 |Acc 92.10000 | Loss: 0.22716 | Task Loss 0.20841 | EWC Loss: 0.01875\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1658 | acc:93.6500\n",
      "[VAL Acc] Target: 93.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8450 | acc:51.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0547 | acc:59.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0276 | acc:55.5344\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 55.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.9545 | acc:58.2680\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6092 | acc:71.5342\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5434 | acc:75.3918\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8381 | acc:58.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.50%\n",
      "[VAL Acc] Avg 65.56%\n",
      "\n",
      "\n",
      "---------- Starting epoch 3 ----------\n",
      "Train Epoch: 004 Batch: 00001/00094 | Loss: 0.1936 | EWC Loss: 0.0205 | CE Loss: 0.1731\n",
      "Train Epoch: 004 Batch: 00002/00094 | Loss: 0.1417 | EWC Loss: 0.0206 | CE Loss: 0.1211\n",
      "Train Epoch: 004 Batch: 00003/00094 | Loss: 0.2572 | EWC Loss: 0.0207 | CE Loss: 0.2366\n",
      "Train Epoch: 004 Batch: 00004/00094 | Loss: 0.1512 | EWC Loss: 0.0206 | CE Loss: 0.1306\n",
      "Train Epoch: 004 Batch: 00005/00094 | Loss: 0.2628 | EWC Loss: 0.0205 | CE Loss: 0.2422\n",
      "Train Epoch: 004 Batch: 00006/00094 | Loss: 0.2757 | EWC Loss: 0.0207 | CE Loss: 0.2550\n",
      "Train Epoch: 004 Batch: 00007/00094 | Loss: 0.2972 | EWC Loss: 0.0209 | CE Loss: 0.2763\n",
      "Train Epoch: 004 Batch: 00008/00094 | Loss: 0.1606 | EWC Loss: 0.0214 | CE Loss: 0.1392\n",
      "Train Epoch: 004 Batch: 00009/00094 | Loss: 0.2312 | EWC Loss: 0.0213 | CE Loss: 0.2100\n",
      "Train Epoch: 004 Batch: 00010/00094 | Loss: 0.1245 | EWC Loss: 0.0212 | CE Loss: 0.1033\n",
      "Train Epoch: 004 Batch: 00011/00094 | Loss: 0.1629 | EWC Loss: 0.0211 | CE Loss: 0.1419\n",
      "Train Epoch: 004 Batch: 00012/00094 | Loss: 0.1355 | EWC Loss: 0.0209 | CE Loss: 0.1146\n",
      "Train Epoch: 004 Batch: 00013/00094 | Loss: 0.1509 | EWC Loss: 0.0208 | CE Loss: 0.1301\n",
      "Train Epoch: 004 Batch: 00014/00094 | Loss: 0.2123 | EWC Loss: 0.0208 | CE Loss: 0.1915\n",
      "Train Epoch: 004 Batch: 00015/00094 | Loss: 0.2276 | EWC Loss: 0.0205 | CE Loss: 0.2071\n",
      "Train Epoch: 004 Batch: 00016/00094 | Loss: 0.1972 | EWC Loss: 0.0203 | CE Loss: 0.1769\n",
      "Train Epoch: 004 Batch: 00017/00094 | Loss: 0.1483 | EWC Loss: 0.0201 | CE Loss: 0.1282\n",
      "Train Epoch: 004 Batch: 00018/00094 | Loss: 0.1381 | EWC Loss: 0.0199 | CE Loss: 0.1181\n",
      "Train Epoch: 004 Batch: 00019/00094 | Loss: 0.2468 | EWC Loss: 0.0200 | CE Loss: 0.2268\n",
      "Train Epoch: 004 Batch: 00020/00094 | Loss: 0.1423 | EWC Loss: 0.0200 | CE Loss: 0.1223\n",
      "Train Epoch: 004 Batch: 00021/00094 | Loss: 0.2935 | EWC Loss: 0.0201 | CE Loss: 0.2734\n",
      "Train Epoch: 004 Batch: 00022/00094 | Loss: 0.2174 | EWC Loss: 0.0200 | CE Loss: 0.1974\n",
      "Train Epoch: 004 Batch: 00023/00094 | Loss: 0.1988 | EWC Loss: 0.0199 | CE Loss: 0.1789\n",
      "Train Epoch: 004 Batch: 00024/00094 | Loss: 0.2006 | EWC Loss: 0.0198 | CE Loss: 0.1808\n",
      "Train Epoch: 004 Batch: 00025/00094 | Loss: 0.2788 | EWC Loss: 0.0199 | CE Loss: 0.2589\n",
      "Train Epoch: 004 Batch: 00026/00094 | Loss: 0.1767 | EWC Loss: 0.0199 | CE Loss: 0.1568\n",
      "Train Epoch: 004 Batch: 00027/00094 | Loss: 0.2269 | EWC Loss: 0.0202 | CE Loss: 0.2068\n",
      "Train Epoch: 004 Batch: 00028/00094 | Loss: 0.1910 | EWC Loss: 0.0201 | CE Loss: 0.1709\n",
      "Train Epoch: 004 Batch: 00029/00094 | Loss: 0.2464 | EWC Loss: 0.0198 | CE Loss: 0.2266\n",
      "Train Epoch: 004 Batch: 00030/00094 | Loss: 0.1391 | EWC Loss: 0.0197 | CE Loss: 0.1194\n",
      "Train Epoch: 004 Batch: 00031/00094 | Loss: 0.2663 | EWC Loss: 0.0196 | CE Loss: 0.2467\n",
      "Train Epoch: 004 Batch: 00032/00094 | Loss: 0.2211 | EWC Loss: 0.0198 | CE Loss: 0.2013\n",
      "Train Epoch: 004 Batch: 00033/00094 | Loss: 0.1635 | EWC Loss: 0.0199 | CE Loss: 0.1436\n",
      "Train Epoch: 004 Batch: 00034/00094 | Loss: 0.1088 | EWC Loss: 0.0203 | CE Loss: 0.0885\n",
      "Train Epoch: 004 Batch: 00035/00094 | Loss: 0.2340 | EWC Loss: 0.0201 | CE Loss: 0.2139\n",
      "Train Epoch: 004 Batch: 00036/00094 | Loss: 0.1625 | EWC Loss: 0.0200 | CE Loss: 0.1425\n",
      "Train Epoch: 004 Batch: 00037/00094 | Loss: 0.2194 | EWC Loss: 0.0199 | CE Loss: 0.1994\n",
      "Train Epoch: 004 Batch: 00038/00094 | Loss: 0.2061 | EWC Loss: 0.0199 | CE Loss: 0.1862\n",
      "Train Epoch: 004 Batch: 00039/00094 | Loss: 0.2285 | EWC Loss: 0.0200 | CE Loss: 0.2084\n",
      "Train Epoch: 004 Batch: 00040/00094 | Loss: 0.1682 | EWC Loss: 0.0200 | CE Loss: 0.1482\n",
      "Train Epoch: 004 Batch: 00041/00094 | Loss: 0.2349 | EWC Loss: 0.0200 | CE Loss: 0.2149\n",
      "Train Epoch: 004 Batch: 00042/00094 | Loss: 0.1352 | EWC Loss: 0.0199 | CE Loss: 0.1153\n",
      "Train Epoch: 004 Batch: 00043/00094 | Loss: 0.1958 | EWC Loss: 0.0198 | CE Loss: 0.1760\n",
      "Train Epoch: 004 Batch: 00044/00094 | Loss: 0.1570 | EWC Loss: 0.0197 | CE Loss: 0.1372\n",
      "Train Epoch: 004 Batch: 00045/00094 | Loss: 0.1407 | EWC Loss: 0.0199 | CE Loss: 0.1208\n",
      "Train Epoch: 004 Batch: 00046/00094 | Loss: 0.1611 | EWC Loss: 0.0198 | CE Loss: 0.1413\n",
      "Train Epoch: 004 Batch: 00047/00094 | Loss: 0.2520 | EWC Loss: 0.0198 | CE Loss: 0.2322\n",
      "Train Epoch: 004 Batch: 00048/00094 | Loss: 0.3045 | EWC Loss: 0.0197 | CE Loss: 0.2848\n",
      "Train Epoch: 004 Batch: 00049/00094 | Loss: 0.2092 | EWC Loss: 0.0198 | CE Loss: 0.1894\n",
      "Train Epoch: 004 Batch: 00050/00094 | Loss: 0.2467 | EWC Loss: 0.0197 | CE Loss: 0.2271\n",
      "Train Epoch: 004 Batch: 00051/00094 | Loss: 0.2341 | EWC Loss: 0.0196 | CE Loss: 0.2145\n",
      "Train Epoch: 004 Batch: 00052/00094 | Loss: 0.2604 | EWC Loss: 0.0196 | CE Loss: 0.2408\n",
      "Train Epoch: 004 Batch: 00053/00094 | Loss: 0.2264 | EWC Loss: 0.0196 | CE Loss: 0.2067\n",
      "Train Epoch: 004 Batch: 00054/00094 | Loss: 0.1583 | EWC Loss: 0.0197 | CE Loss: 0.1385\n",
      "Train Epoch: 004 Batch: 00055/00094 | Loss: 0.1588 | EWC Loss: 0.0196 | CE Loss: 0.1391\n",
      "Train Epoch: 004 Batch: 00056/00094 | Loss: 0.1273 | EWC Loss: 0.0197 | CE Loss: 0.1076\n",
      "Train Epoch: 004 Batch: 00057/00094 | Loss: 0.1631 | EWC Loss: 0.0196 | CE Loss: 0.1435\n",
      "Train Epoch: 004 Batch: 00058/00094 | Loss: 0.1291 | EWC Loss: 0.0196 | CE Loss: 0.1095\n",
      "Train Epoch: 004 Batch: 00059/00094 | Loss: 0.2814 | EWC Loss: 0.0195 | CE Loss: 0.2620\n",
      "Train Epoch: 004 Batch: 00060/00094 | Loss: 0.2102 | EWC Loss: 0.0195 | CE Loss: 0.1907\n",
      "Train Epoch: 004 Batch: 00061/00094 | Loss: 0.1566 | EWC Loss: 0.0197 | CE Loss: 0.1370\n",
      "Train Epoch: 004 Batch: 00062/00094 | Loss: 0.3467 | EWC Loss: 0.0195 | CE Loss: 0.3271\n",
      "Train Epoch: 004 Batch: 00063/00094 | Loss: 0.1622 | EWC Loss: 0.0195 | CE Loss: 0.1426\n",
      "Train Epoch: 004 Batch: 00064/00094 | Loss: 0.1711 | EWC Loss: 0.0196 | CE Loss: 0.1515\n",
      "Train Epoch: 004 Batch: 00065/00094 | Loss: 0.2577 | EWC Loss: 0.0195 | CE Loss: 0.2382\n",
      "Train Epoch: 004 Batch: 00066/00094 | Loss: 0.1198 | EWC Loss: 0.0198 | CE Loss: 0.1000\n",
      "Train Epoch: 004 Batch: 00067/00094 | Loss: 0.3120 | EWC Loss: 0.0197 | CE Loss: 0.2923\n",
      "Train Epoch: 004 Batch: 00068/00094 | Loss: 0.2126 | EWC Loss: 0.0195 | CE Loss: 0.1930\n",
      "Train Epoch: 004 Batch: 00069/00094 | Loss: 0.2045 | EWC Loss: 0.0197 | CE Loss: 0.1848\n",
      "Train Epoch: 004 Batch: 00070/00094 | Loss: 0.1600 | EWC Loss: 0.0199 | CE Loss: 0.1402\n",
      "Train Epoch: 004 Batch: 00071/00094 | Loss: 0.2338 | EWC Loss: 0.0198 | CE Loss: 0.2140\n",
      "Train Epoch: 004 Batch: 00072/00094 | Loss: 0.1676 | EWC Loss: 0.0199 | CE Loss: 0.1477\n",
      "Train Epoch: 004 Batch: 00073/00094 | Loss: 0.1878 | EWC Loss: 0.0200 | CE Loss: 0.1678\n",
      "Train Epoch: 004 Batch: 00074/00094 | Loss: 0.1566 | EWC Loss: 0.0199 | CE Loss: 0.1368\n",
      "Train Epoch: 004 Batch: 00075/00094 | Loss: 0.2193 | EWC Loss: 0.0198 | CE Loss: 0.1995\n",
      "Train Epoch: 004 Batch: 00076/00094 | Loss: 0.1320 | EWC Loss: 0.0200 | CE Loss: 0.1120\n",
      "Train Epoch: 004 Batch: 00077/00094 | Loss: 0.2560 | EWC Loss: 0.0199 | CE Loss: 0.2360\n",
      "Train Epoch: 004 Batch: 00078/00094 | Loss: 0.2016 | EWC Loss: 0.0200 | CE Loss: 0.1816\n",
      "Train Epoch: 004 Batch: 00079/00094 | Loss: 0.1398 | EWC Loss: 0.0202 | CE Loss: 0.1197\n",
      "Train Epoch: 004 Batch: 00080/00094 | Loss: 0.2138 | EWC Loss: 0.0202 | CE Loss: 0.1937\n",
      "Train Epoch: 004 Batch: 00081/00094 | Loss: 0.1360 | EWC Loss: 0.0202 | CE Loss: 0.1159\n",
      "Train Epoch: 004 Batch: 00082/00094 | Loss: 0.2019 | EWC Loss: 0.0202 | CE Loss: 0.1817\n",
      "Train Epoch: 004 Batch: 00083/00094 | Loss: 0.2338 | EWC Loss: 0.0204 | CE Loss: 0.2134\n",
      "Train Epoch: 004 Batch: 00084/00094 | Loss: 0.2100 | EWC Loss: 0.0205 | CE Loss: 0.1895\n",
      "Train Epoch: 004 Batch: 00085/00094 | Loss: 0.1374 | EWC Loss: 0.0206 | CE Loss: 0.1168\n",
      "Train Epoch: 004 Batch: 00086/00094 | Loss: 0.1557 | EWC Loss: 0.0208 | CE Loss: 0.1349\n",
      "Train Epoch: 004 Batch: 00087/00094 | Loss: 0.1228 | EWC Loss: 0.0206 | CE Loss: 0.1022\n",
      "Train Epoch: 004 Batch: 00088/00094 | Loss: 0.2052 | EWC Loss: 0.0207 | CE Loss: 0.1845\n",
      "Train Epoch: 004 Batch: 00089/00094 | Loss: 0.2251 | EWC Loss: 0.0208 | CE Loss: 0.2044\n",
      "Train Epoch: 004 Batch: 00090/00094 | Loss: 0.2363 | EWC Loss: 0.0210 | CE Loss: 0.2153\n",
      "Train Epoch: 004 Batch: 00091/00094 | Loss: 0.1905 | EWC Loss: 0.0208 | CE Loss: 0.1697\n",
      "Train Epoch: 004 Batch: 00092/00094 | Loss: 0.1892 | EWC Loss: 0.0209 | CE Loss: 0.1683\n",
      "Train Epoch: 004 Batch: 00093/00094 | Loss: 0.1528 | EWC Loss: 0.0206 | CE Loss: 0.1321\n",
      "Train Epoch: 004 Batch: 00094/00094 | Loss: 0.1059 | EWC Loss: 0.0205 | CE Loss: 0.0854\n",
      "Train Epoch: 003 |Acc 93.40000 | Loss: 0.19687 | Task Loss 0.17676 | EWC Loss: 0.02011\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1526 | acc:94.5500\n",
      "[VAL Acc] Target: 94.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.8580 | acc:49.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.9971 | acc:60.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0123 | acc:57.6336\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 57.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.0400 | acc:56.7790\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6453 | acc:72.4584\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.46%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5285 | acc:76.1755\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8609 | acc:59.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.50%\n",
      "[VAL Acc] Avg 65.91%\n",
      "\n",
      "\n",
      "---------- Starting epoch 4 ----------\n",
      "Train Epoch: 005 Batch: 00001/00094 | Loss: 0.2157 | EWC Loss: 0.0202 | CE Loss: 0.1955\n",
      "Train Epoch: 005 Batch: 00002/00094 | Loss: 0.1769 | EWC Loss: 0.0202 | CE Loss: 0.1567\n",
      "Train Epoch: 005 Batch: 00003/00094 | Loss: 0.1929 | EWC Loss: 0.0203 | CE Loss: 0.1726\n",
      "Train Epoch: 005 Batch: 00004/00094 | Loss: 0.2107 | EWC Loss: 0.0203 | CE Loss: 0.1904\n",
      "Train Epoch: 005 Batch: 00005/00094 | Loss: 0.2154 | EWC Loss: 0.0203 | CE Loss: 0.1950\n",
      "Train Epoch: 005 Batch: 00006/00094 | Loss: 0.2169 | EWC Loss: 0.0203 | CE Loss: 0.1966\n",
      "Train Epoch: 005 Batch: 00007/00094 | Loss: 0.1629 | EWC Loss: 0.0202 | CE Loss: 0.1427\n",
      "Train Epoch: 005 Batch: 00008/00094 | Loss: 0.1865 | EWC Loss: 0.0203 | CE Loss: 0.1662\n",
      "Train Epoch: 005 Batch: 00009/00094 | Loss: 0.1584 | EWC Loss: 0.0204 | CE Loss: 0.1380\n",
      "Train Epoch: 005 Batch: 00010/00094 | Loss: 0.2640 | EWC Loss: 0.0203 | CE Loss: 0.2437\n",
      "Train Epoch: 005 Batch: 00011/00094 | Loss: 0.2240 | EWC Loss: 0.0203 | CE Loss: 0.2037\n",
      "Train Epoch: 005 Batch: 00012/00094 | Loss: 0.1735 | EWC Loss: 0.0205 | CE Loss: 0.1530\n",
      "Train Epoch: 005 Batch: 00013/00094 | Loss: 0.1648 | EWC Loss: 0.0202 | CE Loss: 0.1445\n",
      "Train Epoch: 005 Batch: 00014/00094 | Loss: 0.1594 | EWC Loss: 0.0201 | CE Loss: 0.1394\n",
      "Train Epoch: 005 Batch: 00015/00094 | Loss: 0.1504 | EWC Loss: 0.0200 | CE Loss: 0.1304\n",
      "Train Epoch: 005 Batch: 00016/00094 | Loss: 0.2836 | EWC Loss: 0.0200 | CE Loss: 0.2636\n",
      "Train Epoch: 005 Batch: 00017/00094 | Loss: 0.1743 | EWC Loss: 0.0201 | CE Loss: 0.1542\n",
      "Train Epoch: 005 Batch: 00018/00094 | Loss: 0.1515 | EWC Loss: 0.0203 | CE Loss: 0.1312\n",
      "Train Epoch: 005 Batch: 00019/00094 | Loss: 0.1895 | EWC Loss: 0.0203 | CE Loss: 0.1692\n",
      "Train Epoch: 005 Batch: 00020/00094 | Loss: 0.2467 | EWC Loss: 0.0206 | CE Loss: 0.2260\n",
      "Train Epoch: 005 Batch: 00021/00094 | Loss: 0.1825 | EWC Loss: 0.0209 | CE Loss: 0.1615\n",
      "Train Epoch: 005 Batch: 00022/00094 | Loss: 0.1348 | EWC Loss: 0.0209 | CE Loss: 0.1138\n",
      "Train Epoch: 005 Batch: 00023/00094 | Loss: 0.2466 | EWC Loss: 0.0211 | CE Loss: 0.2255\n",
      "Train Epoch: 005 Batch: 00024/00094 | Loss: 0.1192 | EWC Loss: 0.0207 | CE Loss: 0.0985\n",
      "Train Epoch: 005 Batch: 00025/00094 | Loss: 0.1602 | EWC Loss: 0.0207 | CE Loss: 0.1395\n",
      "Train Epoch: 005 Batch: 00026/00094 | Loss: 0.2094 | EWC Loss: 0.0207 | CE Loss: 0.1887\n",
      "Train Epoch: 005 Batch: 00027/00094 | Loss: 0.1820 | EWC Loss: 0.0206 | CE Loss: 0.1614\n",
      "Train Epoch: 005 Batch: 00028/00094 | Loss: 0.1263 | EWC Loss: 0.0206 | CE Loss: 0.1056\n",
      "Train Epoch: 005 Batch: 00029/00094 | Loss: 0.1643 | EWC Loss: 0.0208 | CE Loss: 0.1435\n",
      "Train Epoch: 005 Batch: 00030/00094 | Loss: 0.1669 | EWC Loss: 0.0207 | CE Loss: 0.1462\n",
      "Train Epoch: 005 Batch: 00031/00094 | Loss: 0.1475 | EWC Loss: 0.0208 | CE Loss: 0.1268\n",
      "Train Epoch: 005 Batch: 00032/00094 | Loss: 0.2717 | EWC Loss: 0.0207 | CE Loss: 0.2510\n",
      "Train Epoch: 005 Batch: 00033/00094 | Loss: 0.1482 | EWC Loss: 0.0207 | CE Loss: 0.1275\n",
      "Train Epoch: 005 Batch: 00034/00094 | Loss: 0.1831 | EWC Loss: 0.0206 | CE Loss: 0.1625\n",
      "Train Epoch: 005 Batch: 00035/00094 | Loss: 0.1677 | EWC Loss: 0.0205 | CE Loss: 0.1472\n",
      "Train Epoch: 005 Batch: 00036/00094 | Loss: 0.2108 | EWC Loss: 0.0206 | CE Loss: 0.1902\n",
      "Train Epoch: 005 Batch: 00037/00094 | Loss: 0.1457 | EWC Loss: 0.0205 | CE Loss: 0.1252\n",
      "Train Epoch: 005 Batch: 00038/00094 | Loss: 0.2791 | EWC Loss: 0.0205 | CE Loss: 0.2586\n",
      "Train Epoch: 005 Batch: 00039/00094 | Loss: 0.1746 | EWC Loss: 0.0209 | CE Loss: 0.1537\n",
      "Train Epoch: 005 Batch: 00040/00094 | Loss: 0.3312 | EWC Loss: 0.0214 | CE Loss: 0.3098\n",
      "Train Epoch: 005 Batch: 00041/00094 | Loss: 0.1393 | EWC Loss: 0.0214 | CE Loss: 0.1180\n",
      "Train Epoch: 005 Batch: 00042/00094 | Loss: 0.1194 | EWC Loss: 0.0213 | CE Loss: 0.0981\n",
      "Train Epoch: 005 Batch: 00043/00094 | Loss: 0.1999 | EWC Loss: 0.0212 | CE Loss: 0.1787\n",
      "Train Epoch: 005 Batch: 00044/00094 | Loss: 0.1886 | EWC Loss: 0.0207 | CE Loss: 0.1679\n",
      "Train Epoch: 005 Batch: 00045/00094 | Loss: 0.1705 | EWC Loss: 0.0206 | CE Loss: 0.1499\n",
      "Train Epoch: 005 Batch: 00046/00094 | Loss: 0.1650 | EWC Loss: 0.0205 | CE Loss: 0.1445\n",
      "Train Epoch: 005 Batch: 00047/00094 | Loss: 0.2688 | EWC Loss: 0.0208 | CE Loss: 0.2479\n",
      "Train Epoch: 005 Batch: 00048/00094 | Loss: 0.1693 | EWC Loss: 0.0213 | CE Loss: 0.1480\n",
      "Train Epoch: 005 Batch: 00049/00094 | Loss: 0.1780 | EWC Loss: 0.0214 | CE Loss: 0.1566\n",
      "Train Epoch: 005 Batch: 00050/00094 | Loss: 0.1370 | EWC Loss: 0.0218 | CE Loss: 0.1152\n",
      "Train Epoch: 005 Batch: 00051/00094 | Loss: 0.1720 | EWC Loss: 0.0219 | CE Loss: 0.1500\n",
      "Train Epoch: 005 Batch: 00052/00094 | Loss: 0.1940 | EWC Loss: 0.0217 | CE Loss: 0.1723\n",
      "Train Epoch: 005 Batch: 00053/00094 | Loss: 0.1983 | EWC Loss: 0.0216 | CE Loss: 0.1768\n",
      "Train Epoch: 005 Batch: 00054/00094 | Loss: 0.1634 | EWC Loss: 0.0214 | CE Loss: 0.1419\n",
      "Train Epoch: 005 Batch: 00055/00094 | Loss: 0.1929 | EWC Loss: 0.0215 | CE Loss: 0.1714\n",
      "Train Epoch: 005 Batch: 00056/00094 | Loss: 0.2296 | EWC Loss: 0.0215 | CE Loss: 0.2082\n",
      "Train Epoch: 005 Batch: 00057/00094 | Loss: 0.2339 | EWC Loss: 0.0217 | CE Loss: 0.2122\n",
      "Train Epoch: 005 Batch: 00058/00094 | Loss: 0.1197 | EWC Loss: 0.0216 | CE Loss: 0.0981\n",
      "Train Epoch: 005 Batch: 00059/00094 | Loss: 0.1560 | EWC Loss: 0.0213 | CE Loss: 0.1347\n",
      "Train Epoch: 005 Batch: 00060/00094 | Loss: 0.1227 | EWC Loss: 0.0212 | CE Loss: 0.1015\n",
      "Train Epoch: 005 Batch: 00061/00094 | Loss: 0.2431 | EWC Loss: 0.0211 | CE Loss: 0.2220\n",
      "Train Epoch: 005 Batch: 00062/00094 | Loss: 0.0943 | EWC Loss: 0.0211 | CE Loss: 0.0732\n",
      "Train Epoch: 005 Batch: 00063/00094 | Loss: 0.1980 | EWC Loss: 0.0211 | CE Loss: 0.1768\n",
      "Train Epoch: 005 Batch: 00064/00094 | Loss: 0.1316 | EWC Loss: 0.0211 | CE Loss: 0.1105\n",
      "Train Epoch: 005 Batch: 00065/00094 | Loss: 0.1655 | EWC Loss: 0.0210 | CE Loss: 0.1445\n",
      "Train Epoch: 005 Batch: 00066/00094 | Loss: 0.2020 | EWC Loss: 0.0210 | CE Loss: 0.1810\n",
      "Train Epoch: 005 Batch: 00067/00094 | Loss: 0.1175 | EWC Loss: 0.0213 | CE Loss: 0.0962\n",
      "Train Epoch: 005 Batch: 00068/00094 | Loss: 0.2076 | EWC Loss: 0.0213 | CE Loss: 0.1863\n",
      "Train Epoch: 005 Batch: 00069/00094 | Loss: 0.2251 | EWC Loss: 0.0212 | CE Loss: 0.2039\n",
      "Train Epoch: 005 Batch: 00070/00094 | Loss: 0.1555 | EWC Loss: 0.0212 | CE Loss: 0.1342\n",
      "Train Epoch: 005 Batch: 00071/00094 | Loss: 0.1136 | EWC Loss: 0.0211 | CE Loss: 0.0925\n",
      "Train Epoch: 005 Batch: 00072/00094 | Loss: 0.1690 | EWC Loss: 0.0207 | CE Loss: 0.1483\n",
      "Train Epoch: 005 Batch: 00073/00094 | Loss: 0.2298 | EWC Loss: 0.0204 | CE Loss: 0.2094\n",
      "Train Epoch: 005 Batch: 00074/00094 | Loss: 0.1198 | EWC Loss: 0.0205 | CE Loss: 0.0993\n",
      "Train Epoch: 005 Batch: 00075/00094 | Loss: 0.1235 | EWC Loss: 0.0203 | CE Loss: 0.1033\n",
      "Train Epoch: 005 Batch: 00076/00094 | Loss: 0.1709 | EWC Loss: 0.0203 | CE Loss: 0.1505\n",
      "Train Epoch: 005 Batch: 00077/00094 | Loss: 0.1901 | EWC Loss: 0.0206 | CE Loss: 0.1695\n",
      "Train Epoch: 005 Batch: 00078/00094 | Loss: 0.2084 | EWC Loss: 0.0207 | CE Loss: 0.1878\n",
      "Train Epoch: 005 Batch: 00079/00094 | Loss: 0.2025 | EWC Loss: 0.0208 | CE Loss: 0.1817\n",
      "Train Epoch: 005 Batch: 00080/00094 | Loss: 0.1737 | EWC Loss: 0.0205 | CE Loss: 0.1532\n",
      "Train Epoch: 005 Batch: 00081/00094 | Loss: 0.1757 | EWC Loss: 0.0202 | CE Loss: 0.1555\n",
      "Train Epoch: 005 Batch: 00082/00094 | Loss: 0.1897 | EWC Loss: 0.0203 | CE Loss: 0.1694\n",
      "Train Epoch: 005 Batch: 00083/00094 | Loss: 0.1692 | EWC Loss: 0.0203 | CE Loss: 0.1489\n",
      "Train Epoch: 005 Batch: 00084/00094 | Loss: 0.2221 | EWC Loss: 0.0204 | CE Loss: 0.2017\n",
      "Train Epoch: 005 Batch: 00085/00094 | Loss: 0.1579 | EWC Loss: 0.0203 | CE Loss: 0.1376\n",
      "Train Epoch: 005 Batch: 00086/00094 | Loss: 0.2801 | EWC Loss: 0.0203 | CE Loss: 0.2598\n",
      "Train Epoch: 005 Batch: 00087/00094 | Loss: 0.1759 | EWC Loss: 0.0206 | CE Loss: 0.1553\n",
      "Train Epoch: 005 Batch: 00088/00094 | Loss: 0.1970 | EWC Loss: 0.0207 | CE Loss: 0.1764\n",
      "Train Epoch: 005 Batch: 00089/00094 | Loss: 0.2166 | EWC Loss: 0.0210 | CE Loss: 0.1956\n",
      "Train Epoch: 005 Batch: 00090/00094 | Loss: 0.1258 | EWC Loss: 0.0213 | CE Loss: 0.1046\n",
      "Train Epoch: 005 Batch: 00091/00094 | Loss: 0.1598 | EWC Loss: 0.0211 | CE Loss: 0.1387\n",
      "Train Epoch: 005 Batch: 00092/00094 | Loss: 0.1633 | EWC Loss: 0.0209 | CE Loss: 0.1423\n",
      "Train Epoch: 005 Batch: 00093/00094 | Loss: 0.2505 | EWC Loss: 0.0211 | CE Loss: 0.2294\n",
      "Train Epoch: 005 Batch: 00094/00094 | Loss: 0.1915 | EWC Loss: 0.0215 | CE Loss: 0.1700\n",
      "Train Epoch: 004 |Acc 93.58333 | Loss: 0.18409 | Task Loss 0.16330 | EWC Loss: 0.02079\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1341 | acc:95.6500\n",
      "[VAL Acc] Target: 95.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.9736 | acc:50.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0764 | acc:61.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 61.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:0.9907 | acc:56.2977\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 56.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.0536 | acc:57.4843\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.48%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6561 | acc:69.9630\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5223 | acc:77.7821\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 77.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8594 | acc:58.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.25%\n",
      "[VAL Acc] Avg 65.93%\n",
      "\n",
      "\n",
      "---------- Starting epoch 5 ----------\n",
      "Train Epoch: 006 Batch: 00001/00094 | Loss: 0.1820 | EWC Loss: 0.0215 | CE Loss: 0.1605\n",
      "Train Epoch: 006 Batch: 00002/00094 | Loss: 0.2383 | EWC Loss: 0.0214 | CE Loss: 0.2169\n",
      "Train Epoch: 006 Batch: 00003/00094 | Loss: 0.1876 | EWC Loss: 0.0215 | CE Loss: 0.1660\n",
      "Train Epoch: 006 Batch: 00004/00094 | Loss: 0.1602 | EWC Loss: 0.0213 | CE Loss: 0.1389\n",
      "Train Epoch: 006 Batch: 00005/00094 | Loss: 0.1108 | EWC Loss: 0.0210 | CE Loss: 0.0898\n",
      "Train Epoch: 006 Batch: 00006/00094 | Loss: 0.1614 | EWC Loss: 0.0210 | CE Loss: 0.1405\n",
      "Train Epoch: 006 Batch: 00007/00094 | Loss: 0.1584 | EWC Loss: 0.0206 | CE Loss: 0.1378\n",
      "Train Epoch: 006 Batch: 00008/00094 | Loss: 0.1744 | EWC Loss: 0.0208 | CE Loss: 0.1536\n",
      "Train Epoch: 006 Batch: 00009/00094 | Loss: 0.2157 | EWC Loss: 0.0209 | CE Loss: 0.1949\n",
      "Train Epoch: 006 Batch: 00010/00094 | Loss: 0.1462 | EWC Loss: 0.0208 | CE Loss: 0.1254\n",
      "Train Epoch: 006 Batch: 00011/00094 | Loss: 0.2868 | EWC Loss: 0.0209 | CE Loss: 0.2659\n",
      "Train Epoch: 006 Batch: 00012/00094 | Loss: 0.2285 | EWC Loss: 0.0207 | CE Loss: 0.2078\n",
      "Train Epoch: 006 Batch: 00013/00094 | Loss: 0.1414 | EWC Loss: 0.0206 | CE Loss: 0.1208\n",
      "Train Epoch: 006 Batch: 00014/00094 | Loss: 0.1462 | EWC Loss: 0.0205 | CE Loss: 0.1257\n",
      "Train Epoch: 006 Batch: 00015/00094 | Loss: 0.1430 | EWC Loss: 0.0205 | CE Loss: 0.1225\n",
      "Train Epoch: 006 Batch: 00016/00094 | Loss: 0.1223 | EWC Loss: 0.0205 | CE Loss: 0.1018\n",
      "Train Epoch: 006 Batch: 00017/00094 | Loss: 0.1722 | EWC Loss: 0.0204 | CE Loss: 0.1518\n",
      "Train Epoch: 006 Batch: 00018/00094 | Loss: 0.1180 | EWC Loss: 0.0205 | CE Loss: 0.0975\n",
      "Train Epoch: 006 Batch: 00019/00094 | Loss: 0.1703 | EWC Loss: 0.0203 | CE Loss: 0.1500\n",
      "Train Epoch: 006 Batch: 00020/00094 | Loss: 0.1404 | EWC Loss: 0.0203 | CE Loss: 0.1201\n",
      "Train Epoch: 006 Batch: 00021/00094 | Loss: 0.2092 | EWC Loss: 0.0203 | CE Loss: 0.1889\n",
      "Train Epoch: 006 Batch: 00022/00094 | Loss: 0.1568 | EWC Loss: 0.0202 | CE Loss: 0.1365\n",
      "Train Epoch: 006 Batch: 00023/00094 | Loss: 0.1711 | EWC Loss: 0.0202 | CE Loss: 0.1509\n",
      "Train Epoch: 006 Batch: 00024/00094 | Loss: 0.1364 | EWC Loss: 0.0202 | CE Loss: 0.1162\n",
      "Train Epoch: 006 Batch: 00025/00094 | Loss: 0.1767 | EWC Loss: 0.0202 | CE Loss: 0.1566\n",
      "Train Epoch: 006 Batch: 00026/00094 | Loss: 0.2721 | EWC Loss: 0.0203 | CE Loss: 0.2518\n",
      "Train Epoch: 006 Batch: 00027/00094 | Loss: 0.2186 | EWC Loss: 0.0203 | CE Loss: 0.1983\n",
      "Train Epoch: 006 Batch: 00028/00094 | Loss: 0.2083 | EWC Loss: 0.0203 | CE Loss: 0.1880\n",
      "Train Epoch: 006 Batch: 00029/00094 | Loss: 0.2155 | EWC Loss: 0.0201 | CE Loss: 0.1954\n",
      "Train Epoch: 006 Batch: 00030/00094 | Loss: 0.1877 | EWC Loss: 0.0203 | CE Loss: 0.1674\n",
      "Train Epoch: 006 Batch: 00031/00094 | Loss: 0.1717 | EWC Loss: 0.0204 | CE Loss: 0.1513\n",
      "Train Epoch: 006 Batch: 00032/00094 | Loss: 0.1558 | EWC Loss: 0.0204 | CE Loss: 0.1355\n",
      "Train Epoch: 006 Batch: 00033/00094 | Loss: 0.1356 | EWC Loss: 0.0204 | CE Loss: 0.1152\n",
      "Train Epoch: 006 Batch: 00034/00094 | Loss: 0.1749 | EWC Loss: 0.0204 | CE Loss: 0.1545\n",
      "Train Epoch: 006 Batch: 00035/00094 | Loss: 0.2253 | EWC Loss: 0.0206 | CE Loss: 0.2047\n",
      "Train Epoch: 006 Batch: 00036/00094 | Loss: 0.1780 | EWC Loss: 0.0206 | CE Loss: 0.1574\n",
      "Train Epoch: 006 Batch: 00037/00094 | Loss: 0.1541 | EWC Loss: 0.0205 | CE Loss: 0.1335\n",
      "Train Epoch: 006 Batch: 00038/00094 | Loss: 0.2569 | EWC Loss: 0.0205 | CE Loss: 0.2364\n",
      "Train Epoch: 006 Batch: 00039/00094 | Loss: 0.1771 | EWC Loss: 0.0205 | CE Loss: 0.1566\n",
      "Train Epoch: 006 Batch: 00040/00094 | Loss: 0.1098 | EWC Loss: 0.0205 | CE Loss: 0.0893\n",
      "Train Epoch: 006 Batch: 00041/00094 | Loss: 0.1740 | EWC Loss: 0.0204 | CE Loss: 0.1535\n",
      "Train Epoch: 006 Batch: 00042/00094 | Loss: 0.2052 | EWC Loss: 0.0206 | CE Loss: 0.1846\n",
      "Train Epoch: 006 Batch: 00043/00094 | Loss: 0.1354 | EWC Loss: 0.0206 | CE Loss: 0.1148\n",
      "Train Epoch: 006 Batch: 00044/00094 | Loss: 0.1219 | EWC Loss: 0.0204 | CE Loss: 0.1014\n",
      "Train Epoch: 006 Batch: 00045/00094 | Loss: 0.1492 | EWC Loss: 0.0204 | CE Loss: 0.1288\n",
      "Train Epoch: 006 Batch: 00046/00094 | Loss: 0.1157 | EWC Loss: 0.0203 | CE Loss: 0.0954\n",
      "Train Epoch: 006 Batch: 00047/00094 | Loss: 0.1124 | EWC Loss: 0.0203 | CE Loss: 0.0921\n",
      "Train Epoch: 006 Batch: 00048/00094 | Loss: 0.1615 | EWC Loss: 0.0203 | CE Loss: 0.1412\n",
      "Train Epoch: 006 Batch: 00049/00094 | Loss: 0.2230 | EWC Loss: 0.0203 | CE Loss: 0.2027\n",
      "Train Epoch: 006 Batch: 00050/00094 | Loss: 0.1496 | EWC Loss: 0.0203 | CE Loss: 0.1293\n",
      "Train Epoch: 006 Batch: 00051/00094 | Loss: 0.1263 | EWC Loss: 0.0203 | CE Loss: 0.1060\n",
      "Train Epoch: 006 Batch: 00052/00094 | Loss: 0.1859 | EWC Loss: 0.0204 | CE Loss: 0.1655\n",
      "Train Epoch: 006 Batch: 00053/00094 | Loss: 0.1444 | EWC Loss: 0.0203 | CE Loss: 0.1241\n",
      "Train Epoch: 006 Batch: 00054/00094 | Loss: 0.2169 | EWC Loss: 0.0204 | CE Loss: 0.1965\n",
      "Train Epoch: 006 Batch: 00055/00094 | Loss: 0.1267 | EWC Loss: 0.0204 | CE Loss: 0.1063\n",
      "Train Epoch: 006 Batch: 00056/00094 | Loss: 0.1322 | EWC Loss: 0.0203 | CE Loss: 0.1119\n",
      "Train Epoch: 006 Batch: 00057/00094 | Loss: 0.1408 | EWC Loss: 0.0202 | CE Loss: 0.1206\n",
      "Train Epoch: 006 Batch: 00058/00094 | Loss: 0.1012 | EWC Loss: 0.0201 | CE Loss: 0.0811\n",
      "Train Epoch: 006 Batch: 00059/00094 | Loss: 0.1323 | EWC Loss: 0.0200 | CE Loss: 0.1123\n",
      "Train Epoch: 006 Batch: 00060/00094 | Loss: 0.1242 | EWC Loss: 0.0200 | CE Loss: 0.1041\n",
      "Train Epoch: 006 Batch: 00061/00094 | Loss: 0.2790 | EWC Loss: 0.0201 | CE Loss: 0.2589\n",
      "Train Epoch: 006 Batch: 00062/00094 | Loss: 0.1310 | EWC Loss: 0.0203 | CE Loss: 0.1108\n",
      "Train Epoch: 006 Batch: 00063/00094 | Loss: 0.1474 | EWC Loss: 0.0204 | CE Loss: 0.1270\n",
      "Train Epoch: 006 Batch: 00064/00094 | Loss: 0.1520 | EWC Loss: 0.0205 | CE Loss: 0.1315\n",
      "Train Epoch: 006 Batch: 00065/00094 | Loss: 0.1460 | EWC Loss: 0.0205 | CE Loss: 0.1255\n",
      "Train Epoch: 006 Batch: 00066/00094 | Loss: 0.1655 | EWC Loss: 0.0205 | CE Loss: 0.1451\n",
      "Train Epoch: 006 Batch: 00067/00094 | Loss: 0.1733 | EWC Loss: 0.0205 | CE Loss: 0.1528\n",
      "Train Epoch: 006 Batch: 00068/00094 | Loss: 0.1087 | EWC Loss: 0.0204 | CE Loss: 0.0882\n",
      "Train Epoch: 006 Batch: 00069/00094 | Loss: 0.1110 | EWC Loss: 0.0207 | CE Loss: 0.0903\n",
      "Train Epoch: 006 Batch: 00070/00094 | Loss: 0.1480 | EWC Loss: 0.0207 | CE Loss: 0.1273\n",
      "Train Epoch: 006 Batch: 00071/00094 | Loss: 0.1904 | EWC Loss: 0.0207 | CE Loss: 0.1697\n",
      "Train Epoch: 006 Batch: 00072/00094 | Loss: 0.1021 | EWC Loss: 0.0207 | CE Loss: 0.0814\n",
      "Train Epoch: 006 Batch: 00073/00094 | Loss: 0.1555 | EWC Loss: 0.0206 | CE Loss: 0.1349\n",
      "Train Epoch: 006 Batch: 00074/00094 | Loss: 0.1743 | EWC Loss: 0.0206 | CE Loss: 0.1537\n",
      "Train Epoch: 006 Batch: 00075/00094 | Loss: 0.2367 | EWC Loss: 0.0207 | CE Loss: 0.2159\n",
      "Train Epoch: 006 Batch: 00076/00094 | Loss: 0.1579 | EWC Loss: 0.0206 | CE Loss: 0.1373\n",
      "Train Epoch: 006 Batch: 00077/00094 | Loss: 0.1251 | EWC Loss: 0.0208 | CE Loss: 0.1043\n",
      "Train Epoch: 006 Batch: 00078/00094 | Loss: 0.1151 | EWC Loss: 0.0209 | CE Loss: 0.0943\n",
      "Train Epoch: 006 Batch: 00079/00094 | Loss: 0.1286 | EWC Loss: 0.0207 | CE Loss: 0.1079\n",
      "Train Epoch: 006 Batch: 00080/00094 | Loss: 0.0762 | EWC Loss: 0.0208 | CE Loss: 0.0553\n",
      "Train Epoch: 006 Batch: 00081/00094 | Loss: 0.1409 | EWC Loss: 0.0208 | CE Loss: 0.1201\n",
      "Train Epoch: 006 Batch: 00082/00094 | Loss: 0.1403 | EWC Loss: 0.0207 | CE Loss: 0.1196\n",
      "Train Epoch: 006 Batch: 00083/00094 | Loss: 0.2272 | EWC Loss: 0.0209 | CE Loss: 0.2063\n",
      "Train Epoch: 006 Batch: 00084/00094 | Loss: 0.2277 | EWC Loss: 0.0210 | CE Loss: 0.2067\n",
      "Train Epoch: 006 Batch: 00085/00094 | Loss: 0.1745 | EWC Loss: 0.0209 | CE Loss: 0.1536\n",
      "Train Epoch: 006 Batch: 00086/00094 | Loss: 0.1479 | EWC Loss: 0.0209 | CE Loss: 0.1270\n",
      "Train Epoch: 006 Batch: 00087/00094 | Loss: 0.1707 | EWC Loss: 0.0208 | CE Loss: 0.1499\n",
      "Train Epoch: 006 Batch: 00088/00094 | Loss: 0.1456 | EWC Loss: 0.0207 | CE Loss: 0.1249\n",
      "Train Epoch: 006 Batch: 00089/00094 | Loss: 0.1350 | EWC Loss: 0.0206 | CE Loss: 0.1145\n",
      "Train Epoch: 006 Batch: 00090/00094 | Loss: 0.1259 | EWC Loss: 0.0207 | CE Loss: 0.1052\n",
      "Train Epoch: 006 Batch: 00091/00094 | Loss: 0.2089 | EWC Loss: 0.0206 | CE Loss: 0.1882\n",
      "Train Epoch: 006 Batch: 00092/00094 | Loss: 0.2551 | EWC Loss: 0.0207 | CE Loss: 0.2344\n",
      "Train Epoch: 006 Batch: 00093/00094 | Loss: 0.1440 | EWC Loss: 0.0209 | CE Loss: 0.1231\n",
      "Train Epoch: 006 Batch: 00094/00094 | Loss: 0.0842 | EWC Loss: 0.0208 | CE Loss: 0.0634\n",
      "Train Epoch: 005 |Acc 94.56667 | Loss: 0.16411 | Task Loss 0.14355 | EWC Loss: 0.02055\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1367 | acc:95.3500\n",
      "[VAL Acc] Target: 95.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.9566 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0327 | acc:61.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 61.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1477 | acc:54.0076\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 54.01%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.0053 | acc:57.5235\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6833 | acc:70.0555\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5075 | acc:78.0564\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 78.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8544 | acc:59.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.88%\n",
      "[VAL Acc] Avg 65.84%\n",
      "\n",
      "\n",
      "---------- Starting epoch 6 ----------\n",
      "Train Epoch: 007 Batch: 00001/00094 | Loss: 0.1333 | EWC Loss: 0.0207 | CE Loss: 0.1126\n",
      "Train Epoch: 007 Batch: 00002/00094 | Loss: 0.1219 | EWC Loss: 0.0207 | CE Loss: 0.1012\n",
      "Train Epoch: 007 Batch: 00003/00094 | Loss: 0.2215 | EWC Loss: 0.0207 | CE Loss: 0.2008\n",
      "Train Epoch: 007 Batch: 00004/00094 | Loss: 0.2184 | EWC Loss: 0.0208 | CE Loss: 0.1976\n",
      "Train Epoch: 007 Batch: 00005/00094 | Loss: 0.1330 | EWC Loss: 0.0208 | CE Loss: 0.1122\n",
      "Train Epoch: 007 Batch: 00006/00094 | Loss: 0.1652 | EWC Loss: 0.0207 | CE Loss: 0.1445\n",
      "Train Epoch: 007 Batch: 00007/00094 | Loss: 0.1572 | EWC Loss: 0.0208 | CE Loss: 0.1364\n",
      "Train Epoch: 007 Batch: 00008/00094 | Loss: 0.2106 | EWC Loss: 0.0207 | CE Loss: 0.1899\n",
      "Train Epoch: 007 Batch: 00009/00094 | Loss: 0.1245 | EWC Loss: 0.0207 | CE Loss: 0.1038\n",
      "Train Epoch: 007 Batch: 00010/00094 | Loss: 0.1514 | EWC Loss: 0.0208 | CE Loss: 0.1307\n",
      "Train Epoch: 007 Batch: 00011/00094 | Loss: 0.1589 | EWC Loss: 0.0208 | CE Loss: 0.1382\n",
      "Train Epoch: 007 Batch: 00012/00094 | Loss: 0.2656 | EWC Loss: 0.0208 | CE Loss: 0.2448\n",
      "Train Epoch: 007 Batch: 00013/00094 | Loss: 0.1763 | EWC Loss: 0.0208 | CE Loss: 0.1555\n",
      "Train Epoch: 007 Batch: 00014/00094 | Loss: 0.1459 | EWC Loss: 0.0208 | CE Loss: 0.1251\n",
      "Train Epoch: 007 Batch: 00015/00094 | Loss: 0.1678 | EWC Loss: 0.0208 | CE Loss: 0.1470\n",
      "Train Epoch: 007 Batch: 00016/00094 | Loss: 0.1768 | EWC Loss: 0.0208 | CE Loss: 0.1560\n",
      "Train Epoch: 007 Batch: 00017/00094 | Loss: 0.1507 | EWC Loss: 0.0209 | CE Loss: 0.1299\n",
      "Train Epoch: 007 Batch: 00018/00094 | Loss: 0.1834 | EWC Loss: 0.0209 | CE Loss: 0.1625\n",
      "Train Epoch: 007 Batch: 00019/00094 | Loss: 0.1169 | EWC Loss: 0.0208 | CE Loss: 0.0961\n",
      "Train Epoch: 007 Batch: 00020/00094 | Loss: 0.1876 | EWC Loss: 0.0208 | CE Loss: 0.1668\n",
      "Train Epoch: 007 Batch: 00021/00094 | Loss: 0.1900 | EWC Loss: 0.0208 | CE Loss: 0.1691\n",
      "Train Epoch: 007 Batch: 00022/00094 | Loss: 0.1382 | EWC Loss: 0.0208 | CE Loss: 0.1174\n",
      "Train Epoch: 007 Batch: 00023/00094 | Loss: 0.1682 | EWC Loss: 0.0208 | CE Loss: 0.1474\n",
      "Train Epoch: 007 Batch: 00024/00094 | Loss: 0.1447 | EWC Loss: 0.0207 | CE Loss: 0.1240\n",
      "Train Epoch: 007 Batch: 00025/00094 | Loss: 0.1451 | EWC Loss: 0.0208 | CE Loss: 0.1242\n",
      "Train Epoch: 007 Batch: 00026/00094 | Loss: 0.1641 | EWC Loss: 0.0209 | CE Loss: 0.1432\n",
      "Train Epoch: 007 Batch: 00027/00094 | Loss: 0.1122 | EWC Loss: 0.0208 | CE Loss: 0.0914\n",
      "Train Epoch: 007 Batch: 00028/00094 | Loss: 0.2416 | EWC Loss: 0.0208 | CE Loss: 0.2208\n",
      "Train Epoch: 007 Batch: 00029/00094 | Loss: 0.2040 | EWC Loss: 0.0209 | CE Loss: 0.1831\n",
      "Train Epoch: 007 Batch: 00030/00094 | Loss: 0.1696 | EWC Loss: 0.0209 | CE Loss: 0.1487\n",
      "Train Epoch: 007 Batch: 00031/00094 | Loss: 0.1142 | EWC Loss: 0.0210 | CE Loss: 0.0932\n",
      "Train Epoch: 007 Batch: 00032/00094 | Loss: 0.2734 | EWC Loss: 0.0210 | CE Loss: 0.2523\n",
      "Train Epoch: 007 Batch: 00033/00094 | Loss: 0.1307 | EWC Loss: 0.0210 | CE Loss: 0.1097\n",
      "Train Epoch: 007 Batch: 00034/00094 | Loss: 0.1549 | EWC Loss: 0.0210 | CE Loss: 0.1339\n",
      "Train Epoch: 007 Batch: 00035/00094 | Loss: 0.1826 | EWC Loss: 0.0210 | CE Loss: 0.1616\n",
      "Train Epoch: 007 Batch: 00036/00094 | Loss: 0.1734 | EWC Loss: 0.0213 | CE Loss: 0.1521\n",
      "Train Epoch: 007 Batch: 00037/00094 | Loss: 0.1802 | EWC Loss: 0.0214 | CE Loss: 0.1588\n",
      "Train Epoch: 007 Batch: 00038/00094 | Loss: 0.1207 | EWC Loss: 0.0215 | CE Loss: 0.0993\n",
      "Train Epoch: 007 Batch: 00039/00094 | Loss: 0.1063 | EWC Loss: 0.0213 | CE Loss: 0.0850\n",
      "Train Epoch: 007 Batch: 00040/00094 | Loss: 0.2678 | EWC Loss: 0.0212 | CE Loss: 0.2466\n",
      "Train Epoch: 007 Batch: 00041/00094 | Loss: 0.1692 | EWC Loss: 0.0211 | CE Loss: 0.1481\n",
      "Train Epoch: 007 Batch: 00042/00094 | Loss: 0.1545 | EWC Loss: 0.0212 | CE Loss: 0.1333\n",
      "Train Epoch: 007 Batch: 00043/00094 | Loss: 0.2123 | EWC Loss: 0.0212 | CE Loss: 0.1911\n",
      "Train Epoch: 007 Batch: 00044/00094 | Loss: 0.2005 | EWC Loss: 0.0212 | CE Loss: 0.1793\n",
      "Train Epoch: 007 Batch: 00045/00094 | Loss: 0.1673 | EWC Loss: 0.0211 | CE Loss: 0.1463\n",
      "Train Epoch: 007 Batch: 00046/00094 | Loss: 0.0765 | EWC Loss: 0.0210 | CE Loss: 0.0556\n",
      "Train Epoch: 007 Batch: 00047/00094 | Loss: 0.3018 | EWC Loss: 0.0209 | CE Loss: 0.2808\n",
      "Train Epoch: 007 Batch: 00048/00094 | Loss: 0.1439 | EWC Loss: 0.0209 | CE Loss: 0.1231\n",
      "Train Epoch: 007 Batch: 00049/00094 | Loss: 0.1673 | EWC Loss: 0.0210 | CE Loss: 0.1463\n",
      "Train Epoch: 007 Batch: 00050/00094 | Loss: 0.1315 | EWC Loss: 0.0210 | CE Loss: 0.1106\n",
      "Train Epoch: 007 Batch: 00051/00094 | Loss: 0.1979 | EWC Loss: 0.0209 | CE Loss: 0.1770\n",
      "Train Epoch: 007 Batch: 00052/00094 | Loss: 0.1636 | EWC Loss: 0.0209 | CE Loss: 0.1427\n",
      "Train Epoch: 007 Batch: 00053/00094 | Loss: 0.1940 | EWC Loss: 0.0210 | CE Loss: 0.1731\n",
      "Train Epoch: 007 Batch: 00054/00094 | Loss: 0.2090 | EWC Loss: 0.0210 | CE Loss: 0.1880\n",
      "Train Epoch: 007 Batch: 00055/00094 | Loss: 0.1168 | EWC Loss: 0.0209 | CE Loss: 0.0960\n",
      "Train Epoch: 007 Batch: 00056/00094 | Loss: 0.1712 | EWC Loss: 0.0208 | CE Loss: 0.1504\n",
      "Train Epoch: 007 Batch: 00057/00094 | Loss: 0.1183 | EWC Loss: 0.0208 | CE Loss: 0.0974\n",
      "Train Epoch: 007 Batch: 00058/00094 | Loss: 0.3006 | EWC Loss: 0.0209 | CE Loss: 0.2798\n",
      "Train Epoch: 007 Batch: 00059/00094 | Loss: 0.1508 | EWC Loss: 0.0209 | CE Loss: 0.1299\n",
      "Train Epoch: 007 Batch: 00060/00094 | Loss: 0.0915 | EWC Loss: 0.0210 | CE Loss: 0.0705\n",
      "Train Epoch: 007 Batch: 00061/00094 | Loss: 0.1931 | EWC Loss: 0.0209 | CE Loss: 0.1722\n",
      "Train Epoch: 007 Batch: 00062/00094 | Loss: 0.2680 | EWC Loss: 0.0209 | CE Loss: 0.2471\n",
      "Train Epoch: 007 Batch: 00063/00094 | Loss: 0.1509 | EWC Loss: 0.0212 | CE Loss: 0.1297\n",
      "Train Epoch: 007 Batch: 00064/00094 | Loss: 0.1475 | EWC Loss: 0.0213 | CE Loss: 0.1262\n",
      "Train Epoch: 007 Batch: 00065/00094 | Loss: 0.1140 | EWC Loss: 0.0212 | CE Loss: 0.0928\n",
      "Train Epoch: 007 Batch: 00066/00094 | Loss: 0.2053 | EWC Loss: 0.0212 | CE Loss: 0.1841\n",
      "Train Epoch: 007 Batch: 00067/00094 | Loss: 0.1405 | EWC Loss: 0.0212 | CE Loss: 0.1192\n",
      "Train Epoch: 007 Batch: 00068/00094 | Loss: 0.1363 | EWC Loss: 0.0213 | CE Loss: 0.1150\n",
      "Train Epoch: 007 Batch: 00069/00094 | Loss: 0.2532 | EWC Loss: 0.0213 | CE Loss: 0.2319\n",
      "Train Epoch: 007 Batch: 00070/00094 | Loss: 0.1440 | EWC Loss: 0.0213 | CE Loss: 0.1227\n",
      "Train Epoch: 007 Batch: 00071/00094 | Loss: 0.1654 | EWC Loss: 0.0213 | CE Loss: 0.1441\n",
      "Train Epoch: 007 Batch: 00072/00094 | Loss: 0.1740 | EWC Loss: 0.0215 | CE Loss: 0.1525\n",
      "Train Epoch: 007 Batch: 00073/00094 | Loss: 0.1582 | EWC Loss: 0.0215 | CE Loss: 0.1367\n",
      "Train Epoch: 007 Batch: 00074/00094 | Loss: 0.1658 | EWC Loss: 0.0215 | CE Loss: 0.1443\n",
      "Train Epoch: 007 Batch: 00075/00094 | Loss: 0.1474 | EWC Loss: 0.0215 | CE Loss: 0.1258\n",
      "Train Epoch: 007 Batch: 00076/00094 | Loss: 0.1372 | EWC Loss: 0.0215 | CE Loss: 0.1157\n",
      "Train Epoch: 007 Batch: 00077/00094 | Loss: 0.1758 | EWC Loss: 0.0216 | CE Loss: 0.1542\n",
      "Train Epoch: 007 Batch: 00078/00094 | Loss: 0.1444 | EWC Loss: 0.0217 | CE Loss: 0.1228\n",
      "Train Epoch: 007 Batch: 00079/00094 | Loss: 0.1582 | EWC Loss: 0.0218 | CE Loss: 0.1363\n",
      "Train Epoch: 007 Batch: 00080/00094 | Loss: 0.1480 | EWC Loss: 0.0217 | CE Loss: 0.1263\n",
      "Train Epoch: 007 Batch: 00081/00094 | Loss: 0.2126 | EWC Loss: 0.0218 | CE Loss: 0.1908\n",
      "Train Epoch: 007 Batch: 00082/00094 | Loss: 0.1257 | EWC Loss: 0.0218 | CE Loss: 0.1039\n",
      "Train Epoch: 007 Batch: 00083/00094 | Loss: 0.0885 | EWC Loss: 0.0217 | CE Loss: 0.0668\n",
      "Train Epoch: 007 Batch: 00084/00094 | Loss: 0.2076 | EWC Loss: 0.0217 | CE Loss: 0.1859\n",
      "Train Epoch: 007 Batch: 00085/00094 | Loss: 0.1273 | EWC Loss: 0.0216 | CE Loss: 0.1056\n",
      "Train Epoch: 007 Batch: 00086/00094 | Loss: 0.1437 | EWC Loss: 0.0216 | CE Loss: 0.1221\n",
      "Train Epoch: 007 Batch: 00087/00094 | Loss: 0.1424 | EWC Loss: 0.0216 | CE Loss: 0.1208\n",
      "Train Epoch: 007 Batch: 00088/00094 | Loss: 0.1057 | EWC Loss: 0.0217 | CE Loss: 0.0840\n",
      "Train Epoch: 007 Batch: 00089/00094 | Loss: 0.1078 | EWC Loss: 0.0218 | CE Loss: 0.0861\n",
      "Train Epoch: 007 Batch: 00090/00094 | Loss: 0.0912 | EWC Loss: 0.0218 | CE Loss: 0.0694\n",
      "Train Epoch: 007 Batch: 00091/00094 | Loss: 0.1332 | EWC Loss: 0.0217 | CE Loss: 0.1116\n",
      "Train Epoch: 007 Batch: 00092/00094 | Loss: 0.1228 | EWC Loss: 0.0216 | CE Loss: 0.1012\n",
      "Train Epoch: 007 Batch: 00093/00094 | Loss: 0.1282 | EWC Loss: 0.0216 | CE Loss: 0.1067\n",
      "Train Epoch: 007 Batch: 00094/00094 | Loss: 0.1690 | EWC Loss: 0.0215 | CE Loss: 0.1475\n",
      "Train Epoch: 006 |Acc 94.78333 | Loss: 0.16405 | Task Loss 0.14292 | EWC Loss: 0.02113\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1281 | acc:95.5000\n",
      "[VAL Acc] Target: 95.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.0927 | acc:50.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1224 | acc:60.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2159 | acc:54.3893\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 54.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1213 | acc:56.9357\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6243 | acc:71.5342\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5934 | acc:75.2351\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9043 | acc:58.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.38%\n",
      "[VAL Acc] Avg 65.33%\n",
      "\n",
      "\n",
      "---------- Starting epoch 7 ----------\n",
      "Train Epoch: 008 Batch: 00001/00094 | Loss: 0.1865 | EWC Loss: 0.0217 | CE Loss: 0.1648\n",
      "Train Epoch: 008 Batch: 00002/00094 | Loss: 0.1109 | EWC Loss: 0.0216 | CE Loss: 0.0892\n",
      "Train Epoch: 008 Batch: 00003/00094 | Loss: 0.1296 | EWC Loss: 0.0216 | CE Loss: 0.1080\n",
      "Train Epoch: 008 Batch: 00004/00094 | Loss: 0.1801 | EWC Loss: 0.0216 | CE Loss: 0.1585\n",
      "Train Epoch: 008 Batch: 00005/00094 | Loss: 0.1527 | EWC Loss: 0.0216 | CE Loss: 0.1311\n",
      "Train Epoch: 008 Batch: 00006/00094 | Loss: 0.1374 | EWC Loss: 0.0216 | CE Loss: 0.1158\n",
      "Train Epoch: 008 Batch: 00007/00094 | Loss: 0.1471 | EWC Loss: 0.0217 | CE Loss: 0.1254\n",
      "Train Epoch: 008 Batch: 00008/00094 | Loss: 0.1923 | EWC Loss: 0.0216 | CE Loss: 0.1707\n",
      "Train Epoch: 008 Batch: 00009/00094 | Loss: 0.0782 | EWC Loss: 0.0219 | CE Loss: 0.0563\n",
      "Train Epoch: 008 Batch: 00010/00094 | Loss: 0.1672 | EWC Loss: 0.0219 | CE Loss: 0.1453\n",
      "Train Epoch: 008 Batch: 00011/00094 | Loss: 0.2321 | EWC Loss: 0.0219 | CE Loss: 0.2103\n",
      "Train Epoch: 008 Batch: 00012/00094 | Loss: 0.2267 | EWC Loss: 0.0219 | CE Loss: 0.2048\n",
      "Train Epoch: 008 Batch: 00013/00094 | Loss: 0.1994 | EWC Loss: 0.0219 | CE Loss: 0.1775\n",
      "Train Epoch: 008 Batch: 00014/00094 | Loss: 0.2337 | EWC Loss: 0.0220 | CE Loss: 0.2117\n",
      "Train Epoch: 008 Batch: 00015/00094 | Loss: 0.2207 | EWC Loss: 0.0220 | CE Loss: 0.1987\n",
      "Train Epoch: 008 Batch: 00016/00094 | Loss: 0.1922 | EWC Loss: 0.0219 | CE Loss: 0.1703\n",
      "Train Epoch: 008 Batch: 00017/00094 | Loss: 0.1315 | EWC Loss: 0.0219 | CE Loss: 0.1096\n",
      "Train Epoch: 008 Batch: 00018/00094 | Loss: 0.1881 | EWC Loss: 0.0219 | CE Loss: 0.1661\n",
      "Train Epoch: 008 Batch: 00019/00094 | Loss: 0.1120 | EWC Loss: 0.0219 | CE Loss: 0.0901\n",
      "Train Epoch: 008 Batch: 00020/00094 | Loss: 0.1140 | EWC Loss: 0.0219 | CE Loss: 0.0921\n",
      "Train Epoch: 008 Batch: 00021/00094 | Loss: 0.1496 | EWC Loss: 0.0219 | CE Loss: 0.1277\n",
      "Train Epoch: 008 Batch: 00022/00094 | Loss: 0.2043 | EWC Loss: 0.0219 | CE Loss: 0.1824\n",
      "Train Epoch: 008 Batch: 00023/00094 | Loss: 0.1619 | EWC Loss: 0.0219 | CE Loss: 0.1400\n",
      "Train Epoch: 008 Batch: 00024/00094 | Loss: 0.1278 | EWC Loss: 0.0218 | CE Loss: 0.1060\n",
      "Train Epoch: 008 Batch: 00025/00094 | Loss: 0.2237 | EWC Loss: 0.0218 | CE Loss: 0.2019\n",
      "Train Epoch: 008 Batch: 00026/00094 | Loss: 0.1941 | EWC Loss: 0.0218 | CE Loss: 0.1722\n",
      "Train Epoch: 008 Batch: 00027/00094 | Loss: 0.1539 | EWC Loss: 0.0218 | CE Loss: 0.1321\n",
      "Train Epoch: 008 Batch: 00028/00094 | Loss: 0.1045 | EWC Loss: 0.0217 | CE Loss: 0.0827\n",
      "Train Epoch: 008 Batch: 00029/00094 | Loss: 0.1370 | EWC Loss: 0.0218 | CE Loss: 0.1152\n",
      "Train Epoch: 008 Batch: 00030/00094 | Loss: 0.0940 | EWC Loss: 0.0216 | CE Loss: 0.0724\n",
      "Train Epoch: 008 Batch: 00031/00094 | Loss: 0.1068 | EWC Loss: 0.0215 | CE Loss: 0.0853\n",
      "Train Epoch: 008 Batch: 00032/00094 | Loss: 0.2341 | EWC Loss: 0.0215 | CE Loss: 0.2126\n",
      "Train Epoch: 008 Batch: 00033/00094 | Loss: 0.1043 | EWC Loss: 0.0215 | CE Loss: 0.0828\n",
      "Train Epoch: 008 Batch: 00034/00094 | Loss: 0.1498 | EWC Loss: 0.0214 | CE Loss: 0.1284\n",
      "Train Epoch: 008 Batch: 00035/00094 | Loss: 0.2470 | EWC Loss: 0.0214 | CE Loss: 0.2256\n",
      "Train Epoch: 008 Batch: 00036/00094 | Loss: 0.1293 | EWC Loss: 0.0214 | CE Loss: 0.1078\n",
      "Train Epoch: 008 Batch: 00037/00094 | Loss: 0.2638 | EWC Loss: 0.0215 | CE Loss: 0.2424\n",
      "Train Epoch: 008 Batch: 00038/00094 | Loss: 0.2129 | EWC Loss: 0.0215 | CE Loss: 0.1914\n",
      "Train Epoch: 008 Batch: 00039/00094 | Loss: 0.1275 | EWC Loss: 0.0215 | CE Loss: 0.1060\n",
      "Train Epoch: 008 Batch: 00040/00094 | Loss: 0.1121 | EWC Loss: 0.0215 | CE Loss: 0.0906\n",
      "Train Epoch: 008 Batch: 00041/00094 | Loss: 0.1137 | EWC Loss: 0.0215 | CE Loss: 0.0923\n",
      "Train Epoch: 008 Batch: 00042/00094 | Loss: 0.1709 | EWC Loss: 0.0214 | CE Loss: 0.1495\n",
      "Train Epoch: 008 Batch: 00043/00094 | Loss: 0.2120 | EWC Loss: 0.0213 | CE Loss: 0.1907\n",
      "Train Epoch: 008 Batch: 00044/00094 | Loss: 0.1336 | EWC Loss: 0.0214 | CE Loss: 0.1122\n",
      "Train Epoch: 008 Batch: 00045/00094 | Loss: 0.1395 | EWC Loss: 0.0215 | CE Loss: 0.1180\n",
      "Train Epoch: 008 Batch: 00046/00094 | Loss: 0.1916 | EWC Loss: 0.0215 | CE Loss: 0.1700\n",
      "Train Epoch: 008 Batch: 00047/00094 | Loss: 0.1683 | EWC Loss: 0.0216 | CE Loss: 0.1466\n",
      "Train Epoch: 008 Batch: 00048/00094 | Loss: 0.0969 | EWC Loss: 0.0215 | CE Loss: 0.0754\n",
      "Train Epoch: 008 Batch: 00049/00094 | Loss: 0.0962 | EWC Loss: 0.0215 | CE Loss: 0.0747\n",
      "Train Epoch: 008 Batch: 00050/00094 | Loss: 0.2123 | EWC Loss: 0.0216 | CE Loss: 0.1907\n",
      "Train Epoch: 008 Batch: 00051/00094 | Loss: 0.1001 | EWC Loss: 0.0217 | CE Loss: 0.0784\n",
      "Train Epoch: 008 Batch: 00052/00094 | Loss: 0.1547 | EWC Loss: 0.0217 | CE Loss: 0.1331\n",
      "Train Epoch: 008 Batch: 00053/00094 | Loss: 0.1490 | EWC Loss: 0.0216 | CE Loss: 0.1274\n",
      "Train Epoch: 008 Batch: 00054/00094 | Loss: 0.2151 | EWC Loss: 0.0216 | CE Loss: 0.1935\n",
      "Train Epoch: 008 Batch: 00055/00094 | Loss: 0.2127 | EWC Loss: 0.0216 | CE Loss: 0.1912\n",
      "Train Epoch: 008 Batch: 00056/00094 | Loss: 0.1273 | EWC Loss: 0.0215 | CE Loss: 0.1058\n",
      "Train Epoch: 008 Batch: 00057/00094 | Loss: 0.0822 | EWC Loss: 0.0214 | CE Loss: 0.0608\n",
      "Train Epoch: 008 Batch: 00058/00094 | Loss: 0.1798 | EWC Loss: 0.0214 | CE Loss: 0.1584\n",
      "Train Epoch: 008 Batch: 00059/00094 | Loss: 0.0919 | EWC Loss: 0.0214 | CE Loss: 0.0705\n",
      "Train Epoch: 008 Batch: 00060/00094 | Loss: 0.1445 | EWC Loss: 0.0214 | CE Loss: 0.1232\n",
      "Train Epoch: 008 Batch: 00061/00094 | Loss: 0.1655 | EWC Loss: 0.0214 | CE Loss: 0.1441\n",
      "Train Epoch: 008 Batch: 00062/00094 | Loss: 0.1657 | EWC Loss: 0.0214 | CE Loss: 0.1443\n",
      "Train Epoch: 008 Batch: 00063/00094 | Loss: 0.1609 | EWC Loss: 0.0213 | CE Loss: 0.1396\n",
      "Train Epoch: 008 Batch: 00064/00094 | Loss: 0.1902 | EWC Loss: 0.0213 | CE Loss: 0.1689\n",
      "Train Epoch: 008 Batch: 00065/00094 | Loss: 0.2061 | EWC Loss: 0.0212 | CE Loss: 0.1849\n",
      "Train Epoch: 008 Batch: 00066/00094 | Loss: 0.2021 | EWC Loss: 0.0213 | CE Loss: 0.1807\n",
      "Train Epoch: 008 Batch: 00067/00094 | Loss: 0.1692 | EWC Loss: 0.0214 | CE Loss: 0.1478\n",
      "Train Epoch: 008 Batch: 00068/00094 | Loss: 0.1742 | EWC Loss: 0.0216 | CE Loss: 0.1527\n",
      "Train Epoch: 008 Batch: 00069/00094 | Loss: 0.1443 | EWC Loss: 0.0215 | CE Loss: 0.1228\n",
      "Train Epoch: 008 Batch: 00070/00094 | Loss: 0.0995 | EWC Loss: 0.0214 | CE Loss: 0.0781\n",
      "Train Epoch: 008 Batch: 00071/00094 | Loss: 0.2683 | EWC Loss: 0.0214 | CE Loss: 0.2469\n",
      "Train Epoch: 008 Batch: 00072/00094 | Loss: 0.1600 | EWC Loss: 0.0213 | CE Loss: 0.1387\n",
      "Train Epoch: 008 Batch: 00073/00094 | Loss: 0.1715 | EWC Loss: 0.0213 | CE Loss: 0.1502\n",
      "Train Epoch: 008 Batch: 00074/00094 | Loss: 0.1667 | EWC Loss: 0.0214 | CE Loss: 0.1453\n",
      "Train Epoch: 008 Batch: 00075/00094 | Loss: 0.1169 | EWC Loss: 0.0213 | CE Loss: 0.0956\n",
      "Train Epoch: 008 Batch: 00076/00094 | Loss: 0.2095 | EWC Loss: 0.0213 | CE Loss: 0.1881\n",
      "Train Epoch: 008 Batch: 00077/00094 | Loss: 0.1820 | EWC Loss: 0.0214 | CE Loss: 0.1606\n",
      "Train Epoch: 008 Batch: 00078/00094 | Loss: 0.1944 | EWC Loss: 0.0213 | CE Loss: 0.1731\n",
      "Train Epoch: 008 Batch: 00079/00094 | Loss: 0.1668 | EWC Loss: 0.0213 | CE Loss: 0.1455\n",
      "Train Epoch: 008 Batch: 00080/00094 | Loss: 0.1691 | EWC Loss: 0.0213 | CE Loss: 0.1478\n",
      "Train Epoch: 008 Batch: 00081/00094 | Loss: 0.1955 | EWC Loss: 0.0213 | CE Loss: 0.1743\n",
      "Train Epoch: 008 Batch: 00082/00094 | Loss: 0.1774 | EWC Loss: 0.0213 | CE Loss: 0.1561\n",
      "Train Epoch: 008 Batch: 00083/00094 | Loss: 0.0778 | EWC Loss: 0.0213 | CE Loss: 0.0565\n",
      "Train Epoch: 008 Batch: 00084/00094 | Loss: 0.1540 | EWC Loss: 0.0213 | CE Loss: 0.1326\n",
      "Train Epoch: 008 Batch: 00085/00094 | Loss: 0.1236 | EWC Loss: 0.0214 | CE Loss: 0.1022\n",
      "Train Epoch: 008 Batch: 00086/00094 | Loss: 0.1397 | EWC Loss: 0.0213 | CE Loss: 0.1183\n",
      "Train Epoch: 008 Batch: 00087/00094 | Loss: 0.0969 | EWC Loss: 0.0214 | CE Loss: 0.0755\n",
      "Train Epoch: 008 Batch: 00088/00094 | Loss: 0.2076 | EWC Loss: 0.0214 | CE Loss: 0.1862\n",
      "Train Epoch: 008 Batch: 00089/00094 | Loss: 0.1986 | EWC Loss: 0.0213 | CE Loss: 0.1773\n",
      "Train Epoch: 008 Batch: 00090/00094 | Loss: 0.1480 | EWC Loss: 0.0214 | CE Loss: 0.1267\n",
      "Train Epoch: 008 Batch: 00091/00094 | Loss: 0.1975 | EWC Loss: 0.0213 | CE Loss: 0.1762\n",
      "Train Epoch: 008 Batch: 00092/00094 | Loss: 0.1790 | EWC Loss: 0.0214 | CE Loss: 0.1577\n",
      "Train Epoch: 008 Batch: 00093/00094 | Loss: 0.2251 | EWC Loss: 0.0214 | CE Loss: 0.2037\n",
      "Train Epoch: 008 Batch: 00094/00094 | Loss: 0.2273 | EWC Loss: 0.0216 | CE Loss: 0.2058\n",
      "Train Epoch: 007 |Acc 94.78333 | Loss: 0.16377 | Task Loss 0.14222 | EWC Loss: 0.02154\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1209 | acc:95.7000\n",
      "[VAL Acc] Target: 95.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.1023 | acc:51.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1303 | acc:61.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 61.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1433 | acc:54.9618\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 54.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2011 | acc:55.2508\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6754 | acc:70.7024\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5241 | acc:78.0956\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 78.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8983 | acc:59.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.50%\n",
      "[VAL Acc] Avg 65.87%\n",
      "\n",
      "\n",
      "---------- Starting epoch 8 ----------\n",
      "Train Epoch: 009 Batch: 00001/00094 | Loss: 0.1173 | EWC Loss: 0.0216 | CE Loss: 0.0957\n",
      "Train Epoch: 009 Batch: 00002/00094 | Loss: 0.1980 | EWC Loss: 0.0216 | CE Loss: 0.1764\n",
      "Train Epoch: 009 Batch: 00003/00094 | Loss: 0.1437 | EWC Loss: 0.0216 | CE Loss: 0.1221\n",
      "Train Epoch: 009 Batch: 00004/00094 | Loss: 0.1655 | EWC Loss: 0.0216 | CE Loss: 0.1439\n",
      "Train Epoch: 009 Batch: 00005/00094 | Loss: 0.2832 | EWC Loss: 0.0216 | CE Loss: 0.2617\n",
      "Train Epoch: 009 Batch: 00006/00094 | Loss: 0.2489 | EWC Loss: 0.0215 | CE Loss: 0.2274\n",
      "Train Epoch: 009 Batch: 00007/00094 | Loss: 0.1134 | EWC Loss: 0.0215 | CE Loss: 0.0919\n",
      "Train Epoch: 009 Batch: 00008/00094 | Loss: 0.1107 | EWC Loss: 0.0216 | CE Loss: 0.0892\n",
      "Train Epoch: 009 Batch: 00009/00094 | Loss: 0.1447 | EWC Loss: 0.0216 | CE Loss: 0.1232\n",
      "Train Epoch: 009 Batch: 00010/00094 | Loss: 0.1339 | EWC Loss: 0.0216 | CE Loss: 0.1123\n",
      "Train Epoch: 009 Batch: 00011/00094 | Loss: 0.1145 | EWC Loss: 0.0215 | CE Loss: 0.0929\n",
      "Train Epoch: 009 Batch: 00012/00094 | Loss: 0.2313 | EWC Loss: 0.0215 | CE Loss: 0.2098\n",
      "Train Epoch: 009 Batch: 00013/00094 | Loss: 0.1740 | EWC Loss: 0.0215 | CE Loss: 0.1525\n",
      "Train Epoch: 009 Batch: 00014/00094 | Loss: 0.0548 | EWC Loss: 0.0215 | CE Loss: 0.0333\n",
      "Train Epoch: 009 Batch: 00015/00094 | Loss: 0.1274 | EWC Loss: 0.0215 | CE Loss: 0.1059\n",
      "Train Epoch: 009 Batch: 00016/00094 | Loss: 0.1457 | EWC Loss: 0.0215 | CE Loss: 0.1241\n",
      "Train Epoch: 009 Batch: 00017/00094 | Loss: 0.2370 | EWC Loss: 0.0215 | CE Loss: 0.2155\n",
      "Train Epoch: 009 Batch: 00018/00094 | Loss: 0.0848 | EWC Loss: 0.0215 | CE Loss: 0.0633\n",
      "Train Epoch: 009 Batch: 00019/00094 | Loss: 0.1463 | EWC Loss: 0.0214 | CE Loss: 0.1249\n",
      "Train Epoch: 009 Batch: 00020/00094 | Loss: 0.1745 | EWC Loss: 0.0214 | CE Loss: 0.1531\n",
      "Train Epoch: 009 Batch: 00021/00094 | Loss: 0.2589 | EWC Loss: 0.0214 | CE Loss: 0.2375\n",
      "Train Epoch: 009 Batch: 00022/00094 | Loss: 0.1123 | EWC Loss: 0.0214 | CE Loss: 0.0909\n",
      "Train Epoch: 009 Batch: 00023/00094 | Loss: 0.2012 | EWC Loss: 0.0214 | CE Loss: 0.1798\n",
      "Train Epoch: 009 Batch: 00024/00094 | Loss: 0.1507 | EWC Loss: 0.0215 | CE Loss: 0.1292\n",
      "Train Epoch: 009 Batch: 00025/00094 | Loss: 0.1008 | EWC Loss: 0.0215 | CE Loss: 0.0793\n",
      "Train Epoch: 009 Batch: 00026/00094 | Loss: 0.1549 | EWC Loss: 0.0215 | CE Loss: 0.1335\n",
      "Train Epoch: 009 Batch: 00027/00094 | Loss: 0.0966 | EWC Loss: 0.0215 | CE Loss: 0.0752\n",
      "Train Epoch: 009 Batch: 00028/00094 | Loss: 0.1558 | EWC Loss: 0.0215 | CE Loss: 0.1343\n",
      "Train Epoch: 009 Batch: 00029/00094 | Loss: 0.1931 | EWC Loss: 0.0215 | CE Loss: 0.1716\n",
      "Train Epoch: 009 Batch: 00030/00094 | Loss: 0.1621 | EWC Loss: 0.0215 | CE Loss: 0.1406\n",
      "Train Epoch: 009 Batch: 00031/00094 | Loss: 0.1781 | EWC Loss: 0.0215 | CE Loss: 0.1566\n",
      "Train Epoch: 009 Batch: 00032/00094 | Loss: 0.1585 | EWC Loss: 0.0215 | CE Loss: 0.1370\n",
      "Train Epoch: 009 Batch: 00033/00094 | Loss: 0.1422 | EWC Loss: 0.0214 | CE Loss: 0.1208\n",
      "Train Epoch: 009 Batch: 00034/00094 | Loss: 0.1919 | EWC Loss: 0.0213 | CE Loss: 0.1706\n",
      "Train Epoch: 009 Batch: 00035/00094 | Loss: 0.1653 | EWC Loss: 0.0213 | CE Loss: 0.1439\n",
      "Train Epoch: 009 Batch: 00036/00094 | Loss: 0.1210 | EWC Loss: 0.0213 | CE Loss: 0.0997\n",
      "Train Epoch: 009 Batch: 00037/00094 | Loss: 0.1829 | EWC Loss: 0.0214 | CE Loss: 0.1616\n",
      "Train Epoch: 009 Batch: 00038/00094 | Loss: 0.1474 | EWC Loss: 0.0213 | CE Loss: 0.1261\n",
      "Train Epoch: 009 Batch: 00039/00094 | Loss: 0.1087 | EWC Loss: 0.0213 | CE Loss: 0.0874\n",
      "Train Epoch: 009 Batch: 00040/00094 | Loss: 0.0971 | EWC Loss: 0.0213 | CE Loss: 0.0758\n",
      "Train Epoch: 009 Batch: 00041/00094 | Loss: 0.1150 | EWC Loss: 0.0213 | CE Loss: 0.0937\n",
      "Train Epoch: 009 Batch: 00042/00094 | Loss: 0.1196 | EWC Loss: 0.0213 | CE Loss: 0.0983\n",
      "Train Epoch: 009 Batch: 00043/00094 | Loss: 0.1476 | EWC Loss: 0.0213 | CE Loss: 0.1264\n",
      "Train Epoch: 009 Batch: 00044/00094 | Loss: 0.1111 | EWC Loss: 0.0213 | CE Loss: 0.0899\n",
      "Train Epoch: 009 Batch: 00045/00094 | Loss: 0.2355 | EWC Loss: 0.0213 | CE Loss: 0.2142\n",
      "Train Epoch: 009 Batch: 00046/00094 | Loss: 0.1301 | EWC Loss: 0.0213 | CE Loss: 0.1088\n",
      "Train Epoch: 009 Batch: 00047/00094 | Loss: 0.1612 | EWC Loss: 0.0213 | CE Loss: 0.1399\n",
      "Train Epoch: 009 Batch: 00048/00094 | Loss: 0.2267 | EWC Loss: 0.0213 | CE Loss: 0.2055\n",
      "Train Epoch: 009 Batch: 00049/00094 | Loss: 0.1817 | EWC Loss: 0.0213 | CE Loss: 0.1605\n",
      "Train Epoch: 009 Batch: 00050/00094 | Loss: 0.1047 | EWC Loss: 0.0213 | CE Loss: 0.0834\n",
      "Train Epoch: 009 Batch: 00051/00094 | Loss: 0.1170 | EWC Loss: 0.0213 | CE Loss: 0.0957\n",
      "Train Epoch: 009 Batch: 00052/00094 | Loss: 0.1832 | EWC Loss: 0.0212 | CE Loss: 0.1620\n",
      "Train Epoch: 009 Batch: 00053/00094 | Loss: 0.2701 | EWC Loss: 0.0212 | CE Loss: 0.2489\n",
      "Train Epoch: 009 Batch: 00054/00094 | Loss: 0.1301 | EWC Loss: 0.0212 | CE Loss: 0.1089\n",
      "Train Epoch: 009 Batch: 00055/00094 | Loss: 0.1626 | EWC Loss: 0.0212 | CE Loss: 0.1414\n",
      "Train Epoch: 009 Batch: 00056/00094 | Loss: 0.0896 | EWC Loss: 0.0212 | CE Loss: 0.0683\n",
      "Train Epoch: 009 Batch: 00057/00094 | Loss: 0.1626 | EWC Loss: 0.0212 | CE Loss: 0.1414\n",
      "Train Epoch: 009 Batch: 00058/00094 | Loss: 0.1261 | EWC Loss: 0.0212 | CE Loss: 0.1049\n",
      "Train Epoch: 009 Batch: 00059/00094 | Loss: 0.1731 | EWC Loss: 0.0212 | CE Loss: 0.1519\n",
      "Train Epoch: 009 Batch: 00060/00094 | Loss: 0.1151 | EWC Loss: 0.0212 | CE Loss: 0.0939\n",
      "Train Epoch: 009 Batch: 00061/00094 | Loss: 0.1238 | EWC Loss: 0.0212 | CE Loss: 0.1026\n",
      "Train Epoch: 009 Batch: 00062/00094 | Loss: 0.1370 | EWC Loss: 0.0212 | CE Loss: 0.1159\n",
      "Train Epoch: 009 Batch: 00063/00094 | Loss: 0.1844 | EWC Loss: 0.0212 | CE Loss: 0.1632\n",
      "Train Epoch: 009 Batch: 00064/00094 | Loss: 0.1160 | EWC Loss: 0.0212 | CE Loss: 0.0948\n",
      "Train Epoch: 009 Batch: 00065/00094 | Loss: 0.1873 | EWC Loss: 0.0212 | CE Loss: 0.1661\n",
      "Train Epoch: 009 Batch: 00066/00094 | Loss: 0.1115 | EWC Loss: 0.0212 | CE Loss: 0.0903\n",
      "Train Epoch: 009 Batch: 00067/00094 | Loss: 0.1759 | EWC Loss: 0.0212 | CE Loss: 0.1547\n",
      "Train Epoch: 009 Batch: 00068/00094 | Loss: 0.1264 | EWC Loss: 0.0212 | CE Loss: 0.1052\n",
      "Train Epoch: 009 Batch: 00069/00094 | Loss: 0.1152 | EWC Loss: 0.0212 | CE Loss: 0.0940\n",
      "Train Epoch: 009 Batch: 00070/00094 | Loss: 0.1703 | EWC Loss: 0.0212 | CE Loss: 0.1491\n",
      "Train Epoch: 009 Batch: 00071/00094 | Loss: 0.1335 | EWC Loss: 0.0212 | CE Loss: 0.1122\n",
      "Train Epoch: 009 Batch: 00072/00094 | Loss: 0.1341 | EWC Loss: 0.0213 | CE Loss: 0.1128\n",
      "Train Epoch: 009 Batch: 00073/00094 | Loss: 0.1072 | EWC Loss: 0.0212 | CE Loss: 0.0860\n",
      "Train Epoch: 009 Batch: 00074/00094 | Loss: 0.0852 | EWC Loss: 0.0212 | CE Loss: 0.0639\n",
      "Train Epoch: 009 Batch: 00075/00094 | Loss: 0.1160 | EWC Loss: 0.0212 | CE Loss: 0.0948\n",
      "Train Epoch: 009 Batch: 00076/00094 | Loss: 0.1347 | EWC Loss: 0.0212 | CE Loss: 0.1135\n",
      "Train Epoch: 009 Batch: 00077/00094 | Loss: 0.1245 | EWC Loss: 0.0212 | CE Loss: 0.1033\n",
      "Train Epoch: 009 Batch: 00078/00094 | Loss: 0.1702 | EWC Loss: 0.0212 | CE Loss: 0.1490\n",
      "Train Epoch: 009 Batch: 00079/00094 | Loss: 0.1275 | EWC Loss: 0.0213 | CE Loss: 0.1062\n",
      "Train Epoch: 009 Batch: 00080/00094 | Loss: 0.0934 | EWC Loss: 0.0213 | CE Loss: 0.0721\n",
      "Train Epoch: 009 Batch: 00081/00094 | Loss: 0.1455 | EWC Loss: 0.0213 | CE Loss: 0.1242\n",
      "Train Epoch: 009 Batch: 00082/00094 | Loss: 0.1461 | EWC Loss: 0.0213 | CE Loss: 0.1248\n",
      "Train Epoch: 009 Batch: 00083/00094 | Loss: 0.1461 | EWC Loss: 0.0213 | CE Loss: 0.1248\n",
      "Train Epoch: 009 Batch: 00084/00094 | Loss: 0.2528 | EWC Loss: 0.0213 | CE Loss: 0.2315\n",
      "Train Epoch: 009 Batch: 00085/00094 | Loss: 0.1243 | EWC Loss: 0.0213 | CE Loss: 0.1030\n",
      "Train Epoch: 009 Batch: 00086/00094 | Loss: 0.1616 | EWC Loss: 0.0214 | CE Loss: 0.1403\n",
      "Train Epoch: 009 Batch: 00087/00094 | Loss: 0.1819 | EWC Loss: 0.0214 | CE Loss: 0.1606\n",
      "Train Epoch: 009 Batch: 00088/00094 | Loss: 0.1808 | EWC Loss: 0.0214 | CE Loss: 0.1594\n",
      "Train Epoch: 009 Batch: 00089/00094 | Loss: 0.1216 | EWC Loss: 0.0214 | CE Loss: 0.1002\n",
      "Train Epoch: 009 Batch: 00090/00094 | Loss: 0.1313 | EWC Loss: 0.0214 | CE Loss: 0.1099\n",
      "Train Epoch: 009 Batch: 00091/00094 | Loss: 0.0987 | EWC Loss: 0.0214 | CE Loss: 0.0773\n",
      "Train Epoch: 009 Batch: 00092/00094 | Loss: 0.1021 | EWC Loss: 0.0214 | CE Loss: 0.0807\n",
      "Train Epoch: 009 Batch: 00093/00094 | Loss: 0.1411 | EWC Loss: 0.0214 | CE Loss: 0.1197\n",
      "Train Epoch: 009 Batch: 00094/00094 | Loss: 0.1528 | EWC Loss: 0.0215 | CE Loss: 0.1313\n",
      "Train Epoch: 008 |Acc 95.50000 | Loss: 0.14950 | Task Loss 0.12815 | EWC Loss: 0.02135\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1146 | acc:96.0500\n",
      "[VAL Acc] Target: 96.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.1661 | acc:50.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1027 | acc:61.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 61.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1589 | acc:56.2977\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 56.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1874 | acc:55.7602\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6635 | acc:71.1645\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5602 | acc:77.4687\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 77.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9437 | acc:57.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.06%\n",
      "[VAL Acc] Avg 65.62%\n",
      "\n",
      "\n",
      "---------- Starting epoch 9 ----------\n",
      "Train Epoch: 010 Batch: 00001/00094 | Loss: 0.1377 | EWC Loss: 0.0215 | CE Loss: 0.1162\n",
      "Train Epoch: 010 Batch: 00002/00094 | Loss: 0.2182 | EWC Loss: 0.0215 | CE Loss: 0.1967\n",
      "Train Epoch: 010 Batch: 00003/00094 | Loss: 0.1798 | EWC Loss: 0.0215 | CE Loss: 0.1583\n",
      "Train Epoch: 010 Batch: 00004/00094 | Loss: 0.1465 | EWC Loss: 0.0215 | CE Loss: 0.1250\n",
      "Train Epoch: 010 Batch: 00005/00094 | Loss: 0.0925 | EWC Loss: 0.0215 | CE Loss: 0.0710\n",
      "Train Epoch: 010 Batch: 00006/00094 | Loss: 0.1655 | EWC Loss: 0.0215 | CE Loss: 0.1440\n",
      "Train Epoch: 010 Batch: 00007/00094 | Loss: 0.1306 | EWC Loss: 0.0215 | CE Loss: 0.1091\n",
      "Train Epoch: 010 Batch: 00008/00094 | Loss: 0.1231 | EWC Loss: 0.0215 | CE Loss: 0.1016\n",
      "Train Epoch: 010 Batch: 00009/00094 | Loss: 0.0622 | EWC Loss: 0.0215 | CE Loss: 0.0407\n",
      "Train Epoch: 010 Batch: 00010/00094 | Loss: 0.0913 | EWC Loss: 0.0215 | CE Loss: 0.0698\n",
      "Train Epoch: 010 Batch: 00011/00094 | Loss: 0.1040 | EWC Loss: 0.0215 | CE Loss: 0.0825\n",
      "Train Epoch: 010 Batch: 00012/00094 | Loss: 0.1304 | EWC Loss: 0.0215 | CE Loss: 0.1089\n",
      "Train Epoch: 010 Batch: 00013/00094 | Loss: 0.1878 | EWC Loss: 0.0215 | CE Loss: 0.1664\n",
      "Train Epoch: 010 Batch: 00014/00094 | Loss: 0.0582 | EWC Loss: 0.0215 | CE Loss: 0.0367\n",
      "Train Epoch: 010 Batch: 00015/00094 | Loss: 0.1104 | EWC Loss: 0.0215 | CE Loss: 0.0889\n",
      "Train Epoch: 010 Batch: 00016/00094 | Loss: 0.1254 | EWC Loss: 0.0215 | CE Loss: 0.1040\n",
      "Train Epoch: 010 Batch: 00017/00094 | Loss: 0.1772 | EWC Loss: 0.0215 | CE Loss: 0.1558\n",
      "Train Epoch: 010 Batch: 00018/00094 | Loss: 0.0982 | EWC Loss: 0.0215 | CE Loss: 0.0768\n",
      "Train Epoch: 010 Batch: 00019/00094 | Loss: 0.1909 | EWC Loss: 0.0215 | CE Loss: 0.1694\n",
      "Train Epoch: 010 Batch: 00020/00094 | Loss: 0.0731 | EWC Loss: 0.0215 | CE Loss: 0.0517\n",
      "Train Epoch: 010 Batch: 00021/00094 | Loss: 0.1115 | EWC Loss: 0.0215 | CE Loss: 0.0900\n",
      "Train Epoch: 010 Batch: 00022/00094 | Loss: 0.1670 | EWC Loss: 0.0214 | CE Loss: 0.1456\n",
      "Train Epoch: 010 Batch: 00023/00094 | Loss: 0.1663 | EWC Loss: 0.0215 | CE Loss: 0.1448\n",
      "Train Epoch: 010 Batch: 00024/00094 | Loss: 0.1122 | EWC Loss: 0.0214 | CE Loss: 0.0908\n",
      "Train Epoch: 010 Batch: 00025/00094 | Loss: 0.1614 | EWC Loss: 0.0215 | CE Loss: 0.1399\n",
      "Train Epoch: 010 Batch: 00026/00094 | Loss: 0.1540 | EWC Loss: 0.0215 | CE Loss: 0.1326\n",
      "Train Epoch: 010 Batch: 00027/00094 | Loss: 0.0981 | EWC Loss: 0.0215 | CE Loss: 0.0766\n",
      "Train Epoch: 010 Batch: 00028/00094 | Loss: 0.1811 | EWC Loss: 0.0215 | CE Loss: 0.1597\n",
      "Train Epoch: 010 Batch: 00029/00094 | Loss: 0.0787 | EWC Loss: 0.0214 | CE Loss: 0.0573\n",
      "Train Epoch: 010 Batch: 00030/00094 | Loss: 0.1608 | EWC Loss: 0.0214 | CE Loss: 0.1394\n",
      "Train Epoch: 010 Batch: 00031/00094 | Loss: 0.1893 | EWC Loss: 0.0214 | CE Loss: 0.1679\n",
      "Train Epoch: 010 Batch: 00032/00094 | Loss: 0.1780 | EWC Loss: 0.0214 | CE Loss: 0.1566\n",
      "Train Epoch: 010 Batch: 00033/00094 | Loss: 0.1459 | EWC Loss: 0.0214 | CE Loss: 0.1245\n",
      "Train Epoch: 010 Batch: 00034/00094 | Loss: 0.1308 | EWC Loss: 0.0214 | CE Loss: 0.1094\n",
      "Train Epoch: 010 Batch: 00035/00094 | Loss: 0.1489 | EWC Loss: 0.0214 | CE Loss: 0.1274\n",
      "Train Epoch: 010 Batch: 00036/00094 | Loss: 0.0928 | EWC Loss: 0.0214 | CE Loss: 0.0714\n",
      "Train Epoch: 010 Batch: 00037/00094 | Loss: 0.2205 | EWC Loss: 0.0214 | CE Loss: 0.1991\n",
      "Train Epoch: 010 Batch: 00038/00094 | Loss: 0.1350 | EWC Loss: 0.0214 | CE Loss: 0.1135\n",
      "Train Epoch: 010 Batch: 00039/00094 | Loss: 0.1799 | EWC Loss: 0.0214 | CE Loss: 0.1585\n",
      "Train Epoch: 010 Batch: 00040/00094 | Loss: 0.1997 | EWC Loss: 0.0214 | CE Loss: 0.1783\n",
      "Train Epoch: 010 Batch: 00041/00094 | Loss: 0.2350 | EWC Loss: 0.0214 | CE Loss: 0.2136\n",
      "Train Epoch: 010 Batch: 00042/00094 | Loss: 0.0921 | EWC Loss: 0.0214 | CE Loss: 0.0707\n",
      "Train Epoch: 010 Batch: 00043/00094 | Loss: 0.1396 | EWC Loss: 0.0214 | CE Loss: 0.1182\n",
      "Train Epoch: 010 Batch: 00044/00094 | Loss: 0.1548 | EWC Loss: 0.0214 | CE Loss: 0.1334\n",
      "Train Epoch: 010 Batch: 00045/00094 | Loss: 0.1336 | EWC Loss: 0.0214 | CE Loss: 0.1122\n",
      "Train Epoch: 010 Batch: 00046/00094 | Loss: 0.1295 | EWC Loss: 0.0214 | CE Loss: 0.1081\n",
      "Train Epoch: 010 Batch: 00047/00094 | Loss: 0.1128 | EWC Loss: 0.0214 | CE Loss: 0.0914\n",
      "Train Epoch: 010 Batch: 00048/00094 | Loss: 0.1352 | EWC Loss: 0.0214 | CE Loss: 0.1138\n",
      "Train Epoch: 010 Batch: 00049/00094 | Loss: 0.1840 | EWC Loss: 0.0214 | CE Loss: 0.1626\n",
      "Train Epoch: 010 Batch: 00050/00094 | Loss: 0.1904 | EWC Loss: 0.0214 | CE Loss: 0.1690\n",
      "Train Epoch: 010 Batch: 00051/00094 | Loss: 0.1625 | EWC Loss: 0.0214 | CE Loss: 0.1411\n",
      "Train Epoch: 010 Batch: 00052/00094 | Loss: 0.1503 | EWC Loss: 0.0214 | CE Loss: 0.1289\n",
      "Train Epoch: 010 Batch: 00053/00094 | Loss: 0.0972 | EWC Loss: 0.0214 | CE Loss: 0.0759\n",
      "Train Epoch: 010 Batch: 00054/00094 | Loss: 0.0978 | EWC Loss: 0.0214 | CE Loss: 0.0764\n",
      "Train Epoch: 010 Batch: 00055/00094 | Loss: 0.1515 | EWC Loss: 0.0214 | CE Loss: 0.1302\n",
      "Train Epoch: 010 Batch: 00056/00094 | Loss: 0.2021 | EWC Loss: 0.0214 | CE Loss: 0.1807\n",
      "Train Epoch: 010 Batch: 00057/00094 | Loss: 0.2326 | EWC Loss: 0.0214 | CE Loss: 0.2113\n",
      "Train Epoch: 010 Batch: 00058/00094 | Loss: 0.1708 | EWC Loss: 0.0214 | CE Loss: 0.1494\n",
      "Train Epoch: 010 Batch: 00059/00094 | Loss: 0.1232 | EWC Loss: 0.0214 | CE Loss: 0.1019\n",
      "Train Epoch: 010 Batch: 00060/00094 | Loss: 0.1552 | EWC Loss: 0.0214 | CE Loss: 0.1339\n",
      "Train Epoch: 010 Batch: 00061/00094 | Loss: 0.1225 | EWC Loss: 0.0214 | CE Loss: 0.1011\n",
      "Train Epoch: 010 Batch: 00062/00094 | Loss: 0.2412 | EWC Loss: 0.0214 | CE Loss: 0.2198\n",
      "Train Epoch: 010 Batch: 00063/00094 | Loss: 0.1353 | EWC Loss: 0.0214 | CE Loss: 0.1139\n",
      "Train Epoch: 010 Batch: 00064/00094 | Loss: 0.1385 | EWC Loss: 0.0214 | CE Loss: 0.1171\n",
      "Train Epoch: 010 Batch: 00065/00094 | Loss: 0.1154 | EWC Loss: 0.0214 | CE Loss: 0.0940\n",
      "Train Epoch: 010 Batch: 00066/00094 | Loss: 0.1386 | EWC Loss: 0.0214 | CE Loss: 0.1172\n",
      "Train Epoch: 010 Batch: 00067/00094 | Loss: 0.2049 | EWC Loss: 0.0214 | CE Loss: 0.1836\n",
      "Train Epoch: 010 Batch: 00068/00094 | Loss: 0.1380 | EWC Loss: 0.0214 | CE Loss: 0.1166\n",
      "Train Epoch: 010 Batch: 00069/00094 | Loss: 0.2179 | EWC Loss: 0.0214 | CE Loss: 0.1966\n",
      "Train Epoch: 010 Batch: 00070/00094 | Loss: 0.1530 | EWC Loss: 0.0214 | CE Loss: 0.1317\n",
      "Train Epoch: 010 Batch: 00071/00094 | Loss: 0.1515 | EWC Loss: 0.0214 | CE Loss: 0.1301\n",
      "Train Epoch: 010 Batch: 00072/00094 | Loss: 0.1401 | EWC Loss: 0.0214 | CE Loss: 0.1188\n",
      "Train Epoch: 010 Batch: 00073/00094 | Loss: 0.1383 | EWC Loss: 0.0214 | CE Loss: 0.1169\n",
      "Train Epoch: 010 Batch: 00074/00094 | Loss: 0.1068 | EWC Loss: 0.0214 | CE Loss: 0.0855\n",
      "Train Epoch: 010 Batch: 00075/00094 | Loss: 0.1456 | EWC Loss: 0.0214 | CE Loss: 0.1243\n",
      "Train Epoch: 010 Batch: 00076/00094 | Loss: 0.1271 | EWC Loss: 0.0214 | CE Loss: 0.1057\n",
      "Train Epoch: 010 Batch: 00077/00094 | Loss: 0.1237 | EWC Loss: 0.0214 | CE Loss: 0.1023\n",
      "Train Epoch: 010 Batch: 00078/00094 | Loss: 0.1225 | EWC Loss: 0.0214 | CE Loss: 0.1011\n",
      "Train Epoch: 010 Batch: 00079/00094 | Loss: 0.0903 | EWC Loss: 0.0214 | CE Loss: 0.0690\n",
      "Train Epoch: 010 Batch: 00080/00094 | Loss: 0.2908 | EWC Loss: 0.0214 | CE Loss: 0.2695\n",
      "Train Epoch: 010 Batch: 00081/00094 | Loss: 0.1159 | EWC Loss: 0.0214 | CE Loss: 0.0946\n",
      "Train Epoch: 010 Batch: 00082/00094 | Loss: 0.1524 | EWC Loss: 0.0214 | CE Loss: 0.1310\n",
      "Train Epoch: 010 Batch: 00083/00094 | Loss: 0.1353 | EWC Loss: 0.0214 | CE Loss: 0.1139\n",
      "Train Epoch: 010 Batch: 00084/00094 | Loss: 0.1754 | EWC Loss: 0.0214 | CE Loss: 0.1540\n",
      "Train Epoch: 010 Batch: 00085/00094 | Loss: 0.2221 | EWC Loss: 0.0214 | CE Loss: 0.2007\n",
      "Train Epoch: 010 Batch: 00086/00094 | Loss: 0.1647 | EWC Loss: 0.0213 | CE Loss: 0.1433\n",
      "Train Epoch: 010 Batch: 00087/00094 | Loss: 0.2648 | EWC Loss: 0.0213 | CE Loss: 0.2435\n",
      "Train Epoch: 010 Batch: 00088/00094 | Loss: 0.0926 | EWC Loss: 0.0213 | CE Loss: 0.0713\n",
      "Train Epoch: 010 Batch: 00089/00094 | Loss: 0.2707 | EWC Loss: 0.0213 | CE Loss: 0.2493\n",
      "Train Epoch: 010 Batch: 00090/00094 | Loss: 0.2858 | EWC Loss: 0.0213 | CE Loss: 0.2644\n",
      "Train Epoch: 010 Batch: 00091/00094 | Loss: 0.1700 | EWC Loss: 0.0214 | CE Loss: 0.1486\n",
      "Train Epoch: 010 Batch: 00092/00094 | Loss: 0.1430 | EWC Loss: 0.0214 | CE Loss: 0.1216\n",
      "Train Epoch: 010 Batch: 00093/00094 | Loss: 0.3230 | EWC Loss: 0.0214 | CE Loss: 0.3017\n",
      "Train Epoch: 010 Batch: 00094/00094 | Loss: 0.0961 | EWC Loss: 0.0214 | CE Loss: 0.0747\n",
      "Train Epoch: 009 |Acc 95.51667 | Loss: 0.15233 | Task Loss 0.13092 | EWC Loss: 0.02141\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1229 | acc:95.5000\n",
      "[VAL Acc] Target: 95.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.1051 | acc:50.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1357 | acc:60.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2009 | acc:51.1450\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 51.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1509 | acc:55.3683\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6195 | acc:73.1978\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5370 | acc:76.4890\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8860 | acc:60.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.62%\n",
      "[VAL Acc] Avg 65.32%\n",
      "\n",
      "\n",
      "---------- Starting epoch 10 ----------\n",
      "Train Epoch: 011 Batch: 00001/00094 | Loss: 0.0992 | EWC Loss: 0.0214 | CE Loss: 0.0778\n",
      "Train Epoch: 011 Batch: 00002/00094 | Loss: 0.1382 | EWC Loss: 0.0214 | CE Loss: 0.1168\n",
      "Train Epoch: 011 Batch: 00003/00094 | Loss: 0.1052 | EWC Loss: 0.0214 | CE Loss: 0.0838\n",
      "Train Epoch: 011 Batch: 00004/00094 | Loss: 0.1555 | EWC Loss: 0.0214 | CE Loss: 0.1342\n",
      "Train Epoch: 011 Batch: 00005/00094 | Loss: 0.1251 | EWC Loss: 0.0214 | CE Loss: 0.1037\n",
      "Train Epoch: 011 Batch: 00006/00094 | Loss: 0.1441 | EWC Loss: 0.0214 | CE Loss: 0.1227\n",
      "Train Epoch: 011 Batch: 00007/00094 | Loss: 0.1122 | EWC Loss: 0.0214 | CE Loss: 0.0909\n",
      "Train Epoch: 011 Batch: 00008/00094 | Loss: 0.1581 | EWC Loss: 0.0214 | CE Loss: 0.1368\n",
      "Train Epoch: 011 Batch: 00009/00094 | Loss: 0.0869 | EWC Loss: 0.0214 | CE Loss: 0.0655\n",
      "Train Epoch: 011 Batch: 00010/00094 | Loss: 0.1397 | EWC Loss: 0.0214 | CE Loss: 0.1184\n",
      "Train Epoch: 011 Batch: 00011/00094 | Loss: 0.2068 | EWC Loss: 0.0214 | CE Loss: 0.1854\n",
      "Train Epoch: 011 Batch: 00012/00094 | Loss: 0.1208 | EWC Loss: 0.0214 | CE Loss: 0.0994\n",
      "Train Epoch: 011 Batch: 00013/00094 | Loss: 0.1545 | EWC Loss: 0.0214 | CE Loss: 0.1331\n",
      "Train Epoch: 011 Batch: 00014/00094 | Loss: 0.1238 | EWC Loss: 0.0214 | CE Loss: 0.1025\n",
      "Train Epoch: 011 Batch: 00015/00094 | Loss: 0.1929 | EWC Loss: 0.0214 | CE Loss: 0.1715\n",
      "Train Epoch: 011 Batch: 00016/00094 | Loss: 0.1331 | EWC Loss: 0.0214 | CE Loss: 0.1118\n",
      "Train Epoch: 011 Batch: 00017/00094 | Loss: 0.2056 | EWC Loss: 0.0214 | CE Loss: 0.1843\n",
      "Train Epoch: 011 Batch: 00018/00094 | Loss: 0.1812 | EWC Loss: 0.0214 | CE Loss: 0.1598\n",
      "Train Epoch: 011 Batch: 00019/00094 | Loss: 0.1408 | EWC Loss: 0.0214 | CE Loss: 0.1194\n",
      "Train Epoch: 011 Batch: 00020/00094 | Loss: 0.1775 | EWC Loss: 0.0214 | CE Loss: 0.1561\n",
      "Train Epoch: 011 Batch: 00021/00094 | Loss: 0.1840 | EWC Loss: 0.0214 | CE Loss: 0.1626\n",
      "Train Epoch: 011 Batch: 00022/00094 | Loss: 0.1135 | EWC Loss: 0.0214 | CE Loss: 0.0921\n",
      "Train Epoch: 011 Batch: 00023/00094 | Loss: 0.1165 | EWC Loss: 0.0214 | CE Loss: 0.0951\n",
      "Train Epoch: 011 Batch: 00024/00094 | Loss: 0.0837 | EWC Loss: 0.0214 | CE Loss: 0.0623\n",
      "Train Epoch: 011 Batch: 00025/00094 | Loss: 0.1395 | EWC Loss: 0.0214 | CE Loss: 0.1181\n",
      "Train Epoch: 011 Batch: 00026/00094 | Loss: 0.1526 | EWC Loss: 0.0214 | CE Loss: 0.1312\n",
      "Train Epoch: 011 Batch: 00027/00094 | Loss: 0.2214 | EWC Loss: 0.0214 | CE Loss: 0.2000\n",
      "Train Epoch: 011 Batch: 00028/00094 | Loss: 0.1022 | EWC Loss: 0.0214 | CE Loss: 0.0808\n",
      "Train Epoch: 011 Batch: 00029/00094 | Loss: 0.1840 | EWC Loss: 0.0214 | CE Loss: 0.1626\n",
      "Train Epoch: 011 Batch: 00030/00094 | Loss: 0.1081 | EWC Loss: 0.0214 | CE Loss: 0.0867\n",
      "Train Epoch: 011 Batch: 00031/00094 | Loss: 0.1935 | EWC Loss: 0.0214 | CE Loss: 0.1721\n",
      "Train Epoch: 011 Batch: 00032/00094 | Loss: 0.1507 | EWC Loss: 0.0214 | CE Loss: 0.1293\n",
      "Train Epoch: 011 Batch: 00033/00094 | Loss: 0.1382 | EWC Loss: 0.0214 | CE Loss: 0.1168\n",
      "Train Epoch: 011 Batch: 00034/00094 | Loss: 0.1199 | EWC Loss: 0.0214 | CE Loss: 0.0985\n",
      "Train Epoch: 011 Batch: 00035/00094 | Loss: 0.1457 | EWC Loss: 0.0214 | CE Loss: 0.1244\n",
      "Train Epoch: 011 Batch: 00036/00094 | Loss: 0.0863 | EWC Loss: 0.0214 | CE Loss: 0.0650\n",
      "Train Epoch: 011 Batch: 00037/00094 | Loss: 0.1452 | EWC Loss: 0.0214 | CE Loss: 0.1238\n",
      "Train Epoch: 011 Batch: 00038/00094 | Loss: 0.1122 | EWC Loss: 0.0214 | CE Loss: 0.0908\n",
      "Train Epoch: 011 Batch: 00039/00094 | Loss: 0.1393 | EWC Loss: 0.0214 | CE Loss: 0.1180\n",
      "Train Epoch: 011 Batch: 00040/00094 | Loss: 0.0983 | EWC Loss: 0.0214 | CE Loss: 0.0769\n",
      "Train Epoch: 011 Batch: 00041/00094 | Loss: 0.2568 | EWC Loss: 0.0214 | CE Loss: 0.2355\n",
      "Train Epoch: 011 Batch: 00042/00094 | Loss: 0.2202 | EWC Loss: 0.0214 | CE Loss: 0.1988\n",
      "Train Epoch: 011 Batch: 00043/00094 | Loss: 0.1604 | EWC Loss: 0.0214 | CE Loss: 0.1390\n",
      "Train Epoch: 011 Batch: 00044/00094 | Loss: 0.1176 | EWC Loss: 0.0214 | CE Loss: 0.0962\n",
      "Train Epoch: 011 Batch: 00045/00094 | Loss: 0.1303 | EWC Loss: 0.0214 | CE Loss: 0.1090\n",
      "Train Epoch: 011 Batch: 00046/00094 | Loss: 0.1921 | EWC Loss: 0.0214 | CE Loss: 0.1707\n",
      "Train Epoch: 011 Batch: 00047/00094 | Loss: 0.1230 | EWC Loss: 0.0214 | CE Loss: 0.1016\n",
      "Train Epoch: 011 Batch: 00048/00094 | Loss: 0.1978 | EWC Loss: 0.0214 | CE Loss: 0.1764\n",
      "Train Epoch: 011 Batch: 00049/00094 | Loss: 0.1481 | EWC Loss: 0.0214 | CE Loss: 0.1268\n",
      "Train Epoch: 011 Batch: 00050/00094 | Loss: 0.1512 | EWC Loss: 0.0214 | CE Loss: 0.1299\n",
      "Train Epoch: 011 Batch: 00051/00094 | Loss: 0.2095 | EWC Loss: 0.0214 | CE Loss: 0.1881\n",
      "Train Epoch: 011 Batch: 00052/00094 | Loss: 0.1705 | EWC Loss: 0.0214 | CE Loss: 0.1491\n",
      "Train Epoch: 011 Batch: 00053/00094 | Loss: 0.1664 | EWC Loss: 0.0214 | CE Loss: 0.1450\n",
      "Train Epoch: 011 Batch: 00054/00094 | Loss: 0.1536 | EWC Loss: 0.0214 | CE Loss: 0.1323\n",
      "Train Epoch: 011 Batch: 00055/00094 | Loss: 0.1350 | EWC Loss: 0.0214 | CE Loss: 0.1136\n",
      "Train Epoch: 011 Batch: 00056/00094 | Loss: 0.1468 | EWC Loss: 0.0214 | CE Loss: 0.1255\n",
      "Train Epoch: 011 Batch: 00057/00094 | Loss: 0.1528 | EWC Loss: 0.0214 | CE Loss: 0.1314\n",
      "Train Epoch: 011 Batch: 00058/00094 | Loss: 0.1041 | EWC Loss: 0.0214 | CE Loss: 0.0827\n",
      "Train Epoch: 011 Batch: 00059/00094 | Loss: 0.1361 | EWC Loss: 0.0214 | CE Loss: 0.1147\n",
      "Train Epoch: 011 Batch: 00060/00094 | Loss: 0.1613 | EWC Loss: 0.0214 | CE Loss: 0.1399\n",
      "Train Epoch: 011 Batch: 00061/00094 | Loss: 0.2150 | EWC Loss: 0.0214 | CE Loss: 0.1936\n",
      "Train Epoch: 011 Batch: 00062/00094 | Loss: 0.1394 | EWC Loss: 0.0214 | CE Loss: 0.1180\n",
      "Train Epoch: 011 Batch: 00063/00094 | Loss: 0.1432 | EWC Loss: 0.0214 | CE Loss: 0.1219\n",
      "Train Epoch: 011 Batch: 00064/00094 | Loss: 0.2493 | EWC Loss: 0.0214 | CE Loss: 0.2279\n",
      "Train Epoch: 011 Batch: 00065/00094 | Loss: 0.0838 | EWC Loss: 0.0214 | CE Loss: 0.0625\n",
      "Train Epoch: 011 Batch: 00066/00094 | Loss: 0.0953 | EWC Loss: 0.0214 | CE Loss: 0.0739\n",
      "Train Epoch: 011 Batch: 00067/00094 | Loss: 0.2153 | EWC Loss: 0.0214 | CE Loss: 0.1939\n",
      "Train Epoch: 011 Batch: 00068/00094 | Loss: 0.1266 | EWC Loss: 0.0214 | CE Loss: 0.1052\n",
      "Train Epoch: 011 Batch: 00069/00094 | Loss: 0.1672 | EWC Loss: 0.0214 | CE Loss: 0.1458\n",
      "Train Epoch: 011 Batch: 00070/00094 | Loss: 0.1881 | EWC Loss: 0.0214 | CE Loss: 0.1667\n",
      "Train Epoch: 011 Batch: 00071/00094 | Loss: 0.1344 | EWC Loss: 0.0214 | CE Loss: 0.1130\n",
      "Train Epoch: 011 Batch: 00072/00094 | Loss: 0.1516 | EWC Loss: 0.0214 | CE Loss: 0.1303\n",
      "Train Epoch: 011 Batch: 00073/00094 | Loss: 0.1722 | EWC Loss: 0.0214 | CE Loss: 0.1508\n",
      "Train Epoch: 011 Batch: 00074/00094 | Loss: 0.1775 | EWC Loss: 0.0214 | CE Loss: 0.1561\n",
      "Train Epoch: 011 Batch: 00075/00094 | Loss: 0.0911 | EWC Loss: 0.0214 | CE Loss: 0.0697\n",
      "Train Epoch: 011 Batch: 00076/00094 | Loss: 0.1485 | EWC Loss: 0.0214 | CE Loss: 0.1271\n",
      "Train Epoch: 011 Batch: 00077/00094 | Loss: 0.1625 | EWC Loss: 0.0214 | CE Loss: 0.1412\n",
      "Train Epoch: 011 Batch: 00078/00094 | Loss: 0.2236 | EWC Loss: 0.0214 | CE Loss: 0.2022\n",
      "Train Epoch: 011 Batch: 00079/00094 | Loss: 0.1237 | EWC Loss: 0.0214 | CE Loss: 0.1024\n",
      "Train Epoch: 011 Batch: 00080/00094 | Loss: 0.1086 | EWC Loss: 0.0214 | CE Loss: 0.0872\n",
      "Train Epoch: 011 Batch: 00081/00094 | Loss: 0.1326 | EWC Loss: 0.0214 | CE Loss: 0.1113\n",
      "Train Epoch: 011 Batch: 00082/00094 | Loss: 0.1615 | EWC Loss: 0.0214 | CE Loss: 0.1401\n",
      "Train Epoch: 011 Batch: 00083/00094 | Loss: 0.1007 | EWC Loss: 0.0214 | CE Loss: 0.0794\n",
      "Train Epoch: 011 Batch: 00084/00094 | Loss: 0.1545 | EWC Loss: 0.0214 | CE Loss: 0.1332\n",
      "Train Epoch: 011 Batch: 00085/00094 | Loss: 0.1677 | EWC Loss: 0.0214 | CE Loss: 0.1463\n",
      "Train Epoch: 011 Batch: 00086/00094 | Loss: 0.1518 | EWC Loss: 0.0214 | CE Loss: 0.1304\n",
      "Train Epoch: 011 Batch: 00087/00094 | Loss: 0.1285 | EWC Loss: 0.0214 | CE Loss: 0.1071\n",
      "Train Epoch: 011 Batch: 00088/00094 | Loss: 0.1403 | EWC Loss: 0.0214 | CE Loss: 0.1190\n",
      "Train Epoch: 011 Batch: 00089/00094 | Loss: 0.2266 | EWC Loss: 0.0214 | CE Loss: 0.2052\n",
      "Train Epoch: 011 Batch: 00090/00094 | Loss: 0.1510 | EWC Loss: 0.0214 | CE Loss: 0.1297\n",
      "Train Epoch: 011 Batch: 00091/00094 | Loss: 0.1799 | EWC Loss: 0.0214 | CE Loss: 0.1585\n",
      "Train Epoch: 011 Batch: 00092/00094 | Loss: 0.1057 | EWC Loss: 0.0214 | CE Loss: 0.0843\n",
      "Train Epoch: 011 Batch: 00093/00094 | Loss: 0.1736 | EWC Loss: 0.0214 | CE Loss: 0.1522\n",
      "Train Epoch: 011 Batch: 00094/00094 | Loss: 0.0834 | EWC Loss: 0.0214 | CE Loss: 0.0620\n",
      "Train Epoch: 010 |Acc 95.45000 | Loss: 0.14941 | Task Loss 0.12804 | EWC Loss: 0.02137\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1288 | acc:95.6000\n",
      "[VAL Acc] Target: 95.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.0047 | acc:50.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0869 | acc:60.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.0339 | acc:56.1069\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 56.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.0158 | acc:58.6207\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6525 | acc:70.6100\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4863 | acc:79.2320\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 79.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8951 | acc:60.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.12%\n",
      "[VAL Acc] Avg 66.49%\n",
      "\n",
      "\n",
      "---------- Starting epoch 11 ----------\n",
      "Train Epoch: 012 Batch: 00001/00094 | Loss: 0.1594 | EWC Loss: 0.0214 | CE Loss: 0.1381\n",
      "Train Epoch: 012 Batch: 00002/00094 | Loss: 0.1026 | EWC Loss: 0.0214 | CE Loss: 0.0812\n",
      "Train Epoch: 012 Batch: 00003/00094 | Loss: 0.1352 | EWC Loss: 0.0214 | CE Loss: 0.1139\n",
      "Train Epoch: 012 Batch: 00004/00094 | Loss: 0.1491 | EWC Loss: 0.0214 | CE Loss: 0.1277\n",
      "Train Epoch: 012 Batch: 00005/00094 | Loss: 0.0991 | EWC Loss: 0.0214 | CE Loss: 0.0777\n",
      "Train Epoch: 012 Batch: 00006/00094 | Loss: 0.0989 | EWC Loss: 0.0214 | CE Loss: 0.0775\n",
      "Train Epoch: 012 Batch: 00007/00094 | Loss: 0.2155 | EWC Loss: 0.0214 | CE Loss: 0.1941\n",
      "Train Epoch: 012 Batch: 00008/00094 | Loss: 0.1336 | EWC Loss: 0.0214 | CE Loss: 0.1122\n",
      "Train Epoch: 012 Batch: 00009/00094 | Loss: 0.1400 | EWC Loss: 0.0214 | CE Loss: 0.1186\n",
      "Train Epoch: 012 Batch: 00010/00094 | Loss: 0.1050 | EWC Loss: 0.0214 | CE Loss: 0.0836\n",
      "Train Epoch: 012 Batch: 00011/00094 | Loss: 0.1446 | EWC Loss: 0.0214 | CE Loss: 0.1233\n",
      "Train Epoch: 012 Batch: 00012/00094 | Loss: 0.1599 | EWC Loss: 0.0214 | CE Loss: 0.1385\n",
      "Train Epoch: 012 Batch: 00013/00094 | Loss: 0.1722 | EWC Loss: 0.0214 | CE Loss: 0.1508\n",
      "Train Epoch: 012 Batch: 00014/00094 | Loss: 0.0892 | EWC Loss: 0.0214 | CE Loss: 0.0678\n",
      "Train Epoch: 012 Batch: 00015/00094 | Loss: 0.1585 | EWC Loss: 0.0214 | CE Loss: 0.1371\n",
      "Train Epoch: 012 Batch: 00016/00094 | Loss: 0.1344 | EWC Loss: 0.0214 | CE Loss: 0.1130\n",
      "Train Epoch: 012 Batch: 00017/00094 | Loss: 0.1567 | EWC Loss: 0.0214 | CE Loss: 0.1353\n",
      "Train Epoch: 012 Batch: 00018/00094 | Loss: 0.1923 | EWC Loss: 0.0214 | CE Loss: 0.1709\n",
      "Train Epoch: 012 Batch: 00019/00094 | Loss: 0.1127 | EWC Loss: 0.0214 | CE Loss: 0.0913\n",
      "Train Epoch: 012 Batch: 00020/00094 | Loss: 0.1631 | EWC Loss: 0.0214 | CE Loss: 0.1417\n",
      "Train Epoch: 012 Batch: 00021/00094 | Loss: 0.1574 | EWC Loss: 0.0214 | CE Loss: 0.1360\n",
      "Train Epoch: 012 Batch: 00022/00094 | Loss: 0.1516 | EWC Loss: 0.0214 | CE Loss: 0.1301\n",
      "Train Epoch: 012 Batch: 00023/00094 | Loss: 0.2096 | EWC Loss: 0.0214 | CE Loss: 0.1882\n",
      "Train Epoch: 012 Batch: 00024/00094 | Loss: 0.1369 | EWC Loss: 0.0214 | CE Loss: 0.1155\n",
      "Train Epoch: 012 Batch: 00025/00094 | Loss: 0.2504 | EWC Loss: 0.0214 | CE Loss: 0.2290\n",
      "Train Epoch: 012 Batch: 00026/00094 | Loss: 0.1303 | EWC Loss: 0.0214 | CE Loss: 0.1089\n",
      "Train Epoch: 012 Batch: 00027/00094 | Loss: 0.1816 | EWC Loss: 0.0214 | CE Loss: 0.1602\n",
      "Train Epoch: 012 Batch: 00028/00094 | Loss: 0.2280 | EWC Loss: 0.0214 | CE Loss: 0.2066\n",
      "Train Epoch: 012 Batch: 00029/00094 | Loss: 0.1473 | EWC Loss: 0.0214 | CE Loss: 0.1259\n",
      "Train Epoch: 012 Batch: 00030/00094 | Loss: 0.2004 | EWC Loss: 0.0214 | CE Loss: 0.1790\n",
      "Train Epoch: 012 Batch: 00031/00094 | Loss: 0.2172 | EWC Loss: 0.0214 | CE Loss: 0.1958\n",
      "Train Epoch: 012 Batch: 00032/00094 | Loss: 0.1141 | EWC Loss: 0.0214 | CE Loss: 0.0927\n",
      "Train Epoch: 012 Batch: 00033/00094 | Loss: 0.1053 | EWC Loss: 0.0214 | CE Loss: 0.0838\n",
      "Train Epoch: 012 Batch: 00034/00094 | Loss: 0.1199 | EWC Loss: 0.0214 | CE Loss: 0.0985\n",
      "Train Epoch: 012 Batch: 00035/00094 | Loss: 0.1213 | EWC Loss: 0.0214 | CE Loss: 0.0999\n",
      "Train Epoch: 012 Batch: 00036/00094 | Loss: 0.1925 | EWC Loss: 0.0214 | CE Loss: 0.1710\n",
      "Train Epoch: 012 Batch: 00037/00094 | Loss: 0.1645 | EWC Loss: 0.0214 | CE Loss: 0.1430\n",
      "Train Epoch: 012 Batch: 00038/00094 | Loss: 0.2890 | EWC Loss: 0.0214 | CE Loss: 0.2676\n",
      "Train Epoch: 012 Batch: 00039/00094 | Loss: 0.1436 | EWC Loss: 0.0214 | CE Loss: 0.1222\n",
      "Train Epoch: 012 Batch: 00040/00094 | Loss: 0.1838 | EWC Loss: 0.0214 | CE Loss: 0.1624\n",
      "Train Epoch: 012 Batch: 00041/00094 | Loss: 0.1096 | EWC Loss: 0.0214 | CE Loss: 0.0882\n",
      "Train Epoch: 012 Batch: 00042/00094 | Loss: 0.1575 | EWC Loss: 0.0214 | CE Loss: 0.1361\n",
      "Train Epoch: 012 Batch: 00043/00094 | Loss: 0.1083 | EWC Loss: 0.0214 | CE Loss: 0.0869\n",
      "Train Epoch: 012 Batch: 00044/00094 | Loss: 0.1696 | EWC Loss: 0.0214 | CE Loss: 0.1482\n",
      "Train Epoch: 012 Batch: 00045/00094 | Loss: 0.1589 | EWC Loss: 0.0214 | CE Loss: 0.1375\n",
      "Train Epoch: 012 Batch: 00046/00094 | Loss: 0.1846 | EWC Loss: 0.0214 | CE Loss: 0.1632\n",
      "Train Epoch: 012 Batch: 00047/00094 | Loss: 0.2002 | EWC Loss: 0.0214 | CE Loss: 0.1787\n",
      "Train Epoch: 012 Batch: 00048/00094 | Loss: 0.2144 | EWC Loss: 0.0214 | CE Loss: 0.1930\n",
      "Train Epoch: 012 Batch: 00049/00094 | Loss: 0.2267 | EWC Loss: 0.0214 | CE Loss: 0.2052\n",
      "Train Epoch: 012 Batch: 00050/00094 | Loss: 0.1473 | EWC Loss: 0.0214 | CE Loss: 0.1258\n",
      "Train Epoch: 012 Batch: 00051/00094 | Loss: 0.1299 | EWC Loss: 0.0214 | CE Loss: 0.1085\n",
      "Train Epoch: 012 Batch: 00052/00094 | Loss: 0.1226 | EWC Loss: 0.0214 | CE Loss: 0.1011\n",
      "Train Epoch: 012 Batch: 00053/00094 | Loss: 0.1316 | EWC Loss: 0.0214 | CE Loss: 0.1102\n",
      "Train Epoch: 012 Batch: 00054/00094 | Loss: 0.0674 | EWC Loss: 0.0214 | CE Loss: 0.0460\n",
      "Train Epoch: 012 Batch: 00055/00094 | Loss: 0.1575 | EWC Loss: 0.0214 | CE Loss: 0.1361\n",
      "Train Epoch: 012 Batch: 00056/00094 | Loss: 0.1384 | EWC Loss: 0.0214 | CE Loss: 0.1170\n",
      "Train Epoch: 012 Batch: 00057/00094 | Loss: 0.1373 | EWC Loss: 0.0214 | CE Loss: 0.1159\n",
      "Train Epoch: 012 Batch: 00058/00094 | Loss: 0.1052 | EWC Loss: 0.0214 | CE Loss: 0.0838\n",
      "Train Epoch: 012 Batch: 00059/00094 | Loss: 0.1617 | EWC Loss: 0.0214 | CE Loss: 0.1402\n",
      "Train Epoch: 012 Batch: 00060/00094 | Loss: 0.1171 | EWC Loss: 0.0214 | CE Loss: 0.0957\n",
      "Train Epoch: 012 Batch: 00061/00094 | Loss: 0.1279 | EWC Loss: 0.0214 | CE Loss: 0.1065\n",
      "Train Epoch: 012 Batch: 00062/00094 | Loss: 0.1147 | EWC Loss: 0.0214 | CE Loss: 0.0933\n",
      "Train Epoch: 012 Batch: 00063/00094 | Loss: 0.1146 | EWC Loss: 0.0214 | CE Loss: 0.0932\n",
      "Train Epoch: 012 Batch: 00064/00094 | Loss: 0.1686 | EWC Loss: 0.0214 | CE Loss: 0.1472\n",
      "Train Epoch: 012 Batch: 00065/00094 | Loss: 0.1014 | EWC Loss: 0.0214 | CE Loss: 0.0800\n",
      "Train Epoch: 012 Batch: 00066/00094 | Loss: 0.1811 | EWC Loss: 0.0214 | CE Loss: 0.1597\n",
      "Train Epoch: 012 Batch: 00067/00094 | Loss: 0.1506 | EWC Loss: 0.0214 | CE Loss: 0.1292\n",
      "Train Epoch: 012 Batch: 00068/00094 | Loss: 0.0925 | EWC Loss: 0.0214 | CE Loss: 0.0711\n",
      "Train Epoch: 012 Batch: 00069/00094 | Loss: 0.1635 | EWC Loss: 0.0214 | CE Loss: 0.1421\n",
      "Train Epoch: 012 Batch: 00070/00094 | Loss: 0.1413 | EWC Loss: 0.0214 | CE Loss: 0.1199\n",
      "Train Epoch: 012 Batch: 00071/00094 | Loss: 0.2045 | EWC Loss: 0.0214 | CE Loss: 0.1831\n",
      "Train Epoch: 012 Batch: 00072/00094 | Loss: 0.0939 | EWC Loss: 0.0214 | CE Loss: 0.0726\n",
      "Train Epoch: 012 Batch: 00073/00094 | Loss: 0.2475 | EWC Loss: 0.0214 | CE Loss: 0.2261\n",
      "Train Epoch: 012 Batch: 00074/00094 | Loss: 0.1206 | EWC Loss: 0.0214 | CE Loss: 0.0992\n",
      "Train Epoch: 012 Batch: 00075/00094 | Loss: 0.2446 | EWC Loss: 0.0214 | CE Loss: 0.2232\n",
      "Train Epoch: 012 Batch: 00076/00094 | Loss: 0.1501 | EWC Loss: 0.0214 | CE Loss: 0.1288\n",
      "Train Epoch: 012 Batch: 00077/00094 | Loss: 0.2397 | EWC Loss: 0.0214 | CE Loss: 0.2184\n",
      "Train Epoch: 012 Batch: 00078/00094 | Loss: 0.1218 | EWC Loss: 0.0214 | CE Loss: 0.1004\n",
      "Train Epoch: 012 Batch: 00079/00094 | Loss: 0.1150 | EWC Loss: 0.0214 | CE Loss: 0.0936\n",
      "Train Epoch: 012 Batch: 00080/00094 | Loss: 0.2090 | EWC Loss: 0.0214 | CE Loss: 0.1876\n",
      "Train Epoch: 012 Batch: 00081/00094 | Loss: 0.1199 | EWC Loss: 0.0214 | CE Loss: 0.0985\n",
      "Train Epoch: 012 Batch: 00082/00094 | Loss: 0.1435 | EWC Loss: 0.0214 | CE Loss: 0.1222\n",
      "Train Epoch: 012 Batch: 00083/00094 | Loss: 0.1959 | EWC Loss: 0.0214 | CE Loss: 0.1746\n",
      "Train Epoch: 012 Batch: 00084/00094 | Loss: 0.1415 | EWC Loss: 0.0214 | CE Loss: 0.1202\n",
      "Train Epoch: 012 Batch: 00085/00094 | Loss: 0.1920 | EWC Loss: 0.0213 | CE Loss: 0.1707\n",
      "Train Epoch: 012 Batch: 00086/00094 | Loss: 0.1481 | EWC Loss: 0.0214 | CE Loss: 0.1268\n",
      "Train Epoch: 012 Batch: 00087/00094 | Loss: 0.1217 | EWC Loss: 0.0214 | CE Loss: 0.1004\n",
      "Train Epoch: 012 Batch: 00088/00094 | Loss: 0.1344 | EWC Loss: 0.0214 | CE Loss: 0.1130\n",
      "Train Epoch: 012 Batch: 00089/00094 | Loss: 0.1170 | EWC Loss: 0.0214 | CE Loss: 0.0957\n",
      "Train Epoch: 012 Batch: 00090/00094 | Loss: 0.1216 | EWC Loss: 0.0214 | CE Loss: 0.1002\n",
      "Train Epoch: 012 Batch: 00091/00094 | Loss: 0.1679 | EWC Loss: 0.0214 | CE Loss: 0.1466\n",
      "Train Epoch: 012 Batch: 00092/00094 | Loss: 0.1299 | EWC Loss: 0.0214 | CE Loss: 0.1085\n",
      "Train Epoch: 012 Batch: 00093/00094 | Loss: 0.1476 | EWC Loss: 0.0214 | CE Loss: 0.1262\n",
      "Train Epoch: 012 Batch: 00094/00094 | Loss: 0.2516 | EWC Loss: 0.0214 | CE Loss: 0.2302\n",
      "Train Epoch: 011 |Acc 95.35000 | Loss: 0.15371 | Task Loss 0.13231 | EWC Loss: 0.02140\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1243 | acc:95.6500\n",
      "[VAL Acc] Target: 95.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.0950 | acc:51.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0993 | acc:60.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1497 | acc:54.1985\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 54.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1606 | acc:56.7398\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.74%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6648 | acc:69.9630\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 69.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5771 | acc:76.4498\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9016 | acc:58.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.94%\n",
      "[VAL Acc] Avg 65.41%\n",
      "\n",
      "\n",
      "---------- Starting epoch 12 ----------\n",
      "Train Epoch: 013 Batch: 00001/00094 | Loss: 0.2020 | EWC Loss: 0.0213 | CE Loss: 0.1806\n",
      "Train Epoch: 013 Batch: 00002/00094 | Loss: 0.1106 | EWC Loss: 0.0213 | CE Loss: 0.0892\n",
      "Train Epoch: 013 Batch: 00003/00094 | Loss: 0.1969 | EWC Loss: 0.0213 | CE Loss: 0.1756\n",
      "Train Epoch: 013 Batch: 00004/00094 | Loss: 0.1548 | EWC Loss: 0.0213 | CE Loss: 0.1335\n",
      "Train Epoch: 013 Batch: 00005/00094 | Loss: 0.0844 | EWC Loss: 0.0213 | CE Loss: 0.0631\n",
      "Train Epoch: 013 Batch: 00006/00094 | Loss: 0.1519 | EWC Loss: 0.0213 | CE Loss: 0.1306\n",
      "Train Epoch: 013 Batch: 00007/00094 | Loss: 0.1500 | EWC Loss: 0.0213 | CE Loss: 0.1287\n",
      "Train Epoch: 013 Batch: 00008/00094 | Loss: 0.1250 | EWC Loss: 0.0213 | CE Loss: 0.1037\n",
      "Train Epoch: 013 Batch: 00009/00094 | Loss: 0.2079 | EWC Loss: 0.0213 | CE Loss: 0.1866\n",
      "Train Epoch: 013 Batch: 00010/00094 | Loss: 0.1189 | EWC Loss: 0.0213 | CE Loss: 0.0975\n",
      "Train Epoch: 013 Batch: 00011/00094 | Loss: 0.1244 | EWC Loss: 0.0213 | CE Loss: 0.1031\n",
      "Train Epoch: 013 Batch: 00012/00094 | Loss: 0.1044 | EWC Loss: 0.0213 | CE Loss: 0.0831\n",
      "Train Epoch: 013 Batch: 00013/00094 | Loss: 0.1369 | EWC Loss: 0.0213 | CE Loss: 0.1156\n",
      "Train Epoch: 013 Batch: 00014/00094 | Loss: 0.0873 | EWC Loss: 0.0213 | CE Loss: 0.0661\n",
      "Train Epoch: 013 Batch: 00015/00094 | Loss: 0.1059 | EWC Loss: 0.0213 | CE Loss: 0.0846\n",
      "Train Epoch: 013 Batch: 00016/00094 | Loss: 0.1571 | EWC Loss: 0.0213 | CE Loss: 0.1358\n",
      "Train Epoch: 013 Batch: 00017/00094 | Loss: 0.1816 | EWC Loss: 0.0213 | CE Loss: 0.1603\n",
      "Train Epoch: 013 Batch: 00018/00094 | Loss: 0.1321 | EWC Loss: 0.0212 | CE Loss: 0.1109\n",
      "Train Epoch: 013 Batch: 00019/00094 | Loss: 0.1875 | EWC Loss: 0.0213 | CE Loss: 0.1662\n",
      "Train Epoch: 013 Batch: 00020/00094 | Loss: 0.1442 | EWC Loss: 0.0212 | CE Loss: 0.1229\n",
      "Train Epoch: 013 Batch: 00021/00094 | Loss: 0.2071 | EWC Loss: 0.0213 | CE Loss: 0.1858\n",
      "Train Epoch: 013 Batch: 00022/00094 | Loss: 0.1710 | EWC Loss: 0.0213 | CE Loss: 0.1497\n",
      "Train Epoch: 013 Batch: 00023/00094 | Loss: 0.1768 | EWC Loss: 0.0213 | CE Loss: 0.1555\n",
      "Train Epoch: 013 Batch: 00024/00094 | Loss: 0.1055 | EWC Loss: 0.0213 | CE Loss: 0.0842\n",
      "Train Epoch: 013 Batch: 00025/00094 | Loss: 0.1448 | EWC Loss: 0.0213 | CE Loss: 0.1235\n",
      "Train Epoch: 013 Batch: 00026/00094 | Loss: 0.1958 | EWC Loss: 0.0213 | CE Loss: 0.1744\n",
      "Train Epoch: 013 Batch: 00027/00094 | Loss: 0.1285 | EWC Loss: 0.0213 | CE Loss: 0.1072\n",
      "Train Epoch: 013 Batch: 00028/00094 | Loss: 0.1608 | EWC Loss: 0.0213 | CE Loss: 0.1395\n",
      "Train Epoch: 013 Batch: 00029/00094 | Loss: 0.1486 | EWC Loss: 0.0213 | CE Loss: 0.1273\n",
      "Train Epoch: 013 Batch: 00030/00094 | Loss: 0.2450 | EWC Loss: 0.0213 | CE Loss: 0.2237\n",
      "Train Epoch: 013 Batch: 00031/00094 | Loss: 0.1018 | EWC Loss: 0.0213 | CE Loss: 0.0805\n",
      "Train Epoch: 013 Batch: 00032/00094 | Loss: 0.1213 | EWC Loss: 0.0213 | CE Loss: 0.0999\n",
      "Train Epoch: 013 Batch: 00033/00094 | Loss: 0.1172 | EWC Loss: 0.0213 | CE Loss: 0.0958\n",
      "Train Epoch: 013 Batch: 00034/00094 | Loss: 0.0964 | EWC Loss: 0.0213 | CE Loss: 0.0751\n",
      "Train Epoch: 013 Batch: 00035/00094 | Loss: 0.1072 | EWC Loss: 0.0213 | CE Loss: 0.0859\n",
      "Train Epoch: 013 Batch: 00036/00094 | Loss: 0.1350 | EWC Loss: 0.0213 | CE Loss: 0.1138\n",
      "Train Epoch: 013 Batch: 00037/00094 | Loss: 0.1754 | EWC Loss: 0.0213 | CE Loss: 0.1541\n",
      "Train Epoch: 013 Batch: 00038/00094 | Loss: 0.0658 | EWC Loss: 0.0213 | CE Loss: 0.0445\n",
      "Train Epoch: 013 Batch: 00039/00094 | Loss: 0.1472 | EWC Loss: 0.0213 | CE Loss: 0.1259\n",
      "Train Epoch: 013 Batch: 00040/00094 | Loss: 0.1683 | EWC Loss: 0.0213 | CE Loss: 0.1470\n",
      "Train Epoch: 013 Batch: 00041/00094 | Loss: 0.1236 | EWC Loss: 0.0213 | CE Loss: 0.1022\n",
      "Train Epoch: 013 Batch: 00042/00094 | Loss: 0.1403 | EWC Loss: 0.0213 | CE Loss: 0.1190\n",
      "Train Epoch: 013 Batch: 00043/00094 | Loss: 0.2131 | EWC Loss: 0.0213 | CE Loss: 0.1918\n",
      "Train Epoch: 013 Batch: 00044/00094 | Loss: 0.1654 | EWC Loss: 0.0213 | CE Loss: 0.1440\n",
      "Train Epoch: 013 Batch: 00045/00094 | Loss: 0.1947 | EWC Loss: 0.0213 | CE Loss: 0.1734\n",
      "Train Epoch: 013 Batch: 00046/00094 | Loss: 0.0834 | EWC Loss: 0.0213 | CE Loss: 0.0621\n",
      "Train Epoch: 013 Batch: 00047/00094 | Loss: 0.1516 | EWC Loss: 0.0213 | CE Loss: 0.1304\n",
      "Train Epoch: 013 Batch: 00048/00094 | Loss: 0.2069 | EWC Loss: 0.0213 | CE Loss: 0.1856\n",
      "Train Epoch: 013 Batch: 00049/00094 | Loss: 0.1089 | EWC Loss: 0.0213 | CE Loss: 0.0875\n",
      "Train Epoch: 013 Batch: 00050/00094 | Loss: 0.2527 | EWC Loss: 0.0213 | CE Loss: 0.2313\n",
      "Train Epoch: 013 Batch: 00051/00094 | Loss: 0.2203 | EWC Loss: 0.0214 | CE Loss: 0.1989\n",
      "Train Epoch: 013 Batch: 00052/00094 | Loss: 0.1186 | EWC Loss: 0.0214 | CE Loss: 0.0972\n",
      "Train Epoch: 013 Batch: 00053/00094 | Loss: 0.1941 | EWC Loss: 0.0215 | CE Loss: 0.1727\n",
      "Train Epoch: 013 Batch: 00054/00094 | Loss: 0.1016 | EWC Loss: 0.0215 | CE Loss: 0.0802\n",
      "Train Epoch: 013 Batch: 00055/00094 | Loss: 0.1565 | EWC Loss: 0.0215 | CE Loss: 0.1351\n",
      "Train Epoch: 013 Batch: 00056/00094 | Loss: 0.1737 | EWC Loss: 0.0215 | CE Loss: 0.1523\n",
      "Train Epoch: 013 Batch: 00057/00094 | Loss: 0.1109 | EWC Loss: 0.0215 | CE Loss: 0.0894\n",
      "Train Epoch: 013 Batch: 00058/00094 | Loss: 0.1907 | EWC Loss: 0.0214 | CE Loss: 0.1693\n",
      "Train Epoch: 013 Batch: 00059/00094 | Loss: 0.1477 | EWC Loss: 0.0215 | CE Loss: 0.1263\n",
      "Train Epoch: 013 Batch: 00060/00094 | Loss: 0.1618 | EWC Loss: 0.0215 | CE Loss: 0.1403\n",
      "Train Epoch: 013 Batch: 00061/00094 | Loss: 0.2296 | EWC Loss: 0.0215 | CE Loss: 0.2080\n",
      "Train Epoch: 013 Batch: 00062/00094 | Loss: 0.1534 | EWC Loss: 0.0215 | CE Loss: 0.1319\n",
      "Train Epoch: 013 Batch: 00063/00094 | Loss: 0.1378 | EWC Loss: 0.0215 | CE Loss: 0.1162\n",
      "Train Epoch: 013 Batch: 00064/00094 | Loss: 0.0842 | EWC Loss: 0.0215 | CE Loss: 0.0627\n",
      "Train Epoch: 013 Batch: 00065/00094 | Loss: 0.1640 | EWC Loss: 0.0215 | CE Loss: 0.1425\n",
      "Train Epoch: 013 Batch: 00066/00094 | Loss: 0.1755 | EWC Loss: 0.0216 | CE Loss: 0.1540\n",
      "Train Epoch: 013 Batch: 00067/00094 | Loss: 0.2115 | EWC Loss: 0.0215 | CE Loss: 0.1900\n",
      "Train Epoch: 013 Batch: 00068/00094 | Loss: 0.1543 | EWC Loss: 0.0215 | CE Loss: 0.1328\n",
      "Train Epoch: 013 Batch: 00069/00094 | Loss: 0.1288 | EWC Loss: 0.0215 | CE Loss: 0.1073\n",
      "Train Epoch: 013 Batch: 00070/00094 | Loss: 0.0980 | EWC Loss: 0.0215 | CE Loss: 0.0765\n",
      "Train Epoch: 013 Batch: 00071/00094 | Loss: 0.0744 | EWC Loss: 0.0214 | CE Loss: 0.0529\n",
      "Train Epoch: 013 Batch: 00072/00094 | Loss: 0.1217 | EWC Loss: 0.0214 | CE Loss: 0.1002\n",
      "Train Epoch: 013 Batch: 00073/00094 | Loss: 0.2380 | EWC Loss: 0.0215 | CE Loss: 0.2165\n",
      "Train Epoch: 013 Batch: 00074/00094 | Loss: 0.1207 | EWC Loss: 0.0215 | CE Loss: 0.0992\n",
      "Train Epoch: 013 Batch: 00075/00094 | Loss: 0.0882 | EWC Loss: 0.0215 | CE Loss: 0.0667\n",
      "Train Epoch: 013 Batch: 00076/00094 | Loss: 0.1973 | EWC Loss: 0.0215 | CE Loss: 0.1759\n",
      "Train Epoch: 013 Batch: 00077/00094 | Loss: 0.1723 | EWC Loss: 0.0215 | CE Loss: 0.1508\n",
      "Train Epoch: 013 Batch: 00078/00094 | Loss: 0.1403 | EWC Loss: 0.0215 | CE Loss: 0.1188\n",
      "Train Epoch: 013 Batch: 00079/00094 | Loss: 0.1228 | EWC Loss: 0.0215 | CE Loss: 0.1013\n",
      "Train Epoch: 013 Batch: 00080/00094 | Loss: 0.2266 | EWC Loss: 0.0215 | CE Loss: 0.2051\n",
      "Train Epoch: 013 Batch: 00081/00094 | Loss: 0.2637 | EWC Loss: 0.0215 | CE Loss: 0.2422\n",
      "Train Epoch: 013 Batch: 00082/00094 | Loss: 0.1810 | EWC Loss: 0.0215 | CE Loss: 0.1595\n",
      "Train Epoch: 013 Batch: 00083/00094 | Loss: 0.1407 | EWC Loss: 0.0215 | CE Loss: 0.1192\n",
      "Train Epoch: 013 Batch: 00084/00094 | Loss: 0.1385 | EWC Loss: 0.0215 | CE Loss: 0.1170\n",
      "Train Epoch: 013 Batch: 00085/00094 | Loss: 0.1633 | EWC Loss: 0.0215 | CE Loss: 0.1418\n",
      "Train Epoch: 013 Batch: 00086/00094 | Loss: 0.1217 | EWC Loss: 0.0215 | CE Loss: 0.1002\n",
      "Train Epoch: 013 Batch: 00087/00094 | Loss: 0.1574 | EWC Loss: 0.0215 | CE Loss: 0.1359\n",
      "Train Epoch: 013 Batch: 00088/00094 | Loss: 0.1731 | EWC Loss: 0.0215 | CE Loss: 0.1516\n",
      "Train Epoch: 013 Batch: 00089/00094 | Loss: 0.1437 | EWC Loss: 0.0215 | CE Loss: 0.1222\n",
      "Train Epoch: 013 Batch: 00090/00094 | Loss: 0.1650 | EWC Loss: 0.0215 | CE Loss: 0.1435\n",
      "Train Epoch: 013 Batch: 00091/00094 | Loss: 0.1420 | EWC Loss: 0.0215 | CE Loss: 0.1205\n",
      "Train Epoch: 013 Batch: 00092/00094 | Loss: 0.1176 | EWC Loss: 0.0215 | CE Loss: 0.0961\n",
      "Train Epoch: 013 Batch: 00093/00094 | Loss: 0.1818 | EWC Loss: 0.0215 | CE Loss: 0.1603\n",
      "Train Epoch: 013 Batch: 00094/00094 | Loss: 0.0917 | EWC Loss: 0.0215 | CE Loss: 0.0702\n",
      "Train Epoch: 012 |Acc 95.33333 | Loss: 0.15128 | Task Loss 0.12989 | EWC Loss: 0.02139\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1202 | acc:95.5500\n",
      "[VAL Acc] Target: 95.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.1583 | acc:50.8000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1134 | acc:60.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1642 | acc:52.0992\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 52.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2441 | acc:55.4075\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6692 | acc:70.0555\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5859 | acc:75.6270\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9307 | acc:57.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.63%\n",
      "[VAL Acc] Avg 64.69%\n",
      "\n",
      "\n",
      "---------- Starting epoch 13 ----------\n",
      "Train Epoch: 014 Batch: 00001/00094 | Loss: 0.2437 | EWC Loss: 0.0215 | CE Loss: 0.2222\n",
      "Train Epoch: 014 Batch: 00002/00094 | Loss: 0.1800 | EWC Loss: 0.0215 | CE Loss: 0.1584\n",
      "Train Epoch: 014 Batch: 00003/00094 | Loss: 0.2155 | EWC Loss: 0.0216 | CE Loss: 0.1939\n",
      "Train Epoch: 014 Batch: 00004/00094 | Loss: 0.1229 | EWC Loss: 0.0216 | CE Loss: 0.1013\n",
      "Train Epoch: 014 Batch: 00005/00094 | Loss: 0.2855 | EWC Loss: 0.0216 | CE Loss: 0.2639\n",
      "Train Epoch: 014 Batch: 00006/00094 | Loss: 0.2090 | EWC Loss: 0.0216 | CE Loss: 0.1874\n",
      "Train Epoch: 014 Batch: 00007/00094 | Loss: 0.2660 | EWC Loss: 0.0216 | CE Loss: 0.2444\n",
      "Train Epoch: 014 Batch: 00008/00094 | Loss: 0.1306 | EWC Loss: 0.0216 | CE Loss: 0.1089\n",
      "Train Epoch: 014 Batch: 00009/00094 | Loss: 0.1719 | EWC Loss: 0.0216 | CE Loss: 0.1503\n",
      "Train Epoch: 014 Batch: 00010/00094 | Loss: 0.1000 | EWC Loss: 0.0216 | CE Loss: 0.0785\n",
      "Train Epoch: 014 Batch: 00011/00094 | Loss: 0.1896 | EWC Loss: 0.0215 | CE Loss: 0.1682\n",
      "Train Epoch: 014 Batch: 00012/00094 | Loss: 0.1367 | EWC Loss: 0.0215 | CE Loss: 0.1152\n",
      "Train Epoch: 014 Batch: 00013/00094 | Loss: 0.1429 | EWC Loss: 0.0215 | CE Loss: 0.1213\n",
      "Train Epoch: 014 Batch: 00014/00094 | Loss: 0.1070 | EWC Loss: 0.0215 | CE Loss: 0.0854\n",
      "Train Epoch: 014 Batch: 00015/00094 | Loss: 0.1330 | EWC Loss: 0.0215 | CE Loss: 0.1115\n",
      "Train Epoch: 014 Batch: 00016/00094 | Loss: 0.2903 | EWC Loss: 0.0215 | CE Loss: 0.2688\n",
      "Train Epoch: 014 Batch: 00017/00094 | Loss: 0.1454 | EWC Loss: 0.0215 | CE Loss: 0.1239\n",
      "Train Epoch: 014 Batch: 00018/00094 | Loss: 0.1119 | EWC Loss: 0.0215 | CE Loss: 0.0904\n",
      "Train Epoch: 014 Batch: 00019/00094 | Loss: 0.1704 | EWC Loss: 0.0214 | CE Loss: 0.1489\n",
      "Train Epoch: 014 Batch: 00020/00094 | Loss: 0.2076 | EWC Loss: 0.0215 | CE Loss: 0.1861\n",
      "Train Epoch: 014 Batch: 00021/00094 | Loss: 0.1336 | EWC Loss: 0.0215 | CE Loss: 0.1121\n",
      "Train Epoch: 014 Batch: 00022/00094 | Loss: 0.1506 | EWC Loss: 0.0214 | CE Loss: 0.1292\n",
      "Train Epoch: 014 Batch: 00023/00094 | Loss: 0.1775 | EWC Loss: 0.0215 | CE Loss: 0.1560\n",
      "Train Epoch: 014 Batch: 00024/00094 | Loss: 0.1387 | EWC Loss: 0.0215 | CE Loss: 0.1172\n",
      "Train Epoch: 014 Batch: 00025/00094 | Loss: 0.0949 | EWC Loss: 0.0214 | CE Loss: 0.0735\n",
      "Train Epoch: 014 Batch: 00026/00094 | Loss: 0.1273 | EWC Loss: 0.0214 | CE Loss: 0.1058\n",
      "Train Epoch: 014 Batch: 00027/00094 | Loss: 0.1878 | EWC Loss: 0.0215 | CE Loss: 0.1663\n",
      "Train Epoch: 014 Batch: 00028/00094 | Loss: 0.1062 | EWC Loss: 0.0214 | CE Loss: 0.0847\n",
      "Train Epoch: 014 Batch: 00029/00094 | Loss: 0.1387 | EWC Loss: 0.0214 | CE Loss: 0.1173\n",
      "Train Epoch: 014 Batch: 00030/00094 | Loss: 0.2361 | EWC Loss: 0.0214 | CE Loss: 0.2148\n",
      "Train Epoch: 014 Batch: 00031/00094 | Loss: 0.2552 | EWC Loss: 0.0215 | CE Loss: 0.2337\n",
      "Train Epoch: 014 Batch: 00032/00094 | Loss: 0.1182 | EWC Loss: 0.0214 | CE Loss: 0.0967\n",
      "Train Epoch: 014 Batch: 00033/00094 | Loss: 0.1166 | EWC Loss: 0.0214 | CE Loss: 0.0952\n",
      "Train Epoch: 014 Batch: 00034/00094 | Loss: 0.1738 | EWC Loss: 0.0214 | CE Loss: 0.1524\n",
      "Train Epoch: 014 Batch: 00035/00094 | Loss: 0.1410 | EWC Loss: 0.0214 | CE Loss: 0.1195\n",
      "Train Epoch: 014 Batch: 00036/00094 | Loss: 0.1377 | EWC Loss: 0.0214 | CE Loss: 0.1163\n",
      "Train Epoch: 014 Batch: 00037/00094 | Loss: 0.1161 | EWC Loss: 0.0214 | CE Loss: 0.0947\n",
      "Train Epoch: 014 Batch: 00038/00094 | Loss: 0.1787 | EWC Loss: 0.0213 | CE Loss: 0.1573\n",
      "Train Epoch: 014 Batch: 00039/00094 | Loss: 0.1355 | EWC Loss: 0.0214 | CE Loss: 0.1141\n",
      "Train Epoch: 014 Batch: 00040/00094 | Loss: 0.2097 | EWC Loss: 0.0215 | CE Loss: 0.1882\n",
      "Train Epoch: 014 Batch: 00041/00094 | Loss: 0.1019 | EWC Loss: 0.0214 | CE Loss: 0.0805\n",
      "Train Epoch: 014 Batch: 00042/00094 | Loss: 0.2271 | EWC Loss: 0.0214 | CE Loss: 0.2057\n",
      "Train Epoch: 014 Batch: 00043/00094 | Loss: 0.0940 | EWC Loss: 0.0215 | CE Loss: 0.0725\n",
      "Train Epoch: 014 Batch: 00044/00094 | Loss: 0.1338 | EWC Loss: 0.0215 | CE Loss: 0.1123\n",
      "Train Epoch: 014 Batch: 00045/00094 | Loss: 0.1057 | EWC Loss: 0.0215 | CE Loss: 0.0842\n",
      "Train Epoch: 014 Batch: 00046/00094 | Loss: 0.1412 | EWC Loss: 0.0215 | CE Loss: 0.1198\n",
      "Train Epoch: 014 Batch: 00047/00094 | Loss: 0.1204 | EWC Loss: 0.0215 | CE Loss: 0.0989\n",
      "Train Epoch: 014 Batch: 00048/00094 | Loss: 0.1476 | EWC Loss: 0.0214 | CE Loss: 0.1261\n",
      "Train Epoch: 014 Batch: 00049/00094 | Loss: 0.1104 | EWC Loss: 0.0215 | CE Loss: 0.0889\n",
      "Train Epoch: 014 Batch: 00050/00094 | Loss: 0.0991 | EWC Loss: 0.0215 | CE Loss: 0.0776\n",
      "Train Epoch: 014 Batch: 00051/00094 | Loss: 0.1260 | EWC Loss: 0.0215 | CE Loss: 0.1045\n",
      "Train Epoch: 014 Batch: 00052/00094 | Loss: 0.1391 | EWC Loss: 0.0215 | CE Loss: 0.1176\n",
      "Train Epoch: 014 Batch: 00053/00094 | Loss: 0.1307 | EWC Loss: 0.0215 | CE Loss: 0.1092\n",
      "Train Epoch: 014 Batch: 00054/00094 | Loss: 0.1582 | EWC Loss: 0.0217 | CE Loss: 0.1365\n",
      "Train Epoch: 014 Batch: 00055/00094 | Loss: 0.1180 | EWC Loss: 0.0217 | CE Loss: 0.0963\n",
      "Train Epoch: 014 Batch: 00056/00094 | Loss: 0.1114 | EWC Loss: 0.0218 | CE Loss: 0.0895\n",
      "Train Epoch: 014 Batch: 00057/00094 | Loss: 0.1343 | EWC Loss: 0.0218 | CE Loss: 0.1125\n",
      "Train Epoch: 014 Batch: 00058/00094 | Loss: 0.0920 | EWC Loss: 0.0218 | CE Loss: 0.0702\n",
      "Train Epoch: 014 Batch: 00059/00094 | Loss: 0.1978 | EWC Loss: 0.0217 | CE Loss: 0.1761\n",
      "Train Epoch: 014 Batch: 00060/00094 | Loss: 0.1436 | EWC Loss: 0.0217 | CE Loss: 0.1220\n",
      "Train Epoch: 014 Batch: 00061/00094 | Loss: 0.1146 | EWC Loss: 0.0216 | CE Loss: 0.0930\n",
      "Train Epoch: 014 Batch: 00062/00094 | Loss: 0.1369 | EWC Loss: 0.0217 | CE Loss: 0.1153\n",
      "Train Epoch: 014 Batch: 00063/00094 | Loss: 0.1807 | EWC Loss: 0.0216 | CE Loss: 0.1591\n",
      "Train Epoch: 014 Batch: 00064/00094 | Loss: 0.1501 | EWC Loss: 0.0216 | CE Loss: 0.1286\n",
      "Train Epoch: 014 Batch: 00065/00094 | Loss: 0.2312 | EWC Loss: 0.0216 | CE Loss: 0.2097\n",
      "Train Epoch: 014 Batch: 00066/00094 | Loss: 0.1696 | EWC Loss: 0.0215 | CE Loss: 0.1482\n",
      "Train Epoch: 014 Batch: 00067/00094 | Loss: 0.1237 | EWC Loss: 0.0215 | CE Loss: 0.1022\n",
      "Train Epoch: 014 Batch: 00068/00094 | Loss: 0.1088 | EWC Loss: 0.0215 | CE Loss: 0.0873\n",
      "Train Epoch: 014 Batch: 00069/00094 | Loss: 0.1238 | EWC Loss: 0.0214 | CE Loss: 0.1024\n",
      "Train Epoch: 014 Batch: 00070/00094 | Loss: 0.1736 | EWC Loss: 0.0214 | CE Loss: 0.1522\n",
      "Train Epoch: 014 Batch: 00071/00094 | Loss: 0.1313 | EWC Loss: 0.0215 | CE Loss: 0.1098\n",
      "Train Epoch: 014 Batch: 00072/00094 | Loss: 0.1441 | EWC Loss: 0.0215 | CE Loss: 0.1226\n",
      "Train Epoch: 014 Batch: 00073/00094 | Loss: 0.1335 | EWC Loss: 0.0216 | CE Loss: 0.1119\n",
      "Train Epoch: 014 Batch: 00074/00094 | Loss: 0.1852 | EWC Loss: 0.0217 | CE Loss: 0.1635\n",
      "Train Epoch: 014 Batch: 00075/00094 | Loss: 0.0989 | EWC Loss: 0.0217 | CE Loss: 0.0772\n",
      "Train Epoch: 014 Batch: 00076/00094 | Loss: 0.1345 | EWC Loss: 0.0217 | CE Loss: 0.1128\n",
      "Train Epoch: 014 Batch: 00077/00094 | Loss: 0.0889 | EWC Loss: 0.0217 | CE Loss: 0.0671\n",
      "Train Epoch: 014 Batch: 00078/00094 | Loss: 0.2647 | EWC Loss: 0.0217 | CE Loss: 0.2430\n",
      "Train Epoch: 014 Batch: 00079/00094 | Loss: 0.2028 | EWC Loss: 0.0217 | CE Loss: 0.1811\n",
      "Train Epoch: 014 Batch: 00080/00094 | Loss: 0.0916 | EWC Loss: 0.0216 | CE Loss: 0.0700\n",
      "Train Epoch: 014 Batch: 00081/00094 | Loss: 0.1562 | EWC Loss: 0.0216 | CE Loss: 0.1345\n",
      "Train Epoch: 014 Batch: 00082/00094 | Loss: 0.1733 | EWC Loss: 0.0216 | CE Loss: 0.1517\n",
      "Train Epoch: 014 Batch: 00083/00094 | Loss: 0.1347 | EWC Loss: 0.0215 | CE Loss: 0.1132\n",
      "Train Epoch: 014 Batch: 00084/00094 | Loss: 0.1136 | EWC Loss: 0.0215 | CE Loss: 0.0921\n",
      "Train Epoch: 014 Batch: 00085/00094 | Loss: 0.1793 | EWC Loss: 0.0215 | CE Loss: 0.1578\n",
      "Train Epoch: 014 Batch: 00086/00094 | Loss: 0.1207 | EWC Loss: 0.0214 | CE Loss: 0.0992\n",
      "Train Epoch: 014 Batch: 00087/00094 | Loss: 0.1495 | EWC Loss: 0.0214 | CE Loss: 0.1281\n",
      "Train Epoch: 014 Batch: 00088/00094 | Loss: 0.2453 | EWC Loss: 0.0214 | CE Loss: 0.2239\n",
      "Train Epoch: 014 Batch: 00089/00094 | Loss: 0.0854 | EWC Loss: 0.0215 | CE Loss: 0.0639\n",
      "Train Epoch: 014 Batch: 00090/00094 | Loss: 0.1750 | EWC Loss: 0.0215 | CE Loss: 0.1535\n",
      "Train Epoch: 014 Batch: 00091/00094 | Loss: 0.1613 | EWC Loss: 0.0215 | CE Loss: 0.1398\n",
      "Train Epoch: 014 Batch: 00092/00094 | Loss: 0.1625 | EWC Loss: 0.0215 | CE Loss: 0.1409\n",
      "Train Epoch: 014 Batch: 00093/00094 | Loss: 0.1652 | EWC Loss: 0.0215 | CE Loss: 0.1437\n",
      "Train Epoch: 014 Batch: 00094/00094 | Loss: 0.1649 | EWC Loss: 0.0217 | CE Loss: 0.1433\n",
      "Train Epoch: 013 |Acc 95.31667 | Loss: 0.15363 | Task Loss 0.13211 | EWC Loss: 0.02153\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1088 | acc:96.1500\n",
      "[VAL Acc] Target: 96.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.1349 | acc:50.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1257 | acc:62.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 62.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1953 | acc:55.9160\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 55.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2104 | acc:54.5455\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6419 | acc:71.4418\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5273 | acc:77.0768\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 77.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9533 | acc:56.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.81%\n",
      "[VAL Acc] Avg 65.52%\n",
      "\n",
      "\n",
      "---------- Starting epoch 14 ----------\n",
      "Train Epoch: 015 Batch: 00001/00094 | Loss: 0.1210 | EWC Loss: 0.0217 | CE Loss: 0.0994\n",
      "Train Epoch: 015 Batch: 00002/00094 | Loss: 0.1099 | EWC Loss: 0.0217 | CE Loss: 0.0882\n",
      "Train Epoch: 015 Batch: 00003/00094 | Loss: 0.1179 | EWC Loss: 0.0216 | CE Loss: 0.0964\n",
      "Train Epoch: 015 Batch: 00004/00094 | Loss: 0.1348 | EWC Loss: 0.0215 | CE Loss: 0.1133\n",
      "Train Epoch: 015 Batch: 00005/00094 | Loss: 0.1740 | EWC Loss: 0.0215 | CE Loss: 0.1524\n",
      "Train Epoch: 015 Batch: 00006/00094 | Loss: 0.2254 | EWC Loss: 0.0216 | CE Loss: 0.2038\n",
      "Train Epoch: 015 Batch: 00007/00094 | Loss: 0.1596 | EWC Loss: 0.0216 | CE Loss: 0.1379\n",
      "Train Epoch: 015 Batch: 00008/00094 | Loss: 0.1284 | EWC Loss: 0.0216 | CE Loss: 0.1067\n",
      "Train Epoch: 015 Batch: 00009/00094 | Loss: 0.1193 | EWC Loss: 0.0218 | CE Loss: 0.0975\n",
      "Train Epoch: 015 Batch: 00010/00094 | Loss: 0.1225 | EWC Loss: 0.0217 | CE Loss: 0.1008\n",
      "Train Epoch: 015 Batch: 00011/00094 | Loss: 0.1249 | EWC Loss: 0.0217 | CE Loss: 0.1032\n",
      "Train Epoch: 015 Batch: 00012/00094 | Loss: 0.1588 | EWC Loss: 0.0217 | CE Loss: 0.1371\n",
      "Train Epoch: 015 Batch: 00013/00094 | Loss: 0.1985 | EWC Loss: 0.0215 | CE Loss: 0.1770\n",
      "Train Epoch: 015 Batch: 00014/00094 | Loss: 0.2040 | EWC Loss: 0.0215 | CE Loss: 0.1825\n",
      "Train Epoch: 015 Batch: 00015/00094 | Loss: 0.2143 | EWC Loss: 0.0216 | CE Loss: 0.1927\n",
      "Train Epoch: 015 Batch: 00016/00094 | Loss: 0.1290 | EWC Loss: 0.0217 | CE Loss: 0.1073\n",
      "Train Epoch: 015 Batch: 00017/00094 | Loss: 0.1632 | EWC Loss: 0.0217 | CE Loss: 0.1414\n",
      "Train Epoch: 015 Batch: 00018/00094 | Loss: 0.1677 | EWC Loss: 0.0218 | CE Loss: 0.1459\n",
      "Train Epoch: 015 Batch: 00019/00094 | Loss: 0.1582 | EWC Loss: 0.0219 | CE Loss: 0.1363\n",
      "Train Epoch: 015 Batch: 00020/00094 | Loss: 0.1674 | EWC Loss: 0.0220 | CE Loss: 0.1454\n",
      "Train Epoch: 015 Batch: 00021/00094 | Loss: 0.1593 | EWC Loss: 0.0222 | CE Loss: 0.1371\n",
      "Train Epoch: 015 Batch: 00022/00094 | Loss: 0.1814 | EWC Loss: 0.0223 | CE Loss: 0.1591\n",
      "Train Epoch: 015 Batch: 00023/00094 | Loss: 0.1509 | EWC Loss: 0.0224 | CE Loss: 0.1284\n",
      "Train Epoch: 015 Batch: 00024/00094 | Loss: 0.1548 | EWC Loss: 0.0225 | CE Loss: 0.1324\n",
      "Train Epoch: 015 Batch: 00025/00094 | Loss: 0.1271 | EWC Loss: 0.0223 | CE Loss: 0.1048\n",
      "Train Epoch: 015 Batch: 00026/00094 | Loss: 0.1022 | EWC Loss: 0.0220 | CE Loss: 0.0802\n",
      "Train Epoch: 015 Batch: 00027/00094 | Loss: 0.1338 | EWC Loss: 0.0220 | CE Loss: 0.1118\n",
      "Train Epoch: 015 Batch: 00028/00094 | Loss: 0.3764 | EWC Loss: 0.0220 | CE Loss: 0.3543\n",
      "Train Epoch: 015 Batch: 00029/00094 | Loss: 0.1144 | EWC Loss: 0.0222 | CE Loss: 0.0923\n",
      "Train Epoch: 015 Batch: 00030/00094 | Loss: 0.1971 | EWC Loss: 0.0221 | CE Loss: 0.1750\n",
      "Train Epoch: 015 Batch: 00031/00094 | Loss: 0.1192 | EWC Loss: 0.0220 | CE Loss: 0.0972\n",
      "Train Epoch: 015 Batch: 00032/00094 | Loss: 0.1090 | EWC Loss: 0.0220 | CE Loss: 0.0870\n",
      "Train Epoch: 015 Batch: 00033/00094 | Loss: 0.1506 | EWC Loss: 0.0219 | CE Loss: 0.1287\n",
      "Train Epoch: 015 Batch: 00034/00094 | Loss: 0.1091 | EWC Loss: 0.0218 | CE Loss: 0.0872\n",
      "Train Epoch: 015 Batch: 00035/00094 | Loss: 0.1323 | EWC Loss: 0.0217 | CE Loss: 0.1106\n",
      "Train Epoch: 015 Batch: 00036/00094 | Loss: 0.1549 | EWC Loss: 0.0217 | CE Loss: 0.1332\n",
      "Train Epoch: 015 Batch: 00037/00094 | Loss: 0.1118 | EWC Loss: 0.0217 | CE Loss: 0.0901\n",
      "Train Epoch: 015 Batch: 00038/00094 | Loss: 0.0960 | EWC Loss: 0.0216 | CE Loss: 0.0744\n",
      "Train Epoch: 015 Batch: 00039/00094 | Loss: 0.1448 | EWC Loss: 0.0216 | CE Loss: 0.1233\n",
      "Train Epoch: 015 Batch: 00040/00094 | Loss: 0.0940 | EWC Loss: 0.0216 | CE Loss: 0.0723\n",
      "Train Epoch: 015 Batch: 00041/00094 | Loss: 0.1450 | EWC Loss: 0.0215 | CE Loss: 0.1235\n",
      "Train Epoch: 015 Batch: 00042/00094 | Loss: 0.1209 | EWC Loss: 0.0216 | CE Loss: 0.0993\n",
      "Train Epoch: 015 Batch: 00043/00094 | Loss: 0.1224 | EWC Loss: 0.0217 | CE Loss: 0.1008\n",
      "Train Epoch: 015 Batch: 00044/00094 | Loss: 0.1927 | EWC Loss: 0.0218 | CE Loss: 0.1709\n",
      "Train Epoch: 015 Batch: 00045/00094 | Loss: 0.1464 | EWC Loss: 0.0216 | CE Loss: 0.1248\n",
      "Train Epoch: 015 Batch: 00046/00094 | Loss: 0.1884 | EWC Loss: 0.0214 | CE Loss: 0.1670\n",
      "Train Epoch: 015 Batch: 00047/00094 | Loss: 0.0997 | EWC Loss: 0.0214 | CE Loss: 0.0783\n",
      "Train Epoch: 015 Batch: 00048/00094 | Loss: 0.2048 | EWC Loss: 0.0214 | CE Loss: 0.1835\n",
      "Train Epoch: 015 Batch: 00049/00094 | Loss: 0.1477 | EWC Loss: 0.0213 | CE Loss: 0.1264\n",
      "Train Epoch: 015 Batch: 00050/00094 | Loss: 0.0929 | EWC Loss: 0.0213 | CE Loss: 0.0716\n",
      "Train Epoch: 015 Batch: 00051/00094 | Loss: 0.1848 | EWC Loss: 0.0212 | CE Loss: 0.1636\n",
      "Train Epoch: 015 Batch: 00052/00094 | Loss: 0.1224 | EWC Loss: 0.0212 | CE Loss: 0.1012\n",
      "Train Epoch: 015 Batch: 00053/00094 | Loss: 0.1479 | EWC Loss: 0.0213 | CE Loss: 0.1266\n",
      "Train Epoch: 015 Batch: 00054/00094 | Loss: 0.2273 | EWC Loss: 0.0213 | CE Loss: 0.2060\n",
      "Train Epoch: 015 Batch: 00055/00094 | Loss: 0.0805 | EWC Loss: 0.0214 | CE Loss: 0.0591\n",
      "Train Epoch: 015 Batch: 00056/00094 | Loss: 0.1519 | EWC Loss: 0.0213 | CE Loss: 0.1306\n",
      "Train Epoch: 015 Batch: 00057/00094 | Loss: 0.1224 | EWC Loss: 0.0213 | CE Loss: 0.1011\n",
      "Train Epoch: 015 Batch: 00058/00094 | Loss: 0.1382 | EWC Loss: 0.0213 | CE Loss: 0.1170\n",
      "Train Epoch: 015 Batch: 00059/00094 | Loss: 0.1016 | EWC Loss: 0.0213 | CE Loss: 0.0804\n",
      "Train Epoch: 015 Batch: 00060/00094 | Loss: 0.1001 | EWC Loss: 0.0213 | CE Loss: 0.0788\n",
      "Train Epoch: 015 Batch: 00061/00094 | Loss: 0.1440 | EWC Loss: 0.0214 | CE Loss: 0.1227\n",
      "Train Epoch: 015 Batch: 00062/00094 | Loss: 0.1139 | EWC Loss: 0.0212 | CE Loss: 0.0926\n",
      "Train Epoch: 015 Batch: 00063/00094 | Loss: 0.2320 | EWC Loss: 0.0212 | CE Loss: 0.2108\n",
      "Train Epoch: 015 Batch: 00064/00094 | Loss: 0.1174 | EWC Loss: 0.0213 | CE Loss: 0.0961\n",
      "Train Epoch: 015 Batch: 00065/00094 | Loss: 0.0812 | EWC Loss: 0.0213 | CE Loss: 0.0599\n",
      "Train Epoch: 015 Batch: 00066/00094 | Loss: 0.0865 | EWC Loss: 0.0213 | CE Loss: 0.0651\n",
      "Train Epoch: 015 Batch: 00067/00094 | Loss: 0.1030 | EWC Loss: 0.0213 | CE Loss: 0.0818\n",
      "Train Epoch: 015 Batch: 00068/00094 | Loss: 0.2687 | EWC Loss: 0.0212 | CE Loss: 0.2475\n",
      "Train Epoch: 015 Batch: 00069/00094 | Loss: 0.0715 | EWC Loss: 0.0213 | CE Loss: 0.0503\n",
      "Train Epoch: 015 Batch: 00070/00094 | Loss: 0.1423 | EWC Loss: 0.0213 | CE Loss: 0.1210\n",
      "Train Epoch: 015 Batch: 00071/00094 | Loss: 0.1629 | EWC Loss: 0.0213 | CE Loss: 0.1416\n",
      "Train Epoch: 015 Batch: 00072/00094 | Loss: 0.0754 | EWC Loss: 0.0213 | CE Loss: 0.0540\n",
      "Train Epoch: 015 Batch: 00073/00094 | Loss: 0.1883 | EWC Loss: 0.0213 | CE Loss: 0.1670\n",
      "Train Epoch: 015 Batch: 00074/00094 | Loss: 0.1659 | EWC Loss: 0.0213 | CE Loss: 0.1446\n",
      "Train Epoch: 015 Batch: 00075/00094 | Loss: 0.1939 | EWC Loss: 0.0211 | CE Loss: 0.1728\n",
      "Train Epoch: 015 Batch: 00076/00094 | Loss: 0.1115 | EWC Loss: 0.0212 | CE Loss: 0.0903\n",
      "Train Epoch: 015 Batch: 00077/00094 | Loss: 0.1285 | EWC Loss: 0.0212 | CE Loss: 0.1073\n",
      "Train Epoch: 015 Batch: 00078/00094 | Loss: 0.1127 | EWC Loss: 0.0214 | CE Loss: 0.0912\n",
      "Train Epoch: 015 Batch: 00079/00094 | Loss: 0.1454 | EWC Loss: 0.0214 | CE Loss: 0.1240\n",
      "Train Epoch: 015 Batch: 00080/00094 | Loss: 0.1056 | EWC Loss: 0.0214 | CE Loss: 0.0841\n",
      "Train Epoch: 015 Batch: 00081/00094 | Loss: 0.0869 | EWC Loss: 0.0215 | CE Loss: 0.0654\n",
      "Train Epoch: 015 Batch: 00082/00094 | Loss: 0.1063 | EWC Loss: 0.0215 | CE Loss: 0.0849\n",
      "Train Epoch: 015 Batch: 00083/00094 | Loss: 0.1738 | EWC Loss: 0.0215 | CE Loss: 0.1524\n",
      "Train Epoch: 015 Batch: 00084/00094 | Loss: 0.1042 | EWC Loss: 0.0215 | CE Loss: 0.0827\n",
      "Train Epoch: 015 Batch: 00085/00094 | Loss: 0.1320 | EWC Loss: 0.0214 | CE Loss: 0.1105\n",
      "Train Epoch: 015 Batch: 00086/00094 | Loss: 0.1435 | EWC Loss: 0.0214 | CE Loss: 0.1221\n",
      "Train Epoch: 015 Batch: 00087/00094 | Loss: 0.1114 | EWC Loss: 0.0214 | CE Loss: 0.0900\n",
      "Train Epoch: 015 Batch: 00088/00094 | Loss: 0.2344 | EWC Loss: 0.0214 | CE Loss: 0.2130\n",
      "Train Epoch: 015 Batch: 00089/00094 | Loss: 0.0965 | EWC Loss: 0.0214 | CE Loss: 0.0750\n",
      "Train Epoch: 015 Batch: 00090/00094 | Loss: 0.1808 | EWC Loss: 0.0214 | CE Loss: 0.1594\n",
      "Train Epoch: 015 Batch: 00091/00094 | Loss: 0.3553 | EWC Loss: 0.0214 | CE Loss: 0.3340\n",
      "Train Epoch: 015 Batch: 00092/00094 | Loss: 0.2005 | EWC Loss: 0.0216 | CE Loss: 0.1790\n",
      "Train Epoch: 015 Batch: 00093/00094 | Loss: 0.1107 | EWC Loss: 0.0217 | CE Loss: 0.0890\n",
      "Train Epoch: 015 Batch: 00094/00094 | Loss: 0.1788 | EWC Loss: 0.0218 | CE Loss: 0.1570\n",
      "Train Epoch: 014 |Acc 95.48333 | Loss: 0.14704 | Task Loss 0.12547 | EWC Loss: 0.02157\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1071 | acc:96.3500\n",
      "[VAL Acc] Target: 96.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.2559 | acc:50.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1950 | acc:59.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2039 | acc:53.2443\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 53.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2666 | acc:54.6630\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6096 | acc:75.0462\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5699 | acc:76.2539\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9631 | acc:57.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.56%\n",
      "[VAL Acc] Avg 65.37%\n",
      "\n",
      "\n",
      "---------- Starting epoch 15 ----------\n",
      "Train Epoch: 016 Batch: 00001/00094 | Loss: 0.1953 | EWC Loss: 0.0219 | CE Loss: 0.1734\n",
      "Train Epoch: 016 Batch: 00002/00094 | Loss: 0.1175 | EWC Loss: 0.0219 | CE Loss: 0.0956\n",
      "Train Epoch: 016 Batch: 00003/00094 | Loss: 0.1986 | EWC Loss: 0.0220 | CE Loss: 0.1765\n",
      "Train Epoch: 016 Batch: 00004/00094 | Loss: 0.1583 | EWC Loss: 0.0219 | CE Loss: 0.1364\n",
      "Train Epoch: 016 Batch: 00005/00094 | Loss: 0.2748 | EWC Loss: 0.0218 | CE Loss: 0.2530\n",
      "Train Epoch: 016 Batch: 00006/00094 | Loss: 0.0970 | EWC Loss: 0.0218 | CE Loss: 0.0751\n",
      "Train Epoch: 016 Batch: 00007/00094 | Loss: 0.1277 | EWC Loss: 0.0219 | CE Loss: 0.1058\n",
      "Train Epoch: 016 Batch: 00008/00094 | Loss: 0.1784 | EWC Loss: 0.0219 | CE Loss: 0.1565\n",
      "Train Epoch: 016 Batch: 00009/00094 | Loss: 0.1222 | EWC Loss: 0.0220 | CE Loss: 0.1002\n",
      "Train Epoch: 016 Batch: 00010/00094 | Loss: 0.1402 | EWC Loss: 0.0219 | CE Loss: 0.1182\n",
      "Train Epoch: 016 Batch: 00011/00094 | Loss: 0.1323 | EWC Loss: 0.0221 | CE Loss: 0.1102\n",
      "Train Epoch: 016 Batch: 00012/00094 | Loss: 0.1895 | EWC Loss: 0.0221 | CE Loss: 0.1674\n",
      "Train Epoch: 016 Batch: 00013/00094 | Loss: 0.1443 | EWC Loss: 0.0221 | CE Loss: 0.1222\n",
      "Train Epoch: 016 Batch: 00014/00094 | Loss: 0.1012 | EWC Loss: 0.0216 | CE Loss: 0.0796\n",
      "Train Epoch: 016 Batch: 00015/00094 | Loss: 0.2083 | EWC Loss: 0.0216 | CE Loss: 0.1867\n",
      "Train Epoch: 016 Batch: 00016/00094 | Loss: 0.1297 | EWC Loss: 0.0216 | CE Loss: 0.1081\n",
      "Train Epoch: 016 Batch: 00017/00094 | Loss: 0.1131 | EWC Loss: 0.0220 | CE Loss: 0.0911\n",
      "Train Epoch: 016 Batch: 00018/00094 | Loss: 0.0821 | EWC Loss: 0.0223 | CE Loss: 0.0598\n",
      "Train Epoch: 016 Batch: 00019/00094 | Loss: 0.0922 | EWC Loss: 0.0222 | CE Loss: 0.0700\n",
      "Train Epoch: 016 Batch: 00020/00094 | Loss: 0.1057 | EWC Loss: 0.0222 | CE Loss: 0.0835\n",
      "Train Epoch: 016 Batch: 00021/00094 | Loss: 0.2028 | EWC Loss: 0.0221 | CE Loss: 0.1806\n",
      "Train Epoch: 016 Batch: 00022/00094 | Loss: 0.1525 | EWC Loss: 0.0223 | CE Loss: 0.1302\n",
      "Train Epoch: 016 Batch: 00023/00094 | Loss: 0.1002 | EWC Loss: 0.0223 | CE Loss: 0.0779\n",
      "Train Epoch: 016 Batch: 00024/00094 | Loss: 0.1188 | EWC Loss: 0.0221 | CE Loss: 0.0967\n",
      "Train Epoch: 016 Batch: 00025/00094 | Loss: 0.2275 | EWC Loss: 0.0221 | CE Loss: 0.2054\n",
      "Train Epoch: 016 Batch: 00026/00094 | Loss: 0.1906 | EWC Loss: 0.0221 | CE Loss: 0.1685\n",
      "Train Epoch: 016 Batch: 00027/00094 | Loss: 0.1563 | EWC Loss: 0.0224 | CE Loss: 0.1339\n",
      "Train Epoch: 016 Batch: 00028/00094 | Loss: 0.2822 | EWC Loss: 0.0225 | CE Loss: 0.2598\n",
      "Train Epoch: 016 Batch: 00029/00094 | Loss: 0.1486 | EWC Loss: 0.0223 | CE Loss: 0.1264\n",
      "Train Epoch: 016 Batch: 00030/00094 | Loss: 0.1165 | EWC Loss: 0.0224 | CE Loss: 0.0940\n",
      "Train Epoch: 016 Batch: 00031/00094 | Loss: 0.0732 | EWC Loss: 0.0224 | CE Loss: 0.0508\n",
      "Train Epoch: 016 Batch: 00032/00094 | Loss: 0.1496 | EWC Loss: 0.0223 | CE Loss: 0.1273\n",
      "Train Epoch: 016 Batch: 00033/00094 | Loss: 0.0900 | EWC Loss: 0.0225 | CE Loss: 0.0675\n",
      "Train Epoch: 016 Batch: 00034/00094 | Loss: 0.1478 | EWC Loss: 0.0224 | CE Loss: 0.1254\n",
      "Train Epoch: 016 Batch: 00035/00094 | Loss: 0.1230 | EWC Loss: 0.0226 | CE Loss: 0.1004\n",
      "Train Epoch: 016 Batch: 00036/00094 | Loss: 0.1592 | EWC Loss: 0.0228 | CE Loss: 0.1364\n",
      "Train Epoch: 016 Batch: 00037/00094 | Loss: 0.0821 | EWC Loss: 0.0228 | CE Loss: 0.0593\n",
      "Train Epoch: 016 Batch: 00038/00094 | Loss: 0.0987 | EWC Loss: 0.0227 | CE Loss: 0.0760\n",
      "Train Epoch: 016 Batch: 00039/00094 | Loss: 0.1305 | EWC Loss: 0.0227 | CE Loss: 0.1078\n",
      "Train Epoch: 016 Batch: 00040/00094 | Loss: 0.0954 | EWC Loss: 0.0224 | CE Loss: 0.0730\n",
      "Train Epoch: 016 Batch: 00041/00094 | Loss: 0.1264 | EWC Loss: 0.0225 | CE Loss: 0.1038\n",
      "Train Epoch: 016 Batch: 00042/00094 | Loss: 0.2457 | EWC Loss: 0.0224 | CE Loss: 0.2234\n",
      "Train Epoch: 016 Batch: 00043/00094 | Loss: 0.0960 | EWC Loss: 0.0223 | CE Loss: 0.0737\n",
      "Train Epoch: 016 Batch: 00044/00094 | Loss: 0.2024 | EWC Loss: 0.0223 | CE Loss: 0.1801\n",
      "Train Epoch: 016 Batch: 00045/00094 | Loss: 0.0920 | EWC Loss: 0.0222 | CE Loss: 0.0698\n",
      "Train Epoch: 016 Batch: 00046/00094 | Loss: 0.1214 | EWC Loss: 0.0222 | CE Loss: 0.0991\n",
      "Train Epoch: 016 Batch: 00047/00094 | Loss: 0.1338 | EWC Loss: 0.0221 | CE Loss: 0.1116\n",
      "Train Epoch: 016 Batch: 00048/00094 | Loss: 0.1704 | EWC Loss: 0.0220 | CE Loss: 0.1483\n",
      "Train Epoch: 016 Batch: 00049/00094 | Loss: 0.2641 | EWC Loss: 0.0220 | CE Loss: 0.2421\n",
      "Train Epoch: 016 Batch: 00050/00094 | Loss: 0.1247 | EWC Loss: 0.0220 | CE Loss: 0.1028\n",
      "Train Epoch: 016 Batch: 00051/00094 | Loss: 0.1292 | EWC Loss: 0.0219 | CE Loss: 0.1074\n",
      "Train Epoch: 016 Batch: 00052/00094 | Loss: 0.1488 | EWC Loss: 0.0218 | CE Loss: 0.1270\n",
      "Train Epoch: 016 Batch: 00053/00094 | Loss: 0.2723 | EWC Loss: 0.0217 | CE Loss: 0.2507\n",
      "Train Epoch: 016 Batch: 00054/00094 | Loss: 0.1398 | EWC Loss: 0.0217 | CE Loss: 0.1181\n",
      "Train Epoch: 016 Batch: 00055/00094 | Loss: 0.0951 | EWC Loss: 0.0218 | CE Loss: 0.0733\n",
      "Train Epoch: 016 Batch: 00056/00094 | Loss: 0.2204 | EWC Loss: 0.0218 | CE Loss: 0.1986\n",
      "Train Epoch: 016 Batch: 00057/00094 | Loss: 0.1166 | EWC Loss: 0.0218 | CE Loss: 0.0948\n",
      "Train Epoch: 016 Batch: 00058/00094 | Loss: 0.1114 | EWC Loss: 0.0218 | CE Loss: 0.0895\n",
      "Train Epoch: 016 Batch: 00059/00094 | Loss: 0.2573 | EWC Loss: 0.0217 | CE Loss: 0.2356\n",
      "Train Epoch: 016 Batch: 00060/00094 | Loss: 0.2138 | EWC Loss: 0.0220 | CE Loss: 0.1918\n",
      "Train Epoch: 016 Batch: 00061/00094 | Loss: 0.1152 | EWC Loss: 0.0220 | CE Loss: 0.0932\n",
      "Train Epoch: 016 Batch: 00062/00094 | Loss: 0.1201 | EWC Loss: 0.0220 | CE Loss: 0.0981\n",
      "Train Epoch: 016 Batch: 00063/00094 | Loss: 0.1610 | EWC Loss: 0.0221 | CE Loss: 0.1389\n",
      "Train Epoch: 016 Batch: 00064/00094 | Loss: 0.0874 | EWC Loss: 0.0219 | CE Loss: 0.0654\n",
      "Train Epoch: 016 Batch: 00065/00094 | Loss: 0.1618 | EWC Loss: 0.0219 | CE Loss: 0.1399\n",
      "Train Epoch: 016 Batch: 00066/00094 | Loss: 0.1449 | EWC Loss: 0.0220 | CE Loss: 0.1229\n",
      "Train Epoch: 016 Batch: 00067/00094 | Loss: 0.1879 | EWC Loss: 0.0220 | CE Loss: 0.1659\n",
      "Train Epoch: 016 Batch: 00068/00094 | Loss: 0.1161 | EWC Loss: 0.0219 | CE Loss: 0.0941\n",
      "Train Epoch: 016 Batch: 00069/00094 | Loss: 0.2010 | EWC Loss: 0.0218 | CE Loss: 0.1791\n",
      "Train Epoch: 016 Batch: 00070/00094 | Loss: 0.1504 | EWC Loss: 0.0219 | CE Loss: 0.1285\n",
      "Train Epoch: 016 Batch: 00071/00094 | Loss: 0.1611 | EWC Loss: 0.0220 | CE Loss: 0.1391\n",
      "Train Epoch: 016 Batch: 00072/00094 | Loss: 0.1316 | EWC Loss: 0.0221 | CE Loss: 0.1095\n",
      "Train Epoch: 016 Batch: 00073/00094 | Loss: 0.1550 | EWC Loss: 0.0224 | CE Loss: 0.1326\n",
      "Train Epoch: 016 Batch: 00074/00094 | Loss: 0.1846 | EWC Loss: 0.0223 | CE Loss: 0.1623\n",
      "Train Epoch: 016 Batch: 00075/00094 | Loss: 0.0686 | EWC Loss: 0.0224 | CE Loss: 0.0462\n",
      "Train Epoch: 016 Batch: 00076/00094 | Loss: 0.1680 | EWC Loss: 0.0223 | CE Loss: 0.1458\n",
      "Train Epoch: 016 Batch: 00077/00094 | Loss: 0.1163 | EWC Loss: 0.0222 | CE Loss: 0.0941\n",
      "Train Epoch: 016 Batch: 00078/00094 | Loss: 0.1033 | EWC Loss: 0.0221 | CE Loss: 0.0812\n",
      "Train Epoch: 016 Batch: 00079/00094 | Loss: 0.0927 | EWC Loss: 0.0222 | CE Loss: 0.0704\n",
      "Train Epoch: 016 Batch: 00080/00094 | Loss: 0.1600 | EWC Loss: 0.0222 | CE Loss: 0.1378\n",
      "Train Epoch: 016 Batch: 00081/00094 | Loss: 0.2323 | EWC Loss: 0.0221 | CE Loss: 0.2101\n",
      "Train Epoch: 016 Batch: 00082/00094 | Loss: 0.1549 | EWC Loss: 0.0223 | CE Loss: 0.1326\n",
      "Train Epoch: 016 Batch: 00083/00094 | Loss: 0.1285 | EWC Loss: 0.0222 | CE Loss: 0.1063\n",
      "Train Epoch: 016 Batch: 00084/00094 | Loss: 0.1486 | EWC Loss: 0.0221 | CE Loss: 0.1265\n",
      "Train Epoch: 016 Batch: 00085/00094 | Loss: 0.1682 | EWC Loss: 0.0222 | CE Loss: 0.1461\n",
      "Train Epoch: 016 Batch: 00086/00094 | Loss: 0.1967 | EWC Loss: 0.0222 | CE Loss: 0.1744\n",
      "Train Epoch: 016 Batch: 00087/00094 | Loss: 0.2546 | EWC Loss: 0.0220 | CE Loss: 0.2326\n",
      "Train Epoch: 016 Batch: 00088/00094 | Loss: 0.1946 | EWC Loss: 0.0222 | CE Loss: 0.1724\n",
      "Train Epoch: 016 Batch: 00089/00094 | Loss: 0.1261 | EWC Loss: 0.0223 | CE Loss: 0.1038\n",
      "Train Epoch: 016 Batch: 00090/00094 | Loss: 0.1487 | EWC Loss: 0.0223 | CE Loss: 0.1265\n",
      "Train Epoch: 016 Batch: 00091/00094 | Loss: 0.1257 | EWC Loss: 0.0221 | CE Loss: 0.1036\n",
      "Train Epoch: 016 Batch: 00092/00094 | Loss: 0.0982 | EWC Loss: 0.0219 | CE Loss: 0.0763\n",
      "Train Epoch: 016 Batch: 00093/00094 | Loss: 0.0974 | EWC Loss: 0.0220 | CE Loss: 0.0753\n",
      "Train Epoch: 016 Batch: 00094/00094 | Loss: 0.1584 | EWC Loss: 0.0220 | CE Loss: 0.1364\n",
      "Train Epoch: 015 |Acc 95.21667 | Loss: 0.14953 | Task Loss 0.12741 | EWC Loss: 0.02212\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1066 | acc:96.8500\n",
      "[VAL Acc] Target: 96.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.3311 | acc:51.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 51.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1976 | acc:60.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.1561 | acc:53.4351\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 53.44%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2767 | acc:56.2304\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6331 | acc:71.7190\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 71.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5791 | acc:75.7837\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 75.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9117 | acc:58.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.31%\n",
      "[VAL Acc] Avg 65.49%\n",
      "\n",
      "\n",
      "---------- Starting epoch 16 ----------\n",
      "Train Epoch: 017 Batch: 00001/00094 | Loss: 0.1212 | EWC Loss: 0.0219 | CE Loss: 0.0993\n",
      "Train Epoch: 017 Batch: 00002/00094 | Loss: 0.1407 | EWC Loss: 0.0221 | CE Loss: 0.1186\n",
      "Train Epoch: 017 Batch: 00003/00094 | Loss: 0.1256 | EWC Loss: 0.0222 | CE Loss: 0.1034\n",
      "Train Epoch: 017 Batch: 00004/00094 | Loss: 0.1774 | EWC Loss: 0.0221 | CE Loss: 0.1554\n",
      "Train Epoch: 017 Batch: 00005/00094 | Loss: 0.2229 | EWC Loss: 0.0222 | CE Loss: 0.2007\n",
      "Train Epoch: 017 Batch: 00006/00094 | Loss: 0.2887 | EWC Loss: 0.0223 | CE Loss: 0.2664\n",
      "Train Epoch: 017 Batch: 00007/00094 | Loss: 0.1235 | EWC Loss: 0.0226 | CE Loss: 0.1009\n",
      "Train Epoch: 017 Batch: 00008/00094 | Loss: 0.1498 | EWC Loss: 0.0226 | CE Loss: 0.1272\n",
      "Train Epoch: 017 Batch: 00009/00094 | Loss: 0.0775 | EWC Loss: 0.0233 | CE Loss: 0.0543\n",
      "Train Epoch: 017 Batch: 00010/00094 | Loss: 0.0605 | EWC Loss: 0.0234 | CE Loss: 0.0372\n",
      "Train Epoch: 017 Batch: 00011/00094 | Loss: 0.1078 | EWC Loss: 0.0233 | CE Loss: 0.0845\n",
      "Train Epoch: 017 Batch: 00012/00094 | Loss: 0.1119 | EWC Loss: 0.0231 | CE Loss: 0.0888\n",
      "Train Epoch: 017 Batch: 00013/00094 | Loss: 0.1523 | EWC Loss: 0.0234 | CE Loss: 0.1288\n",
      "Train Epoch: 017 Batch: 00014/00094 | Loss: 0.1568 | EWC Loss: 0.0236 | CE Loss: 0.1332\n",
      "Train Epoch: 017 Batch: 00015/00094 | Loss: 0.0932 | EWC Loss: 0.0237 | CE Loss: 0.0696\n",
      "Train Epoch: 017 Batch: 00016/00094 | Loss: 0.1574 | EWC Loss: 0.0234 | CE Loss: 0.1340\n",
      "Train Epoch: 017 Batch: 00017/00094 | Loss: 0.1085 | EWC Loss: 0.0231 | CE Loss: 0.0854\n",
      "Train Epoch: 017 Batch: 00018/00094 | Loss: 0.1001 | EWC Loss: 0.0230 | CE Loss: 0.0771\n",
      "Train Epoch: 017 Batch: 00019/00094 | Loss: 0.1755 | EWC Loss: 0.0231 | CE Loss: 0.1524\n",
      "Train Epoch: 017 Batch: 00020/00094 | Loss: 0.1345 | EWC Loss: 0.0230 | CE Loss: 0.1115\n",
      "Train Epoch: 017 Batch: 00021/00094 | Loss: 0.2386 | EWC Loss: 0.0234 | CE Loss: 0.2152\n",
      "Train Epoch: 017 Batch: 00022/00094 | Loss: 0.2145 | EWC Loss: 0.0232 | CE Loss: 0.1912\n",
      "Train Epoch: 017 Batch: 00023/00094 | Loss: 0.1245 | EWC Loss: 0.0229 | CE Loss: 0.1017\n",
      "Train Epoch: 017 Batch: 00024/00094 | Loss: 0.1391 | EWC Loss: 0.0227 | CE Loss: 0.1163\n",
      "Train Epoch: 017 Batch: 00025/00094 | Loss: 0.1211 | EWC Loss: 0.0226 | CE Loss: 0.0985\n",
      "Train Epoch: 017 Batch: 00026/00094 | Loss: 0.1235 | EWC Loss: 0.0224 | CE Loss: 0.1011\n",
      "Train Epoch: 017 Batch: 00027/00094 | Loss: 0.1708 | EWC Loss: 0.0225 | CE Loss: 0.1483\n",
      "Train Epoch: 017 Batch: 00028/00094 | Loss: 0.1221 | EWC Loss: 0.0224 | CE Loss: 0.0997\n",
      "Train Epoch: 017 Batch: 00029/00094 | Loss: 0.1049 | EWC Loss: 0.0224 | CE Loss: 0.0825\n",
      "Train Epoch: 017 Batch: 00030/00094 | Loss: 0.1534 | EWC Loss: 0.0225 | CE Loss: 0.1309\n",
      "Train Epoch: 017 Batch: 00031/00094 | Loss: 0.0735 | EWC Loss: 0.0226 | CE Loss: 0.0508\n",
      "Train Epoch: 017 Batch: 00032/00094 | Loss: 0.1359 | EWC Loss: 0.0226 | CE Loss: 0.1133\n",
      "Train Epoch: 017 Batch: 00033/00094 | Loss: 0.1217 | EWC Loss: 0.0226 | CE Loss: 0.0991\n",
      "Train Epoch: 017 Batch: 00034/00094 | Loss: 0.0945 | EWC Loss: 0.0228 | CE Loss: 0.0718\n",
      "Train Epoch: 017 Batch: 00035/00094 | Loss: 0.1186 | EWC Loss: 0.0228 | CE Loss: 0.0958\n",
      "Train Epoch: 017 Batch: 00036/00094 | Loss: 0.2106 | EWC Loss: 0.0229 | CE Loss: 0.1877\n",
      "Train Epoch: 017 Batch: 00037/00094 | Loss: 0.1123 | EWC Loss: 0.0228 | CE Loss: 0.0895\n",
      "Train Epoch: 017 Batch: 00038/00094 | Loss: 0.1852 | EWC Loss: 0.0227 | CE Loss: 0.1625\n",
      "Train Epoch: 017 Batch: 00039/00094 | Loss: 0.1423 | EWC Loss: 0.0228 | CE Loss: 0.1195\n",
      "Train Epoch: 017 Batch: 00040/00094 | Loss: 0.1211 | EWC Loss: 0.0228 | CE Loss: 0.0983\n",
      "Train Epoch: 017 Batch: 00041/00094 | Loss: 0.1488 | EWC Loss: 0.0231 | CE Loss: 0.1257\n",
      "Train Epoch: 017 Batch: 00042/00094 | Loss: 0.1271 | EWC Loss: 0.0234 | CE Loss: 0.1037\n",
      "Train Epoch: 017 Batch: 00043/00094 | Loss: 0.1007 | EWC Loss: 0.0233 | CE Loss: 0.0775\n",
      "Train Epoch: 017 Batch: 00044/00094 | Loss: 0.2596 | EWC Loss: 0.0234 | CE Loss: 0.2362\n",
      "Train Epoch: 017 Batch: 00045/00094 | Loss: 0.1924 | EWC Loss: 0.0233 | CE Loss: 0.1691\n",
      "Train Epoch: 017 Batch: 00046/00094 | Loss: 0.1573 | EWC Loss: 0.0231 | CE Loss: 0.1342\n",
      "Train Epoch: 017 Batch: 00047/00094 | Loss: 0.1408 | EWC Loss: 0.0235 | CE Loss: 0.1173\n",
      "Train Epoch: 017 Batch: 00048/00094 | Loss: 0.1193 | EWC Loss: 0.0235 | CE Loss: 0.0958\n",
      "Train Epoch: 017 Batch: 00049/00094 | Loss: 0.2530 | EWC Loss: 0.0235 | CE Loss: 0.2295\n",
      "Train Epoch: 017 Batch: 00050/00094 | Loss: 0.1199 | EWC Loss: 0.0233 | CE Loss: 0.0966\n",
      "Train Epoch: 017 Batch: 00051/00094 | Loss: 0.1505 | EWC Loss: 0.0235 | CE Loss: 0.1270\n",
      "Train Epoch: 017 Batch: 00052/00094 | Loss: 0.0854 | EWC Loss: 0.0234 | CE Loss: 0.0619\n",
      "Train Epoch: 017 Batch: 00053/00094 | Loss: 0.1465 | EWC Loss: 0.0234 | CE Loss: 0.1230\n",
      "Train Epoch: 017 Batch: 00054/00094 | Loss: 0.0760 | EWC Loss: 0.0231 | CE Loss: 0.0528\n",
      "Train Epoch: 017 Batch: 00055/00094 | Loss: 0.1037 | EWC Loss: 0.0232 | CE Loss: 0.0806\n",
      "Train Epoch: 017 Batch: 00056/00094 | Loss: 0.1305 | EWC Loss: 0.0231 | CE Loss: 0.1074\n",
      "Train Epoch: 017 Batch: 00057/00094 | Loss: 0.1242 | EWC Loss: 0.0231 | CE Loss: 0.1011\n",
      "Train Epoch: 017 Batch: 00058/00094 | Loss: 0.1618 | EWC Loss: 0.0233 | CE Loss: 0.1385\n",
      "Train Epoch: 017 Batch: 00059/00094 | Loss: 0.1415 | EWC Loss: 0.0231 | CE Loss: 0.1184\n",
      "Train Epoch: 017 Batch: 00060/00094 | Loss: 0.0903 | EWC Loss: 0.0231 | CE Loss: 0.0672\n",
      "Train Epoch: 017 Batch: 00061/00094 | Loss: 0.1266 | EWC Loss: 0.0233 | CE Loss: 0.1033\n",
      "Train Epoch: 017 Batch: 00062/00094 | Loss: 0.1559 | EWC Loss: 0.0232 | CE Loss: 0.1327\n",
      "Train Epoch: 017 Batch: 00063/00094 | Loss: 0.1063 | EWC Loss: 0.0231 | CE Loss: 0.0832\n",
      "Train Epoch: 017 Batch: 00064/00094 | Loss: 0.1315 | EWC Loss: 0.0231 | CE Loss: 0.1084\n",
      "Train Epoch: 017 Batch: 00065/00094 | Loss: 0.1000 | EWC Loss: 0.0228 | CE Loss: 0.0773\n",
      "Train Epoch: 017 Batch: 00066/00094 | Loss: 0.2443 | EWC Loss: 0.0228 | CE Loss: 0.2215\n",
      "Train Epoch: 017 Batch: 00067/00094 | Loss: 0.1469 | EWC Loss: 0.0233 | CE Loss: 0.1236\n",
      "Train Epoch: 017 Batch: 00068/00094 | Loss: 0.1177 | EWC Loss: 0.0236 | CE Loss: 0.0941\n",
      "Train Epoch: 017 Batch: 00069/00094 | Loss: 0.1406 | EWC Loss: 0.0231 | CE Loss: 0.1175\n",
      "Train Epoch: 017 Batch: 00070/00094 | Loss: 0.1007 | EWC Loss: 0.0233 | CE Loss: 0.0774\n",
      "Train Epoch: 017 Batch: 00071/00094 | Loss: 0.0925 | EWC Loss: 0.0239 | CE Loss: 0.0686\n",
      "Train Epoch: 017 Batch: 00072/00094 | Loss: 0.1009 | EWC Loss: 0.0236 | CE Loss: 0.0774\n",
      "Train Epoch: 017 Batch: 00073/00094 | Loss: 0.1042 | EWC Loss: 0.0237 | CE Loss: 0.0805\n",
      "Train Epoch: 017 Batch: 00074/00094 | Loss: 0.0977 | EWC Loss: 0.0237 | CE Loss: 0.0740\n",
      "Train Epoch: 017 Batch: 00075/00094 | Loss: 0.1282 | EWC Loss: 0.0238 | CE Loss: 0.1045\n",
      "Train Epoch: 017 Batch: 00076/00094 | Loss: 0.1552 | EWC Loss: 0.0238 | CE Loss: 0.1313\n",
      "Train Epoch: 017 Batch: 00077/00094 | Loss: 0.1337 | EWC Loss: 0.0233 | CE Loss: 0.1104\n",
      "Train Epoch: 017 Batch: 00078/00094 | Loss: 0.2289 | EWC Loss: 0.0231 | CE Loss: 0.2057\n",
      "Train Epoch: 017 Batch: 00079/00094 | Loss: 0.1192 | EWC Loss: 0.0231 | CE Loss: 0.0960\n",
      "Train Epoch: 017 Batch: 00080/00094 | Loss: 0.1560 | EWC Loss: 0.0232 | CE Loss: 0.1328\n",
      "Train Epoch: 017 Batch: 00081/00094 | Loss: 0.1205 | EWC Loss: 0.0230 | CE Loss: 0.0975\n",
      "Train Epoch: 017 Batch: 00082/00094 | Loss: 0.1104 | EWC Loss: 0.0228 | CE Loss: 0.0877\n",
      "Train Epoch: 017 Batch: 00083/00094 | Loss: 0.1385 | EWC Loss: 0.0225 | CE Loss: 0.1160\n",
      "Train Epoch: 017 Batch: 00084/00094 | Loss: 0.1290 | EWC Loss: 0.0227 | CE Loss: 0.1063\n",
      "Train Epoch: 017 Batch: 00085/00094 | Loss: 0.1556 | EWC Loss: 0.0227 | CE Loss: 0.1329\n",
      "Train Epoch: 017 Batch: 00086/00094 | Loss: 0.1053 | EWC Loss: 0.0227 | CE Loss: 0.0826\n",
      "Train Epoch: 017 Batch: 00087/00094 | Loss: 0.0814 | EWC Loss: 0.0230 | CE Loss: 0.0584\n",
      "Train Epoch: 017 Batch: 00088/00094 | Loss: 0.1134 | EWC Loss: 0.0229 | CE Loss: 0.0905\n",
      "Train Epoch: 017 Batch: 00089/00094 | Loss: 0.1081 | EWC Loss: 0.0229 | CE Loss: 0.0852\n",
      "Train Epoch: 017 Batch: 00090/00094 | Loss: 0.1805 | EWC Loss: 0.0229 | CE Loss: 0.1576\n",
      "Train Epoch: 017 Batch: 00091/00094 | Loss: 0.1501 | EWC Loss: 0.0228 | CE Loss: 0.1273\n",
      "Train Epoch: 017 Batch: 00092/00094 | Loss: 0.1319 | EWC Loss: 0.0225 | CE Loss: 0.1095\n",
      "Train Epoch: 017 Batch: 00093/00094 | Loss: 0.1071 | EWC Loss: 0.0226 | CE Loss: 0.0845\n",
      "Train Epoch: 017 Batch: 00094/00094 | Loss: 0.1026 | EWC Loss: 0.0224 | CE Loss: 0.0802\n",
      "Train Epoch: 016 |Acc 95.63333 | Loss: 0.13686 | Task Loss 0.11386 | EWC Loss: 0.02300\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1162 | acc:96.0500\n",
      "[VAL Acc] Target: 96.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.2112 | acc:50.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1776 | acc:59.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2125 | acc:51.1450\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 51.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.1804 | acc:55.9561\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6292 | acc:73.1978\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5898 | acc:76.0188\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 76.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9644 | acc:57.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.19%\n",
      "[VAL Acc] Avg 64.99%\n",
      "\n",
      "\n",
      "---------- Starting epoch 17 ----------\n",
      "Train Epoch: 018 Batch: 00001/00094 | Loss: 0.1473 | EWC Loss: 0.0224 | CE Loss: 0.1249\n",
      "Train Epoch: 018 Batch: 00002/00094 | Loss: 0.1102 | EWC Loss: 0.0225 | CE Loss: 0.0877\n",
      "Train Epoch: 018 Batch: 00003/00094 | Loss: 0.0718 | EWC Loss: 0.0224 | CE Loss: 0.0494\n",
      "Train Epoch: 018 Batch: 00004/00094 | Loss: 0.0836 | EWC Loss: 0.0222 | CE Loss: 0.0614\n",
      "Train Epoch: 018 Batch: 00005/00094 | Loss: 0.1263 | EWC Loss: 0.0222 | CE Loss: 0.1041\n",
      "Train Epoch: 018 Batch: 00006/00094 | Loss: 0.1350 | EWC Loss: 0.0223 | CE Loss: 0.1126\n",
      "Train Epoch: 018 Batch: 00007/00094 | Loss: 0.1015 | EWC Loss: 0.0224 | CE Loss: 0.0792\n",
      "Train Epoch: 018 Batch: 00008/00094 | Loss: 0.0651 | EWC Loss: 0.0224 | CE Loss: 0.0427\n",
      "Train Epoch: 018 Batch: 00009/00094 | Loss: 0.1283 | EWC Loss: 0.0223 | CE Loss: 0.1060\n",
      "Train Epoch: 018 Batch: 00010/00094 | Loss: 0.1354 | EWC Loss: 0.0222 | CE Loss: 0.1132\n",
      "Train Epoch: 018 Batch: 00011/00094 | Loss: 0.1283 | EWC Loss: 0.0220 | CE Loss: 0.1063\n",
      "Train Epoch: 018 Batch: 00012/00094 | Loss: 0.1199 | EWC Loss: 0.0222 | CE Loss: 0.0977\n",
      "Train Epoch: 018 Batch: 00013/00094 | Loss: 0.0698 | EWC Loss: 0.0223 | CE Loss: 0.0474\n",
      "Train Epoch: 018 Batch: 00014/00094 | Loss: 0.1617 | EWC Loss: 0.0224 | CE Loss: 0.1393\n",
      "Train Epoch: 018 Batch: 00015/00094 | Loss: 0.1777 | EWC Loss: 0.0224 | CE Loss: 0.1553\n",
      "Train Epoch: 018 Batch: 00016/00094 | Loss: 0.0968 | EWC Loss: 0.0223 | CE Loss: 0.0745\n",
      "Train Epoch: 018 Batch: 00017/00094 | Loss: 0.2235 | EWC Loss: 0.0221 | CE Loss: 0.2015\n",
      "Train Epoch: 018 Batch: 00018/00094 | Loss: 0.1185 | EWC Loss: 0.0221 | CE Loss: 0.0964\n",
      "Train Epoch: 018 Batch: 00019/00094 | Loss: 0.1466 | EWC Loss: 0.0226 | CE Loss: 0.1240\n",
      "Train Epoch: 018 Batch: 00020/00094 | Loss: 0.1242 | EWC Loss: 0.0234 | CE Loss: 0.1008\n",
      "Train Epoch: 018 Batch: 00021/00094 | Loss: 0.1920 | EWC Loss: 0.0235 | CE Loss: 0.1685\n",
      "Train Epoch: 018 Batch: 00022/00094 | Loss: 0.1439 | EWC Loss: 0.0237 | CE Loss: 0.1202\n",
      "Train Epoch: 018 Batch: 00023/00094 | Loss: 0.0856 | EWC Loss: 0.0235 | CE Loss: 0.0621\n",
      "Train Epoch: 018 Batch: 00024/00094 | Loss: 0.1256 | EWC Loss: 0.0234 | CE Loss: 0.1022\n",
      "Train Epoch: 018 Batch: 00025/00094 | Loss: 0.1320 | EWC Loss: 0.0236 | CE Loss: 0.1084\n",
      "Train Epoch: 018 Batch: 00026/00094 | Loss: 0.0969 | EWC Loss: 0.0237 | CE Loss: 0.0733\n",
      "Train Epoch: 018 Batch: 00027/00094 | Loss: 0.0872 | EWC Loss: 0.0240 | CE Loss: 0.0633\n",
      "Train Epoch: 018 Batch: 00028/00094 | Loss: 0.2125 | EWC Loss: 0.0238 | CE Loss: 0.1887\n",
      "Train Epoch: 018 Batch: 00029/00094 | Loss: 0.2116 | EWC Loss: 0.0235 | CE Loss: 0.1880\n",
      "Train Epoch: 018 Batch: 00030/00094 | Loss: 0.1442 | EWC Loss: 0.0236 | CE Loss: 0.1205\n",
      "Train Epoch: 018 Batch: 00031/00094 | Loss: 0.1924 | EWC Loss: 0.0236 | CE Loss: 0.1687\n",
      "Train Epoch: 018 Batch: 00032/00094 | Loss: 0.1266 | EWC Loss: 0.0237 | CE Loss: 0.1029\n",
      "Train Epoch: 018 Batch: 00033/00094 | Loss: 0.1490 | EWC Loss: 0.0239 | CE Loss: 0.1251\n",
      "Train Epoch: 018 Batch: 00034/00094 | Loss: 0.2236 | EWC Loss: 0.0233 | CE Loss: 0.2003\n",
      "Train Epoch: 018 Batch: 00035/00094 | Loss: 0.1186 | EWC Loss: 0.0239 | CE Loss: 0.0947\n",
      "Train Epoch: 018 Batch: 00036/00094 | Loss: 0.0948 | EWC Loss: 0.0242 | CE Loss: 0.0705\n",
      "Train Epoch: 018 Batch: 00037/00094 | Loss: 0.1222 | EWC Loss: 0.0242 | CE Loss: 0.0980\n",
      "Train Epoch: 018 Batch: 00038/00094 | Loss: 0.1174 | EWC Loss: 0.0245 | CE Loss: 0.0929\n",
      "Train Epoch: 018 Batch: 00039/00094 | Loss: 0.1564 | EWC Loss: 0.0241 | CE Loss: 0.1323\n",
      "Train Epoch: 018 Batch: 00040/00094 | Loss: 0.1275 | EWC Loss: 0.0239 | CE Loss: 0.1037\n",
      "Train Epoch: 018 Batch: 00041/00094 | Loss: 0.1076 | EWC Loss: 0.0240 | CE Loss: 0.0836\n",
      "Train Epoch: 018 Batch: 00042/00094 | Loss: 0.0710 | EWC Loss: 0.0237 | CE Loss: 0.0473\n",
      "Train Epoch: 018 Batch: 00043/00094 | Loss: 0.1558 | EWC Loss: 0.0238 | CE Loss: 0.1320\n",
      "Train Epoch: 018 Batch: 00044/00094 | Loss: 0.0831 | EWC Loss: 0.0234 | CE Loss: 0.0597\n",
      "Train Epoch: 018 Batch: 00045/00094 | Loss: 0.1524 | EWC Loss: 0.0232 | CE Loss: 0.1291\n",
      "Train Epoch: 018 Batch: 00046/00094 | Loss: 0.0819 | EWC Loss: 0.0236 | CE Loss: 0.0583\n",
      "Train Epoch: 018 Batch: 00047/00094 | Loss: 0.1213 | EWC Loss: 0.0237 | CE Loss: 0.0976\n",
      "Train Epoch: 018 Batch: 00048/00094 | Loss: 0.1475 | EWC Loss: 0.0231 | CE Loss: 0.1244\n",
      "Train Epoch: 018 Batch: 00049/00094 | Loss: 0.1295 | EWC Loss: 0.0231 | CE Loss: 0.1063\n",
      "Train Epoch: 018 Batch: 00050/00094 | Loss: 0.0963 | EWC Loss: 0.0225 | CE Loss: 0.0738\n",
      "Train Epoch: 018 Batch: 00051/00094 | Loss: 0.0804 | EWC Loss: 0.0223 | CE Loss: 0.0581\n",
      "Train Epoch: 018 Batch: 00052/00094 | Loss: 0.0968 | EWC Loss: 0.0222 | CE Loss: 0.0746\n",
      "Train Epoch: 018 Batch: 00053/00094 | Loss: 0.1640 | EWC Loss: 0.0223 | CE Loss: 0.1418\n",
      "Train Epoch: 018 Batch: 00054/00094 | Loss: 0.1888 | EWC Loss: 0.0221 | CE Loss: 0.1667\n",
      "Train Epoch: 018 Batch: 00055/00094 | Loss: 0.1422 | EWC Loss: 0.0221 | CE Loss: 0.1201\n",
      "Train Epoch: 018 Batch: 00056/00094 | Loss: 0.1003 | EWC Loss: 0.0222 | CE Loss: 0.0781\n",
      "Train Epoch: 018 Batch: 00057/00094 | Loss: 0.0565 | EWC Loss: 0.0222 | CE Loss: 0.0343\n",
      "Train Epoch: 018 Batch: 00058/00094 | Loss: 0.1111 | EWC Loss: 0.0221 | CE Loss: 0.0890\n",
      "Train Epoch: 018 Batch: 00059/00094 | Loss: 0.2078 | EWC Loss: 0.0213 | CE Loss: 0.1864\n",
      "Train Epoch: 018 Batch: 00060/00094 | Loss: 0.0773 | EWC Loss: 0.0212 | CE Loss: 0.0561\n",
      "Train Epoch: 018 Batch: 00061/00094 | Loss: 0.1168 | EWC Loss: 0.0213 | CE Loss: 0.0955\n",
      "Train Epoch: 018 Batch: 00062/00094 | Loss: 0.1289 | EWC Loss: 0.0215 | CE Loss: 0.1073\n",
      "Train Epoch: 018 Batch: 00063/00094 | Loss: 0.1009 | EWC Loss: 0.0214 | CE Loss: 0.0795\n",
      "Train Epoch: 018 Batch: 00064/00094 | Loss: 0.0920 | EWC Loss: 0.0215 | CE Loss: 0.0705\n",
      "Train Epoch: 018 Batch: 00065/00094 | Loss: 0.1391 | EWC Loss: 0.0214 | CE Loss: 0.1177\n",
      "Train Epoch: 018 Batch: 00066/00094 | Loss: 0.0887 | EWC Loss: 0.0214 | CE Loss: 0.0673\n",
      "Train Epoch: 018 Batch: 00067/00094 | Loss: 0.1384 | EWC Loss: 0.0215 | CE Loss: 0.1168\n",
      "Train Epoch: 018 Batch: 00068/00094 | Loss: 0.1380 | EWC Loss: 0.0217 | CE Loss: 0.1163\n",
      "Train Epoch: 018 Batch: 00069/00094 | Loss: 0.1142 | EWC Loss: 0.0214 | CE Loss: 0.0928\n",
      "Train Epoch: 018 Batch: 00070/00094 | Loss: 0.0790 | EWC Loss: 0.0214 | CE Loss: 0.0576\n",
      "Train Epoch: 018 Batch: 00071/00094 | Loss: 0.1448 | EWC Loss: 0.0215 | CE Loss: 0.1233\n",
      "Train Epoch: 018 Batch: 00072/00094 | Loss: 0.1718 | EWC Loss: 0.0215 | CE Loss: 0.1503\n",
      "Train Epoch: 018 Batch: 00073/00094 | Loss: 0.1548 | EWC Loss: 0.0216 | CE Loss: 0.1332\n",
      "Train Epoch: 018 Batch: 00074/00094 | Loss: 0.1606 | EWC Loss: 0.0216 | CE Loss: 0.1390\n",
      "Train Epoch: 018 Batch: 00075/00094 | Loss: 0.1356 | EWC Loss: 0.0216 | CE Loss: 0.1140\n",
      "Train Epoch: 018 Batch: 00076/00094 | Loss: 0.0973 | EWC Loss: 0.0221 | CE Loss: 0.0753\n",
      "Train Epoch: 018 Batch: 00077/00094 | Loss: 0.1171 | EWC Loss: 0.0220 | CE Loss: 0.0951\n",
      "Train Epoch: 018 Batch: 00078/00094 | Loss: 0.1082 | EWC Loss: 0.0222 | CE Loss: 0.0861\n",
      "Train Epoch: 018 Batch: 00079/00094 | Loss: 0.1067 | EWC Loss: 0.0221 | CE Loss: 0.0846\n",
      "Train Epoch: 018 Batch: 00080/00094 | Loss: 0.0818 | EWC Loss: 0.0221 | CE Loss: 0.0597\n",
      "Train Epoch: 018 Batch: 00081/00094 | Loss: 0.1146 | EWC Loss: 0.0223 | CE Loss: 0.0924\n",
      "Train Epoch: 018 Batch: 00082/00094 | Loss: 0.0894 | EWC Loss: 0.0224 | CE Loss: 0.0671\n",
      "Train Epoch: 018 Batch: 00083/00094 | Loss: 0.1155 | EWC Loss: 0.0225 | CE Loss: 0.0930\n",
      "Train Epoch: 018 Batch: 00084/00094 | Loss: 0.1040 | EWC Loss: 0.0225 | CE Loss: 0.0815\n",
      "Train Epoch: 018 Batch: 00085/00094 | Loss: 0.1193 | EWC Loss: 0.0223 | CE Loss: 0.0970\n",
      "Train Epoch: 018 Batch: 00086/00094 | Loss: 0.1919 | EWC Loss: 0.0222 | CE Loss: 0.1697\n",
      "Train Epoch: 018 Batch: 00087/00094 | Loss: 0.1670 | EWC Loss: 0.0223 | CE Loss: 0.1447\n",
      "Train Epoch: 018 Batch: 00088/00094 | Loss: 0.0769 | EWC Loss: 0.0225 | CE Loss: 0.0544\n",
      "Train Epoch: 018 Batch: 00089/00094 | Loss: 0.1164 | EWC Loss: 0.0223 | CE Loss: 0.0941\n",
      "Train Epoch: 018 Batch: 00090/00094 | Loss: 0.1560 | EWC Loss: 0.0223 | CE Loss: 0.1336\n",
      "Train Epoch: 018 Batch: 00091/00094 | Loss: 0.1180 | EWC Loss: 0.0223 | CE Loss: 0.0957\n",
      "Train Epoch: 018 Batch: 00092/00094 | Loss: 0.0814 | EWC Loss: 0.0225 | CE Loss: 0.0589\n",
      "Train Epoch: 018 Batch: 00093/00094 | Loss: 0.1823 | EWC Loss: 0.0228 | CE Loss: 0.1594\n",
      "Train Epoch: 018 Batch: 00094/00094 | Loss: 0.0924 | EWC Loss: 0.0231 | CE Loss: 0.0693\n",
      "Train Epoch: 017 |Acc 96.23333 | Loss: 0.12599 | Task Loss 0.10339 | EWC Loss: 0.02260\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0904 | acc:96.8000\n",
      "[VAL Acc] Target: 96.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.3248 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1888 | acc:60.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2744 | acc:52.0992\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 52.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2187 | acc:57.0925\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6422 | acc:73.7523\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5297 | acc:78.6442\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 78.64%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9781 | acc:57.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.81%\n",
      "[VAL Acc] Avg 65.90%\n",
      "\n",
      "\n",
      "---------- Starting epoch 18 ----------\n",
      "Train Epoch: 019 Batch: 00001/00094 | Loss: 0.1160 | EWC Loss: 0.0231 | CE Loss: 0.0929\n",
      "Train Epoch: 019 Batch: 00002/00094 | Loss: 0.1005 | EWC Loss: 0.0234 | CE Loss: 0.0771\n",
      "Train Epoch: 019 Batch: 00003/00094 | Loss: 0.1251 | EWC Loss: 0.0235 | CE Loss: 0.1016\n",
      "Train Epoch: 019 Batch: 00004/00094 | Loss: 0.1106 | EWC Loss: 0.0232 | CE Loss: 0.0874\n",
      "Train Epoch: 019 Batch: 00005/00094 | Loss: 0.1714 | EWC Loss: 0.0231 | CE Loss: 0.1482\n",
      "Train Epoch: 019 Batch: 00006/00094 | Loss: 0.1250 | EWC Loss: 0.0238 | CE Loss: 0.1012\n",
      "Train Epoch: 019 Batch: 00007/00094 | Loss: 0.1891 | EWC Loss: 0.0240 | CE Loss: 0.1650\n",
      "Train Epoch: 019 Batch: 00008/00094 | Loss: 0.1225 | EWC Loss: 0.0241 | CE Loss: 0.0983\n",
      "Train Epoch: 019 Batch: 00009/00094 | Loss: 0.1404 | EWC Loss: 0.0239 | CE Loss: 0.1165\n",
      "Train Epoch: 019 Batch: 00010/00094 | Loss: 0.0766 | EWC Loss: 0.0239 | CE Loss: 0.0526\n",
      "Train Epoch: 019 Batch: 00011/00094 | Loss: 0.1109 | EWC Loss: 0.0238 | CE Loss: 0.0871\n",
      "Train Epoch: 019 Batch: 00012/00094 | Loss: 0.2701 | EWC Loss: 0.0239 | CE Loss: 0.2462\n",
      "Train Epoch: 019 Batch: 00013/00094 | Loss: 0.0886 | EWC Loss: 0.0236 | CE Loss: 0.0650\n",
      "Train Epoch: 019 Batch: 00014/00094 | Loss: 0.1234 | EWC Loss: 0.0235 | CE Loss: 0.0999\n",
      "Train Epoch: 019 Batch: 00015/00094 | Loss: 0.1558 | EWC Loss: 0.0233 | CE Loss: 0.1325\n",
      "Train Epoch: 019 Batch: 00016/00094 | Loss: 0.1153 | EWC Loss: 0.0239 | CE Loss: 0.0914\n",
      "Train Epoch: 019 Batch: 00017/00094 | Loss: 0.1268 | EWC Loss: 0.0242 | CE Loss: 0.1026\n",
      "Train Epoch: 019 Batch: 00018/00094 | Loss: 0.1360 | EWC Loss: 0.0242 | CE Loss: 0.1118\n",
      "Train Epoch: 019 Batch: 00019/00094 | Loss: 0.0971 | EWC Loss: 0.0247 | CE Loss: 0.0724\n",
      "Train Epoch: 019 Batch: 00020/00094 | Loss: 0.1987 | EWC Loss: 0.0243 | CE Loss: 0.1743\n",
      "Train Epoch: 019 Batch: 00021/00094 | Loss: 0.1268 | EWC Loss: 0.0252 | CE Loss: 0.1016\n",
      "Train Epoch: 019 Batch: 00022/00094 | Loss: 0.1157 | EWC Loss: 0.0250 | CE Loss: 0.0907\n",
      "Train Epoch: 019 Batch: 00023/00094 | Loss: 0.1342 | EWC Loss: 0.0250 | CE Loss: 0.1092\n",
      "Train Epoch: 019 Batch: 00024/00094 | Loss: 0.2220 | EWC Loss: 0.0250 | CE Loss: 0.1970\n",
      "Train Epoch: 019 Batch: 00025/00094 | Loss: 0.0756 | EWC Loss: 0.0249 | CE Loss: 0.0506\n",
      "Train Epoch: 019 Batch: 00026/00094 | Loss: 0.1495 | EWC Loss: 0.0248 | CE Loss: 0.1247\n",
      "Train Epoch: 019 Batch: 00027/00094 | Loss: 0.1739 | EWC Loss: 0.0243 | CE Loss: 0.1496\n",
      "Train Epoch: 019 Batch: 00028/00094 | Loss: 0.0796 | EWC Loss: 0.0254 | CE Loss: 0.0542\n",
      "Train Epoch: 019 Batch: 00029/00094 | Loss: 0.2571 | EWC Loss: 0.0251 | CE Loss: 0.2320\n",
      "Train Epoch: 019 Batch: 00030/00094 | Loss: 0.0868 | EWC Loss: 0.0245 | CE Loss: 0.0623\n",
      "Train Epoch: 019 Batch: 00031/00094 | Loss: 0.2196 | EWC Loss: 0.0241 | CE Loss: 0.1955\n",
      "Train Epoch: 019 Batch: 00032/00094 | Loss: 0.0980 | EWC Loss: 0.0239 | CE Loss: 0.0741\n",
      "Train Epoch: 019 Batch: 00033/00094 | Loss: 0.1514 | EWC Loss: 0.0239 | CE Loss: 0.1275\n",
      "Train Epoch: 019 Batch: 00034/00094 | Loss: 0.1427 | EWC Loss: 0.0238 | CE Loss: 0.1188\n",
      "Train Epoch: 019 Batch: 00035/00094 | Loss: 0.0702 | EWC Loss: 0.0240 | CE Loss: 0.0462\n",
      "Train Epoch: 019 Batch: 00036/00094 | Loss: 0.1850 | EWC Loss: 0.0241 | CE Loss: 0.1610\n",
      "Train Epoch: 019 Batch: 00037/00094 | Loss: 0.0916 | EWC Loss: 0.0243 | CE Loss: 0.0672\n",
      "Train Epoch: 019 Batch: 00038/00094 | Loss: 0.0832 | EWC Loss: 0.0242 | CE Loss: 0.0591\n",
      "Train Epoch: 019 Batch: 00039/00094 | Loss: 0.1022 | EWC Loss: 0.0242 | CE Loss: 0.0780\n",
      "Train Epoch: 019 Batch: 00040/00094 | Loss: 0.1178 | EWC Loss: 0.0243 | CE Loss: 0.0935\n",
      "Train Epoch: 019 Batch: 00041/00094 | Loss: 0.0885 | EWC Loss: 0.0250 | CE Loss: 0.0635\n",
      "Train Epoch: 019 Batch: 00042/00094 | Loss: 0.1143 | EWC Loss: 0.0248 | CE Loss: 0.0895\n",
      "Train Epoch: 019 Batch: 00043/00094 | Loss: 0.1495 | EWC Loss: 0.0252 | CE Loss: 0.1242\n",
      "Train Epoch: 019 Batch: 00044/00094 | Loss: 0.1113 | EWC Loss: 0.0258 | CE Loss: 0.0856\n",
      "Train Epoch: 019 Batch: 00045/00094 | Loss: 0.1434 | EWC Loss: 0.0259 | CE Loss: 0.1175\n",
      "Train Epoch: 019 Batch: 00046/00094 | Loss: 0.1285 | EWC Loss: 0.0264 | CE Loss: 0.1021\n",
      "Train Epoch: 019 Batch: 00047/00094 | Loss: 0.1350 | EWC Loss: 0.0264 | CE Loss: 0.1087\n",
      "Train Epoch: 019 Batch: 00048/00094 | Loss: 0.1800 | EWC Loss: 0.0258 | CE Loss: 0.1542\n",
      "Train Epoch: 019 Batch: 00049/00094 | Loss: 0.1288 | EWC Loss: 0.0257 | CE Loss: 0.1030\n",
      "Train Epoch: 019 Batch: 00050/00094 | Loss: 0.0828 | EWC Loss: 0.0260 | CE Loss: 0.0567\n",
      "Train Epoch: 019 Batch: 00051/00094 | Loss: 0.0921 | EWC Loss: 0.0257 | CE Loss: 0.0664\n",
      "Train Epoch: 019 Batch: 00052/00094 | Loss: 0.1822 | EWC Loss: 0.0254 | CE Loss: 0.1567\n",
      "Train Epoch: 019 Batch: 00053/00094 | Loss: 0.0890 | EWC Loss: 0.0246 | CE Loss: 0.0644\n",
      "Train Epoch: 019 Batch: 00054/00094 | Loss: 0.1146 | EWC Loss: 0.0245 | CE Loss: 0.0901\n",
      "Train Epoch: 019 Batch: 00055/00094 | Loss: 0.1745 | EWC Loss: 0.0243 | CE Loss: 0.1502\n",
      "Train Epoch: 019 Batch: 00056/00094 | Loss: 0.1082 | EWC Loss: 0.0245 | CE Loss: 0.0837\n",
      "Train Epoch: 019 Batch: 00057/00094 | Loss: 0.1673 | EWC Loss: 0.0246 | CE Loss: 0.1427\n",
      "Train Epoch: 019 Batch: 00058/00094 | Loss: 0.1597 | EWC Loss: 0.0251 | CE Loss: 0.1345\n",
      "Train Epoch: 019 Batch: 00059/00094 | Loss: 0.1192 | EWC Loss: 0.0261 | CE Loss: 0.0931\n",
      "Train Epoch: 019 Batch: 00060/00094 | Loss: 0.1311 | EWC Loss: 0.0261 | CE Loss: 0.1050\n",
      "Train Epoch: 019 Batch: 00061/00094 | Loss: 0.0868 | EWC Loss: 0.0257 | CE Loss: 0.0611\n",
      "Train Epoch: 019 Batch: 00062/00094 | Loss: 0.1268 | EWC Loss: 0.0262 | CE Loss: 0.1006\n",
      "Train Epoch: 019 Batch: 00063/00094 | Loss: 0.1548 | EWC Loss: 0.0257 | CE Loss: 0.1291\n",
      "Train Epoch: 019 Batch: 00064/00094 | Loss: 0.0837 | EWC Loss: 0.0248 | CE Loss: 0.0589\n",
      "Train Epoch: 019 Batch: 00065/00094 | Loss: 0.1131 | EWC Loss: 0.0248 | CE Loss: 0.0883\n",
      "Train Epoch: 019 Batch: 00066/00094 | Loss: 0.1194 | EWC Loss: 0.0244 | CE Loss: 0.0950\n",
      "Train Epoch: 019 Batch: 00067/00094 | Loss: 0.2201 | EWC Loss: 0.0240 | CE Loss: 0.1961\n",
      "Train Epoch: 019 Batch: 00068/00094 | Loss: 0.1384 | EWC Loss: 0.0245 | CE Loss: 0.1139\n",
      "Train Epoch: 019 Batch: 00069/00094 | Loss: 0.1122 | EWC Loss: 0.0248 | CE Loss: 0.0874\n",
      "Train Epoch: 019 Batch: 00070/00094 | Loss: 0.1636 | EWC Loss: 0.0244 | CE Loss: 0.1392\n",
      "Train Epoch: 019 Batch: 00071/00094 | Loss: 0.0923 | EWC Loss: 0.0257 | CE Loss: 0.0666\n",
      "Train Epoch: 019 Batch: 00072/00094 | Loss: 0.1542 | EWC Loss: 0.0260 | CE Loss: 0.1282\n",
      "Train Epoch: 019 Batch: 00073/00094 | Loss: 0.0996 | EWC Loss: 0.0259 | CE Loss: 0.0737\n",
      "Train Epoch: 019 Batch: 00074/00094 | Loss: 0.0815 | EWC Loss: 0.0261 | CE Loss: 0.0554\n",
      "Train Epoch: 019 Batch: 00075/00094 | Loss: 0.1335 | EWC Loss: 0.0263 | CE Loss: 0.1072\n",
      "Train Epoch: 019 Batch: 00076/00094 | Loss: 0.0862 | EWC Loss: 0.0260 | CE Loss: 0.0602\n",
      "Train Epoch: 019 Batch: 00077/00094 | Loss: 0.1126 | EWC Loss: 0.0262 | CE Loss: 0.0864\n",
      "Train Epoch: 019 Batch: 00078/00094 | Loss: 0.0946 | EWC Loss: 0.0259 | CE Loss: 0.0688\n",
      "Train Epoch: 019 Batch: 00079/00094 | Loss: 0.1082 | EWC Loss: 0.0255 | CE Loss: 0.0827\n",
      "Train Epoch: 019 Batch: 00080/00094 | Loss: 0.0990 | EWC Loss: 0.0255 | CE Loss: 0.0735\n",
      "Train Epoch: 019 Batch: 00081/00094 | Loss: 0.1431 | EWC Loss: 0.0255 | CE Loss: 0.1176\n",
      "Train Epoch: 019 Batch: 00082/00094 | Loss: 0.0563 | EWC Loss: 0.0254 | CE Loss: 0.0309\n",
      "Train Epoch: 019 Batch: 00083/00094 | Loss: 0.0905 | EWC Loss: 0.0252 | CE Loss: 0.0652\n",
      "Train Epoch: 019 Batch: 00084/00094 | Loss: 0.1431 | EWC Loss: 0.0247 | CE Loss: 0.1185\n",
      "Train Epoch: 019 Batch: 00085/00094 | Loss: 0.1846 | EWC Loss: 0.0245 | CE Loss: 0.1601\n",
      "Train Epoch: 019 Batch: 00086/00094 | Loss: 0.1228 | EWC Loss: 0.0243 | CE Loss: 0.0985\n",
      "Train Epoch: 019 Batch: 00087/00094 | Loss: 0.0862 | EWC Loss: 0.0243 | CE Loss: 0.0619\n",
      "Train Epoch: 019 Batch: 00088/00094 | Loss: 0.1426 | EWC Loss: 0.0240 | CE Loss: 0.1186\n",
      "Train Epoch: 019 Batch: 00089/00094 | Loss: 0.1182 | EWC Loss: 0.0252 | CE Loss: 0.0931\n",
      "Train Epoch: 019 Batch: 00090/00094 | Loss: 0.1013 | EWC Loss: 0.0254 | CE Loss: 0.0759\n",
      "Train Epoch: 019 Batch: 00091/00094 | Loss: 0.1387 | EWC Loss: 0.0245 | CE Loss: 0.1142\n",
      "Train Epoch: 019 Batch: 00092/00094 | Loss: 0.1338 | EWC Loss: 0.0241 | CE Loss: 0.1097\n",
      "Train Epoch: 019 Batch: 00093/00094 | Loss: 0.1018 | EWC Loss: 0.0238 | CE Loss: 0.0780\n",
      "Train Epoch: 019 Batch: 00094/00094 | Loss: 0.1523 | EWC Loss: 0.0234 | CE Loss: 0.1289\n",
      "Train Epoch: 018 |Acc 96.43333 | Loss: 0.12849 | Task Loss 0.10374 | EWC Loss: 0.02475\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0820 | acc:97.2500\n",
      "[VAL Acc] Target: 97.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.3270 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1409 | acc:61.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 61.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2970 | acc:52.0992\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 52.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3661 | acc:55.8386\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5924 | acc:76.5250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5891 | acc:77.3119\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 77.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0186 | acc:56.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.88%\n",
      "[VAL Acc] Avg 66.00%\n",
      "\n",
      "\n",
      "---------- Starting epoch 19 ----------\n",
      "Train Epoch: 020 Batch: 00001/00094 | Loss: 0.2001 | EWC Loss: 0.0236 | CE Loss: 0.1765\n",
      "Train Epoch: 020 Batch: 00002/00094 | Loss: 0.0970 | EWC Loss: 0.0244 | CE Loss: 0.0726\n",
      "Train Epoch: 020 Batch: 00003/00094 | Loss: 0.1641 | EWC Loss: 0.0236 | CE Loss: 0.1405\n",
      "Train Epoch: 020 Batch: 00004/00094 | Loss: 0.1063 | EWC Loss: 0.0233 | CE Loss: 0.0830\n",
      "Train Epoch: 020 Batch: 00005/00094 | Loss: 0.1413 | EWC Loss: 0.0233 | CE Loss: 0.1180\n",
      "Train Epoch: 020 Batch: 00006/00094 | Loss: 0.0752 | EWC Loss: 0.0231 | CE Loss: 0.0521\n",
      "Train Epoch: 020 Batch: 00007/00094 | Loss: 0.0720 | EWC Loss: 0.0231 | CE Loss: 0.0489\n",
      "Train Epoch: 020 Batch: 00008/00094 | Loss: 0.0778 | EWC Loss: 0.0229 | CE Loss: 0.0549\n",
      "Train Epoch: 020 Batch: 00009/00094 | Loss: 0.1551 | EWC Loss: 0.0228 | CE Loss: 0.1323\n",
      "Train Epoch: 020 Batch: 00010/00094 | Loss: 0.1788 | EWC Loss: 0.0227 | CE Loss: 0.1562\n",
      "Train Epoch: 020 Batch: 00011/00094 | Loss: 0.0700 | EWC Loss: 0.0231 | CE Loss: 0.0469\n",
      "Train Epoch: 020 Batch: 00012/00094 | Loss: 0.1167 | EWC Loss: 0.0230 | CE Loss: 0.0937\n",
      "Train Epoch: 020 Batch: 00013/00094 | Loss: 0.0760 | EWC Loss: 0.0228 | CE Loss: 0.0532\n",
      "Train Epoch: 020 Batch: 00014/00094 | Loss: 0.0982 | EWC Loss: 0.0227 | CE Loss: 0.0755\n",
      "Train Epoch: 020 Batch: 00015/00094 | Loss: 0.0707 | EWC Loss: 0.0228 | CE Loss: 0.0479\n",
      "Train Epoch: 020 Batch: 00016/00094 | Loss: 0.1327 | EWC Loss: 0.0227 | CE Loss: 0.1100\n",
      "Train Epoch: 020 Batch: 00017/00094 | Loss: 0.0555 | EWC Loss: 0.0228 | CE Loss: 0.0327\n",
      "Train Epoch: 020 Batch: 00018/00094 | Loss: 0.0760 | EWC Loss: 0.0227 | CE Loss: 0.0533\n",
      "Train Epoch: 020 Batch: 00019/00094 | Loss: 0.0899 | EWC Loss: 0.0227 | CE Loss: 0.0672\n",
      "Train Epoch: 020 Batch: 00020/00094 | Loss: 0.1927 | EWC Loss: 0.0224 | CE Loss: 0.1703\n",
      "Train Epoch: 020 Batch: 00021/00094 | Loss: 0.1267 | EWC Loss: 0.0227 | CE Loss: 0.1039\n",
      "Train Epoch: 020 Batch: 00022/00094 | Loss: 0.1037 | EWC Loss: 0.0228 | CE Loss: 0.0809\n",
      "Train Epoch: 020 Batch: 00023/00094 | Loss: 0.1145 | EWC Loss: 0.0230 | CE Loss: 0.0915\n",
      "Train Epoch: 020 Batch: 00024/00094 | Loss: 0.1159 | EWC Loss: 0.0232 | CE Loss: 0.0927\n",
      "Train Epoch: 020 Batch: 00025/00094 | Loss: 0.0753 | EWC Loss: 0.0233 | CE Loss: 0.0520\n",
      "Train Epoch: 020 Batch: 00026/00094 | Loss: 0.1188 | EWC Loss: 0.0232 | CE Loss: 0.0956\n",
      "Train Epoch: 020 Batch: 00027/00094 | Loss: 0.1587 | EWC Loss: 0.0234 | CE Loss: 0.1353\n",
      "Train Epoch: 020 Batch: 00028/00094 | Loss: 0.1316 | EWC Loss: 0.0236 | CE Loss: 0.1080\n",
      "Train Epoch: 020 Batch: 00029/00094 | Loss: 0.0705 | EWC Loss: 0.0233 | CE Loss: 0.0472\n",
      "Train Epoch: 020 Batch: 00030/00094 | Loss: 0.0813 | EWC Loss: 0.0233 | CE Loss: 0.0580\n",
      "Train Epoch: 020 Batch: 00031/00094 | Loss: 0.1217 | EWC Loss: 0.0231 | CE Loss: 0.0985\n",
      "Train Epoch: 020 Batch: 00032/00094 | Loss: 0.1482 | EWC Loss: 0.0238 | CE Loss: 0.1245\n",
      "Train Epoch: 020 Batch: 00033/00094 | Loss: 0.1365 | EWC Loss: 0.0241 | CE Loss: 0.1124\n",
      "Train Epoch: 020 Batch: 00034/00094 | Loss: 0.0525 | EWC Loss: 0.0239 | CE Loss: 0.0286\n",
      "Train Epoch: 020 Batch: 00035/00094 | Loss: 0.1136 | EWC Loss: 0.0237 | CE Loss: 0.0899\n",
      "Train Epoch: 020 Batch: 00036/00094 | Loss: 0.0880 | EWC Loss: 0.0234 | CE Loss: 0.0645\n",
      "Train Epoch: 020 Batch: 00037/00094 | Loss: 0.0782 | EWC Loss: 0.0231 | CE Loss: 0.0550\n",
      "Train Epoch: 020 Batch: 00038/00094 | Loss: 0.0929 | EWC Loss: 0.0230 | CE Loss: 0.0698\n",
      "Train Epoch: 020 Batch: 00039/00094 | Loss: 0.0670 | EWC Loss: 0.0227 | CE Loss: 0.0443\n",
      "Train Epoch: 020 Batch: 00040/00094 | Loss: 0.0698 | EWC Loss: 0.0227 | CE Loss: 0.0471\n",
      "Train Epoch: 020 Batch: 00041/00094 | Loss: 0.0907 | EWC Loss: 0.0227 | CE Loss: 0.0680\n",
      "Train Epoch: 020 Batch: 00042/00094 | Loss: 0.1266 | EWC Loss: 0.0226 | CE Loss: 0.1040\n",
      "Train Epoch: 020 Batch: 00043/00094 | Loss: 0.1642 | EWC Loss: 0.0226 | CE Loss: 0.1415\n",
      "Train Epoch: 020 Batch: 00044/00094 | Loss: 0.1327 | EWC Loss: 0.0226 | CE Loss: 0.1101\n",
      "Train Epoch: 020 Batch: 00045/00094 | Loss: 0.1101 | EWC Loss: 0.0221 | CE Loss: 0.0880\n",
      "Train Epoch: 020 Batch: 00046/00094 | Loss: 0.1045 | EWC Loss: 0.0219 | CE Loss: 0.0826\n",
      "Train Epoch: 020 Batch: 00047/00094 | Loss: 0.1949 | EWC Loss: 0.0221 | CE Loss: 0.1728\n",
      "Train Epoch: 020 Batch: 00048/00094 | Loss: 0.1112 | EWC Loss: 0.0226 | CE Loss: 0.0886\n",
      "Train Epoch: 020 Batch: 00049/00094 | Loss: 0.0701 | EWC Loss: 0.0230 | CE Loss: 0.0471\n",
      "Train Epoch: 020 Batch: 00050/00094 | Loss: 0.1599 | EWC Loss: 0.0230 | CE Loss: 0.1369\n",
      "Train Epoch: 020 Batch: 00051/00094 | Loss: 0.0905 | EWC Loss: 0.0235 | CE Loss: 0.0670\n",
      "Train Epoch: 020 Batch: 00052/00094 | Loss: 0.1271 | EWC Loss: 0.0234 | CE Loss: 0.1037\n",
      "Train Epoch: 020 Batch: 00053/00094 | Loss: 0.0965 | EWC Loss: 0.0235 | CE Loss: 0.0731\n",
      "Train Epoch: 020 Batch: 00054/00094 | Loss: 0.0674 | EWC Loss: 0.0236 | CE Loss: 0.0437\n",
      "Train Epoch: 020 Batch: 00055/00094 | Loss: 0.1193 | EWC Loss: 0.0238 | CE Loss: 0.0955\n",
      "Train Epoch: 020 Batch: 00056/00094 | Loss: 0.1104 | EWC Loss: 0.0239 | CE Loss: 0.0865\n",
      "Train Epoch: 020 Batch: 00057/00094 | Loss: 0.1483 | EWC Loss: 0.0232 | CE Loss: 0.1251\n",
      "Train Epoch: 020 Batch: 00058/00094 | Loss: 0.1467 | EWC Loss: 0.0227 | CE Loss: 0.1239\n",
      "Train Epoch: 020 Batch: 00059/00094 | Loss: 0.1185 | EWC Loss: 0.0226 | CE Loss: 0.0959\n",
      "Train Epoch: 020 Batch: 00060/00094 | Loss: 0.0937 | EWC Loss: 0.0223 | CE Loss: 0.0713\n",
      "Train Epoch: 020 Batch: 00061/00094 | Loss: 0.1277 | EWC Loss: 0.0225 | CE Loss: 0.1053\n",
      "Train Epoch: 020 Batch: 00062/00094 | Loss: 0.1002 | EWC Loss: 0.0226 | CE Loss: 0.0775\n",
      "Train Epoch: 020 Batch: 00063/00094 | Loss: 0.1166 | EWC Loss: 0.0226 | CE Loss: 0.0940\n",
      "Train Epoch: 020 Batch: 00064/00094 | Loss: 0.1230 | EWC Loss: 0.0225 | CE Loss: 0.1005\n",
      "Train Epoch: 020 Batch: 00065/00094 | Loss: 0.2593 | EWC Loss: 0.0228 | CE Loss: 0.2366\n",
      "Train Epoch: 020 Batch: 00066/00094 | Loss: 0.1395 | EWC Loss: 0.0227 | CE Loss: 0.1167\n",
      "Train Epoch: 020 Batch: 00067/00094 | Loss: 0.0901 | EWC Loss: 0.0225 | CE Loss: 0.0676\n",
      "Train Epoch: 020 Batch: 00068/00094 | Loss: 0.1226 | EWC Loss: 0.0221 | CE Loss: 0.1005\n",
      "Train Epoch: 020 Batch: 00069/00094 | Loss: 0.0654 | EWC Loss: 0.0222 | CE Loss: 0.0432\n",
      "Train Epoch: 020 Batch: 00070/00094 | Loss: 0.0653 | EWC Loss: 0.0221 | CE Loss: 0.0431\n",
      "Train Epoch: 020 Batch: 00071/00094 | Loss: 0.1804 | EWC Loss: 0.0220 | CE Loss: 0.1584\n",
      "Train Epoch: 020 Batch: 00072/00094 | Loss: 0.1311 | EWC Loss: 0.0218 | CE Loss: 0.1092\n",
      "Train Epoch: 020 Batch: 00073/00094 | Loss: 0.0696 | EWC Loss: 0.0219 | CE Loss: 0.0477\n",
      "Train Epoch: 020 Batch: 00074/00094 | Loss: 0.1774 | EWC Loss: 0.0218 | CE Loss: 0.1556\n",
      "Train Epoch: 020 Batch: 00075/00094 | Loss: 0.1081 | EWC Loss: 0.0220 | CE Loss: 0.0861\n",
      "Train Epoch: 020 Batch: 00076/00094 | Loss: 0.0820 | EWC Loss: 0.0220 | CE Loss: 0.0600\n",
      "Train Epoch: 020 Batch: 00077/00094 | Loss: 0.1553 | EWC Loss: 0.0220 | CE Loss: 0.1334\n",
      "Train Epoch: 020 Batch: 00078/00094 | Loss: 0.0903 | EWC Loss: 0.0221 | CE Loss: 0.0681\n",
      "Train Epoch: 020 Batch: 00079/00094 | Loss: 0.1024 | EWC Loss: 0.0222 | CE Loss: 0.0802\n",
      "Train Epoch: 020 Batch: 00080/00094 | Loss: 0.1331 | EWC Loss: 0.0222 | CE Loss: 0.1109\n",
      "Train Epoch: 020 Batch: 00081/00094 | Loss: 0.1094 | EWC Loss: 0.0225 | CE Loss: 0.0869\n",
      "Train Epoch: 020 Batch: 00082/00094 | Loss: 0.0936 | EWC Loss: 0.0226 | CE Loss: 0.0710\n",
      "Train Epoch: 020 Batch: 00083/00094 | Loss: 0.1194 | EWC Loss: 0.0227 | CE Loss: 0.0967\n",
      "Train Epoch: 020 Batch: 00084/00094 | Loss: 0.1598 | EWC Loss: 0.0228 | CE Loss: 0.1371\n",
      "Train Epoch: 020 Batch: 00085/00094 | Loss: 0.1350 | EWC Loss: 0.0227 | CE Loss: 0.1123\n",
      "Train Epoch: 020 Batch: 00086/00094 | Loss: 0.1184 | EWC Loss: 0.0227 | CE Loss: 0.0957\n",
      "Train Epoch: 020 Batch: 00087/00094 | Loss: 0.1104 | EWC Loss: 0.0229 | CE Loss: 0.0875\n",
      "Train Epoch: 020 Batch: 00088/00094 | Loss: 0.1235 | EWC Loss: 0.0231 | CE Loss: 0.1004\n",
      "Train Epoch: 020 Batch: 00089/00094 | Loss: 0.1243 | EWC Loss: 0.0234 | CE Loss: 0.1008\n",
      "Train Epoch: 020 Batch: 00090/00094 | Loss: 0.0784 | EWC Loss: 0.0234 | CE Loss: 0.0549\n",
      "Train Epoch: 020 Batch: 00091/00094 | Loss: 0.1178 | EWC Loss: 0.0234 | CE Loss: 0.0944\n",
      "Train Epoch: 020 Batch: 00092/00094 | Loss: 0.1328 | EWC Loss: 0.0228 | CE Loss: 0.1100\n",
      "Train Epoch: 020 Batch: 00093/00094 | Loss: 0.0887 | EWC Loss: 0.0229 | CE Loss: 0.0658\n",
      "Train Epoch: 020 Batch: 00094/00094 | Loss: 0.1613 | EWC Loss: 0.0230 | CE Loss: 0.1383\n",
      "Train Epoch: 019 |Acc 96.48333 | Loss: 0.11496 | Task Loss 0.09210 | EWC Loss: 0.02286\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0794 | acc:97.6500\n",
      "[VAL Acc] Target: 97.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.3698 | acc:50.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1126 | acc:61.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 61.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2929 | acc:52.4809\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 52.48%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2779 | acc:58.2288\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7316 | acc:72.2736\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5357 | acc:79.5455\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 79.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0702 | acc:57.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.81%\n",
      "[VAL Acc] Avg 66.19%\n",
      "\n",
      "\n",
      "---------- Starting epoch 20 ----------\n",
      "Train Epoch: 021 Batch: 00001/00094 | Loss: 0.1203 | EWC Loss: 0.0229 | CE Loss: 0.0974\n",
      "Train Epoch: 021 Batch: 00002/00094 | Loss: 0.1153 | EWC Loss: 0.0231 | CE Loss: 0.0922\n",
      "Train Epoch: 021 Batch: 00003/00094 | Loss: 0.0740 | EWC Loss: 0.0232 | CE Loss: 0.0508\n",
      "Train Epoch: 021 Batch: 00004/00094 | Loss: 0.1231 | EWC Loss: 0.0230 | CE Loss: 0.1001\n",
      "Train Epoch: 021 Batch: 00005/00094 | Loss: 0.1316 | EWC Loss: 0.0236 | CE Loss: 0.1080\n",
      "Train Epoch: 021 Batch: 00006/00094 | Loss: 0.0986 | EWC Loss: 0.0237 | CE Loss: 0.0749\n",
      "Train Epoch: 021 Batch: 00007/00094 | Loss: 0.0749 | EWC Loss: 0.0235 | CE Loss: 0.0513\n",
      "Train Epoch: 021 Batch: 00008/00094 | Loss: 0.0864 | EWC Loss: 0.0235 | CE Loss: 0.0628\n",
      "Train Epoch: 021 Batch: 00009/00094 | Loss: 0.1265 | EWC Loss: 0.0235 | CE Loss: 0.1029\n",
      "Train Epoch: 021 Batch: 00010/00094 | Loss: 0.1732 | EWC Loss: 0.0236 | CE Loss: 0.1496\n",
      "Train Epoch: 021 Batch: 00011/00094 | Loss: 0.0691 | EWC Loss: 0.0238 | CE Loss: 0.0453\n",
      "Train Epoch: 021 Batch: 00012/00094 | Loss: 0.1703 | EWC Loss: 0.0239 | CE Loss: 0.1464\n",
      "Train Epoch: 021 Batch: 00013/00094 | Loss: 0.0921 | EWC Loss: 0.0245 | CE Loss: 0.0676\n",
      "Train Epoch: 021 Batch: 00014/00094 | Loss: 0.1516 | EWC Loss: 0.0246 | CE Loss: 0.1270\n",
      "Train Epoch: 021 Batch: 00015/00094 | Loss: 0.0982 | EWC Loss: 0.0250 | CE Loss: 0.0732\n",
      "Train Epoch: 021 Batch: 00016/00094 | Loss: 0.1527 | EWC Loss: 0.0249 | CE Loss: 0.1278\n",
      "Train Epoch: 021 Batch: 00017/00094 | Loss: 0.1483 | EWC Loss: 0.0250 | CE Loss: 0.1233\n",
      "Train Epoch: 021 Batch: 00018/00094 | Loss: 0.1089 | EWC Loss: 0.0251 | CE Loss: 0.0838\n",
      "Train Epoch: 021 Batch: 00019/00094 | Loss: 0.1203 | EWC Loss: 0.0257 | CE Loss: 0.0946\n",
      "Train Epoch: 021 Batch: 00020/00094 | Loss: 0.1048 | EWC Loss: 0.0258 | CE Loss: 0.0790\n",
      "Train Epoch: 021 Batch: 00021/00094 | Loss: 0.1044 | EWC Loss: 0.0255 | CE Loss: 0.0789\n",
      "Train Epoch: 021 Batch: 00022/00094 | Loss: 0.1266 | EWC Loss: 0.0263 | CE Loss: 0.1003\n",
      "Train Epoch: 021 Batch: 00023/00094 | Loss: 0.0728 | EWC Loss: 0.0264 | CE Loss: 0.0464\n",
      "Train Epoch: 021 Batch: 00024/00094 | Loss: 0.1173 | EWC Loss: 0.0260 | CE Loss: 0.0913\n",
      "Train Epoch: 021 Batch: 00025/00094 | Loss: 0.0765 | EWC Loss: 0.0258 | CE Loss: 0.0506\n",
      "Train Epoch: 021 Batch: 00026/00094 | Loss: 0.1241 | EWC Loss: 0.0247 | CE Loss: 0.0994\n",
      "Train Epoch: 021 Batch: 00027/00094 | Loss: 0.1006 | EWC Loss: 0.0247 | CE Loss: 0.0758\n",
      "Train Epoch: 021 Batch: 00028/00094 | Loss: 0.1286 | EWC Loss: 0.0246 | CE Loss: 0.1040\n",
      "Train Epoch: 021 Batch: 00029/00094 | Loss: 0.1196 | EWC Loss: 0.0245 | CE Loss: 0.0952\n",
      "Train Epoch: 021 Batch: 00030/00094 | Loss: 0.1674 | EWC Loss: 0.0245 | CE Loss: 0.1429\n",
      "Train Epoch: 021 Batch: 00031/00094 | Loss: 0.1773 | EWC Loss: 0.0245 | CE Loss: 0.1528\n",
      "Train Epoch: 021 Batch: 00032/00094 | Loss: 0.0910 | EWC Loss: 0.0257 | CE Loss: 0.0653\n",
      "Train Epoch: 021 Batch: 00033/00094 | Loss: 0.0979 | EWC Loss: 0.0255 | CE Loss: 0.0723\n",
      "Train Epoch: 021 Batch: 00034/00094 | Loss: 0.1515 | EWC Loss: 0.0255 | CE Loss: 0.1260\n",
      "Train Epoch: 021 Batch: 00035/00094 | Loss: 0.1359 | EWC Loss: 0.0251 | CE Loss: 0.1108\n",
      "Train Epoch: 021 Batch: 00036/00094 | Loss: 0.0739 | EWC Loss: 0.0249 | CE Loss: 0.0491\n",
      "Train Epoch: 021 Batch: 00037/00094 | Loss: 0.1176 | EWC Loss: 0.0246 | CE Loss: 0.0930\n",
      "Train Epoch: 021 Batch: 00038/00094 | Loss: 0.1085 | EWC Loss: 0.0246 | CE Loss: 0.0839\n",
      "Train Epoch: 021 Batch: 00039/00094 | Loss: 0.1368 | EWC Loss: 0.0246 | CE Loss: 0.1122\n",
      "Train Epoch: 021 Batch: 00040/00094 | Loss: 0.1955 | EWC Loss: 0.0245 | CE Loss: 0.1709\n",
      "Train Epoch: 021 Batch: 00041/00094 | Loss: 0.1474 | EWC Loss: 0.0246 | CE Loss: 0.1229\n",
      "Train Epoch: 021 Batch: 00042/00094 | Loss: 0.1885 | EWC Loss: 0.0247 | CE Loss: 0.1637\n",
      "Train Epoch: 021 Batch: 00043/00094 | Loss: 0.1037 | EWC Loss: 0.0251 | CE Loss: 0.0786\n",
      "Train Epoch: 021 Batch: 00044/00094 | Loss: 0.0927 | EWC Loss: 0.0255 | CE Loss: 0.0672\n",
      "Train Epoch: 021 Batch: 00045/00094 | Loss: 0.0563 | EWC Loss: 0.0253 | CE Loss: 0.0310\n",
      "Train Epoch: 021 Batch: 00046/00094 | Loss: 0.1034 | EWC Loss: 0.0251 | CE Loss: 0.0783\n",
      "Train Epoch: 021 Batch: 00047/00094 | Loss: 0.0839 | EWC Loss: 0.0252 | CE Loss: 0.0587\n",
      "Train Epoch: 021 Batch: 00048/00094 | Loss: 0.0627 | EWC Loss: 0.0251 | CE Loss: 0.0375\n",
      "Train Epoch: 021 Batch: 00049/00094 | Loss: 0.1478 | EWC Loss: 0.0254 | CE Loss: 0.1224\n",
      "Train Epoch: 021 Batch: 00050/00094 | Loss: 0.1084 | EWC Loss: 0.0250 | CE Loss: 0.0834\n",
      "Train Epoch: 021 Batch: 00051/00094 | Loss: 0.0871 | EWC Loss: 0.0253 | CE Loss: 0.0619\n",
      "Train Epoch: 021 Batch: 00052/00094 | Loss: 0.0951 | EWC Loss: 0.0246 | CE Loss: 0.0705\n",
      "Train Epoch: 021 Batch: 00053/00094 | Loss: 0.1468 | EWC Loss: 0.0245 | CE Loss: 0.1223\n",
      "Train Epoch: 021 Batch: 00054/00094 | Loss: 0.1076 | EWC Loss: 0.0244 | CE Loss: 0.0832\n",
      "Train Epoch: 021 Batch: 00055/00094 | Loss: 0.1029 | EWC Loss: 0.0241 | CE Loss: 0.0788\n",
      "Train Epoch: 021 Batch: 00056/00094 | Loss: 0.0767 | EWC Loss: 0.0244 | CE Loss: 0.0523\n",
      "Train Epoch: 021 Batch: 00057/00094 | Loss: 0.0945 | EWC Loss: 0.0241 | CE Loss: 0.0704\n",
      "Train Epoch: 021 Batch: 00058/00094 | Loss: 0.1342 | EWC Loss: 0.0240 | CE Loss: 0.1102\n",
      "Train Epoch: 021 Batch: 00059/00094 | Loss: 0.1365 | EWC Loss: 0.0239 | CE Loss: 0.1126\n",
      "Train Epoch: 021 Batch: 00060/00094 | Loss: 0.0689 | EWC Loss: 0.0247 | CE Loss: 0.0442\n",
      "Train Epoch: 021 Batch: 00061/00094 | Loss: 0.1221 | EWC Loss: 0.0249 | CE Loss: 0.0972\n",
      "Train Epoch: 021 Batch: 00062/00094 | Loss: 0.0882 | EWC Loss: 0.0245 | CE Loss: 0.0637\n",
      "Train Epoch: 021 Batch: 00063/00094 | Loss: 0.1376 | EWC Loss: 0.0246 | CE Loss: 0.1130\n",
      "Train Epoch: 021 Batch: 00064/00094 | Loss: 0.1268 | EWC Loss: 0.0246 | CE Loss: 0.1023\n",
      "Train Epoch: 021 Batch: 00065/00094 | Loss: 0.0966 | EWC Loss: 0.0244 | CE Loss: 0.0722\n",
      "Train Epoch: 021 Batch: 00066/00094 | Loss: 0.1051 | EWC Loss: 0.0243 | CE Loss: 0.0808\n",
      "Train Epoch: 021 Batch: 00067/00094 | Loss: 0.0605 | EWC Loss: 0.0240 | CE Loss: 0.0365\n",
      "Train Epoch: 021 Batch: 00068/00094 | Loss: 0.0781 | EWC Loss: 0.0243 | CE Loss: 0.0538\n",
      "Train Epoch: 021 Batch: 00069/00094 | Loss: 0.1303 | EWC Loss: 0.0243 | CE Loss: 0.1061\n",
      "Train Epoch: 021 Batch: 00070/00094 | Loss: 0.1298 | EWC Loss: 0.0247 | CE Loss: 0.1051\n",
      "Train Epoch: 021 Batch: 00071/00094 | Loss: 0.1173 | EWC Loss: 0.0248 | CE Loss: 0.0925\n",
      "Train Epoch: 021 Batch: 00072/00094 | Loss: 0.1268 | EWC Loss: 0.0257 | CE Loss: 0.1011\n",
      "Train Epoch: 021 Batch: 00073/00094 | Loss: 0.1590 | EWC Loss: 0.0261 | CE Loss: 0.1329\n",
      "Train Epoch: 021 Batch: 00074/00094 | Loss: 0.0694 | EWC Loss: 0.0264 | CE Loss: 0.0430\n",
      "Train Epoch: 021 Batch: 00075/00094 | Loss: 0.1467 | EWC Loss: 0.0262 | CE Loss: 0.1204\n",
      "Train Epoch: 021 Batch: 00076/00094 | Loss: 0.1170 | EWC Loss: 0.0258 | CE Loss: 0.0913\n",
      "Train Epoch: 021 Batch: 00077/00094 | Loss: 0.1289 | EWC Loss: 0.0252 | CE Loss: 0.1037\n",
      "Train Epoch: 021 Batch: 00078/00094 | Loss: 0.1197 | EWC Loss: 0.0254 | CE Loss: 0.0944\n",
      "Train Epoch: 021 Batch: 00079/00094 | Loss: 0.0795 | EWC Loss: 0.0255 | CE Loss: 0.0540\n",
      "Train Epoch: 021 Batch: 00080/00094 | Loss: 0.0927 | EWC Loss: 0.0258 | CE Loss: 0.0669\n",
      "Train Epoch: 021 Batch: 00081/00094 | Loss: 0.1202 | EWC Loss: 0.0257 | CE Loss: 0.0946\n",
      "Train Epoch: 021 Batch: 00082/00094 | Loss: 0.0606 | EWC Loss: 0.0251 | CE Loss: 0.0354\n",
      "Train Epoch: 021 Batch: 00083/00094 | Loss: 0.0723 | EWC Loss: 0.0250 | CE Loss: 0.0473\n",
      "Train Epoch: 021 Batch: 00084/00094 | Loss: 0.0943 | EWC Loss: 0.0247 | CE Loss: 0.0696\n",
      "Train Epoch: 021 Batch: 00085/00094 | Loss: 0.1293 | EWC Loss: 0.0246 | CE Loss: 0.1047\n",
      "Train Epoch: 021 Batch: 00086/00094 | Loss: 0.1094 | EWC Loss: 0.0248 | CE Loss: 0.0847\n",
      "Train Epoch: 021 Batch: 00087/00094 | Loss: 0.1128 | EWC Loss: 0.0243 | CE Loss: 0.0885\n",
      "Train Epoch: 021 Batch: 00088/00094 | Loss: 0.0823 | EWC Loss: 0.0239 | CE Loss: 0.0583\n",
      "Train Epoch: 021 Batch: 00089/00094 | Loss: 0.1870 | EWC Loss: 0.0239 | CE Loss: 0.1631\n",
      "Train Epoch: 021 Batch: 00090/00094 | Loss: 0.1029 | EWC Loss: 0.0248 | CE Loss: 0.0780\n",
      "Train Epoch: 021 Batch: 00091/00094 | Loss: 0.1277 | EWC Loss: 0.0251 | CE Loss: 0.1026\n",
      "Train Epoch: 021 Batch: 00092/00094 | Loss: 0.2734 | EWC Loss: 0.0240 | CE Loss: 0.2493\n",
      "Train Epoch: 021 Batch: 00093/00094 | Loss: 0.1580 | EWC Loss: 0.0241 | CE Loss: 0.1338\n",
      "Train Epoch: 021 Batch: 00094/00094 | Loss: 0.0607 | EWC Loss: 0.0239 | CE Loss: 0.0368\n",
      "Train Epoch: 020 |Acc 96.56667 | Loss: 0.11523 | Task Loss 0.09050 | EWC Loss: 0.02473\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0782 | acc:97.5000\n",
      "[VAL Acc] Target: 97.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.4053 | acc:49.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1189 | acc:63.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 63.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.3075 | acc:52.8626\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 52.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3458 | acc:55.7210\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6478 | acc:73.1054\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 73.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5387 | acc:80.4075\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 80.41%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0704 | acc:55.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.62%\n",
      "[VAL Acc] Avg 65.95%\n",
      "\n",
      "\n",
      "---------- Starting epoch 21 ----------\n",
      "Train Epoch: 022 Batch: 00001/00094 | Loss: 0.1421 | EWC Loss: 0.0237 | CE Loss: 0.1183\n",
      "Train Epoch: 022 Batch: 00002/00094 | Loss: 0.0471 | EWC Loss: 0.0237 | CE Loss: 0.0233\n",
      "Train Epoch: 022 Batch: 00003/00094 | Loss: 0.0716 | EWC Loss: 0.0237 | CE Loss: 0.0479\n",
      "Train Epoch: 022 Batch: 00004/00094 | Loss: 0.1434 | EWC Loss: 0.0236 | CE Loss: 0.1198\n",
      "Train Epoch: 022 Batch: 00005/00094 | Loss: 0.1279 | EWC Loss: 0.0237 | CE Loss: 0.1042\n",
      "Train Epoch: 022 Batch: 00006/00094 | Loss: 0.1162 | EWC Loss: 0.0238 | CE Loss: 0.0924\n",
      "Train Epoch: 022 Batch: 00007/00094 | Loss: 0.0651 | EWC Loss: 0.0233 | CE Loss: 0.0417\n",
      "Train Epoch: 022 Batch: 00008/00094 | Loss: 0.0775 | EWC Loss: 0.0230 | CE Loss: 0.0545\n",
      "Train Epoch: 022 Batch: 00009/00094 | Loss: 0.0970 | EWC Loss: 0.0231 | CE Loss: 0.0739\n",
      "Train Epoch: 022 Batch: 00010/00094 | Loss: 0.0553 | EWC Loss: 0.0231 | CE Loss: 0.0322\n",
      "Train Epoch: 022 Batch: 00011/00094 | Loss: 0.0930 | EWC Loss: 0.0230 | CE Loss: 0.0700\n",
      "Train Epoch: 022 Batch: 00012/00094 | Loss: 0.1343 | EWC Loss: 0.0230 | CE Loss: 0.1113\n",
      "Train Epoch: 022 Batch: 00013/00094 | Loss: 0.1165 | EWC Loss: 0.0240 | CE Loss: 0.0925\n",
      "Train Epoch: 022 Batch: 00014/00094 | Loss: 0.0712 | EWC Loss: 0.0250 | CE Loss: 0.0462\n",
      "Train Epoch: 022 Batch: 00015/00094 | Loss: 0.2054 | EWC Loss: 0.0251 | CE Loss: 0.1803\n",
      "Train Epoch: 022 Batch: 00016/00094 | Loss: 0.0881 | EWC Loss: 0.0260 | CE Loss: 0.0622\n",
      "Train Epoch: 022 Batch: 00017/00094 | Loss: 0.1349 | EWC Loss: 0.0265 | CE Loss: 0.1084\n",
      "Train Epoch: 022 Batch: 00018/00094 | Loss: 0.1783 | EWC Loss: 0.0257 | CE Loss: 0.1526\n",
      "Train Epoch: 022 Batch: 00019/00094 | Loss: 0.1314 | EWC Loss: 0.0258 | CE Loss: 0.1056\n",
      "Train Epoch: 022 Batch: 00020/00094 | Loss: 0.0934 | EWC Loss: 0.0258 | CE Loss: 0.0676\n",
      "Train Epoch: 022 Batch: 00021/00094 | Loss: 0.0860 | EWC Loss: 0.0257 | CE Loss: 0.0603\n",
      "Train Epoch: 022 Batch: 00022/00094 | Loss: 0.1424 | EWC Loss: 0.0252 | CE Loss: 0.1171\n",
      "Train Epoch: 022 Batch: 00023/00094 | Loss: 0.1649 | EWC Loss: 0.0257 | CE Loss: 0.1391\n",
      "Train Epoch: 022 Batch: 00024/00094 | Loss: 0.1105 | EWC Loss: 0.0258 | CE Loss: 0.0846\n",
      "Train Epoch: 022 Batch: 00025/00094 | Loss: 0.1185 | EWC Loss: 0.0253 | CE Loss: 0.0932\n",
      "Train Epoch: 022 Batch: 00026/00094 | Loss: 0.0614 | EWC Loss: 0.0255 | CE Loss: 0.0358\n",
      "Train Epoch: 022 Batch: 00027/00094 | Loss: 0.1631 | EWC Loss: 0.0251 | CE Loss: 0.1379\n",
      "Train Epoch: 022 Batch: 00028/00094 | Loss: 0.0713 | EWC Loss: 0.0249 | CE Loss: 0.0464\n",
      "Train Epoch: 022 Batch: 00029/00094 | Loss: 0.0807 | EWC Loss: 0.0249 | CE Loss: 0.0559\n",
      "Train Epoch: 022 Batch: 00030/00094 | Loss: 0.0958 | EWC Loss: 0.0245 | CE Loss: 0.0713\n",
      "Train Epoch: 022 Batch: 00031/00094 | Loss: 0.1382 | EWC Loss: 0.0242 | CE Loss: 0.1140\n",
      "Train Epoch: 022 Batch: 00032/00094 | Loss: 0.1732 | EWC Loss: 0.0242 | CE Loss: 0.1489\n",
      "Train Epoch: 022 Batch: 00033/00094 | Loss: 0.0796 | EWC Loss: 0.0249 | CE Loss: 0.0547\n",
      "Train Epoch: 022 Batch: 00034/00094 | Loss: 0.1282 | EWC Loss: 0.0248 | CE Loss: 0.1034\n",
      "Train Epoch: 022 Batch: 00035/00094 | Loss: 0.0692 | EWC Loss: 0.0246 | CE Loss: 0.0446\n",
      "Train Epoch: 022 Batch: 00036/00094 | Loss: 0.0837 | EWC Loss: 0.0247 | CE Loss: 0.0590\n",
      "Train Epoch: 022 Batch: 00037/00094 | Loss: 0.0714 | EWC Loss: 0.0245 | CE Loss: 0.0469\n",
      "Train Epoch: 022 Batch: 00038/00094 | Loss: 0.0784 | EWC Loss: 0.0241 | CE Loss: 0.0544\n",
      "Train Epoch: 022 Batch: 00039/00094 | Loss: 0.0790 | EWC Loss: 0.0239 | CE Loss: 0.0551\n",
      "Train Epoch: 022 Batch: 00040/00094 | Loss: 0.1234 | EWC Loss: 0.0241 | CE Loss: 0.0993\n",
      "Train Epoch: 022 Batch: 00041/00094 | Loss: 0.1450 | EWC Loss: 0.0243 | CE Loss: 0.1207\n",
      "Train Epoch: 022 Batch: 00042/00094 | Loss: 0.1278 | EWC Loss: 0.0240 | CE Loss: 0.1038\n",
      "Train Epoch: 022 Batch: 00043/00094 | Loss: 0.0764 | EWC Loss: 0.0244 | CE Loss: 0.0519\n",
      "Train Epoch: 022 Batch: 00044/00094 | Loss: 0.0953 | EWC Loss: 0.0244 | CE Loss: 0.0709\n",
      "Train Epoch: 022 Batch: 00045/00094 | Loss: 0.1706 | EWC Loss: 0.0246 | CE Loss: 0.1460\n",
      "Train Epoch: 022 Batch: 00046/00094 | Loss: 0.1211 | EWC Loss: 0.0250 | CE Loss: 0.0961\n",
      "Train Epoch: 022 Batch: 00047/00094 | Loss: 0.0573 | EWC Loss: 0.0255 | CE Loss: 0.0319\n",
      "Train Epoch: 022 Batch: 00048/00094 | Loss: 0.1212 | EWC Loss: 0.0253 | CE Loss: 0.0959\n",
      "Train Epoch: 022 Batch: 00049/00094 | Loss: 0.1317 | EWC Loss: 0.0248 | CE Loss: 0.1070\n",
      "Train Epoch: 022 Batch: 00050/00094 | Loss: 0.2296 | EWC Loss: 0.0250 | CE Loss: 0.2046\n",
      "Train Epoch: 022 Batch: 00051/00094 | Loss: 0.0699 | EWC Loss: 0.0247 | CE Loss: 0.0453\n",
      "Train Epoch: 022 Batch: 00052/00094 | Loss: 0.0608 | EWC Loss: 0.0244 | CE Loss: 0.0364\n",
      "Train Epoch: 022 Batch: 00053/00094 | Loss: 0.0876 | EWC Loss: 0.0242 | CE Loss: 0.0634\n",
      "Train Epoch: 022 Batch: 00054/00094 | Loss: 0.0778 | EWC Loss: 0.0242 | CE Loss: 0.0535\n",
      "Train Epoch: 022 Batch: 00055/00094 | Loss: 0.1177 | EWC Loss: 0.0244 | CE Loss: 0.0933\n",
      "Train Epoch: 022 Batch: 00056/00094 | Loss: 0.0731 | EWC Loss: 0.0244 | CE Loss: 0.0487\n",
      "Train Epoch: 022 Batch: 00057/00094 | Loss: 0.1114 | EWC Loss: 0.0238 | CE Loss: 0.0876\n",
      "Train Epoch: 022 Batch: 00058/00094 | Loss: 0.1443 | EWC Loss: 0.0231 | CE Loss: 0.1212\n",
      "Train Epoch: 022 Batch: 00059/00094 | Loss: 0.1623 | EWC Loss: 0.0230 | CE Loss: 0.1394\n",
      "Train Epoch: 022 Batch: 00060/00094 | Loss: 0.0968 | EWC Loss: 0.0234 | CE Loss: 0.0734\n",
      "Train Epoch: 022 Batch: 00061/00094 | Loss: 0.1751 | EWC Loss: 0.0237 | CE Loss: 0.1514\n",
      "Train Epoch: 022 Batch: 00062/00094 | Loss: 0.2095 | EWC Loss: 0.0235 | CE Loss: 0.1860\n",
      "Train Epoch: 022 Batch: 00063/00094 | Loss: 0.1175 | EWC Loss: 0.0232 | CE Loss: 0.0943\n",
      "Train Epoch: 022 Batch: 00064/00094 | Loss: 0.1457 | EWC Loss: 0.0229 | CE Loss: 0.1228\n",
      "Train Epoch: 022 Batch: 00065/00094 | Loss: 0.0890 | EWC Loss: 0.0234 | CE Loss: 0.0657\n",
      "Train Epoch: 022 Batch: 00066/00094 | Loss: 0.1134 | EWC Loss: 0.0235 | CE Loss: 0.0899\n",
      "Train Epoch: 022 Batch: 00067/00094 | Loss: 0.0972 | EWC Loss: 0.0235 | CE Loss: 0.0737\n",
      "Train Epoch: 022 Batch: 00068/00094 | Loss: 0.1028 | EWC Loss: 0.0234 | CE Loss: 0.0795\n",
      "Train Epoch: 022 Batch: 00069/00094 | Loss: 0.1070 | EWC Loss: 0.0236 | CE Loss: 0.0834\n",
      "Train Epoch: 022 Batch: 00070/00094 | Loss: 0.1893 | EWC Loss: 0.0234 | CE Loss: 0.1659\n",
      "Train Epoch: 022 Batch: 00071/00094 | Loss: 0.1179 | EWC Loss: 0.0232 | CE Loss: 0.0947\n",
      "Train Epoch: 022 Batch: 00072/00094 | Loss: 0.0939 | EWC Loss: 0.0236 | CE Loss: 0.0703\n",
      "Train Epoch: 022 Batch: 00073/00094 | Loss: 0.2376 | EWC Loss: 0.0236 | CE Loss: 0.2139\n",
      "Train Epoch: 022 Batch: 00074/00094 | Loss: 0.0747 | EWC Loss: 0.0233 | CE Loss: 0.0514\n",
      "Train Epoch: 022 Batch: 00075/00094 | Loss: 0.0693 | EWC Loss: 0.0231 | CE Loss: 0.0462\n",
      "Train Epoch: 022 Batch: 00076/00094 | Loss: 0.1559 | EWC Loss: 0.0228 | CE Loss: 0.1331\n",
      "Train Epoch: 022 Batch: 00077/00094 | Loss: 0.1361 | EWC Loss: 0.0227 | CE Loss: 0.1134\n",
      "Train Epoch: 022 Batch: 00078/00094 | Loss: 0.0771 | EWC Loss: 0.0232 | CE Loss: 0.0538\n",
      "Train Epoch: 022 Batch: 00079/00094 | Loss: 0.0946 | EWC Loss: 0.0233 | CE Loss: 0.0712\n",
      "Train Epoch: 022 Batch: 00080/00094 | Loss: 0.0600 | EWC Loss: 0.0234 | CE Loss: 0.0366\n",
      "Train Epoch: 022 Batch: 00081/00094 | Loss: 0.1146 | EWC Loss: 0.0232 | CE Loss: 0.0914\n",
      "Train Epoch: 022 Batch: 00082/00094 | Loss: 0.0908 | EWC Loss: 0.0234 | CE Loss: 0.0674\n",
      "Train Epoch: 022 Batch: 00083/00094 | Loss: 0.0620 | EWC Loss: 0.0235 | CE Loss: 0.0384\n",
      "Train Epoch: 022 Batch: 00084/00094 | Loss: 0.1001 | EWC Loss: 0.0234 | CE Loss: 0.0767\n",
      "Train Epoch: 022 Batch: 00085/00094 | Loss: 0.0905 | EWC Loss: 0.0236 | CE Loss: 0.0669\n",
      "Train Epoch: 022 Batch: 00086/00094 | Loss: 0.0925 | EWC Loss: 0.0236 | CE Loss: 0.0690\n",
      "Train Epoch: 022 Batch: 00087/00094 | Loss: 0.0819 | EWC Loss: 0.0239 | CE Loss: 0.0580\n",
      "Train Epoch: 022 Batch: 00088/00094 | Loss: 0.0811 | EWC Loss: 0.0238 | CE Loss: 0.0573\n",
      "Train Epoch: 022 Batch: 00089/00094 | Loss: 0.0870 | EWC Loss: 0.0238 | CE Loss: 0.0632\n",
      "Train Epoch: 022 Batch: 00090/00094 | Loss: 0.0909 | EWC Loss: 0.0235 | CE Loss: 0.0673\n",
      "Train Epoch: 022 Batch: 00091/00094 | Loss: 0.0813 | EWC Loss: 0.0239 | CE Loss: 0.0574\n",
      "Train Epoch: 022 Batch: 00092/00094 | Loss: 0.0677 | EWC Loss: 0.0237 | CE Loss: 0.0440\n",
      "Train Epoch: 022 Batch: 00093/00094 | Loss: 0.1385 | EWC Loss: 0.0238 | CE Loss: 0.1147\n",
      "Train Epoch: 022 Batch: 00094/00094 | Loss: 0.0647 | EWC Loss: 0.0240 | CE Loss: 0.0407\n",
      "Train Epoch: 021 |Acc 96.76667 | Loss: 0.10950 | Task Loss 0.08540 | EWC Loss: 0.02410\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0766 | acc:97.4000\n",
      "[VAL Acc] Target: 97.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.5040 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2025 | acc:59.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.4048 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3540 | acc:56.9749\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.97%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6509 | acc:72.9205\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5554 | acc:78.7618\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 78.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1060 | acc:55.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.62%\n",
      "[VAL Acc] Avg 65.01%\n",
      "\n",
      "\n",
      "---------- Starting epoch 22 ----------\n",
      "Train Epoch: 023 Batch: 00001/00094 | Loss: 0.0595 | EWC Loss: 0.0243 | CE Loss: 0.0352\n",
      "Train Epoch: 023 Batch: 00002/00094 | Loss: 0.0687 | EWC Loss: 0.0242 | CE Loss: 0.0445\n",
      "Train Epoch: 023 Batch: 00003/00094 | Loss: 0.1110 | EWC Loss: 0.0240 | CE Loss: 0.0870\n",
      "Train Epoch: 023 Batch: 00004/00094 | Loss: 0.0554 | EWC Loss: 0.0237 | CE Loss: 0.0317\n",
      "Train Epoch: 023 Batch: 00005/00094 | Loss: 0.1370 | EWC Loss: 0.0235 | CE Loss: 0.1135\n",
      "Train Epoch: 023 Batch: 00006/00094 | Loss: 0.1195 | EWC Loss: 0.0239 | CE Loss: 0.0956\n",
      "Train Epoch: 023 Batch: 00007/00094 | Loss: 0.1256 | EWC Loss: 0.0245 | CE Loss: 0.1011\n",
      "Train Epoch: 023 Batch: 00008/00094 | Loss: 0.1034 | EWC Loss: 0.0246 | CE Loss: 0.0788\n",
      "Train Epoch: 023 Batch: 00009/00094 | Loss: 0.1459 | EWC Loss: 0.0253 | CE Loss: 0.1206\n",
      "Train Epoch: 023 Batch: 00010/00094 | Loss: 0.1005 | EWC Loss: 0.0265 | CE Loss: 0.0740\n",
      "Train Epoch: 023 Batch: 00011/00094 | Loss: 0.0836 | EWC Loss: 0.0266 | CE Loss: 0.0570\n",
      "Train Epoch: 023 Batch: 00012/00094 | Loss: 0.0841 | EWC Loss: 0.0264 | CE Loss: 0.0577\n",
      "Train Epoch: 023 Batch: 00013/00094 | Loss: 0.0712 | EWC Loss: 0.0261 | CE Loss: 0.0450\n",
      "Train Epoch: 023 Batch: 00014/00094 | Loss: 0.0668 | EWC Loss: 0.0262 | CE Loss: 0.0406\n",
      "Train Epoch: 023 Batch: 00015/00094 | Loss: 0.0861 | EWC Loss: 0.0262 | CE Loss: 0.0599\n",
      "Train Epoch: 023 Batch: 00016/00094 | Loss: 0.0742 | EWC Loss: 0.0263 | CE Loss: 0.0480\n",
      "Train Epoch: 023 Batch: 00017/00094 | Loss: 0.0956 | EWC Loss: 0.0254 | CE Loss: 0.0702\n",
      "Train Epoch: 023 Batch: 00018/00094 | Loss: 0.0644 | EWC Loss: 0.0252 | CE Loss: 0.0392\n",
      "Train Epoch: 023 Batch: 00019/00094 | Loss: 0.0526 | EWC Loss: 0.0253 | CE Loss: 0.0273\n",
      "Train Epoch: 023 Batch: 00020/00094 | Loss: 0.1138 | EWC Loss: 0.0250 | CE Loss: 0.0888\n",
      "Train Epoch: 023 Batch: 00021/00094 | Loss: 0.0957 | EWC Loss: 0.0251 | CE Loss: 0.0706\n",
      "Train Epoch: 023 Batch: 00022/00094 | Loss: 0.0752 | EWC Loss: 0.0247 | CE Loss: 0.0505\n",
      "Train Epoch: 023 Batch: 00023/00094 | Loss: 0.0973 | EWC Loss: 0.0243 | CE Loss: 0.0730\n",
      "Train Epoch: 023 Batch: 00024/00094 | Loss: 0.1242 | EWC Loss: 0.0240 | CE Loss: 0.1002\n",
      "Train Epoch: 023 Batch: 00025/00094 | Loss: 0.1396 | EWC Loss: 0.0242 | CE Loss: 0.1154\n",
      "Train Epoch: 023 Batch: 00026/00094 | Loss: 0.0671 | EWC Loss: 0.0239 | CE Loss: 0.0432\n",
      "Train Epoch: 023 Batch: 00027/00094 | Loss: 0.2239 | EWC Loss: 0.0238 | CE Loss: 0.2001\n",
      "Train Epoch: 023 Batch: 00028/00094 | Loss: 0.0549 | EWC Loss: 0.0242 | CE Loss: 0.0307\n",
      "Train Epoch: 023 Batch: 00029/00094 | Loss: 0.1198 | EWC Loss: 0.0242 | CE Loss: 0.0957\n",
      "Train Epoch: 023 Batch: 00030/00094 | Loss: 0.0934 | EWC Loss: 0.0247 | CE Loss: 0.0688\n",
      "Train Epoch: 023 Batch: 00031/00094 | Loss: 0.0776 | EWC Loss: 0.0240 | CE Loss: 0.0536\n",
      "Train Epoch: 023 Batch: 00032/00094 | Loss: 0.1132 | EWC Loss: 0.0239 | CE Loss: 0.0893\n",
      "Train Epoch: 023 Batch: 00033/00094 | Loss: 0.0486 | EWC Loss: 0.0236 | CE Loss: 0.0250\n",
      "Train Epoch: 023 Batch: 00034/00094 | Loss: 0.0975 | EWC Loss: 0.0234 | CE Loss: 0.0741\n",
      "Train Epoch: 023 Batch: 00035/00094 | Loss: 0.1194 | EWC Loss: 0.0230 | CE Loss: 0.0964\n",
      "Train Epoch: 023 Batch: 00036/00094 | Loss: 0.1375 | EWC Loss: 0.0229 | CE Loss: 0.1146\n",
      "Train Epoch: 023 Batch: 00037/00094 | Loss: 0.0497 | EWC Loss: 0.0232 | CE Loss: 0.0265\n",
      "Train Epoch: 023 Batch: 00038/00094 | Loss: 0.0794 | EWC Loss: 0.0231 | CE Loss: 0.0562\n",
      "Train Epoch: 023 Batch: 00039/00094 | Loss: 0.1194 | EWC Loss: 0.0230 | CE Loss: 0.0965\n",
      "Train Epoch: 023 Batch: 00040/00094 | Loss: 0.0766 | EWC Loss: 0.0229 | CE Loss: 0.0537\n",
      "Train Epoch: 023 Batch: 00041/00094 | Loss: 0.1290 | EWC Loss: 0.0228 | CE Loss: 0.1062\n",
      "Train Epoch: 023 Batch: 00042/00094 | Loss: 0.1290 | EWC Loss: 0.0227 | CE Loss: 0.1062\n",
      "Train Epoch: 023 Batch: 00043/00094 | Loss: 0.1858 | EWC Loss: 0.0230 | CE Loss: 0.1628\n",
      "Train Epoch: 023 Batch: 00044/00094 | Loss: 0.1178 | EWC Loss: 0.0234 | CE Loss: 0.0944\n",
      "Train Epoch: 023 Batch: 00045/00094 | Loss: 0.1319 | EWC Loss: 0.0236 | CE Loss: 0.1083\n",
      "Train Epoch: 023 Batch: 00046/00094 | Loss: 0.1025 | EWC Loss: 0.0237 | CE Loss: 0.0788\n",
      "Train Epoch: 023 Batch: 00047/00094 | Loss: 0.0832 | EWC Loss: 0.0234 | CE Loss: 0.0598\n",
      "Train Epoch: 023 Batch: 00048/00094 | Loss: 0.1128 | EWC Loss: 0.0234 | CE Loss: 0.0894\n",
      "Train Epoch: 023 Batch: 00049/00094 | Loss: 0.1141 | EWC Loss: 0.0241 | CE Loss: 0.0900\n",
      "Train Epoch: 023 Batch: 00050/00094 | Loss: 0.1160 | EWC Loss: 0.0258 | CE Loss: 0.0902\n",
      "Train Epoch: 023 Batch: 00051/00094 | Loss: 0.0790 | EWC Loss: 0.0265 | CE Loss: 0.0525\n",
      "Train Epoch: 023 Batch: 00052/00094 | Loss: 0.1027 | EWC Loss: 0.0264 | CE Loss: 0.0764\n",
      "Train Epoch: 023 Batch: 00053/00094 | Loss: 0.1085 | EWC Loss: 0.0258 | CE Loss: 0.0827\n",
      "Train Epoch: 023 Batch: 00054/00094 | Loss: 0.1551 | EWC Loss: 0.0257 | CE Loss: 0.1293\n",
      "Train Epoch: 023 Batch: 00055/00094 | Loss: 0.0809 | EWC Loss: 0.0259 | CE Loss: 0.0550\n",
      "Train Epoch: 023 Batch: 00056/00094 | Loss: 0.0739 | EWC Loss: 0.0256 | CE Loss: 0.0483\n",
      "Train Epoch: 023 Batch: 00057/00094 | Loss: 0.2115 | EWC Loss: 0.0252 | CE Loss: 0.1863\n",
      "Train Epoch: 023 Batch: 00058/00094 | Loss: 0.1158 | EWC Loss: 0.0251 | CE Loss: 0.0907\n",
      "Train Epoch: 023 Batch: 00059/00094 | Loss: 0.1022 | EWC Loss: 0.0250 | CE Loss: 0.0772\n",
      "Train Epoch: 023 Batch: 00060/00094 | Loss: 0.1040 | EWC Loss: 0.0254 | CE Loss: 0.0786\n",
      "Train Epoch: 023 Batch: 00061/00094 | Loss: 0.0795 | EWC Loss: 0.0253 | CE Loss: 0.0543\n",
      "Train Epoch: 023 Batch: 00062/00094 | Loss: 0.0794 | EWC Loss: 0.0252 | CE Loss: 0.0542\n",
      "Train Epoch: 023 Batch: 00063/00094 | Loss: 0.1091 | EWC Loss: 0.0251 | CE Loss: 0.0840\n",
      "Train Epoch: 023 Batch: 00064/00094 | Loss: 0.1399 | EWC Loss: 0.0253 | CE Loss: 0.1146\n",
      "Train Epoch: 023 Batch: 00065/00094 | Loss: 0.0571 | EWC Loss: 0.0253 | CE Loss: 0.0318\n",
      "Train Epoch: 023 Batch: 00066/00094 | Loss: 0.1254 | EWC Loss: 0.0253 | CE Loss: 0.1001\n",
      "Train Epoch: 023 Batch: 00067/00094 | Loss: 0.0583 | EWC Loss: 0.0257 | CE Loss: 0.0326\n",
      "Train Epoch: 023 Batch: 00068/00094 | Loss: 0.1045 | EWC Loss: 0.0254 | CE Loss: 0.0792\n",
      "Train Epoch: 023 Batch: 00069/00094 | Loss: 0.1383 | EWC Loss: 0.0257 | CE Loss: 0.1127\n",
      "Train Epoch: 023 Batch: 00070/00094 | Loss: 0.0813 | EWC Loss: 0.0254 | CE Loss: 0.0559\n",
      "Train Epoch: 023 Batch: 00071/00094 | Loss: 0.0888 | EWC Loss: 0.0250 | CE Loss: 0.0638\n",
      "Train Epoch: 023 Batch: 00072/00094 | Loss: 0.0788 | EWC Loss: 0.0249 | CE Loss: 0.0540\n",
      "Train Epoch: 023 Batch: 00073/00094 | Loss: 0.1400 | EWC Loss: 0.0246 | CE Loss: 0.1153\n",
      "Train Epoch: 023 Batch: 00074/00094 | Loss: 0.1239 | EWC Loss: 0.0243 | CE Loss: 0.0996\n",
      "Train Epoch: 023 Batch: 00075/00094 | Loss: 0.0933 | EWC Loss: 0.0241 | CE Loss: 0.0693\n",
      "Train Epoch: 023 Batch: 00076/00094 | Loss: 0.0758 | EWC Loss: 0.0240 | CE Loss: 0.0519\n",
      "Train Epoch: 023 Batch: 00077/00094 | Loss: 0.0663 | EWC Loss: 0.0237 | CE Loss: 0.0426\n",
      "Train Epoch: 023 Batch: 00078/00094 | Loss: 0.0876 | EWC Loss: 0.0237 | CE Loss: 0.0638\n",
      "Train Epoch: 023 Batch: 00079/00094 | Loss: 0.1715 | EWC Loss: 0.0237 | CE Loss: 0.1478\n",
      "Train Epoch: 023 Batch: 00080/00094 | Loss: 0.0951 | EWC Loss: 0.0235 | CE Loss: 0.0716\n",
      "Train Epoch: 023 Batch: 00081/00094 | Loss: 0.1011 | EWC Loss: 0.0236 | CE Loss: 0.0775\n",
      "Train Epoch: 023 Batch: 00082/00094 | Loss: 0.0640 | EWC Loss: 0.0236 | CE Loss: 0.0403\n",
      "Train Epoch: 023 Batch: 00083/00094 | Loss: 0.0481 | EWC Loss: 0.0237 | CE Loss: 0.0244\n",
      "Train Epoch: 023 Batch: 00084/00094 | Loss: 0.0908 | EWC Loss: 0.0236 | CE Loss: 0.0672\n",
      "Train Epoch: 023 Batch: 00085/00094 | Loss: 0.0841 | EWC Loss: 0.0234 | CE Loss: 0.0606\n",
      "Train Epoch: 023 Batch: 00086/00094 | Loss: 0.0584 | EWC Loss: 0.0235 | CE Loss: 0.0349\n",
      "Train Epoch: 023 Batch: 00087/00094 | Loss: 0.0929 | EWC Loss: 0.0235 | CE Loss: 0.0694\n",
      "Train Epoch: 023 Batch: 00088/00094 | Loss: 0.0651 | EWC Loss: 0.0234 | CE Loss: 0.0418\n",
      "Train Epoch: 023 Batch: 00089/00094 | Loss: 0.1405 | EWC Loss: 0.0233 | CE Loss: 0.1172\n",
      "Train Epoch: 023 Batch: 00090/00094 | Loss: 0.1026 | EWC Loss: 0.0234 | CE Loss: 0.0792\n",
      "Train Epoch: 023 Batch: 00091/00094 | Loss: 0.1050 | EWC Loss: 0.0237 | CE Loss: 0.0813\n",
      "Train Epoch: 023 Batch: 00092/00094 | Loss: 0.1003 | EWC Loss: 0.0243 | CE Loss: 0.0760\n",
      "Train Epoch: 023 Batch: 00093/00094 | Loss: 0.0979 | EWC Loss: 0.0241 | CE Loss: 0.0737\n",
      "Train Epoch: 023 Batch: 00094/00094 | Loss: 0.1162 | EWC Loss: 0.0240 | CE Loss: 0.0922\n",
      "Train Epoch: 022 |Acc 97.46667 | Loss: 0.10049 | Task Loss 0.07604 | EWC Loss: 0.02445\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0669 | acc:97.7500\n",
      "[VAL Acc] Target: 97.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.5988 | acc:49.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.1999 | acc:61.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 61.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5549 | acc:49.2366\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3980 | acc:56.8574\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6562 | acc:75.7856\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5882 | acc:78.0956\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 78.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1285 | acc:56.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.44%\n",
      "[VAL Acc] Avg 65.73%\n",
      "\n",
      "\n",
      "---------- Starting epoch 23 ----------\n",
      "Train Epoch: 024 Batch: 00001/00094 | Loss: 0.1122 | EWC Loss: 0.0241 | CE Loss: 0.0881\n",
      "Train Epoch: 024 Batch: 00002/00094 | Loss: 0.0720 | EWC Loss: 0.0238 | CE Loss: 0.0483\n",
      "Train Epoch: 024 Batch: 00003/00094 | Loss: 0.0658 | EWC Loss: 0.0237 | CE Loss: 0.0421\n",
      "Train Epoch: 024 Batch: 00004/00094 | Loss: 0.1331 | EWC Loss: 0.0236 | CE Loss: 0.1095\n",
      "Train Epoch: 024 Batch: 00005/00094 | Loss: 0.0717 | EWC Loss: 0.0233 | CE Loss: 0.0485\n",
      "Train Epoch: 024 Batch: 00006/00094 | Loss: 0.0829 | EWC Loss: 0.0233 | CE Loss: 0.0596\n",
      "Train Epoch: 024 Batch: 00007/00094 | Loss: 0.0658 | EWC Loss: 0.0234 | CE Loss: 0.0424\n",
      "Train Epoch: 024 Batch: 00008/00094 | Loss: 0.0652 | EWC Loss: 0.0233 | CE Loss: 0.0419\n",
      "Train Epoch: 024 Batch: 00009/00094 | Loss: 0.1075 | EWC Loss: 0.0233 | CE Loss: 0.0843\n",
      "Train Epoch: 024 Batch: 00010/00094 | Loss: 0.1050 | EWC Loss: 0.0229 | CE Loss: 0.0820\n",
      "Train Epoch: 024 Batch: 00011/00094 | Loss: 0.0753 | EWC Loss: 0.0231 | CE Loss: 0.0523\n",
      "Train Epoch: 024 Batch: 00012/00094 | Loss: 0.1269 | EWC Loss: 0.0231 | CE Loss: 0.1038\n",
      "Train Epoch: 024 Batch: 00013/00094 | Loss: 0.1130 | EWC Loss: 0.0232 | CE Loss: 0.0898\n",
      "Train Epoch: 024 Batch: 00014/00094 | Loss: 0.1034 | EWC Loss: 0.0232 | CE Loss: 0.0802\n",
      "Train Epoch: 024 Batch: 00015/00094 | Loss: 0.0858 | EWC Loss: 0.0233 | CE Loss: 0.0625\n",
      "Train Epoch: 024 Batch: 00016/00094 | Loss: 0.1152 | EWC Loss: 0.0234 | CE Loss: 0.0919\n",
      "Train Epoch: 024 Batch: 00017/00094 | Loss: 0.0469 | EWC Loss: 0.0233 | CE Loss: 0.0236\n",
      "Train Epoch: 024 Batch: 00018/00094 | Loss: 0.1277 | EWC Loss: 0.0232 | CE Loss: 0.1045\n",
      "Train Epoch: 024 Batch: 00019/00094 | Loss: 0.0821 | EWC Loss: 0.0230 | CE Loss: 0.0591\n",
      "Train Epoch: 024 Batch: 00020/00094 | Loss: 0.0529 | EWC Loss: 0.0228 | CE Loss: 0.0301\n",
      "Train Epoch: 024 Batch: 00021/00094 | Loss: 0.0520 | EWC Loss: 0.0228 | CE Loss: 0.0292\n",
      "Train Epoch: 024 Batch: 00022/00094 | Loss: 0.1163 | EWC Loss: 0.0228 | CE Loss: 0.0935\n",
      "Train Epoch: 024 Batch: 00023/00094 | Loss: 0.0819 | EWC Loss: 0.0228 | CE Loss: 0.0591\n",
      "Train Epoch: 024 Batch: 00024/00094 | Loss: 0.1178 | EWC Loss: 0.0229 | CE Loss: 0.0949\n",
      "Train Epoch: 024 Batch: 00025/00094 | Loss: 0.0510 | EWC Loss: 0.0228 | CE Loss: 0.0282\n",
      "Train Epoch: 024 Batch: 00026/00094 | Loss: 0.0742 | EWC Loss: 0.0228 | CE Loss: 0.0514\n",
      "Train Epoch: 024 Batch: 00027/00094 | Loss: 0.2136 | EWC Loss: 0.0227 | CE Loss: 0.1909\n",
      "Train Epoch: 024 Batch: 00028/00094 | Loss: 0.0661 | EWC Loss: 0.0228 | CE Loss: 0.0433\n",
      "Train Epoch: 024 Batch: 00029/00094 | Loss: 0.0604 | EWC Loss: 0.0229 | CE Loss: 0.0375\n",
      "Train Epoch: 024 Batch: 00030/00094 | Loss: 0.0668 | EWC Loss: 0.0228 | CE Loss: 0.0440\n",
      "Train Epoch: 024 Batch: 00031/00094 | Loss: 0.0660 | EWC Loss: 0.0228 | CE Loss: 0.0432\n",
      "Train Epoch: 024 Batch: 00032/00094 | Loss: 0.0521 | EWC Loss: 0.0228 | CE Loss: 0.0293\n",
      "Train Epoch: 024 Batch: 00033/00094 | Loss: 0.0808 | EWC Loss: 0.0227 | CE Loss: 0.0581\n",
      "Train Epoch: 024 Batch: 00034/00094 | Loss: 0.1176 | EWC Loss: 0.0225 | CE Loss: 0.0951\n",
      "Train Epoch: 024 Batch: 00035/00094 | Loss: 0.0848 | EWC Loss: 0.0232 | CE Loss: 0.0616\n",
      "Train Epoch: 024 Batch: 00036/00094 | Loss: 0.1371 | EWC Loss: 0.0230 | CE Loss: 0.1141\n",
      "Train Epoch: 024 Batch: 00037/00094 | Loss: 0.0857 | EWC Loss: 0.0227 | CE Loss: 0.0629\n",
      "Train Epoch: 024 Batch: 00038/00094 | Loss: 0.0638 | EWC Loss: 0.0225 | CE Loss: 0.0413\n",
      "Train Epoch: 024 Batch: 00039/00094 | Loss: 0.1066 | EWC Loss: 0.0224 | CE Loss: 0.0842\n",
      "Train Epoch: 024 Batch: 00040/00094 | Loss: 0.1217 | EWC Loss: 0.0222 | CE Loss: 0.0994\n",
      "Train Epoch: 024 Batch: 00041/00094 | Loss: 0.0717 | EWC Loss: 0.0224 | CE Loss: 0.0493\n",
      "Train Epoch: 024 Batch: 00042/00094 | Loss: 0.0649 | EWC Loss: 0.0224 | CE Loss: 0.0425\n",
      "Train Epoch: 024 Batch: 00043/00094 | Loss: 0.2064 | EWC Loss: 0.0224 | CE Loss: 0.1840\n",
      "Train Epoch: 024 Batch: 00044/00094 | Loss: 0.1452 | EWC Loss: 0.0229 | CE Loss: 0.1223\n",
      "Train Epoch: 024 Batch: 00045/00094 | Loss: 0.0681 | EWC Loss: 0.0229 | CE Loss: 0.0452\n",
      "Train Epoch: 024 Batch: 00046/00094 | Loss: 0.0819 | EWC Loss: 0.0227 | CE Loss: 0.0592\n",
      "Train Epoch: 024 Batch: 00047/00094 | Loss: 0.0465 | EWC Loss: 0.0226 | CE Loss: 0.0239\n",
      "Train Epoch: 024 Batch: 00048/00094 | Loss: 0.0949 | EWC Loss: 0.0226 | CE Loss: 0.0724\n",
      "Train Epoch: 024 Batch: 00049/00094 | Loss: 0.1070 | EWC Loss: 0.0226 | CE Loss: 0.0844\n",
      "Train Epoch: 024 Batch: 00050/00094 | Loss: 0.0755 | EWC Loss: 0.0227 | CE Loss: 0.0528\n",
      "Train Epoch: 024 Batch: 00051/00094 | Loss: 0.0700 | EWC Loss: 0.0227 | CE Loss: 0.0474\n",
      "Train Epoch: 024 Batch: 00052/00094 | Loss: 0.1707 | EWC Loss: 0.0228 | CE Loss: 0.1479\n",
      "Train Epoch: 024 Batch: 00053/00094 | Loss: 0.1022 | EWC Loss: 0.0229 | CE Loss: 0.0793\n",
      "Train Epoch: 024 Batch: 00054/00094 | Loss: 0.0816 | EWC Loss: 0.0228 | CE Loss: 0.0588\n",
      "Train Epoch: 024 Batch: 00055/00094 | Loss: 0.1604 | EWC Loss: 0.0226 | CE Loss: 0.1377\n",
      "Train Epoch: 024 Batch: 00056/00094 | Loss: 0.1057 | EWC Loss: 0.0225 | CE Loss: 0.0832\n",
      "Train Epoch: 024 Batch: 00057/00094 | Loss: 0.0603 | EWC Loss: 0.0224 | CE Loss: 0.0379\n",
      "Train Epoch: 024 Batch: 00058/00094 | Loss: 0.1054 | EWC Loss: 0.0224 | CE Loss: 0.0829\n",
      "Train Epoch: 024 Batch: 00059/00094 | Loss: 0.1021 | EWC Loss: 0.0225 | CE Loss: 0.0796\n",
      "Train Epoch: 024 Batch: 00060/00094 | Loss: 0.1972 | EWC Loss: 0.0224 | CE Loss: 0.1747\n",
      "Train Epoch: 024 Batch: 00061/00094 | Loss: 0.1003 | EWC Loss: 0.0225 | CE Loss: 0.0778\n",
      "Train Epoch: 024 Batch: 00062/00094 | Loss: 0.0584 | EWC Loss: 0.0227 | CE Loss: 0.0357\n",
      "Train Epoch: 024 Batch: 00063/00094 | Loss: 0.0678 | EWC Loss: 0.0227 | CE Loss: 0.0451\n",
      "Train Epoch: 024 Batch: 00064/00094 | Loss: 0.0742 | EWC Loss: 0.0225 | CE Loss: 0.0518\n",
      "Train Epoch: 024 Batch: 00065/00094 | Loss: 0.1152 | EWC Loss: 0.0225 | CE Loss: 0.0927\n",
      "Train Epoch: 024 Batch: 00066/00094 | Loss: 0.0781 | EWC Loss: 0.0225 | CE Loss: 0.0556\n",
      "Train Epoch: 024 Batch: 00067/00094 | Loss: 0.0820 | EWC Loss: 0.0226 | CE Loss: 0.0594\n",
      "Train Epoch: 024 Batch: 00068/00094 | Loss: 0.0717 | EWC Loss: 0.0226 | CE Loss: 0.0490\n",
      "Train Epoch: 024 Batch: 00069/00094 | Loss: 0.0961 | EWC Loss: 0.0226 | CE Loss: 0.0735\n",
      "Train Epoch: 024 Batch: 00070/00094 | Loss: 0.1063 | EWC Loss: 0.0228 | CE Loss: 0.0835\n",
      "Train Epoch: 024 Batch: 00071/00094 | Loss: 0.1239 | EWC Loss: 0.0227 | CE Loss: 0.1012\n",
      "Train Epoch: 024 Batch: 00072/00094 | Loss: 0.1092 | EWC Loss: 0.0227 | CE Loss: 0.0865\n",
      "Train Epoch: 024 Batch: 00073/00094 | Loss: 0.0571 | EWC Loss: 0.0228 | CE Loss: 0.0343\n",
      "Train Epoch: 024 Batch: 00074/00094 | Loss: 0.0784 | EWC Loss: 0.0226 | CE Loss: 0.0558\n",
      "Train Epoch: 024 Batch: 00075/00094 | Loss: 0.0715 | EWC Loss: 0.0226 | CE Loss: 0.0489\n",
      "Train Epoch: 024 Batch: 00076/00094 | Loss: 0.0836 | EWC Loss: 0.0225 | CE Loss: 0.0612\n",
      "Train Epoch: 024 Batch: 00077/00094 | Loss: 0.1075 | EWC Loss: 0.0225 | CE Loss: 0.0850\n",
      "Train Epoch: 024 Batch: 00078/00094 | Loss: 0.1534 | EWC Loss: 0.0223 | CE Loss: 0.1311\n",
      "Train Epoch: 024 Batch: 00079/00094 | Loss: 0.0939 | EWC Loss: 0.0222 | CE Loss: 0.0718\n",
      "Train Epoch: 024 Batch: 00080/00094 | Loss: 0.1199 | EWC Loss: 0.0221 | CE Loss: 0.0978\n",
      "Train Epoch: 024 Batch: 00081/00094 | Loss: 0.1372 | EWC Loss: 0.0222 | CE Loss: 0.1150\n",
      "Train Epoch: 024 Batch: 00082/00094 | Loss: 0.1395 | EWC Loss: 0.0221 | CE Loss: 0.1174\n",
      "Train Epoch: 024 Batch: 00083/00094 | Loss: 0.0802 | EWC Loss: 0.0220 | CE Loss: 0.0581\n",
      "Train Epoch: 024 Batch: 00084/00094 | Loss: 0.0805 | EWC Loss: 0.0219 | CE Loss: 0.0586\n",
      "Train Epoch: 024 Batch: 00085/00094 | Loss: 0.0527 | EWC Loss: 0.0219 | CE Loss: 0.0308\n",
      "Train Epoch: 024 Batch: 00086/00094 | Loss: 0.1074 | EWC Loss: 0.0219 | CE Loss: 0.0855\n",
      "Train Epoch: 024 Batch: 00087/00094 | Loss: 0.1257 | EWC Loss: 0.0219 | CE Loss: 0.1038\n",
      "Train Epoch: 024 Batch: 00088/00094 | Loss: 0.0848 | EWC Loss: 0.0219 | CE Loss: 0.0629\n",
      "Train Epoch: 024 Batch: 00089/00094 | Loss: 0.0840 | EWC Loss: 0.0220 | CE Loss: 0.0621\n",
      "Train Epoch: 024 Batch: 00090/00094 | Loss: 0.0689 | EWC Loss: 0.0219 | CE Loss: 0.0471\n",
      "Train Epoch: 024 Batch: 00091/00094 | Loss: 0.1196 | EWC Loss: 0.0219 | CE Loss: 0.0977\n",
      "Train Epoch: 024 Batch: 00092/00094 | Loss: 0.1236 | EWC Loss: 0.0218 | CE Loss: 0.1018\n",
      "Train Epoch: 024 Batch: 00093/00094 | Loss: 0.0815 | EWC Loss: 0.0220 | CE Loss: 0.0594\n",
      "Train Epoch: 024 Batch: 00094/00094 | Loss: 0.1145 | EWC Loss: 0.0221 | CE Loss: 0.0925\n",
      "Train Epoch: 023 |Acc 97.40000 | Loss: 0.09567 | Task Loss 0.07299 | EWC Loss: 0.02268\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0638 | acc:98.1000\n",
      "[VAL Acc] Target: 98.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7586 | acc:49.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.3600 | acc:60.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5058 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6017 | acc:55.2900\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.29%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6605 | acc:75.6932\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5920 | acc:78.9577\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 78.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.2122 | acc:55.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.75%\n",
      "[VAL Acc] Avg 65.44%\n",
      "\n",
      "\n",
      "---------- Starting epoch 24 ----------\n",
      "Train Epoch: 025 Batch: 00001/00094 | Loss: 0.1562 | EWC Loss: 0.0220 | CE Loss: 0.1342\n",
      "Train Epoch: 025 Batch: 00002/00094 | Loss: 0.1122 | EWC Loss: 0.0220 | CE Loss: 0.0902\n",
      "Train Epoch: 025 Batch: 00003/00094 | Loss: 0.1281 | EWC Loss: 0.0220 | CE Loss: 0.1061\n",
      "Train Epoch: 025 Batch: 00004/00094 | Loss: 0.0536 | EWC Loss: 0.0219 | CE Loss: 0.0317\n",
      "Train Epoch: 025 Batch: 00005/00094 | Loss: 0.0895 | EWC Loss: 0.0219 | CE Loss: 0.0676\n",
      "Train Epoch: 025 Batch: 00006/00094 | Loss: 0.0780 | EWC Loss: 0.0220 | CE Loss: 0.0560\n",
      "Train Epoch: 025 Batch: 00007/00094 | Loss: 0.0715 | EWC Loss: 0.0221 | CE Loss: 0.0494\n",
      "Train Epoch: 025 Batch: 00008/00094 | Loss: 0.1257 | EWC Loss: 0.0222 | CE Loss: 0.1035\n",
      "Train Epoch: 025 Batch: 00009/00094 | Loss: 0.1121 | EWC Loss: 0.0222 | CE Loss: 0.0899\n",
      "Train Epoch: 025 Batch: 00010/00094 | Loss: 0.1292 | EWC Loss: 0.0222 | CE Loss: 0.1070\n",
      "Train Epoch: 025 Batch: 00011/00094 | Loss: 0.0559 | EWC Loss: 0.0223 | CE Loss: 0.0336\n",
      "Train Epoch: 025 Batch: 00012/00094 | Loss: 0.1540 | EWC Loss: 0.0224 | CE Loss: 0.1316\n",
      "Train Epoch: 025 Batch: 00013/00094 | Loss: 0.1932 | EWC Loss: 0.0224 | CE Loss: 0.1708\n",
      "Train Epoch: 025 Batch: 00014/00094 | Loss: 0.1169 | EWC Loss: 0.0226 | CE Loss: 0.0943\n",
      "Train Epoch: 025 Batch: 00015/00094 | Loss: 0.0575 | EWC Loss: 0.0227 | CE Loss: 0.0348\n",
      "Train Epoch: 025 Batch: 00016/00094 | Loss: 0.1007 | EWC Loss: 0.0226 | CE Loss: 0.0781\n",
      "Train Epoch: 025 Batch: 00017/00094 | Loss: 0.1257 | EWC Loss: 0.0226 | CE Loss: 0.1031\n",
      "Train Epoch: 025 Batch: 00018/00094 | Loss: 0.1708 | EWC Loss: 0.0229 | CE Loss: 0.1480\n",
      "Train Epoch: 025 Batch: 00019/00094 | Loss: 0.1225 | EWC Loss: 0.0229 | CE Loss: 0.0995\n",
      "Train Epoch: 025 Batch: 00020/00094 | Loss: 0.0823 | EWC Loss: 0.0231 | CE Loss: 0.0593\n",
      "Train Epoch: 025 Batch: 00021/00094 | Loss: 0.1031 | EWC Loss: 0.0230 | CE Loss: 0.0801\n",
      "Train Epoch: 025 Batch: 00022/00094 | Loss: 0.0619 | EWC Loss: 0.0231 | CE Loss: 0.0388\n",
      "Train Epoch: 025 Batch: 00023/00094 | Loss: 0.0948 | EWC Loss: 0.0231 | CE Loss: 0.0718\n",
      "Train Epoch: 025 Batch: 00024/00094 | Loss: 0.0753 | EWC Loss: 0.0229 | CE Loss: 0.0523\n",
      "Train Epoch: 025 Batch: 00025/00094 | Loss: 0.0605 | EWC Loss: 0.0229 | CE Loss: 0.0375\n",
      "Train Epoch: 025 Batch: 00026/00094 | Loss: 0.0386 | EWC Loss: 0.0228 | CE Loss: 0.0157\n",
      "Train Epoch: 025 Batch: 00027/00094 | Loss: 0.0645 | EWC Loss: 0.0227 | CE Loss: 0.0418\n",
      "Train Epoch: 025 Batch: 00028/00094 | Loss: 0.1324 | EWC Loss: 0.0226 | CE Loss: 0.1098\n",
      "Train Epoch: 025 Batch: 00029/00094 | Loss: 0.0485 | EWC Loss: 0.0226 | CE Loss: 0.0259\n",
      "Train Epoch: 025 Batch: 00030/00094 | Loss: 0.0698 | EWC Loss: 0.0226 | CE Loss: 0.0473\n",
      "Train Epoch: 025 Batch: 00031/00094 | Loss: 0.0819 | EWC Loss: 0.0225 | CE Loss: 0.0594\n",
      "Train Epoch: 025 Batch: 00032/00094 | Loss: 0.1286 | EWC Loss: 0.0226 | CE Loss: 0.1061\n",
      "Train Epoch: 025 Batch: 00033/00094 | Loss: 0.0887 | EWC Loss: 0.0227 | CE Loss: 0.0660\n",
      "Train Epoch: 025 Batch: 00034/00094 | Loss: 0.0602 | EWC Loss: 0.0228 | CE Loss: 0.0374\n",
      "Train Epoch: 025 Batch: 00035/00094 | Loss: 0.1044 | EWC Loss: 0.0228 | CE Loss: 0.0816\n",
      "Train Epoch: 025 Batch: 00036/00094 | Loss: 0.0624 | EWC Loss: 0.0230 | CE Loss: 0.0393\n",
      "Train Epoch: 025 Batch: 00037/00094 | Loss: 0.0688 | EWC Loss: 0.0231 | CE Loss: 0.0457\n",
      "Train Epoch: 025 Batch: 00038/00094 | Loss: 0.0834 | EWC Loss: 0.0230 | CE Loss: 0.0604\n",
      "Train Epoch: 025 Batch: 00039/00094 | Loss: 0.1638 | EWC Loss: 0.0230 | CE Loss: 0.1408\n",
      "Train Epoch: 025 Batch: 00040/00094 | Loss: 0.0864 | EWC Loss: 0.0228 | CE Loss: 0.0636\n",
      "Train Epoch: 025 Batch: 00041/00094 | Loss: 0.0596 | EWC Loss: 0.0228 | CE Loss: 0.0368\n",
      "Train Epoch: 025 Batch: 00042/00094 | Loss: 0.1064 | EWC Loss: 0.0227 | CE Loss: 0.0837\n",
      "Train Epoch: 025 Batch: 00043/00094 | Loss: 0.1285 | EWC Loss: 0.0227 | CE Loss: 0.1058\n",
      "Train Epoch: 025 Batch: 00044/00094 | Loss: 0.1531 | EWC Loss: 0.0229 | CE Loss: 0.1303\n",
      "Train Epoch: 025 Batch: 00045/00094 | Loss: 0.1189 | EWC Loss: 0.0230 | CE Loss: 0.0959\n",
      "Train Epoch: 025 Batch: 00046/00094 | Loss: 0.1487 | EWC Loss: 0.0232 | CE Loss: 0.1256\n",
      "Train Epoch: 025 Batch: 00047/00094 | Loss: 0.1002 | EWC Loss: 0.0233 | CE Loss: 0.0769\n",
      "Train Epoch: 025 Batch: 00048/00094 | Loss: 0.1098 | EWC Loss: 0.0233 | CE Loss: 0.0865\n",
      "Train Epoch: 025 Batch: 00049/00094 | Loss: 0.0955 | EWC Loss: 0.0232 | CE Loss: 0.0724\n",
      "Train Epoch: 025 Batch: 00050/00094 | Loss: 0.1005 | EWC Loss: 0.0232 | CE Loss: 0.0773\n",
      "Train Epoch: 025 Batch: 00051/00094 | Loss: 0.0669 | EWC Loss: 0.0229 | CE Loss: 0.0439\n",
      "Train Epoch: 025 Batch: 00052/00094 | Loss: 0.0553 | EWC Loss: 0.0228 | CE Loss: 0.0325\n",
      "Train Epoch: 025 Batch: 00053/00094 | Loss: 0.0787 | EWC Loss: 0.0228 | CE Loss: 0.0560\n",
      "Train Epoch: 025 Batch: 00054/00094 | Loss: 0.0795 | EWC Loss: 0.0228 | CE Loss: 0.0567\n",
      "Train Epoch: 025 Batch: 00055/00094 | Loss: 0.0829 | EWC Loss: 0.0230 | CE Loss: 0.0599\n",
      "Train Epoch: 025 Batch: 00056/00094 | Loss: 0.0836 | EWC Loss: 0.0229 | CE Loss: 0.0607\n",
      "Train Epoch: 025 Batch: 00057/00094 | Loss: 0.0903 | EWC Loss: 0.0228 | CE Loss: 0.0675\n",
      "Train Epoch: 025 Batch: 00058/00094 | Loss: 0.0994 | EWC Loss: 0.0225 | CE Loss: 0.0769\n",
      "Train Epoch: 025 Batch: 00059/00094 | Loss: 0.0870 | EWC Loss: 0.0226 | CE Loss: 0.0644\n",
      "Train Epoch: 025 Batch: 00060/00094 | Loss: 0.0774 | EWC Loss: 0.0225 | CE Loss: 0.0548\n",
      "Train Epoch: 025 Batch: 00061/00094 | Loss: 0.0695 | EWC Loss: 0.0225 | CE Loss: 0.0470\n",
      "Train Epoch: 025 Batch: 00062/00094 | Loss: 0.1085 | EWC Loss: 0.0225 | CE Loss: 0.0860\n",
      "Train Epoch: 025 Batch: 00063/00094 | Loss: 0.1093 | EWC Loss: 0.0225 | CE Loss: 0.0869\n",
      "Train Epoch: 025 Batch: 00064/00094 | Loss: 0.1176 | EWC Loss: 0.0227 | CE Loss: 0.0949\n",
      "Train Epoch: 025 Batch: 00065/00094 | Loss: 0.0782 | EWC Loss: 0.0226 | CE Loss: 0.0556\n",
      "Train Epoch: 025 Batch: 00066/00094 | Loss: 0.0710 | EWC Loss: 0.0226 | CE Loss: 0.0484\n",
      "Train Epoch: 025 Batch: 00067/00094 | Loss: 0.1582 | EWC Loss: 0.0226 | CE Loss: 0.1356\n",
      "Train Epoch: 025 Batch: 00068/00094 | Loss: 0.1809 | EWC Loss: 0.0227 | CE Loss: 0.1582\n",
      "Train Epoch: 025 Batch: 00069/00094 | Loss: 0.0412 | EWC Loss: 0.0230 | CE Loss: 0.0182\n",
      "Train Epoch: 025 Batch: 00070/00094 | Loss: 0.1288 | EWC Loss: 0.0229 | CE Loss: 0.1059\n",
      "Train Epoch: 025 Batch: 00071/00094 | Loss: 0.0849 | EWC Loss: 0.0229 | CE Loss: 0.0619\n",
      "Train Epoch: 025 Batch: 00072/00094 | Loss: 0.1087 | EWC Loss: 0.0231 | CE Loss: 0.0856\n",
      "Train Epoch: 025 Batch: 00073/00094 | Loss: 0.0820 | EWC Loss: 0.0230 | CE Loss: 0.0590\n",
      "Train Epoch: 025 Batch: 00074/00094 | Loss: 0.1983 | EWC Loss: 0.0229 | CE Loss: 0.1754\n",
      "Train Epoch: 025 Batch: 00075/00094 | Loss: 0.0469 | EWC Loss: 0.0230 | CE Loss: 0.0239\n",
      "Train Epoch: 025 Batch: 00076/00094 | Loss: 0.0681 | EWC Loss: 0.0230 | CE Loss: 0.0451\n",
      "Train Epoch: 025 Batch: 00077/00094 | Loss: 0.0938 | EWC Loss: 0.0229 | CE Loss: 0.0708\n",
      "Train Epoch: 025 Batch: 00078/00094 | Loss: 0.0876 | EWC Loss: 0.0228 | CE Loss: 0.0648\n",
      "Train Epoch: 025 Batch: 00079/00094 | Loss: 0.1119 | EWC Loss: 0.0228 | CE Loss: 0.0892\n",
      "Train Epoch: 025 Batch: 00080/00094 | Loss: 0.0892 | EWC Loss: 0.0228 | CE Loss: 0.0664\n",
      "Train Epoch: 025 Batch: 00081/00094 | Loss: 0.1292 | EWC Loss: 0.0229 | CE Loss: 0.1063\n",
      "Train Epoch: 025 Batch: 00082/00094 | Loss: 0.1046 | EWC Loss: 0.0230 | CE Loss: 0.0816\n",
      "Train Epoch: 025 Batch: 00083/00094 | Loss: 0.1426 | EWC Loss: 0.0231 | CE Loss: 0.1195\n",
      "Train Epoch: 025 Batch: 00084/00094 | Loss: 0.0866 | EWC Loss: 0.0229 | CE Loss: 0.0636\n",
      "Train Epoch: 025 Batch: 00085/00094 | Loss: 0.0869 | EWC Loss: 0.0229 | CE Loss: 0.0640\n",
      "Train Epoch: 025 Batch: 00086/00094 | Loss: 0.0631 | EWC Loss: 0.0228 | CE Loss: 0.0403\n",
      "Train Epoch: 025 Batch: 00087/00094 | Loss: 0.1104 | EWC Loss: 0.0228 | CE Loss: 0.0876\n",
      "Train Epoch: 025 Batch: 00088/00094 | Loss: 0.1051 | EWC Loss: 0.0227 | CE Loss: 0.0824\n",
      "Train Epoch: 025 Batch: 00089/00094 | Loss: 0.0624 | EWC Loss: 0.0226 | CE Loss: 0.0398\n",
      "Train Epoch: 025 Batch: 00090/00094 | Loss: 0.0740 | EWC Loss: 0.0226 | CE Loss: 0.0514\n",
      "Train Epoch: 025 Batch: 00091/00094 | Loss: 0.0737 | EWC Loss: 0.0225 | CE Loss: 0.0512\n",
      "Train Epoch: 025 Batch: 00092/00094 | Loss: 0.0981 | EWC Loss: 0.0226 | CE Loss: 0.0755\n",
      "Train Epoch: 025 Batch: 00093/00094 | Loss: 0.1365 | EWC Loss: 0.0224 | CE Loss: 0.1141\n",
      "Train Epoch: 025 Batch: 00094/00094 | Loss: 0.1001 | EWC Loss: 0.0225 | CE Loss: 0.0775\n",
      "Train Epoch: 024 |Acc 97.06667 | Loss: 0.09834 | Task Loss 0.07562 | EWC Loss: 0.02271\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0667 | acc:97.6000\n",
      "[VAL Acc] Target: 97.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.6200 | acc:50.1500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2898 | acc:59.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5099 | acc:47.9008\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5162 | acc:55.3292\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6692 | acc:75.8780\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5722 | acc:79.8589\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 79.86%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1628 | acc:56.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.12%\n",
      "[VAL Acc] Avg 65.32%\n",
      "\n",
      "\n",
      "---------- Starting epoch 25 ----------\n",
      "Train Epoch: 026 Batch: 00001/00094 | Loss: 0.1035 | EWC Loss: 0.0227 | CE Loss: 0.0808\n",
      "Train Epoch: 026 Batch: 00002/00094 | Loss: 0.1215 | EWC Loss: 0.0226 | CE Loss: 0.0989\n",
      "Train Epoch: 026 Batch: 00003/00094 | Loss: 0.0684 | EWC Loss: 0.0228 | CE Loss: 0.0455\n",
      "Train Epoch: 026 Batch: 00004/00094 | Loss: 0.0716 | EWC Loss: 0.0228 | CE Loss: 0.0488\n",
      "Train Epoch: 026 Batch: 00005/00094 | Loss: 0.0664 | EWC Loss: 0.0229 | CE Loss: 0.0436\n",
      "Train Epoch: 026 Batch: 00006/00094 | Loss: 0.1014 | EWC Loss: 0.0227 | CE Loss: 0.0787\n",
      "Train Epoch: 026 Batch: 00007/00094 | Loss: 0.0732 | EWC Loss: 0.0227 | CE Loss: 0.0505\n",
      "Train Epoch: 026 Batch: 00008/00094 | Loss: 0.1418 | EWC Loss: 0.0228 | CE Loss: 0.1190\n",
      "Train Epoch: 026 Batch: 00009/00094 | Loss: 0.0709 | EWC Loss: 0.0227 | CE Loss: 0.0482\n",
      "Train Epoch: 026 Batch: 00010/00094 | Loss: 0.1033 | EWC Loss: 0.0227 | CE Loss: 0.0806\n",
      "Train Epoch: 026 Batch: 00011/00094 | Loss: 0.1252 | EWC Loss: 0.0229 | CE Loss: 0.1024\n",
      "Train Epoch: 026 Batch: 00012/00094 | Loss: 0.0536 | EWC Loss: 0.0229 | CE Loss: 0.0307\n",
      "Train Epoch: 026 Batch: 00013/00094 | Loss: 0.0826 | EWC Loss: 0.0229 | CE Loss: 0.0597\n",
      "Train Epoch: 026 Batch: 00014/00094 | Loss: 0.0551 | EWC Loss: 0.0228 | CE Loss: 0.0323\n",
      "Train Epoch: 026 Batch: 00015/00094 | Loss: 0.0644 | EWC Loss: 0.0228 | CE Loss: 0.0416\n",
      "Train Epoch: 026 Batch: 00016/00094 | Loss: 0.0655 | EWC Loss: 0.0228 | CE Loss: 0.0427\n",
      "Train Epoch: 026 Batch: 00017/00094 | Loss: 0.1000 | EWC Loss: 0.0228 | CE Loss: 0.0773\n",
      "Train Epoch: 026 Batch: 00018/00094 | Loss: 0.0835 | EWC Loss: 0.0229 | CE Loss: 0.0606\n",
      "Train Epoch: 026 Batch: 00019/00094 | Loss: 0.1050 | EWC Loss: 0.0229 | CE Loss: 0.0821\n",
      "Train Epoch: 026 Batch: 00020/00094 | Loss: 0.0753 | EWC Loss: 0.0230 | CE Loss: 0.0523\n",
      "Train Epoch: 026 Batch: 00021/00094 | Loss: 0.0411 | EWC Loss: 0.0230 | CE Loss: 0.0182\n",
      "Train Epoch: 026 Batch: 00022/00094 | Loss: 0.0483 | EWC Loss: 0.0229 | CE Loss: 0.0254\n",
      "Train Epoch: 026 Batch: 00023/00094 | Loss: 0.0989 | EWC Loss: 0.0229 | CE Loss: 0.0760\n",
      "Train Epoch: 026 Batch: 00024/00094 | Loss: 0.0743 | EWC Loss: 0.0230 | CE Loss: 0.0514\n",
      "Train Epoch: 026 Batch: 00025/00094 | Loss: 0.0914 | EWC Loss: 0.0230 | CE Loss: 0.0684\n",
      "Train Epoch: 026 Batch: 00026/00094 | Loss: 0.1175 | EWC Loss: 0.0230 | CE Loss: 0.0945\n",
      "Train Epoch: 026 Batch: 00027/00094 | Loss: 0.1121 | EWC Loss: 0.0231 | CE Loss: 0.0890\n",
      "Train Epoch: 026 Batch: 00028/00094 | Loss: 0.0535 | EWC Loss: 0.0231 | CE Loss: 0.0303\n",
      "Train Epoch: 026 Batch: 00029/00094 | Loss: 0.0475 | EWC Loss: 0.0231 | CE Loss: 0.0244\n",
      "Train Epoch: 026 Batch: 00030/00094 | Loss: 0.1359 | EWC Loss: 0.0231 | CE Loss: 0.1128\n",
      "Train Epoch: 026 Batch: 00031/00094 | Loss: 0.1344 | EWC Loss: 0.0232 | CE Loss: 0.1113\n",
      "Train Epoch: 026 Batch: 00032/00094 | Loss: 0.0688 | EWC Loss: 0.0231 | CE Loss: 0.0457\n",
      "Train Epoch: 026 Batch: 00033/00094 | Loss: 0.0450 | EWC Loss: 0.0230 | CE Loss: 0.0220\n",
      "Train Epoch: 026 Batch: 00034/00094 | Loss: 0.0667 | EWC Loss: 0.0229 | CE Loss: 0.0438\n",
      "Train Epoch: 026 Batch: 00035/00094 | Loss: 0.0944 | EWC Loss: 0.0227 | CE Loss: 0.0717\n",
      "Train Epoch: 026 Batch: 00036/00094 | Loss: 0.0693 | EWC Loss: 0.0227 | CE Loss: 0.0466\n",
      "Train Epoch: 026 Batch: 00037/00094 | Loss: 0.1060 | EWC Loss: 0.0228 | CE Loss: 0.0833\n",
      "Train Epoch: 026 Batch: 00038/00094 | Loss: 0.1035 | EWC Loss: 0.0230 | CE Loss: 0.0805\n",
      "Train Epoch: 026 Batch: 00039/00094 | Loss: 0.0833 | EWC Loss: 0.0230 | CE Loss: 0.0603\n",
      "Train Epoch: 026 Batch: 00040/00094 | Loss: 0.0983 | EWC Loss: 0.0231 | CE Loss: 0.0753\n",
      "Train Epoch: 026 Batch: 00041/00094 | Loss: 0.0678 | EWC Loss: 0.0230 | CE Loss: 0.0449\n",
      "Train Epoch: 026 Batch: 00042/00094 | Loss: 0.1592 | EWC Loss: 0.0232 | CE Loss: 0.1361\n",
      "Train Epoch: 026 Batch: 00043/00094 | Loss: 0.1049 | EWC Loss: 0.0238 | CE Loss: 0.0810\n",
      "Train Epoch: 026 Batch: 00044/00094 | Loss: 0.1136 | EWC Loss: 0.0238 | CE Loss: 0.0899\n",
      "Train Epoch: 026 Batch: 00045/00094 | Loss: 0.0860 | EWC Loss: 0.0239 | CE Loss: 0.0621\n",
      "Train Epoch: 026 Batch: 00046/00094 | Loss: 0.0473 | EWC Loss: 0.0238 | CE Loss: 0.0235\n",
      "Train Epoch: 026 Batch: 00047/00094 | Loss: 0.1327 | EWC Loss: 0.0236 | CE Loss: 0.1091\n",
      "Train Epoch: 026 Batch: 00048/00094 | Loss: 0.0573 | EWC Loss: 0.0236 | CE Loss: 0.0337\n",
      "Train Epoch: 026 Batch: 00049/00094 | Loss: 0.1329 | EWC Loss: 0.0235 | CE Loss: 0.1094\n",
      "Train Epoch: 026 Batch: 00050/00094 | Loss: 0.1307 | EWC Loss: 0.0233 | CE Loss: 0.1074\n",
      "Train Epoch: 026 Batch: 00051/00094 | Loss: 0.0979 | EWC Loss: 0.0235 | CE Loss: 0.0744\n",
      "Train Epoch: 026 Batch: 00052/00094 | Loss: 0.1034 | EWC Loss: 0.0233 | CE Loss: 0.0801\n",
      "Train Epoch: 026 Batch: 00053/00094 | Loss: 0.0658 | EWC Loss: 0.0236 | CE Loss: 0.0422\n",
      "Train Epoch: 026 Batch: 00054/00094 | Loss: 0.0513 | EWC Loss: 0.0236 | CE Loss: 0.0277\n",
      "Train Epoch: 026 Batch: 00055/00094 | Loss: 0.1086 | EWC Loss: 0.0236 | CE Loss: 0.0850\n",
      "Train Epoch: 026 Batch: 00056/00094 | Loss: 0.0728 | EWC Loss: 0.0236 | CE Loss: 0.0493\n",
      "Train Epoch: 026 Batch: 00057/00094 | Loss: 0.0837 | EWC Loss: 0.0236 | CE Loss: 0.0601\n",
      "Train Epoch: 026 Batch: 00058/00094 | Loss: 0.0908 | EWC Loss: 0.0235 | CE Loss: 0.0672\n",
      "Train Epoch: 026 Batch: 00059/00094 | Loss: 0.1151 | EWC Loss: 0.0235 | CE Loss: 0.0917\n",
      "Train Epoch: 026 Batch: 00060/00094 | Loss: 0.0971 | EWC Loss: 0.0235 | CE Loss: 0.0736\n",
      "Train Epoch: 026 Batch: 00061/00094 | Loss: 0.1253 | EWC Loss: 0.0234 | CE Loss: 0.1019\n",
      "Train Epoch: 026 Batch: 00062/00094 | Loss: 0.1121 | EWC Loss: 0.0238 | CE Loss: 0.0883\n",
      "Train Epoch: 026 Batch: 00063/00094 | Loss: 0.0932 | EWC Loss: 0.0240 | CE Loss: 0.0692\n",
      "Train Epoch: 026 Batch: 00064/00094 | Loss: 0.0560 | EWC Loss: 0.0236 | CE Loss: 0.0323\n",
      "Train Epoch: 026 Batch: 00065/00094 | Loss: 0.0672 | EWC Loss: 0.0236 | CE Loss: 0.0436\n",
      "Train Epoch: 026 Batch: 00066/00094 | Loss: 0.0901 | EWC Loss: 0.0235 | CE Loss: 0.0665\n",
      "Train Epoch: 026 Batch: 00067/00094 | Loss: 0.1413 | EWC Loss: 0.0234 | CE Loss: 0.1179\n",
      "Train Epoch: 026 Batch: 00068/00094 | Loss: 0.1083 | EWC Loss: 0.0235 | CE Loss: 0.0848\n",
      "Train Epoch: 026 Batch: 00069/00094 | Loss: 0.0665 | EWC Loss: 0.0235 | CE Loss: 0.0430\n",
      "Train Epoch: 026 Batch: 00070/00094 | Loss: 0.0758 | EWC Loss: 0.0234 | CE Loss: 0.0524\n",
      "Train Epoch: 026 Batch: 00071/00094 | Loss: 0.0832 | EWC Loss: 0.0233 | CE Loss: 0.0600\n",
      "Train Epoch: 026 Batch: 00072/00094 | Loss: 0.0515 | EWC Loss: 0.0233 | CE Loss: 0.0282\n",
      "Train Epoch: 026 Batch: 00073/00094 | Loss: 0.0778 | EWC Loss: 0.0233 | CE Loss: 0.0545\n",
      "Train Epoch: 026 Batch: 00074/00094 | Loss: 0.0915 | EWC Loss: 0.0233 | CE Loss: 0.0683\n",
      "Train Epoch: 026 Batch: 00075/00094 | Loss: 0.0653 | EWC Loss: 0.0236 | CE Loss: 0.0417\n",
      "Train Epoch: 026 Batch: 00076/00094 | Loss: 0.0802 | EWC Loss: 0.0236 | CE Loss: 0.0566\n",
      "Train Epoch: 026 Batch: 00077/00094 | Loss: 0.1107 | EWC Loss: 0.0236 | CE Loss: 0.0871\n",
      "Train Epoch: 026 Batch: 00078/00094 | Loss: 0.0707 | EWC Loss: 0.0236 | CE Loss: 0.0471\n",
      "Train Epoch: 026 Batch: 00079/00094 | Loss: 0.1127 | EWC Loss: 0.0238 | CE Loss: 0.0889\n",
      "Train Epoch: 026 Batch: 00080/00094 | Loss: 0.0546 | EWC Loss: 0.0237 | CE Loss: 0.0309\n",
      "Train Epoch: 026 Batch: 00081/00094 | Loss: 0.1046 | EWC Loss: 0.0236 | CE Loss: 0.0810\n",
      "Train Epoch: 026 Batch: 00082/00094 | Loss: 0.0531 | EWC Loss: 0.0238 | CE Loss: 0.0293\n",
      "Train Epoch: 026 Batch: 00083/00094 | Loss: 0.0613 | EWC Loss: 0.0238 | CE Loss: 0.0375\n",
      "Train Epoch: 026 Batch: 00084/00094 | Loss: 0.0723 | EWC Loss: 0.0238 | CE Loss: 0.0485\n",
      "Train Epoch: 026 Batch: 00085/00094 | Loss: 0.1338 | EWC Loss: 0.0237 | CE Loss: 0.1101\n",
      "Train Epoch: 026 Batch: 00086/00094 | Loss: 0.0850 | EWC Loss: 0.0239 | CE Loss: 0.0611\n",
      "Train Epoch: 026 Batch: 00087/00094 | Loss: 0.1226 | EWC Loss: 0.0238 | CE Loss: 0.0988\n",
      "Train Epoch: 026 Batch: 00088/00094 | Loss: 0.0977 | EWC Loss: 0.0239 | CE Loss: 0.0738\n",
      "Train Epoch: 026 Batch: 00089/00094 | Loss: 0.0645 | EWC Loss: 0.0239 | CE Loss: 0.0406\n",
      "Train Epoch: 026 Batch: 00090/00094 | Loss: 0.0854 | EWC Loss: 0.0237 | CE Loss: 0.0617\n",
      "Train Epoch: 026 Batch: 00091/00094 | Loss: 0.1082 | EWC Loss: 0.0238 | CE Loss: 0.0844\n",
      "Train Epoch: 026 Batch: 00092/00094 | Loss: 0.0938 | EWC Loss: 0.0237 | CE Loss: 0.0700\n",
      "Train Epoch: 026 Batch: 00093/00094 | Loss: 0.1533 | EWC Loss: 0.0238 | CE Loss: 0.1295\n",
      "Train Epoch: 026 Batch: 00094/00094 | Loss: 0.0528 | EWC Loss: 0.0237 | CE Loss: 0.0291\n",
      "Train Epoch: 025 |Acc 97.68333 | Loss: 0.08898 | Task Loss 0.06568 | EWC Loss: 0.02330\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0539 | acc:98.1000\n",
      "[VAL Acc] Target: 98.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8091 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.3216 | acc:60.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.6089 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5932 | acc:56.1520\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6248 | acc:78.1885\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6205 | acc:78.0564\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 78.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.2040 | acc:54.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.75%\n",
      "[VAL Acc] Avg 65.62%\n",
      "\n",
      "\n",
      "---------- Starting epoch 26 ----------\n",
      "Train Epoch: 027 Batch: 00001/00094 | Loss: 0.0662 | EWC Loss: 0.0238 | CE Loss: 0.0425\n",
      "Train Epoch: 027 Batch: 00002/00094 | Loss: 0.1535 | EWC Loss: 0.0237 | CE Loss: 0.1297\n",
      "Train Epoch: 027 Batch: 00003/00094 | Loss: 0.0730 | EWC Loss: 0.0235 | CE Loss: 0.0495\n",
      "Train Epoch: 027 Batch: 00004/00094 | Loss: 0.0709 | EWC Loss: 0.0235 | CE Loss: 0.0474\n",
      "Train Epoch: 027 Batch: 00005/00094 | Loss: 0.0507 | EWC Loss: 0.0236 | CE Loss: 0.0270\n",
      "Train Epoch: 027 Batch: 00006/00094 | Loss: 0.1492 | EWC Loss: 0.0236 | CE Loss: 0.1257\n",
      "Train Epoch: 027 Batch: 00007/00094 | Loss: 0.1851 | EWC Loss: 0.0236 | CE Loss: 0.1615\n",
      "Train Epoch: 027 Batch: 00008/00094 | Loss: 0.0742 | EWC Loss: 0.0236 | CE Loss: 0.0506\n",
      "Train Epoch: 027 Batch: 00009/00094 | Loss: 0.0485 | EWC Loss: 0.0234 | CE Loss: 0.0251\n",
      "Train Epoch: 027 Batch: 00010/00094 | Loss: 0.1389 | EWC Loss: 0.0234 | CE Loss: 0.1155\n",
      "Train Epoch: 027 Batch: 00011/00094 | Loss: 0.0945 | EWC Loss: 0.0234 | CE Loss: 0.0711\n",
      "Train Epoch: 027 Batch: 00012/00094 | Loss: 0.0708 | EWC Loss: 0.0234 | CE Loss: 0.0474\n",
      "Train Epoch: 027 Batch: 00013/00094 | Loss: 0.0679 | EWC Loss: 0.0233 | CE Loss: 0.0446\n",
      "Train Epoch: 027 Batch: 00014/00094 | Loss: 0.0606 | EWC Loss: 0.0234 | CE Loss: 0.0371\n",
      "Train Epoch: 027 Batch: 00015/00094 | Loss: 0.0904 | EWC Loss: 0.0235 | CE Loss: 0.0670\n",
      "Train Epoch: 027 Batch: 00016/00094 | Loss: 0.0966 | EWC Loss: 0.0235 | CE Loss: 0.0731\n",
      "Train Epoch: 027 Batch: 00017/00094 | Loss: 0.0597 | EWC Loss: 0.0236 | CE Loss: 0.0361\n",
      "Train Epoch: 027 Batch: 00018/00094 | Loss: 0.0393 | EWC Loss: 0.0236 | CE Loss: 0.0157\n",
      "Train Epoch: 027 Batch: 00019/00094 | Loss: 0.0943 | EWC Loss: 0.0235 | CE Loss: 0.0708\n",
      "Train Epoch: 027 Batch: 00020/00094 | Loss: 0.0575 | EWC Loss: 0.0236 | CE Loss: 0.0339\n",
      "Train Epoch: 027 Batch: 00021/00094 | Loss: 0.1158 | EWC Loss: 0.0236 | CE Loss: 0.0922\n",
      "Train Epoch: 027 Batch: 00022/00094 | Loss: 0.0808 | EWC Loss: 0.0236 | CE Loss: 0.0572\n",
      "Train Epoch: 027 Batch: 00023/00094 | Loss: 0.0887 | EWC Loss: 0.0235 | CE Loss: 0.0651\n",
      "Train Epoch: 027 Batch: 00024/00094 | Loss: 0.0749 | EWC Loss: 0.0235 | CE Loss: 0.0514\n",
      "Train Epoch: 027 Batch: 00025/00094 | Loss: 0.2471 | EWC Loss: 0.0236 | CE Loss: 0.2235\n",
      "Train Epoch: 027 Batch: 00026/00094 | Loss: 0.0468 | EWC Loss: 0.0241 | CE Loss: 0.0227\n",
      "Train Epoch: 027 Batch: 00027/00094 | Loss: 0.0364 | EWC Loss: 0.0241 | CE Loss: 0.0123\n",
      "Train Epoch: 027 Batch: 00028/00094 | Loss: 0.1521 | EWC Loss: 0.0241 | CE Loss: 0.1281\n",
      "Train Epoch: 027 Batch: 00029/00094 | Loss: 0.0751 | EWC Loss: 0.0240 | CE Loss: 0.0511\n",
      "Train Epoch: 027 Batch: 00030/00094 | Loss: 0.0651 | EWC Loss: 0.0239 | CE Loss: 0.0412\n",
      "Train Epoch: 027 Batch: 00031/00094 | Loss: 0.0567 | EWC Loss: 0.0238 | CE Loss: 0.0329\n",
      "Train Epoch: 027 Batch: 00032/00094 | Loss: 0.1157 | EWC Loss: 0.0238 | CE Loss: 0.0918\n",
      "Train Epoch: 027 Batch: 00033/00094 | Loss: 0.0635 | EWC Loss: 0.0238 | CE Loss: 0.0397\n",
      "Train Epoch: 027 Batch: 00034/00094 | Loss: 0.0472 | EWC Loss: 0.0237 | CE Loss: 0.0235\n",
      "Train Epoch: 027 Batch: 00035/00094 | Loss: 0.0536 | EWC Loss: 0.0236 | CE Loss: 0.0300\n",
      "Train Epoch: 027 Batch: 00036/00094 | Loss: 0.0773 | EWC Loss: 0.0236 | CE Loss: 0.0537\n",
      "Train Epoch: 027 Batch: 00037/00094 | Loss: 0.0980 | EWC Loss: 0.0236 | CE Loss: 0.0744\n",
      "Train Epoch: 027 Batch: 00038/00094 | Loss: 0.0881 | EWC Loss: 0.0236 | CE Loss: 0.0645\n",
      "Train Epoch: 027 Batch: 00039/00094 | Loss: 0.0736 | EWC Loss: 0.0235 | CE Loss: 0.0501\n",
      "Train Epoch: 027 Batch: 00040/00094 | Loss: 0.0588 | EWC Loss: 0.0235 | CE Loss: 0.0353\n",
      "Train Epoch: 027 Batch: 00041/00094 | Loss: 0.1169 | EWC Loss: 0.0234 | CE Loss: 0.0935\n",
      "Train Epoch: 027 Batch: 00042/00094 | Loss: 0.0812 | EWC Loss: 0.0235 | CE Loss: 0.0577\n",
      "Train Epoch: 027 Batch: 00043/00094 | Loss: 0.1480 | EWC Loss: 0.0234 | CE Loss: 0.1247\n",
      "Train Epoch: 027 Batch: 00044/00094 | Loss: 0.0856 | EWC Loss: 0.0231 | CE Loss: 0.0625\n",
      "Train Epoch: 027 Batch: 00045/00094 | Loss: 0.0867 | EWC Loss: 0.0232 | CE Loss: 0.0636\n",
      "Train Epoch: 027 Batch: 00046/00094 | Loss: 0.0652 | EWC Loss: 0.0232 | CE Loss: 0.0419\n",
      "Train Epoch: 027 Batch: 00047/00094 | Loss: 0.1033 | EWC Loss: 0.0233 | CE Loss: 0.0799\n",
      "Train Epoch: 027 Batch: 00048/00094 | Loss: 0.0716 | EWC Loss: 0.0233 | CE Loss: 0.0483\n",
      "Train Epoch: 027 Batch: 00049/00094 | Loss: 0.1059 | EWC Loss: 0.0233 | CE Loss: 0.0826\n",
      "Train Epoch: 027 Batch: 00050/00094 | Loss: 0.1110 | EWC Loss: 0.0235 | CE Loss: 0.0875\n",
      "Train Epoch: 027 Batch: 00051/00094 | Loss: 0.0877 | EWC Loss: 0.0236 | CE Loss: 0.0641\n",
      "Train Epoch: 027 Batch: 00052/00094 | Loss: 0.0585 | EWC Loss: 0.0238 | CE Loss: 0.0348\n",
      "Train Epoch: 027 Batch: 00053/00094 | Loss: 0.0739 | EWC Loss: 0.0238 | CE Loss: 0.0502\n",
      "Train Epoch: 027 Batch: 00054/00094 | Loss: 0.1101 | EWC Loss: 0.0238 | CE Loss: 0.0864\n",
      "Train Epoch: 027 Batch: 00055/00094 | Loss: 0.0986 | EWC Loss: 0.0238 | CE Loss: 0.0748\n",
      "Train Epoch: 027 Batch: 00056/00094 | Loss: 0.0546 | EWC Loss: 0.0239 | CE Loss: 0.0307\n",
      "Train Epoch: 027 Batch: 00057/00094 | Loss: 0.0765 | EWC Loss: 0.0239 | CE Loss: 0.0526\n",
      "Train Epoch: 027 Batch: 00058/00094 | Loss: 0.0757 | EWC Loss: 0.0239 | CE Loss: 0.0519\n",
      "Train Epoch: 027 Batch: 00059/00094 | Loss: 0.0547 | EWC Loss: 0.0238 | CE Loss: 0.0308\n",
      "Train Epoch: 027 Batch: 00060/00094 | Loss: 0.0809 | EWC Loss: 0.0238 | CE Loss: 0.0571\n",
      "Train Epoch: 027 Batch: 00061/00094 | Loss: 0.0620 | EWC Loss: 0.0239 | CE Loss: 0.0382\n",
      "Train Epoch: 027 Batch: 00062/00094 | Loss: 0.0847 | EWC Loss: 0.0239 | CE Loss: 0.0608\n",
      "Train Epoch: 027 Batch: 00063/00094 | Loss: 0.0637 | EWC Loss: 0.0240 | CE Loss: 0.0397\n",
      "Train Epoch: 027 Batch: 00064/00094 | Loss: 0.1077 | EWC Loss: 0.0238 | CE Loss: 0.0839\n",
      "Train Epoch: 027 Batch: 00065/00094 | Loss: 0.0453 | EWC Loss: 0.0238 | CE Loss: 0.0215\n",
      "Train Epoch: 027 Batch: 00066/00094 | Loss: 0.0614 | EWC Loss: 0.0238 | CE Loss: 0.0376\n",
      "Train Epoch: 027 Batch: 00067/00094 | Loss: 0.0483 | EWC Loss: 0.0240 | CE Loss: 0.0243\n",
      "Train Epoch: 027 Batch: 00068/00094 | Loss: 0.0812 | EWC Loss: 0.0238 | CE Loss: 0.0574\n",
      "Train Epoch: 027 Batch: 00069/00094 | Loss: 0.0656 | EWC Loss: 0.0238 | CE Loss: 0.0417\n",
      "Train Epoch: 027 Batch: 00070/00094 | Loss: 0.0554 | EWC Loss: 0.0237 | CE Loss: 0.0317\n",
      "Train Epoch: 027 Batch: 00071/00094 | Loss: 0.0782 | EWC Loss: 0.0237 | CE Loss: 0.0545\n",
      "Train Epoch: 027 Batch: 00072/00094 | Loss: 0.0767 | EWC Loss: 0.0237 | CE Loss: 0.0530\n",
      "Train Epoch: 027 Batch: 00073/00094 | Loss: 0.0918 | EWC Loss: 0.0236 | CE Loss: 0.0682\n",
      "Train Epoch: 027 Batch: 00074/00094 | Loss: 0.0811 | EWC Loss: 0.0237 | CE Loss: 0.0574\n",
      "Train Epoch: 027 Batch: 00075/00094 | Loss: 0.0734 | EWC Loss: 0.0237 | CE Loss: 0.0498\n",
      "Train Epoch: 027 Batch: 00076/00094 | Loss: 0.1141 | EWC Loss: 0.0236 | CE Loss: 0.0905\n",
      "Train Epoch: 027 Batch: 00077/00094 | Loss: 0.0623 | EWC Loss: 0.0239 | CE Loss: 0.0384\n",
      "Train Epoch: 027 Batch: 00078/00094 | Loss: 0.0577 | EWC Loss: 0.0239 | CE Loss: 0.0338\n",
      "Train Epoch: 027 Batch: 00079/00094 | Loss: 0.1353 | EWC Loss: 0.0239 | CE Loss: 0.1114\n",
      "Train Epoch: 027 Batch: 00080/00094 | Loss: 0.1074 | EWC Loss: 0.0240 | CE Loss: 0.0834\n",
      "Train Epoch: 027 Batch: 00081/00094 | Loss: 0.1079 | EWC Loss: 0.0239 | CE Loss: 0.0840\n",
      "Train Epoch: 027 Batch: 00082/00094 | Loss: 0.0489 | EWC Loss: 0.0239 | CE Loss: 0.0250\n",
      "Train Epoch: 027 Batch: 00083/00094 | Loss: 0.1014 | EWC Loss: 0.0239 | CE Loss: 0.0776\n",
      "Train Epoch: 027 Batch: 00084/00094 | Loss: 0.0934 | EWC Loss: 0.0239 | CE Loss: 0.0696\n",
      "Train Epoch: 027 Batch: 00085/00094 | Loss: 0.0768 | EWC Loss: 0.0238 | CE Loss: 0.0529\n",
      "Train Epoch: 027 Batch: 00086/00094 | Loss: 0.0846 | EWC Loss: 0.0239 | CE Loss: 0.0607\n",
      "Train Epoch: 027 Batch: 00087/00094 | Loss: 0.1792 | EWC Loss: 0.0239 | CE Loss: 0.1553\n",
      "Train Epoch: 027 Batch: 00088/00094 | Loss: 0.1201 | EWC Loss: 0.0239 | CE Loss: 0.0962\n",
      "Train Epoch: 027 Batch: 00089/00094 | Loss: 0.0785 | EWC Loss: 0.0240 | CE Loss: 0.0545\n",
      "Train Epoch: 027 Batch: 00090/00094 | Loss: 0.1021 | EWC Loss: 0.0238 | CE Loss: 0.0783\n",
      "Train Epoch: 027 Batch: 00091/00094 | Loss: 0.0679 | EWC Loss: 0.0238 | CE Loss: 0.0442\n",
      "Train Epoch: 027 Batch: 00092/00094 | Loss: 0.0802 | EWC Loss: 0.0236 | CE Loss: 0.0566\n",
      "Train Epoch: 027 Batch: 00093/00094 | Loss: 0.0744 | EWC Loss: 0.0234 | CE Loss: 0.0510\n",
      "Train Epoch: 027 Batch: 00094/00094 | Loss: 0.1177 | EWC Loss: 0.0234 | CE Loss: 0.0944\n",
      "Train Epoch: 026 |Acc 97.86667 | Loss: 0.08607 | Task Loss 0.06240 | EWC Loss: 0.02367\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0639 | acc:97.6000\n",
      "[VAL Acc] Target: 97.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7500 | acc:49.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2954 | acc:62.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 62.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.6356 | acc:46.5649\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6111 | acc:55.8777\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6401 | acc:78.4658\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6006 | acc:79.3495\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 79.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1554 | acc:55.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.56%\n",
      "[VAL Acc] Avg 65.69%\n",
      "\n",
      "\n",
      "---------- Starting epoch 27 ----------\n",
      "Train Epoch: 028 Batch: 00001/00094 | Loss: 0.0754 | EWC Loss: 0.0232 | CE Loss: 0.0522\n",
      "Train Epoch: 028 Batch: 00002/00094 | Loss: 0.1173 | EWC Loss: 0.0231 | CE Loss: 0.0941\n",
      "Train Epoch: 028 Batch: 00003/00094 | Loss: 0.0736 | EWC Loss: 0.0231 | CE Loss: 0.0505\n",
      "Train Epoch: 028 Batch: 00004/00094 | Loss: 0.0922 | EWC Loss: 0.0232 | CE Loss: 0.0690\n",
      "Train Epoch: 028 Batch: 00005/00094 | Loss: 0.2236 | EWC Loss: 0.0231 | CE Loss: 0.2005\n",
      "Train Epoch: 028 Batch: 00006/00094 | Loss: 0.1053 | EWC Loss: 0.0231 | CE Loss: 0.0822\n",
      "Train Epoch: 028 Batch: 00007/00094 | Loss: 0.0639 | EWC Loss: 0.0231 | CE Loss: 0.0408\n",
      "Train Epoch: 028 Batch: 00008/00094 | Loss: 0.0617 | EWC Loss: 0.0231 | CE Loss: 0.0386\n",
      "Train Epoch: 028 Batch: 00009/00094 | Loss: 0.1094 | EWC Loss: 0.0231 | CE Loss: 0.0863\n",
      "Train Epoch: 028 Batch: 00010/00094 | Loss: 0.0951 | EWC Loss: 0.0230 | CE Loss: 0.0722\n",
      "Train Epoch: 028 Batch: 00011/00094 | Loss: 0.0743 | EWC Loss: 0.0230 | CE Loss: 0.0513\n",
      "Train Epoch: 028 Batch: 00012/00094 | Loss: 0.0384 | EWC Loss: 0.0230 | CE Loss: 0.0155\n",
      "Train Epoch: 028 Batch: 00013/00094 | Loss: 0.1328 | EWC Loss: 0.0229 | CE Loss: 0.1098\n",
      "Train Epoch: 028 Batch: 00014/00094 | Loss: 0.0626 | EWC Loss: 0.0229 | CE Loss: 0.0397\n",
      "Train Epoch: 028 Batch: 00015/00094 | Loss: 0.0452 | EWC Loss: 0.0229 | CE Loss: 0.0223\n",
      "Train Epoch: 028 Batch: 00016/00094 | Loss: 0.1196 | EWC Loss: 0.0229 | CE Loss: 0.0967\n",
      "Train Epoch: 028 Batch: 00017/00094 | Loss: 0.1095 | EWC Loss: 0.0229 | CE Loss: 0.0866\n",
      "Train Epoch: 028 Batch: 00018/00094 | Loss: 0.0548 | EWC Loss: 0.0229 | CE Loss: 0.0319\n",
      "Train Epoch: 028 Batch: 00019/00094 | Loss: 0.0711 | EWC Loss: 0.0229 | CE Loss: 0.0483\n",
      "Train Epoch: 028 Batch: 00020/00094 | Loss: 0.1711 | EWC Loss: 0.0229 | CE Loss: 0.1482\n",
      "Train Epoch: 028 Batch: 00021/00094 | Loss: 0.0971 | EWC Loss: 0.0229 | CE Loss: 0.0741\n",
      "Train Epoch: 028 Batch: 00022/00094 | Loss: 0.0515 | EWC Loss: 0.0229 | CE Loss: 0.0285\n",
      "Train Epoch: 028 Batch: 00023/00094 | Loss: 0.1281 | EWC Loss: 0.0229 | CE Loss: 0.1051\n",
      "Train Epoch: 028 Batch: 00024/00094 | Loss: 0.0983 | EWC Loss: 0.0230 | CE Loss: 0.0753\n",
      "Train Epoch: 028 Batch: 00025/00094 | Loss: 0.0624 | EWC Loss: 0.0230 | CE Loss: 0.0394\n",
      "Train Epoch: 028 Batch: 00026/00094 | Loss: 0.0881 | EWC Loss: 0.0231 | CE Loss: 0.0650\n",
      "Train Epoch: 028 Batch: 00027/00094 | Loss: 0.1430 | EWC Loss: 0.0231 | CE Loss: 0.1198\n",
      "Train Epoch: 028 Batch: 00028/00094 | Loss: 0.0679 | EWC Loss: 0.0230 | CE Loss: 0.0449\n",
      "Train Epoch: 028 Batch: 00029/00094 | Loss: 0.0517 | EWC Loss: 0.0231 | CE Loss: 0.0285\n",
      "Train Epoch: 028 Batch: 00030/00094 | Loss: 0.0991 | EWC Loss: 0.0231 | CE Loss: 0.0759\n",
      "Train Epoch: 028 Batch: 00031/00094 | Loss: 0.0757 | EWC Loss: 0.0231 | CE Loss: 0.0526\n",
      "Train Epoch: 028 Batch: 00032/00094 | Loss: 0.0819 | EWC Loss: 0.0231 | CE Loss: 0.0588\n",
      "Train Epoch: 028 Batch: 00033/00094 | Loss: 0.1073 | EWC Loss: 0.0232 | CE Loss: 0.0841\n",
      "Train Epoch: 028 Batch: 00034/00094 | Loss: 0.1649 | EWC Loss: 0.0232 | CE Loss: 0.1417\n",
      "Train Epoch: 028 Batch: 00035/00094 | Loss: 0.0647 | EWC Loss: 0.0232 | CE Loss: 0.0415\n",
      "Train Epoch: 028 Batch: 00036/00094 | Loss: 0.1388 | EWC Loss: 0.0233 | CE Loss: 0.1156\n",
      "Train Epoch: 028 Batch: 00037/00094 | Loss: 0.0775 | EWC Loss: 0.0233 | CE Loss: 0.0542\n",
      "Train Epoch: 028 Batch: 00038/00094 | Loss: 0.1411 | EWC Loss: 0.0232 | CE Loss: 0.1179\n",
      "Train Epoch: 028 Batch: 00039/00094 | Loss: 0.1095 | EWC Loss: 0.0235 | CE Loss: 0.0860\n",
      "Train Epoch: 028 Batch: 00040/00094 | Loss: 0.0492 | EWC Loss: 0.0235 | CE Loss: 0.0257\n",
      "Train Epoch: 028 Batch: 00041/00094 | Loss: 0.2068 | EWC Loss: 0.0234 | CE Loss: 0.1834\n",
      "Train Epoch: 028 Batch: 00042/00094 | Loss: 0.0960 | EWC Loss: 0.0233 | CE Loss: 0.0727\n",
      "Train Epoch: 028 Batch: 00043/00094 | Loss: 0.0837 | EWC Loss: 0.0233 | CE Loss: 0.0604\n",
      "Train Epoch: 028 Batch: 00044/00094 | Loss: 0.0567 | EWC Loss: 0.0232 | CE Loss: 0.0335\n",
      "Train Epoch: 028 Batch: 00045/00094 | Loss: 0.2692 | EWC Loss: 0.0232 | CE Loss: 0.2461\n",
      "Train Epoch: 028 Batch: 00046/00094 | Loss: 0.0638 | EWC Loss: 0.0232 | CE Loss: 0.0406\n",
      "Train Epoch: 028 Batch: 00047/00094 | Loss: 0.1022 | EWC Loss: 0.0232 | CE Loss: 0.0791\n",
      "Train Epoch: 028 Batch: 00048/00094 | Loss: 0.1511 | EWC Loss: 0.0231 | CE Loss: 0.1280\n",
      "Train Epoch: 028 Batch: 00049/00094 | Loss: 0.0742 | EWC Loss: 0.0231 | CE Loss: 0.0511\n",
      "Train Epoch: 028 Batch: 00050/00094 | Loss: 0.1259 | EWC Loss: 0.0231 | CE Loss: 0.1028\n",
      "Train Epoch: 028 Batch: 00051/00094 | Loss: 0.0656 | EWC Loss: 0.0230 | CE Loss: 0.0426\n",
      "Train Epoch: 028 Batch: 00052/00094 | Loss: 0.0707 | EWC Loss: 0.0230 | CE Loss: 0.0477\n",
      "Train Epoch: 028 Batch: 00053/00094 | Loss: 0.1170 | EWC Loss: 0.0230 | CE Loss: 0.0941\n",
      "Train Epoch: 028 Batch: 00054/00094 | Loss: 0.1065 | EWC Loss: 0.0230 | CE Loss: 0.0835\n",
      "Train Epoch: 028 Batch: 00055/00094 | Loss: 0.1259 | EWC Loss: 0.0230 | CE Loss: 0.1029\n",
      "Train Epoch: 028 Batch: 00056/00094 | Loss: 0.0708 | EWC Loss: 0.0230 | CE Loss: 0.0478\n",
      "Train Epoch: 028 Batch: 00057/00094 | Loss: 0.1390 | EWC Loss: 0.0229 | CE Loss: 0.1161\n",
      "Train Epoch: 028 Batch: 00058/00094 | Loss: 0.0768 | EWC Loss: 0.0229 | CE Loss: 0.0539\n",
      "Train Epoch: 028 Batch: 00059/00094 | Loss: 0.0951 | EWC Loss: 0.0229 | CE Loss: 0.0722\n",
      "Train Epoch: 028 Batch: 00060/00094 | Loss: 0.1008 | EWC Loss: 0.0229 | CE Loss: 0.0779\n",
      "Train Epoch: 028 Batch: 00061/00094 | Loss: 0.1181 | EWC Loss: 0.0229 | CE Loss: 0.0952\n",
      "Train Epoch: 028 Batch: 00062/00094 | Loss: 0.0699 | EWC Loss: 0.0229 | CE Loss: 0.0470\n",
      "Train Epoch: 028 Batch: 00063/00094 | Loss: 0.0662 | EWC Loss: 0.0229 | CE Loss: 0.0433\n",
      "Train Epoch: 028 Batch: 00064/00094 | Loss: 0.1105 | EWC Loss: 0.0229 | CE Loss: 0.0876\n",
      "Train Epoch: 028 Batch: 00065/00094 | Loss: 0.0651 | EWC Loss: 0.0229 | CE Loss: 0.0422\n",
      "Train Epoch: 028 Batch: 00066/00094 | Loss: 0.1370 | EWC Loss: 0.0229 | CE Loss: 0.1141\n",
      "Train Epoch: 028 Batch: 00067/00094 | Loss: 0.1030 | EWC Loss: 0.0229 | CE Loss: 0.0801\n",
      "Train Epoch: 028 Batch: 00068/00094 | Loss: 0.0581 | EWC Loss: 0.0229 | CE Loss: 0.0352\n",
      "Train Epoch: 028 Batch: 00069/00094 | Loss: 0.0509 | EWC Loss: 0.0229 | CE Loss: 0.0279\n",
      "Train Epoch: 028 Batch: 00070/00094 | Loss: 0.0710 | EWC Loss: 0.0229 | CE Loss: 0.0481\n",
      "Train Epoch: 028 Batch: 00071/00094 | Loss: 0.1014 | EWC Loss: 0.0230 | CE Loss: 0.0784\n",
      "Train Epoch: 028 Batch: 00072/00094 | Loss: 0.1559 | EWC Loss: 0.0230 | CE Loss: 0.1329\n",
      "Train Epoch: 028 Batch: 00073/00094 | Loss: 0.0502 | EWC Loss: 0.0230 | CE Loss: 0.0272\n",
      "Train Epoch: 028 Batch: 00074/00094 | Loss: 0.1524 | EWC Loss: 0.0229 | CE Loss: 0.1294\n",
      "Train Epoch: 028 Batch: 00075/00094 | Loss: 0.0627 | EWC Loss: 0.0229 | CE Loss: 0.0399\n",
      "Train Epoch: 028 Batch: 00076/00094 | Loss: 0.0656 | EWC Loss: 0.0228 | CE Loss: 0.0428\n",
      "Train Epoch: 028 Batch: 00077/00094 | Loss: 0.0772 | EWC Loss: 0.0228 | CE Loss: 0.0544\n",
      "Train Epoch: 028 Batch: 00078/00094 | Loss: 0.1512 | EWC Loss: 0.0228 | CE Loss: 0.1284\n",
      "Train Epoch: 028 Batch: 00079/00094 | Loss: 0.1275 | EWC Loss: 0.0228 | CE Loss: 0.1046\n",
      "Train Epoch: 028 Batch: 00080/00094 | Loss: 0.1528 | EWC Loss: 0.0229 | CE Loss: 0.1300\n",
      "Train Epoch: 028 Batch: 00081/00094 | Loss: 0.0469 | EWC Loss: 0.0230 | CE Loss: 0.0239\n",
      "Train Epoch: 028 Batch: 00082/00094 | Loss: 0.0661 | EWC Loss: 0.0230 | CE Loss: 0.0430\n",
      "Train Epoch: 028 Batch: 00083/00094 | Loss: 0.0923 | EWC Loss: 0.0230 | CE Loss: 0.0693\n",
      "Train Epoch: 028 Batch: 00084/00094 | Loss: 0.0744 | EWC Loss: 0.0231 | CE Loss: 0.0513\n",
      "Train Epoch: 028 Batch: 00085/00094 | Loss: 0.1176 | EWC Loss: 0.0232 | CE Loss: 0.0944\n",
      "Train Epoch: 028 Batch: 00086/00094 | Loss: 0.0890 | EWC Loss: 0.0232 | CE Loss: 0.0658\n",
      "Train Epoch: 028 Batch: 00087/00094 | Loss: 0.2479 | EWC Loss: 0.0231 | CE Loss: 0.2248\n",
      "Train Epoch: 028 Batch: 00088/00094 | Loss: 0.0606 | EWC Loss: 0.0232 | CE Loss: 0.0374\n",
      "Train Epoch: 028 Batch: 00089/00094 | Loss: 0.0837 | EWC Loss: 0.0232 | CE Loss: 0.0605\n",
      "Train Epoch: 028 Batch: 00090/00094 | Loss: 0.0525 | EWC Loss: 0.0232 | CE Loss: 0.0293\n",
      "Train Epoch: 028 Batch: 00091/00094 | Loss: 0.1296 | EWC Loss: 0.0232 | CE Loss: 0.1064\n",
      "Train Epoch: 028 Batch: 00092/00094 | Loss: 0.1114 | EWC Loss: 0.0233 | CE Loss: 0.0881\n",
      "Train Epoch: 028 Batch: 00093/00094 | Loss: 0.1040 | EWC Loss: 0.0233 | CE Loss: 0.0807\n",
      "Train Epoch: 028 Batch: 00094/00094 | Loss: 0.1425 | EWC Loss: 0.0232 | CE Loss: 0.1193\n",
      "Train Epoch: 027 |Acc 97.28333 | Loss: 0.09955 | Task Loss 0.07650 | EWC Loss: 0.02306\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0542 | acc:98.2500\n",
      "[VAL Acc] Target: 98.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7131 | acc:49.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2673 | acc:60.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5819 | acc:44.8473\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6986 | acc:54.2320\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6909 | acc:75.7856\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6048 | acc:78.4875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 78.49%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1894 | acc:57.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.44%\n",
      "[VAL Acc] Avg 64.98%\n",
      "\n",
      "\n",
      "---------- Starting epoch 28 ----------\n",
      "Train Epoch: 029 Batch: 00001/00094 | Loss: 0.0512 | EWC Loss: 0.0232 | CE Loss: 0.0281\n",
      "Train Epoch: 029 Batch: 00002/00094 | Loss: 0.0792 | EWC Loss: 0.0232 | CE Loss: 0.0560\n",
      "Train Epoch: 029 Batch: 00003/00094 | Loss: 0.0591 | EWC Loss: 0.0232 | CE Loss: 0.0359\n",
      "Train Epoch: 029 Batch: 00004/00094 | Loss: 0.1102 | EWC Loss: 0.0232 | CE Loss: 0.0870\n",
      "Train Epoch: 029 Batch: 00005/00094 | Loss: 0.1603 | EWC Loss: 0.0231 | CE Loss: 0.1371\n",
      "Train Epoch: 029 Batch: 00006/00094 | Loss: 0.0648 | EWC Loss: 0.0232 | CE Loss: 0.0416\n",
      "Train Epoch: 029 Batch: 00007/00094 | Loss: 0.0596 | EWC Loss: 0.0232 | CE Loss: 0.0364\n",
      "Train Epoch: 029 Batch: 00008/00094 | Loss: 0.1475 | EWC Loss: 0.0232 | CE Loss: 0.1244\n",
      "Train Epoch: 029 Batch: 00009/00094 | Loss: 0.0644 | EWC Loss: 0.0231 | CE Loss: 0.0413\n",
      "Train Epoch: 029 Batch: 00010/00094 | Loss: 0.0981 | EWC Loss: 0.0231 | CE Loss: 0.0750\n",
      "Train Epoch: 029 Batch: 00011/00094 | Loss: 0.0876 | EWC Loss: 0.0231 | CE Loss: 0.0644\n",
      "Train Epoch: 029 Batch: 00012/00094 | Loss: 0.0703 | EWC Loss: 0.0231 | CE Loss: 0.0472\n",
      "Train Epoch: 029 Batch: 00013/00094 | Loss: 0.1267 | EWC Loss: 0.0231 | CE Loss: 0.1036\n",
      "Train Epoch: 029 Batch: 00014/00094 | Loss: 0.0575 | EWC Loss: 0.0231 | CE Loss: 0.0344\n",
      "Train Epoch: 029 Batch: 00015/00094 | Loss: 0.1069 | EWC Loss: 0.0231 | CE Loss: 0.0838\n",
      "Train Epoch: 029 Batch: 00016/00094 | Loss: 0.0474 | EWC Loss: 0.0231 | CE Loss: 0.0243\n",
      "Train Epoch: 029 Batch: 00017/00094 | Loss: 0.1479 | EWC Loss: 0.0230 | CE Loss: 0.1249\n",
      "Train Epoch: 029 Batch: 00018/00094 | Loss: 0.0664 | EWC Loss: 0.0230 | CE Loss: 0.0434\n",
      "Train Epoch: 029 Batch: 00019/00094 | Loss: 0.1121 | EWC Loss: 0.0230 | CE Loss: 0.0891\n",
      "Train Epoch: 029 Batch: 00020/00094 | Loss: 0.0596 | EWC Loss: 0.0229 | CE Loss: 0.0366\n",
      "Train Epoch: 029 Batch: 00021/00094 | Loss: 0.0604 | EWC Loss: 0.0229 | CE Loss: 0.0375\n",
      "Train Epoch: 029 Batch: 00022/00094 | Loss: 0.0526 | EWC Loss: 0.0229 | CE Loss: 0.0297\n",
      "Train Epoch: 029 Batch: 00023/00094 | Loss: 0.1428 | EWC Loss: 0.0229 | CE Loss: 0.1199\n",
      "Train Epoch: 029 Batch: 00024/00094 | Loss: 0.0856 | EWC Loss: 0.0229 | CE Loss: 0.0627\n",
      "Train Epoch: 029 Batch: 00025/00094 | Loss: 0.0846 | EWC Loss: 0.0229 | CE Loss: 0.0617\n",
      "Train Epoch: 029 Batch: 00026/00094 | Loss: 0.0483 | EWC Loss: 0.0229 | CE Loss: 0.0254\n",
      "Train Epoch: 029 Batch: 00027/00094 | Loss: 0.1625 | EWC Loss: 0.0229 | CE Loss: 0.1396\n",
      "Train Epoch: 029 Batch: 00028/00094 | Loss: 0.1046 | EWC Loss: 0.0229 | CE Loss: 0.0817\n",
      "Train Epoch: 029 Batch: 00029/00094 | Loss: 0.1156 | EWC Loss: 0.0230 | CE Loss: 0.0926\n",
      "Train Epoch: 029 Batch: 00030/00094 | Loss: 0.3016 | EWC Loss: 0.0230 | CE Loss: 0.2785\n",
      "Train Epoch: 029 Batch: 00031/00094 | Loss: 0.0526 | EWC Loss: 0.0231 | CE Loss: 0.0295\n",
      "Train Epoch: 029 Batch: 00032/00094 | Loss: 0.0579 | EWC Loss: 0.0231 | CE Loss: 0.0349\n",
      "Train Epoch: 029 Batch: 00033/00094 | Loss: 0.1613 | EWC Loss: 0.0231 | CE Loss: 0.1382\n",
      "Train Epoch: 029 Batch: 00034/00094 | Loss: 0.0423 | EWC Loss: 0.0231 | CE Loss: 0.0192\n",
      "Train Epoch: 029 Batch: 00035/00094 | Loss: 0.0566 | EWC Loss: 0.0231 | CE Loss: 0.0335\n",
      "Train Epoch: 029 Batch: 00036/00094 | Loss: 0.0868 | EWC Loss: 0.0231 | CE Loss: 0.0637\n",
      "Train Epoch: 029 Batch: 00037/00094 | Loss: 0.1150 | EWC Loss: 0.0231 | CE Loss: 0.0919\n",
      "Train Epoch: 029 Batch: 00038/00094 | Loss: 0.0834 | EWC Loss: 0.0231 | CE Loss: 0.0604\n",
      "Train Epoch: 029 Batch: 00039/00094 | Loss: 0.0690 | EWC Loss: 0.0231 | CE Loss: 0.0459\n",
      "Train Epoch: 029 Batch: 00040/00094 | Loss: 0.0764 | EWC Loss: 0.0231 | CE Loss: 0.0533\n",
      "Train Epoch: 029 Batch: 00041/00094 | Loss: 0.0522 | EWC Loss: 0.0231 | CE Loss: 0.0291\n",
      "Train Epoch: 029 Batch: 00042/00094 | Loss: 0.0582 | EWC Loss: 0.0231 | CE Loss: 0.0352\n",
      "Train Epoch: 029 Batch: 00043/00094 | Loss: 0.1045 | EWC Loss: 0.0231 | CE Loss: 0.0814\n",
      "Train Epoch: 029 Batch: 00044/00094 | Loss: 0.0500 | EWC Loss: 0.0230 | CE Loss: 0.0270\n",
      "Train Epoch: 029 Batch: 00045/00094 | Loss: 0.0771 | EWC Loss: 0.0230 | CE Loss: 0.0541\n",
      "Train Epoch: 029 Batch: 00046/00094 | Loss: 0.0630 | EWC Loss: 0.0230 | CE Loss: 0.0400\n",
      "Train Epoch: 029 Batch: 00047/00094 | Loss: 0.0624 | EWC Loss: 0.0230 | CE Loss: 0.0394\n",
      "Train Epoch: 029 Batch: 00048/00094 | Loss: 0.0512 | EWC Loss: 0.0230 | CE Loss: 0.0282\n",
      "Train Epoch: 029 Batch: 00049/00094 | Loss: 0.0611 | EWC Loss: 0.0230 | CE Loss: 0.0381\n",
      "Train Epoch: 029 Batch: 00050/00094 | Loss: 0.0733 | EWC Loss: 0.0230 | CE Loss: 0.0503\n",
      "Train Epoch: 029 Batch: 00051/00094 | Loss: 0.0779 | EWC Loss: 0.0229 | CE Loss: 0.0549\n",
      "Train Epoch: 029 Batch: 00052/00094 | Loss: 0.1477 | EWC Loss: 0.0229 | CE Loss: 0.1247\n",
      "Train Epoch: 029 Batch: 00053/00094 | Loss: 0.0619 | EWC Loss: 0.0229 | CE Loss: 0.0389\n",
      "Train Epoch: 029 Batch: 00054/00094 | Loss: 0.1165 | EWC Loss: 0.0229 | CE Loss: 0.0935\n",
      "Train Epoch: 029 Batch: 00055/00094 | Loss: 0.0835 | EWC Loss: 0.0229 | CE Loss: 0.0606\n",
      "Train Epoch: 029 Batch: 00056/00094 | Loss: 0.0865 | EWC Loss: 0.0230 | CE Loss: 0.0635\n",
      "Train Epoch: 029 Batch: 00057/00094 | Loss: 0.0546 | EWC Loss: 0.0230 | CE Loss: 0.0316\n",
      "Train Epoch: 029 Batch: 00058/00094 | Loss: 0.0571 | EWC Loss: 0.0230 | CE Loss: 0.0341\n",
      "Train Epoch: 029 Batch: 00059/00094 | Loss: 0.0385 | EWC Loss: 0.0230 | CE Loss: 0.0155\n",
      "Train Epoch: 029 Batch: 00060/00094 | Loss: 0.1140 | EWC Loss: 0.0230 | CE Loss: 0.0910\n",
      "Train Epoch: 029 Batch: 00061/00094 | Loss: 0.1015 | EWC Loss: 0.0230 | CE Loss: 0.0784\n",
      "Train Epoch: 029 Batch: 00062/00094 | Loss: 0.0889 | EWC Loss: 0.0233 | CE Loss: 0.0657\n",
      "Train Epoch: 029 Batch: 00063/00094 | Loss: 0.0609 | EWC Loss: 0.0233 | CE Loss: 0.0376\n",
      "Train Epoch: 029 Batch: 00064/00094 | Loss: 0.0550 | EWC Loss: 0.0233 | CE Loss: 0.0317\n",
      "Train Epoch: 029 Batch: 00065/00094 | Loss: 0.1039 | EWC Loss: 0.0233 | CE Loss: 0.0806\n",
      "Train Epoch: 029 Batch: 00066/00094 | Loss: 0.0724 | EWC Loss: 0.0233 | CE Loss: 0.0491\n",
      "Train Epoch: 029 Batch: 00067/00094 | Loss: 0.2395 | EWC Loss: 0.0233 | CE Loss: 0.2162\n",
      "Train Epoch: 029 Batch: 00068/00094 | Loss: 0.0806 | EWC Loss: 0.0232 | CE Loss: 0.0574\n",
      "Train Epoch: 029 Batch: 00069/00094 | Loss: 0.2042 | EWC Loss: 0.0232 | CE Loss: 0.1810\n",
      "Train Epoch: 029 Batch: 00070/00094 | Loss: 0.4153 | EWC Loss: 0.0232 | CE Loss: 0.3922\n",
      "Train Epoch: 029 Batch: 00071/00094 | Loss: 0.0998 | EWC Loss: 0.0231 | CE Loss: 0.0767\n",
      "Train Epoch: 029 Batch: 00072/00094 | Loss: 0.0950 | EWC Loss: 0.0231 | CE Loss: 0.0719\n",
      "Train Epoch: 029 Batch: 00073/00094 | Loss: 0.0487 | EWC Loss: 0.0231 | CE Loss: 0.0256\n",
      "Train Epoch: 029 Batch: 00074/00094 | Loss: 0.0582 | EWC Loss: 0.0231 | CE Loss: 0.0350\n",
      "Train Epoch: 029 Batch: 00075/00094 | Loss: 0.1912 | EWC Loss: 0.0231 | CE Loss: 0.1680\n",
      "Train Epoch: 029 Batch: 00076/00094 | Loss: 0.0758 | EWC Loss: 0.0232 | CE Loss: 0.0527\n",
      "Train Epoch: 029 Batch: 00077/00094 | Loss: 0.0818 | EWC Loss: 0.0232 | CE Loss: 0.0587\n",
      "Train Epoch: 029 Batch: 00078/00094 | Loss: 0.0974 | EWC Loss: 0.0232 | CE Loss: 0.0742\n",
      "Train Epoch: 029 Batch: 00079/00094 | Loss: 0.1213 | EWC Loss: 0.0231 | CE Loss: 0.0982\n",
      "Train Epoch: 029 Batch: 00080/00094 | Loss: 0.0769 | EWC Loss: 0.0231 | CE Loss: 0.0537\n",
      "Train Epoch: 029 Batch: 00081/00094 | Loss: 0.1792 | EWC Loss: 0.0231 | CE Loss: 0.1561\n",
      "Train Epoch: 029 Batch: 00082/00094 | Loss: 0.1660 | EWC Loss: 0.0232 | CE Loss: 0.1428\n",
      "Train Epoch: 029 Batch: 00083/00094 | Loss: 0.0742 | EWC Loss: 0.0232 | CE Loss: 0.0510\n",
      "Train Epoch: 029 Batch: 00084/00094 | Loss: 0.1137 | EWC Loss: 0.0232 | CE Loss: 0.0905\n",
      "Train Epoch: 029 Batch: 00085/00094 | Loss: 0.0856 | EWC Loss: 0.0232 | CE Loss: 0.0624\n",
      "Train Epoch: 029 Batch: 00086/00094 | Loss: 0.1631 | EWC Loss: 0.0232 | CE Loss: 0.1399\n",
      "Train Epoch: 029 Batch: 00087/00094 | Loss: 0.1180 | EWC Loss: 0.0232 | CE Loss: 0.0948\n",
      "Train Epoch: 029 Batch: 00088/00094 | Loss: 0.0689 | EWC Loss: 0.0232 | CE Loss: 0.0457\n",
      "Train Epoch: 029 Batch: 00089/00094 | Loss: 0.0665 | EWC Loss: 0.0233 | CE Loss: 0.0432\n",
      "Train Epoch: 029 Batch: 00090/00094 | Loss: 0.0817 | EWC Loss: 0.0233 | CE Loss: 0.0584\n",
      "Train Epoch: 029 Batch: 00091/00094 | Loss: 0.0689 | EWC Loss: 0.0233 | CE Loss: 0.0456\n",
      "Train Epoch: 029 Batch: 00092/00094 | Loss: 0.0638 | EWC Loss: 0.0233 | CE Loss: 0.0405\n",
      "Train Epoch: 029 Batch: 00093/00094 | Loss: 0.0617 | EWC Loss: 0.0233 | CE Loss: 0.0384\n",
      "Train Epoch: 029 Batch: 00094/00094 | Loss: 0.1347 | EWC Loss: 0.0233 | CE Loss: 0.1114\n",
      "Train Epoch: 028 |Acc 97.66667 | Loss: 0.09619 | Task Loss 0.07310 | EWC Loss: 0.02309\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0504 | acc:98.4000\n",
      "[VAL Acc] Target: 98.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7909 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2675 | acc:60.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5836 | acc:48.6641\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.7199 | acc:54.6630\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6739 | acc:76.8946\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6086 | acc:79.5063\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 79.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.2091 | acc:55.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.19%\n",
      "[VAL Acc] Avg 65.40%\n",
      "\n",
      "\n",
      "---------- Starting epoch 29 ----------\n",
      "Train Epoch: 030 Batch: 00001/00094 | Loss: 0.1591 | EWC Loss: 0.0233 | CE Loss: 0.1358\n",
      "Train Epoch: 030 Batch: 00002/00094 | Loss: 0.0758 | EWC Loss: 0.0233 | CE Loss: 0.0525\n",
      "Train Epoch: 030 Batch: 00003/00094 | Loss: 0.0507 | EWC Loss: 0.0233 | CE Loss: 0.0274\n",
      "Train Epoch: 030 Batch: 00004/00094 | Loss: 0.0690 | EWC Loss: 0.0233 | CE Loss: 0.0457\n",
      "Train Epoch: 030 Batch: 00005/00094 | Loss: 0.1261 | EWC Loss: 0.0233 | CE Loss: 0.1028\n",
      "Train Epoch: 030 Batch: 00006/00094 | Loss: 0.1033 | EWC Loss: 0.0233 | CE Loss: 0.0800\n",
      "Train Epoch: 030 Batch: 00007/00094 | Loss: 0.1164 | EWC Loss: 0.0233 | CE Loss: 0.0931\n",
      "Train Epoch: 030 Batch: 00008/00094 | Loss: 0.0971 | EWC Loss: 0.0233 | CE Loss: 0.0738\n",
      "Train Epoch: 030 Batch: 00009/00094 | Loss: 0.0316 | EWC Loss: 0.0233 | CE Loss: 0.0084\n",
      "Train Epoch: 030 Batch: 00010/00094 | Loss: 0.1182 | EWC Loss: 0.0233 | CE Loss: 0.0949\n",
      "Train Epoch: 030 Batch: 00011/00094 | Loss: 0.0634 | EWC Loss: 0.0233 | CE Loss: 0.0402\n",
      "Train Epoch: 030 Batch: 00012/00094 | Loss: 0.0635 | EWC Loss: 0.0233 | CE Loss: 0.0402\n",
      "Train Epoch: 030 Batch: 00013/00094 | Loss: 0.1167 | EWC Loss: 0.0233 | CE Loss: 0.0935\n",
      "Train Epoch: 030 Batch: 00014/00094 | Loss: 0.1672 | EWC Loss: 0.0233 | CE Loss: 0.1440\n",
      "Train Epoch: 030 Batch: 00015/00094 | Loss: 0.2094 | EWC Loss: 0.0233 | CE Loss: 0.1860\n",
      "Train Epoch: 030 Batch: 00016/00094 | Loss: 0.1094 | EWC Loss: 0.0233 | CE Loss: 0.0861\n",
      "Train Epoch: 030 Batch: 00017/00094 | Loss: 0.1631 | EWC Loss: 0.0233 | CE Loss: 0.1398\n",
      "Train Epoch: 030 Batch: 00018/00094 | Loss: 0.0703 | EWC Loss: 0.0233 | CE Loss: 0.0470\n",
      "Train Epoch: 030 Batch: 00019/00094 | Loss: 0.0464 | EWC Loss: 0.0233 | CE Loss: 0.0231\n",
      "Train Epoch: 030 Batch: 00020/00094 | Loss: 0.1166 | EWC Loss: 0.0233 | CE Loss: 0.0933\n",
      "Train Epoch: 030 Batch: 00021/00094 | Loss: 0.1179 | EWC Loss: 0.0233 | CE Loss: 0.0945\n",
      "Train Epoch: 030 Batch: 00022/00094 | Loss: 0.1252 | EWC Loss: 0.0233 | CE Loss: 0.1019\n",
      "Train Epoch: 030 Batch: 00023/00094 | Loss: 0.1075 | EWC Loss: 0.0233 | CE Loss: 0.0842\n",
      "Train Epoch: 030 Batch: 00024/00094 | Loss: 0.1212 | EWC Loss: 0.0233 | CE Loss: 0.0979\n",
      "Train Epoch: 030 Batch: 00025/00094 | Loss: 0.0525 | EWC Loss: 0.0233 | CE Loss: 0.0292\n",
      "Train Epoch: 030 Batch: 00026/00094 | Loss: 0.0800 | EWC Loss: 0.0233 | CE Loss: 0.0566\n",
      "Train Epoch: 030 Batch: 00027/00094 | Loss: 0.1620 | EWC Loss: 0.0233 | CE Loss: 0.1387\n",
      "Train Epoch: 030 Batch: 00028/00094 | Loss: 0.0554 | EWC Loss: 0.0233 | CE Loss: 0.0321\n",
      "Train Epoch: 030 Batch: 00029/00094 | Loss: 0.0724 | EWC Loss: 0.0233 | CE Loss: 0.0491\n",
      "Train Epoch: 030 Batch: 00030/00094 | Loss: 0.1426 | EWC Loss: 0.0233 | CE Loss: 0.1193\n",
      "Train Epoch: 030 Batch: 00031/00094 | Loss: 0.0771 | EWC Loss: 0.0233 | CE Loss: 0.0538\n",
      "Train Epoch: 030 Batch: 00032/00094 | Loss: 0.0951 | EWC Loss: 0.0233 | CE Loss: 0.0718\n",
      "Train Epoch: 030 Batch: 00033/00094 | Loss: 0.0524 | EWC Loss: 0.0233 | CE Loss: 0.0292\n",
      "Train Epoch: 030 Batch: 00034/00094 | Loss: 0.0642 | EWC Loss: 0.0233 | CE Loss: 0.0409\n",
      "Train Epoch: 030 Batch: 00035/00094 | Loss: 0.0683 | EWC Loss: 0.0233 | CE Loss: 0.0450\n",
      "Train Epoch: 030 Batch: 00036/00094 | Loss: 0.0811 | EWC Loss: 0.0233 | CE Loss: 0.0578\n",
      "Train Epoch: 030 Batch: 00037/00094 | Loss: 0.0539 | EWC Loss: 0.0233 | CE Loss: 0.0307\n",
      "Train Epoch: 030 Batch: 00038/00094 | Loss: 0.0776 | EWC Loss: 0.0232 | CE Loss: 0.0543\n",
      "Train Epoch: 030 Batch: 00039/00094 | Loss: 0.0515 | EWC Loss: 0.0233 | CE Loss: 0.0282\n",
      "Train Epoch: 030 Batch: 00040/00094 | Loss: 0.0614 | EWC Loss: 0.0232 | CE Loss: 0.0382\n",
      "Train Epoch: 030 Batch: 00041/00094 | Loss: 0.0592 | EWC Loss: 0.0232 | CE Loss: 0.0360\n",
      "Train Epoch: 030 Batch: 00042/00094 | Loss: 0.1332 | EWC Loss: 0.0232 | CE Loss: 0.1099\n",
      "Train Epoch: 030 Batch: 00043/00094 | Loss: 0.0416 | EWC Loss: 0.0233 | CE Loss: 0.0183\n",
      "Train Epoch: 030 Batch: 00044/00094 | Loss: 0.1287 | EWC Loss: 0.0233 | CE Loss: 0.1055\n",
      "Train Epoch: 030 Batch: 00045/00094 | Loss: 0.0448 | EWC Loss: 0.0232 | CE Loss: 0.0216\n",
      "Train Epoch: 030 Batch: 00046/00094 | Loss: 0.0640 | EWC Loss: 0.0232 | CE Loss: 0.0408\n",
      "Train Epoch: 030 Batch: 00047/00094 | Loss: 0.0753 | EWC Loss: 0.0232 | CE Loss: 0.0520\n",
      "Train Epoch: 030 Batch: 00048/00094 | Loss: 0.1627 | EWC Loss: 0.0232 | CE Loss: 0.1394\n",
      "Train Epoch: 030 Batch: 00049/00094 | Loss: 0.0559 | EWC Loss: 0.0232 | CE Loss: 0.0327\n",
      "Train Epoch: 030 Batch: 00050/00094 | Loss: 0.0524 | EWC Loss: 0.0232 | CE Loss: 0.0292\n",
      "Train Epoch: 030 Batch: 00051/00094 | Loss: 0.1226 | EWC Loss: 0.0232 | CE Loss: 0.0993\n",
      "Train Epoch: 030 Batch: 00052/00094 | Loss: 0.0815 | EWC Loss: 0.0232 | CE Loss: 0.0583\n",
      "Train Epoch: 030 Batch: 00053/00094 | Loss: 0.0742 | EWC Loss: 0.0232 | CE Loss: 0.0510\n",
      "Train Epoch: 030 Batch: 00054/00094 | Loss: 0.1558 | EWC Loss: 0.0232 | CE Loss: 0.1325\n",
      "Train Epoch: 030 Batch: 00055/00094 | Loss: 0.1660 | EWC Loss: 0.0232 | CE Loss: 0.1427\n",
      "Train Epoch: 030 Batch: 00056/00094 | Loss: 0.1275 | EWC Loss: 0.0232 | CE Loss: 0.1042\n",
      "Train Epoch: 030 Batch: 00057/00094 | Loss: 0.0965 | EWC Loss: 0.0233 | CE Loss: 0.0733\n",
      "Train Epoch: 030 Batch: 00058/00094 | Loss: 0.0881 | EWC Loss: 0.0233 | CE Loss: 0.0649\n",
      "Train Epoch: 030 Batch: 00059/00094 | Loss: 0.0837 | EWC Loss: 0.0233 | CE Loss: 0.0604\n",
      "Train Epoch: 030 Batch: 00060/00094 | Loss: 0.1033 | EWC Loss: 0.0233 | CE Loss: 0.0801\n",
      "Train Epoch: 030 Batch: 00061/00094 | Loss: 0.0738 | EWC Loss: 0.0233 | CE Loss: 0.0505\n",
      "Train Epoch: 030 Batch: 00062/00094 | Loss: 0.0674 | EWC Loss: 0.0233 | CE Loss: 0.0441\n",
      "Train Epoch: 030 Batch: 00063/00094 | Loss: 0.0866 | EWC Loss: 0.0233 | CE Loss: 0.0634\n",
      "Train Epoch: 030 Batch: 00064/00094 | Loss: 0.0467 | EWC Loss: 0.0233 | CE Loss: 0.0234\n",
      "Train Epoch: 030 Batch: 00065/00094 | Loss: 0.0874 | EWC Loss: 0.0233 | CE Loss: 0.0641\n",
      "Train Epoch: 030 Batch: 00066/00094 | Loss: 0.0824 | EWC Loss: 0.0233 | CE Loss: 0.0592\n",
      "Train Epoch: 030 Batch: 00067/00094 | Loss: 0.1110 | EWC Loss: 0.0233 | CE Loss: 0.0878\n",
      "Train Epoch: 030 Batch: 00068/00094 | Loss: 0.0573 | EWC Loss: 0.0233 | CE Loss: 0.0340\n",
      "Train Epoch: 030 Batch: 00069/00094 | Loss: 0.0392 | EWC Loss: 0.0233 | CE Loss: 0.0159\n",
      "Train Epoch: 030 Batch: 00070/00094 | Loss: 0.1450 | EWC Loss: 0.0232 | CE Loss: 0.1217\n",
      "Train Epoch: 030 Batch: 00071/00094 | Loss: 0.0722 | EWC Loss: 0.0232 | CE Loss: 0.0490\n",
      "Train Epoch: 030 Batch: 00072/00094 | Loss: 0.0861 | EWC Loss: 0.0232 | CE Loss: 0.0628\n",
      "Train Epoch: 030 Batch: 00073/00094 | Loss: 0.0499 | EWC Loss: 0.0232 | CE Loss: 0.0267\n",
      "Train Epoch: 030 Batch: 00074/00094 | Loss: 0.0834 | EWC Loss: 0.0232 | CE Loss: 0.0601\n",
      "Train Epoch: 030 Batch: 00075/00094 | Loss: 0.0750 | EWC Loss: 0.0232 | CE Loss: 0.0518\n",
      "Train Epoch: 030 Batch: 00076/00094 | Loss: 0.1597 | EWC Loss: 0.0232 | CE Loss: 0.1365\n",
      "Train Epoch: 030 Batch: 00077/00094 | Loss: 0.1865 | EWC Loss: 0.0232 | CE Loss: 0.1632\n",
      "Train Epoch: 030 Batch: 00078/00094 | Loss: 0.1365 | EWC Loss: 0.0232 | CE Loss: 0.1133\n",
      "Train Epoch: 030 Batch: 00079/00094 | Loss: 0.1167 | EWC Loss: 0.0232 | CE Loss: 0.0935\n",
      "Train Epoch: 030 Batch: 00080/00094 | Loss: 0.0642 | EWC Loss: 0.0232 | CE Loss: 0.0409\n",
      "Train Epoch: 030 Batch: 00081/00094 | Loss: 0.0509 | EWC Loss: 0.0232 | CE Loss: 0.0277\n",
      "Train Epoch: 030 Batch: 00082/00094 | Loss: 0.1375 | EWC Loss: 0.0232 | CE Loss: 0.1143\n",
      "Train Epoch: 030 Batch: 00083/00094 | Loss: 0.0625 | EWC Loss: 0.0232 | CE Loss: 0.0393\n",
      "Train Epoch: 030 Batch: 00084/00094 | Loss: 0.1340 | EWC Loss: 0.0232 | CE Loss: 0.1108\n",
      "Train Epoch: 030 Batch: 00085/00094 | Loss: 0.1190 | EWC Loss: 0.0232 | CE Loss: 0.0958\n",
      "Train Epoch: 030 Batch: 00086/00094 | Loss: 0.1924 | EWC Loss: 0.0232 | CE Loss: 0.1692\n",
      "Train Epoch: 030 Batch: 00087/00094 | Loss: 0.0596 | EWC Loss: 0.0232 | CE Loss: 0.0364\n",
      "Train Epoch: 030 Batch: 00088/00094 | Loss: 0.0858 | EWC Loss: 0.0232 | CE Loss: 0.0625\n",
      "Train Epoch: 030 Batch: 00089/00094 | Loss: 0.1423 | EWC Loss: 0.0232 | CE Loss: 0.1191\n",
      "Train Epoch: 030 Batch: 00090/00094 | Loss: 0.0513 | EWC Loss: 0.0232 | CE Loss: 0.0280\n",
      "Train Epoch: 030 Batch: 00091/00094 | Loss: 0.0694 | EWC Loss: 0.0232 | CE Loss: 0.0462\n",
      "Train Epoch: 030 Batch: 00092/00094 | Loss: 0.0631 | EWC Loss: 0.0232 | CE Loss: 0.0399\n",
      "Train Epoch: 030 Batch: 00093/00094 | Loss: 0.0667 | EWC Loss: 0.0232 | CE Loss: 0.0435\n",
      "Train Epoch: 030 Batch: 00094/00094 | Loss: 0.0542 | EWC Loss: 0.0232 | CE Loss: 0.0309\n",
      "Train Epoch: 029 |Acc 97.36667 | Loss: 0.09389 | Task Loss 0.07063 | EWC Loss: 0.02326\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0617 | acc:98.0000\n",
      "[VAL Acc] Target: 98.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7694 | acc:49.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2349 | acc:60.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.6283 | acc:50.1908\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5271 | acc:55.8386\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.84%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6737 | acc:77.1719\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 77.17%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5558 | acc:80.0940\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 80.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1940 | acc:57.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.06%\n",
      "[VAL Acc] Avg 66.13%\n",
      "\n",
      "\n",
      "---------- Starting epoch 30 ----------\n",
      "Train Epoch: 031 Batch: 00001/00094 | Loss: 0.0744 | EWC Loss: 0.0232 | CE Loss: 0.0512\n",
      "Train Epoch: 031 Batch: 00002/00094 | Loss: 0.1781 | EWC Loss: 0.0232 | CE Loss: 0.1549\n",
      "Train Epoch: 031 Batch: 00003/00094 | Loss: 0.0680 | EWC Loss: 0.0232 | CE Loss: 0.0448\n",
      "Train Epoch: 031 Batch: 00004/00094 | Loss: 0.1081 | EWC Loss: 0.0232 | CE Loss: 0.0849\n",
      "Train Epoch: 031 Batch: 00005/00094 | Loss: 0.1310 | EWC Loss: 0.0232 | CE Loss: 0.1078\n",
      "Train Epoch: 031 Batch: 00006/00094 | Loss: 0.1430 | EWC Loss: 0.0232 | CE Loss: 0.1198\n",
      "Train Epoch: 031 Batch: 00007/00094 | Loss: 0.1431 | EWC Loss: 0.0232 | CE Loss: 0.1199\n",
      "Train Epoch: 031 Batch: 00008/00094 | Loss: 0.0983 | EWC Loss: 0.0232 | CE Loss: 0.0751\n",
      "Train Epoch: 031 Batch: 00009/00094 | Loss: 0.0823 | EWC Loss: 0.0232 | CE Loss: 0.0591\n",
      "Train Epoch: 031 Batch: 00010/00094 | Loss: 0.1119 | EWC Loss: 0.0232 | CE Loss: 0.0887\n",
      "Train Epoch: 031 Batch: 00011/00094 | Loss: 0.1037 | EWC Loss: 0.0232 | CE Loss: 0.0805\n",
      "Train Epoch: 031 Batch: 00012/00094 | Loss: 0.0946 | EWC Loss: 0.0232 | CE Loss: 0.0714\n",
      "Train Epoch: 031 Batch: 00013/00094 | Loss: 0.0740 | EWC Loss: 0.0232 | CE Loss: 0.0508\n",
      "Train Epoch: 031 Batch: 00014/00094 | Loss: 0.0924 | EWC Loss: 0.0232 | CE Loss: 0.0692\n",
      "Train Epoch: 031 Batch: 00015/00094 | Loss: 0.0775 | EWC Loss: 0.0232 | CE Loss: 0.0543\n",
      "Train Epoch: 031 Batch: 00016/00094 | Loss: 0.1800 | EWC Loss: 0.0232 | CE Loss: 0.1568\n",
      "Train Epoch: 031 Batch: 00017/00094 | Loss: 0.1330 | EWC Loss: 0.0232 | CE Loss: 0.1098\n",
      "Train Epoch: 031 Batch: 00018/00094 | Loss: 0.0703 | EWC Loss: 0.0232 | CE Loss: 0.0471\n",
      "Train Epoch: 031 Batch: 00019/00094 | Loss: 0.0628 | EWC Loss: 0.0232 | CE Loss: 0.0396\n",
      "Train Epoch: 031 Batch: 00020/00094 | Loss: 0.1354 | EWC Loss: 0.0232 | CE Loss: 0.1122\n",
      "Train Epoch: 031 Batch: 00021/00094 | Loss: 0.0824 | EWC Loss: 0.0232 | CE Loss: 0.0592\n",
      "Train Epoch: 031 Batch: 00022/00094 | Loss: 0.1100 | EWC Loss: 0.0232 | CE Loss: 0.0868\n",
      "Train Epoch: 031 Batch: 00023/00094 | Loss: 0.0605 | EWC Loss: 0.0232 | CE Loss: 0.0373\n",
      "Train Epoch: 031 Batch: 00024/00094 | Loss: 0.0575 | EWC Loss: 0.0232 | CE Loss: 0.0343\n",
      "Train Epoch: 031 Batch: 00025/00094 | Loss: 0.0918 | EWC Loss: 0.0232 | CE Loss: 0.0686\n",
      "Train Epoch: 031 Batch: 00026/00094 | Loss: 0.0423 | EWC Loss: 0.0232 | CE Loss: 0.0191\n",
      "Train Epoch: 031 Batch: 00027/00094 | Loss: 0.0889 | EWC Loss: 0.0232 | CE Loss: 0.0657\n",
      "Train Epoch: 031 Batch: 00028/00094 | Loss: 0.0379 | EWC Loss: 0.0232 | CE Loss: 0.0147\n",
      "Train Epoch: 031 Batch: 00029/00094 | Loss: 0.0810 | EWC Loss: 0.0232 | CE Loss: 0.0578\n",
      "Train Epoch: 031 Batch: 00030/00094 | Loss: 0.0679 | EWC Loss: 0.0232 | CE Loss: 0.0446\n",
      "Train Epoch: 031 Batch: 00031/00094 | Loss: 0.1490 | EWC Loss: 0.0232 | CE Loss: 0.1258\n",
      "Train Epoch: 031 Batch: 00032/00094 | Loss: 0.1398 | EWC Loss: 0.0232 | CE Loss: 0.1166\n",
      "Train Epoch: 031 Batch: 00033/00094 | Loss: 0.0711 | EWC Loss: 0.0232 | CE Loss: 0.0479\n",
      "Train Epoch: 031 Batch: 00034/00094 | Loss: 0.1084 | EWC Loss: 0.0232 | CE Loss: 0.0852\n",
      "Train Epoch: 031 Batch: 00035/00094 | Loss: 0.2043 | EWC Loss: 0.0232 | CE Loss: 0.1811\n",
      "Train Epoch: 031 Batch: 00036/00094 | Loss: 0.2140 | EWC Loss: 0.0232 | CE Loss: 0.1908\n",
      "Train Epoch: 031 Batch: 00037/00094 | Loss: 0.0675 | EWC Loss: 0.0232 | CE Loss: 0.0443\n",
      "Train Epoch: 031 Batch: 00038/00094 | Loss: 0.0521 | EWC Loss: 0.0232 | CE Loss: 0.0289\n",
      "Train Epoch: 031 Batch: 00039/00094 | Loss: 0.1125 | EWC Loss: 0.0232 | CE Loss: 0.0893\n",
      "Train Epoch: 031 Batch: 00040/00094 | Loss: 0.0731 | EWC Loss: 0.0232 | CE Loss: 0.0499\n",
      "Train Epoch: 031 Batch: 00041/00094 | Loss: 0.1324 | EWC Loss: 0.0232 | CE Loss: 0.1092\n",
      "Train Epoch: 031 Batch: 00042/00094 | Loss: 0.0753 | EWC Loss: 0.0232 | CE Loss: 0.0521\n",
      "Train Epoch: 031 Batch: 00043/00094 | Loss: 0.1001 | EWC Loss: 0.0232 | CE Loss: 0.0769\n",
      "Train Epoch: 031 Batch: 00044/00094 | Loss: 0.0662 | EWC Loss: 0.0232 | CE Loss: 0.0430\n",
      "Train Epoch: 031 Batch: 00045/00094 | Loss: 0.0634 | EWC Loss: 0.0232 | CE Loss: 0.0402\n",
      "Train Epoch: 031 Batch: 00046/00094 | Loss: 0.0496 | EWC Loss: 0.0232 | CE Loss: 0.0264\n",
      "Train Epoch: 031 Batch: 00047/00094 | Loss: 0.0722 | EWC Loss: 0.0232 | CE Loss: 0.0490\n",
      "Train Epoch: 031 Batch: 00048/00094 | Loss: 0.0852 | EWC Loss: 0.0232 | CE Loss: 0.0620\n",
      "Train Epoch: 031 Batch: 00049/00094 | Loss: 0.0551 | EWC Loss: 0.0232 | CE Loss: 0.0319\n",
      "Train Epoch: 031 Batch: 00050/00094 | Loss: 0.0418 | EWC Loss: 0.0232 | CE Loss: 0.0186\n",
      "Train Epoch: 031 Batch: 00051/00094 | Loss: 0.1070 | EWC Loss: 0.0232 | CE Loss: 0.0838\n",
      "Train Epoch: 031 Batch: 00052/00094 | Loss: 0.1139 | EWC Loss: 0.0232 | CE Loss: 0.0907\n",
      "Train Epoch: 031 Batch: 00053/00094 | Loss: 0.1015 | EWC Loss: 0.0232 | CE Loss: 0.0783\n",
      "Train Epoch: 031 Batch: 00054/00094 | Loss: 0.0546 | EWC Loss: 0.0232 | CE Loss: 0.0314\n",
      "Train Epoch: 031 Batch: 00055/00094 | Loss: 0.0610 | EWC Loss: 0.0232 | CE Loss: 0.0378\n",
      "Train Epoch: 031 Batch: 00056/00094 | Loss: 0.0721 | EWC Loss: 0.0232 | CE Loss: 0.0489\n",
      "Train Epoch: 031 Batch: 00057/00094 | Loss: 0.1248 | EWC Loss: 0.0232 | CE Loss: 0.1015\n",
      "Train Epoch: 031 Batch: 00058/00094 | Loss: 0.0690 | EWC Loss: 0.0232 | CE Loss: 0.0458\n",
      "Train Epoch: 031 Batch: 00059/00094 | Loss: 0.0890 | EWC Loss: 0.0232 | CE Loss: 0.0658\n",
      "Train Epoch: 031 Batch: 00060/00094 | Loss: 0.1202 | EWC Loss: 0.0232 | CE Loss: 0.0970\n",
      "Train Epoch: 031 Batch: 00061/00094 | Loss: 0.1012 | EWC Loss: 0.0232 | CE Loss: 0.0780\n",
      "Train Epoch: 031 Batch: 00062/00094 | Loss: 0.0963 | EWC Loss: 0.0232 | CE Loss: 0.0731\n",
      "Train Epoch: 031 Batch: 00063/00094 | Loss: 0.0724 | EWC Loss: 0.0232 | CE Loss: 0.0492\n",
      "Train Epoch: 031 Batch: 00064/00094 | Loss: 0.0675 | EWC Loss: 0.0232 | CE Loss: 0.0443\n",
      "Train Epoch: 031 Batch: 00065/00094 | Loss: 0.1482 | EWC Loss: 0.0232 | CE Loss: 0.1250\n",
      "Train Epoch: 031 Batch: 00066/00094 | Loss: 0.0995 | EWC Loss: 0.0232 | CE Loss: 0.0763\n",
      "Train Epoch: 031 Batch: 00067/00094 | Loss: 0.0887 | EWC Loss: 0.0232 | CE Loss: 0.0655\n",
      "Train Epoch: 031 Batch: 00068/00094 | Loss: 0.1027 | EWC Loss: 0.0232 | CE Loss: 0.0795\n",
      "Train Epoch: 031 Batch: 00069/00094 | Loss: 0.0640 | EWC Loss: 0.0232 | CE Loss: 0.0408\n",
      "Train Epoch: 031 Batch: 00070/00094 | Loss: 0.0814 | EWC Loss: 0.0232 | CE Loss: 0.0582\n",
      "Train Epoch: 031 Batch: 00071/00094 | Loss: 0.0615 | EWC Loss: 0.0232 | CE Loss: 0.0383\n",
      "Train Epoch: 031 Batch: 00072/00094 | Loss: 0.1311 | EWC Loss: 0.0232 | CE Loss: 0.1079\n",
      "Train Epoch: 031 Batch: 00073/00094 | Loss: 0.1361 | EWC Loss: 0.0232 | CE Loss: 0.1129\n",
      "Train Epoch: 031 Batch: 00074/00094 | Loss: 0.0851 | EWC Loss: 0.0232 | CE Loss: 0.0619\n",
      "Train Epoch: 031 Batch: 00075/00094 | Loss: 0.0981 | EWC Loss: 0.0232 | CE Loss: 0.0749\n",
      "Train Epoch: 031 Batch: 00076/00094 | Loss: 0.1076 | EWC Loss: 0.0232 | CE Loss: 0.0844\n",
      "Train Epoch: 031 Batch: 00077/00094 | Loss: 0.0935 | EWC Loss: 0.0232 | CE Loss: 0.0703\n",
      "Train Epoch: 031 Batch: 00078/00094 | Loss: 0.1208 | EWC Loss: 0.0232 | CE Loss: 0.0976\n",
      "Train Epoch: 031 Batch: 00079/00094 | Loss: 0.0414 | EWC Loss: 0.0232 | CE Loss: 0.0182\n",
      "Train Epoch: 031 Batch: 00080/00094 | Loss: 0.1142 | EWC Loss: 0.0232 | CE Loss: 0.0910\n",
      "Train Epoch: 031 Batch: 00081/00094 | Loss: 0.1338 | EWC Loss: 0.0232 | CE Loss: 0.1106\n",
      "Train Epoch: 031 Batch: 00082/00094 | Loss: 0.0489 | EWC Loss: 0.0232 | CE Loss: 0.0257\n",
      "Train Epoch: 031 Batch: 00083/00094 | Loss: 0.0704 | EWC Loss: 0.0232 | CE Loss: 0.0471\n",
      "Train Epoch: 031 Batch: 00084/00094 | Loss: 0.1423 | EWC Loss: 0.0232 | CE Loss: 0.1191\n",
      "Train Epoch: 031 Batch: 00085/00094 | Loss: 0.1199 | EWC Loss: 0.0232 | CE Loss: 0.0967\n",
      "Train Epoch: 031 Batch: 00086/00094 | Loss: 0.0591 | EWC Loss: 0.0232 | CE Loss: 0.0359\n",
      "Train Epoch: 031 Batch: 00087/00094 | Loss: 0.0657 | EWC Loss: 0.0232 | CE Loss: 0.0425\n",
      "Train Epoch: 031 Batch: 00088/00094 | Loss: 0.1986 | EWC Loss: 0.0232 | CE Loss: 0.1754\n",
      "Train Epoch: 031 Batch: 00089/00094 | Loss: 0.0729 | EWC Loss: 0.0232 | CE Loss: 0.0497\n",
      "Train Epoch: 031 Batch: 00090/00094 | Loss: 0.1181 | EWC Loss: 0.0232 | CE Loss: 0.0949\n",
      "Train Epoch: 031 Batch: 00091/00094 | Loss: 0.0975 | EWC Loss: 0.0232 | CE Loss: 0.0743\n",
      "Train Epoch: 031 Batch: 00092/00094 | Loss: 0.0883 | EWC Loss: 0.0232 | CE Loss: 0.0651\n",
      "Train Epoch: 031 Batch: 00093/00094 | Loss: 0.0772 | EWC Loss: 0.0232 | CE Loss: 0.0540\n",
      "Train Epoch: 031 Batch: 00094/00094 | Loss: 0.1085 | EWC Loss: 0.0232 | CE Loss: 0.0853\n",
      "Train Epoch: 030 |Acc 97.28333 | Loss: 0.09618 | Task Loss 0.07297 | EWC Loss: 0.02321\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0546 | acc:98.1000\n",
      "[VAL Acc] Target: 98.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7327 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2645 | acc:61.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 61.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5304 | acc:50.3817\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4433 | acc:56.8182\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6644 | acc:77.9113\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 77.91%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5441 | acc:81.2696\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 81.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1560 | acc:56.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.88%\n",
      "[VAL Acc] Avg 66.62%\n",
      "\n",
      "\n",
      "---------- Starting epoch 31 ----------\n",
      "Train Epoch: 032 Batch: 00001/00094 | Loss: 0.0767 | EWC Loss: 0.0232 | CE Loss: 0.0535\n",
      "Train Epoch: 032 Batch: 00002/00094 | Loss: 0.1221 | EWC Loss: 0.0232 | CE Loss: 0.0989\n",
      "Train Epoch: 032 Batch: 00003/00094 | Loss: 0.1718 | EWC Loss: 0.0232 | CE Loss: 0.1485\n",
      "Train Epoch: 032 Batch: 00004/00094 | Loss: 0.1146 | EWC Loss: 0.0232 | CE Loss: 0.0914\n",
      "Train Epoch: 032 Batch: 00005/00094 | Loss: 0.1212 | EWC Loss: 0.0232 | CE Loss: 0.0980\n",
      "Train Epoch: 032 Batch: 00006/00094 | Loss: 0.0491 | EWC Loss: 0.0232 | CE Loss: 0.0259\n",
      "Train Epoch: 032 Batch: 00007/00094 | Loss: 0.1057 | EWC Loss: 0.0232 | CE Loss: 0.0825\n",
      "Train Epoch: 032 Batch: 00008/00094 | Loss: 0.1012 | EWC Loss: 0.0232 | CE Loss: 0.0779\n",
      "Train Epoch: 032 Batch: 00009/00094 | Loss: 0.0503 | EWC Loss: 0.0232 | CE Loss: 0.0271\n",
      "Train Epoch: 032 Batch: 00010/00094 | Loss: 0.0836 | EWC Loss: 0.0232 | CE Loss: 0.0604\n",
      "Train Epoch: 032 Batch: 00011/00094 | Loss: 0.0809 | EWC Loss: 0.0232 | CE Loss: 0.0577\n",
      "Train Epoch: 032 Batch: 00012/00094 | Loss: 0.0745 | EWC Loss: 0.0232 | CE Loss: 0.0512\n",
      "Train Epoch: 032 Batch: 00013/00094 | Loss: 0.0533 | EWC Loss: 0.0232 | CE Loss: 0.0301\n",
      "Train Epoch: 032 Batch: 00014/00094 | Loss: 0.0693 | EWC Loss: 0.0232 | CE Loss: 0.0461\n",
      "Train Epoch: 032 Batch: 00015/00094 | Loss: 0.0712 | EWC Loss: 0.0232 | CE Loss: 0.0480\n",
      "Train Epoch: 032 Batch: 00016/00094 | Loss: 0.1678 | EWC Loss: 0.0232 | CE Loss: 0.1446\n",
      "Train Epoch: 032 Batch: 00017/00094 | Loss: 0.0512 | EWC Loss: 0.0232 | CE Loss: 0.0279\n",
      "Train Epoch: 032 Batch: 00018/00094 | Loss: 0.1186 | EWC Loss: 0.0232 | CE Loss: 0.0954\n",
      "Train Epoch: 032 Batch: 00019/00094 | Loss: 0.1499 | EWC Loss: 0.0232 | CE Loss: 0.1266\n",
      "Train Epoch: 032 Batch: 00020/00094 | Loss: 0.1476 | EWC Loss: 0.0232 | CE Loss: 0.1243\n",
      "Train Epoch: 032 Batch: 00021/00094 | Loss: 0.0828 | EWC Loss: 0.0232 | CE Loss: 0.0596\n",
      "Train Epoch: 032 Batch: 00022/00094 | Loss: 0.0729 | EWC Loss: 0.0232 | CE Loss: 0.0497\n",
      "Train Epoch: 032 Batch: 00023/00094 | Loss: 0.0922 | EWC Loss: 0.0232 | CE Loss: 0.0690\n",
      "Train Epoch: 032 Batch: 00024/00094 | Loss: 0.0700 | EWC Loss: 0.0232 | CE Loss: 0.0468\n",
      "Train Epoch: 032 Batch: 00025/00094 | Loss: 0.0843 | EWC Loss: 0.0232 | CE Loss: 0.0611\n",
      "Train Epoch: 032 Batch: 00026/00094 | Loss: 0.1728 | EWC Loss: 0.0232 | CE Loss: 0.1496\n",
      "Train Epoch: 032 Batch: 00027/00094 | Loss: 0.0461 | EWC Loss: 0.0232 | CE Loss: 0.0229\n",
      "Train Epoch: 032 Batch: 00028/00094 | Loss: 0.0519 | EWC Loss: 0.0232 | CE Loss: 0.0287\n",
      "Train Epoch: 032 Batch: 00029/00094 | Loss: 0.0822 | EWC Loss: 0.0232 | CE Loss: 0.0590\n",
      "Train Epoch: 032 Batch: 00030/00094 | Loss: 0.0599 | EWC Loss: 0.0232 | CE Loss: 0.0367\n",
      "Train Epoch: 032 Batch: 00031/00094 | Loss: 0.0598 | EWC Loss: 0.0232 | CE Loss: 0.0366\n",
      "Train Epoch: 032 Batch: 00032/00094 | Loss: 0.0611 | EWC Loss: 0.0232 | CE Loss: 0.0379\n",
      "Train Epoch: 032 Batch: 00033/00094 | Loss: 0.0430 | EWC Loss: 0.0232 | CE Loss: 0.0198\n",
      "Train Epoch: 032 Batch: 00034/00094 | Loss: 0.0562 | EWC Loss: 0.0232 | CE Loss: 0.0329\n",
      "Train Epoch: 032 Batch: 00035/00094 | Loss: 0.1253 | EWC Loss: 0.0232 | CE Loss: 0.1021\n",
      "Train Epoch: 032 Batch: 00036/00094 | Loss: 0.1131 | EWC Loss: 0.0232 | CE Loss: 0.0899\n",
      "Train Epoch: 032 Batch: 00037/00094 | Loss: 0.0531 | EWC Loss: 0.0232 | CE Loss: 0.0298\n",
      "Train Epoch: 032 Batch: 00038/00094 | Loss: 0.0944 | EWC Loss: 0.0232 | CE Loss: 0.0712\n",
      "Train Epoch: 032 Batch: 00039/00094 | Loss: 0.0664 | EWC Loss: 0.0232 | CE Loss: 0.0431\n",
      "Train Epoch: 032 Batch: 00040/00094 | Loss: 0.1255 | EWC Loss: 0.0232 | CE Loss: 0.1022\n",
      "Train Epoch: 032 Batch: 00041/00094 | Loss: 0.1257 | EWC Loss: 0.0232 | CE Loss: 0.1024\n",
      "Train Epoch: 032 Batch: 00042/00094 | Loss: 0.1065 | EWC Loss: 0.0232 | CE Loss: 0.0833\n",
      "Train Epoch: 032 Batch: 00043/00094 | Loss: 0.1130 | EWC Loss: 0.0233 | CE Loss: 0.0897\n",
      "Train Epoch: 032 Batch: 00044/00094 | Loss: 0.0544 | EWC Loss: 0.0233 | CE Loss: 0.0311\n",
      "Train Epoch: 032 Batch: 00045/00094 | Loss: 0.0657 | EWC Loss: 0.0233 | CE Loss: 0.0424\n",
      "Train Epoch: 032 Batch: 00046/00094 | Loss: 0.1030 | EWC Loss: 0.0233 | CE Loss: 0.0797\n",
      "Train Epoch: 032 Batch: 00047/00094 | Loss: 0.0709 | EWC Loss: 0.0233 | CE Loss: 0.0477\n",
      "Train Epoch: 032 Batch: 00048/00094 | Loss: 0.0893 | EWC Loss: 0.0233 | CE Loss: 0.0660\n",
      "Train Epoch: 032 Batch: 00049/00094 | Loss: 0.1970 | EWC Loss: 0.0233 | CE Loss: 0.1737\n",
      "Train Epoch: 032 Batch: 00050/00094 | Loss: 0.0463 | EWC Loss: 0.0233 | CE Loss: 0.0230\n",
      "Train Epoch: 032 Batch: 00051/00094 | Loss: 0.0678 | EWC Loss: 0.0233 | CE Loss: 0.0445\n",
      "Train Epoch: 032 Batch: 00052/00094 | Loss: 0.1088 | EWC Loss: 0.0233 | CE Loss: 0.0855\n",
      "Train Epoch: 032 Batch: 00053/00094 | Loss: 0.0797 | EWC Loss: 0.0233 | CE Loss: 0.0564\n",
      "Train Epoch: 032 Batch: 00054/00094 | Loss: 0.2270 | EWC Loss: 0.0233 | CE Loss: 0.2036\n",
      "Train Epoch: 032 Batch: 00055/00094 | Loss: 0.0834 | EWC Loss: 0.0233 | CE Loss: 0.0600\n",
      "Train Epoch: 032 Batch: 00056/00094 | Loss: 0.0950 | EWC Loss: 0.0233 | CE Loss: 0.0717\n",
      "Train Epoch: 032 Batch: 00057/00094 | Loss: 0.1073 | EWC Loss: 0.0233 | CE Loss: 0.0841\n",
      "Train Epoch: 032 Batch: 00058/00094 | Loss: 0.0703 | EWC Loss: 0.0233 | CE Loss: 0.0470\n",
      "Train Epoch: 032 Batch: 00059/00094 | Loss: 0.0846 | EWC Loss: 0.0233 | CE Loss: 0.0613\n",
      "Train Epoch: 032 Batch: 00060/00094 | Loss: 0.0710 | EWC Loss: 0.0233 | CE Loss: 0.0478\n",
      "Train Epoch: 032 Batch: 00061/00094 | Loss: 0.0656 | EWC Loss: 0.0233 | CE Loss: 0.0423\n",
      "Train Epoch: 032 Batch: 00062/00094 | Loss: 0.0590 | EWC Loss: 0.0233 | CE Loss: 0.0358\n",
      "Train Epoch: 032 Batch: 00063/00094 | Loss: 0.0729 | EWC Loss: 0.0233 | CE Loss: 0.0496\n",
      "Train Epoch: 032 Batch: 00064/00094 | Loss: 0.1847 | EWC Loss: 0.0233 | CE Loss: 0.1614\n",
      "Train Epoch: 032 Batch: 00065/00094 | Loss: 0.0661 | EWC Loss: 0.0233 | CE Loss: 0.0428\n",
      "Train Epoch: 032 Batch: 00066/00094 | Loss: 0.0933 | EWC Loss: 0.0233 | CE Loss: 0.0701\n",
      "Train Epoch: 032 Batch: 00067/00094 | Loss: 0.0750 | EWC Loss: 0.0233 | CE Loss: 0.0517\n",
      "Train Epoch: 032 Batch: 00068/00094 | Loss: 0.0772 | EWC Loss: 0.0233 | CE Loss: 0.0539\n",
      "Train Epoch: 032 Batch: 00069/00094 | Loss: 0.0898 | EWC Loss: 0.0233 | CE Loss: 0.0666\n",
      "Train Epoch: 032 Batch: 00070/00094 | Loss: 0.1713 | EWC Loss: 0.0233 | CE Loss: 0.1480\n",
      "Train Epoch: 032 Batch: 00071/00094 | Loss: 0.0843 | EWC Loss: 0.0233 | CE Loss: 0.0611\n",
      "Train Epoch: 032 Batch: 00072/00094 | Loss: 0.0844 | EWC Loss: 0.0233 | CE Loss: 0.0612\n",
      "Train Epoch: 032 Batch: 00073/00094 | Loss: 0.1029 | EWC Loss: 0.0233 | CE Loss: 0.0796\n",
      "Train Epoch: 032 Batch: 00074/00094 | Loss: 0.0876 | EWC Loss: 0.0233 | CE Loss: 0.0643\n",
      "Train Epoch: 032 Batch: 00075/00094 | Loss: 0.0778 | EWC Loss: 0.0233 | CE Loss: 0.0545\n",
      "Train Epoch: 032 Batch: 00076/00094 | Loss: 0.0680 | EWC Loss: 0.0233 | CE Loss: 0.0447\n",
      "Train Epoch: 032 Batch: 00077/00094 | Loss: 0.0952 | EWC Loss: 0.0233 | CE Loss: 0.0719\n",
      "Train Epoch: 032 Batch: 00078/00094 | Loss: 0.0433 | EWC Loss: 0.0233 | CE Loss: 0.0200\n",
      "Train Epoch: 032 Batch: 00079/00094 | Loss: 0.0751 | EWC Loss: 0.0233 | CE Loss: 0.0519\n",
      "Train Epoch: 032 Batch: 00080/00094 | Loss: 0.0576 | EWC Loss: 0.0233 | CE Loss: 0.0344\n",
      "Train Epoch: 032 Batch: 00081/00094 | Loss: 0.1239 | EWC Loss: 0.0233 | CE Loss: 0.1006\n",
      "Train Epoch: 032 Batch: 00082/00094 | Loss: 0.0598 | EWC Loss: 0.0233 | CE Loss: 0.0365\n",
      "Train Epoch: 032 Batch: 00083/00094 | Loss: 0.0937 | EWC Loss: 0.0233 | CE Loss: 0.0704\n",
      "Train Epoch: 032 Batch: 00084/00094 | Loss: 0.1734 | EWC Loss: 0.0233 | CE Loss: 0.1501\n",
      "Train Epoch: 032 Batch: 00085/00094 | Loss: 0.0399 | EWC Loss: 0.0233 | CE Loss: 0.0167\n",
      "Train Epoch: 032 Batch: 00086/00094 | Loss: 0.1419 | EWC Loss: 0.0233 | CE Loss: 0.1186\n",
      "Train Epoch: 032 Batch: 00087/00094 | Loss: 0.0754 | EWC Loss: 0.0233 | CE Loss: 0.0521\n",
      "Train Epoch: 032 Batch: 00088/00094 | Loss: 0.1035 | EWC Loss: 0.0233 | CE Loss: 0.0802\n",
      "Train Epoch: 032 Batch: 00089/00094 | Loss: 0.0389 | EWC Loss: 0.0233 | CE Loss: 0.0156\n",
      "Train Epoch: 032 Batch: 00090/00094 | Loss: 0.0852 | EWC Loss: 0.0233 | CE Loss: 0.0619\n",
      "Train Epoch: 032 Batch: 00091/00094 | Loss: 0.0548 | EWC Loss: 0.0233 | CE Loss: 0.0315\n",
      "Train Epoch: 032 Batch: 00092/00094 | Loss: 0.0610 | EWC Loss: 0.0233 | CE Loss: 0.0377\n",
      "Train Epoch: 032 Batch: 00093/00094 | Loss: 0.1367 | EWC Loss: 0.0233 | CE Loss: 0.1134\n",
      "Train Epoch: 032 Batch: 00094/00094 | Loss: 0.0528 | EWC Loss: 0.0233 | CE Loss: 0.0295\n",
      "Train Epoch: 031 |Acc 97.70000 | Loss: 0.09077 | Task Loss 0.06751 | EWC Loss: 0.02326\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0595 | acc:97.5000\n",
      "[VAL Acc] Target: 97.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8420 | acc:49.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.3170 | acc:60.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5978 | acc:49.4275\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6061 | acc:55.5251\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.53%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6861 | acc:77.3567\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 77.36%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5809 | acc:79.3103\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 79.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1435 | acc:55.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.69%\n",
      "[VAL Acc] Avg 65.60%\n",
      "\n",
      "\n",
      "---------- Starting epoch 32 ----------\n",
      "Train Epoch: 033 Batch: 00001/00094 | Loss: 0.1033 | EWC Loss: 0.0233 | CE Loss: 0.0800\n",
      "Train Epoch: 033 Batch: 00002/00094 | Loss: 0.0776 | EWC Loss: 0.0233 | CE Loss: 0.0543\n",
      "Train Epoch: 033 Batch: 00003/00094 | Loss: 0.0560 | EWC Loss: 0.0233 | CE Loss: 0.0327\n",
      "Train Epoch: 033 Batch: 00004/00094 | Loss: 0.0643 | EWC Loss: 0.0233 | CE Loss: 0.0410\n",
      "Train Epoch: 033 Batch: 00005/00094 | Loss: 0.0458 | EWC Loss: 0.0233 | CE Loss: 0.0224\n",
      "Train Epoch: 033 Batch: 00006/00094 | Loss: 0.1449 | EWC Loss: 0.0233 | CE Loss: 0.1215\n",
      "Train Epoch: 033 Batch: 00007/00094 | Loss: 0.1170 | EWC Loss: 0.0233 | CE Loss: 0.0936\n",
      "Train Epoch: 033 Batch: 00008/00094 | Loss: 0.0596 | EWC Loss: 0.0234 | CE Loss: 0.0362\n",
      "Train Epoch: 033 Batch: 00009/00094 | Loss: 0.0667 | EWC Loss: 0.0234 | CE Loss: 0.0433\n",
      "Train Epoch: 033 Batch: 00010/00094 | Loss: 0.2967 | EWC Loss: 0.0234 | CE Loss: 0.2733\n",
      "Train Epoch: 033 Batch: 00011/00094 | Loss: 0.0918 | EWC Loss: 0.0234 | CE Loss: 0.0683\n",
      "Train Epoch: 033 Batch: 00012/00094 | Loss: 0.1254 | EWC Loss: 0.0234 | CE Loss: 0.1020\n",
      "Train Epoch: 033 Batch: 00013/00094 | Loss: 0.0705 | EWC Loss: 0.0234 | CE Loss: 0.0472\n",
      "Train Epoch: 033 Batch: 00014/00094 | Loss: 0.0794 | EWC Loss: 0.0234 | CE Loss: 0.0561\n",
      "Train Epoch: 033 Batch: 00015/00094 | Loss: 0.0687 | EWC Loss: 0.0234 | CE Loss: 0.0454\n",
      "Train Epoch: 033 Batch: 00016/00094 | Loss: 0.1113 | EWC Loss: 0.0234 | CE Loss: 0.0879\n",
      "Train Epoch: 033 Batch: 00017/00094 | Loss: 0.1291 | EWC Loss: 0.0234 | CE Loss: 0.1057\n",
      "Train Epoch: 033 Batch: 00018/00094 | Loss: 0.0457 | EWC Loss: 0.0234 | CE Loss: 0.0222\n",
      "Train Epoch: 033 Batch: 00019/00094 | Loss: 0.1112 | EWC Loss: 0.0234 | CE Loss: 0.0877\n",
      "Train Epoch: 033 Batch: 00020/00094 | Loss: 0.0562 | EWC Loss: 0.0235 | CE Loss: 0.0328\n",
      "Train Epoch: 033 Batch: 00021/00094 | Loss: 0.1395 | EWC Loss: 0.0235 | CE Loss: 0.1161\n",
      "Train Epoch: 033 Batch: 00022/00094 | Loss: 0.1674 | EWC Loss: 0.0235 | CE Loss: 0.1440\n",
      "Train Epoch: 033 Batch: 00023/00094 | Loss: 0.0755 | EWC Loss: 0.0235 | CE Loss: 0.0521\n",
      "Train Epoch: 033 Batch: 00024/00094 | Loss: 0.0510 | EWC Loss: 0.0235 | CE Loss: 0.0275\n",
      "Train Epoch: 033 Batch: 00025/00094 | Loss: 0.0632 | EWC Loss: 0.0234 | CE Loss: 0.0397\n",
      "Train Epoch: 033 Batch: 00026/00094 | Loss: 0.0702 | EWC Loss: 0.0234 | CE Loss: 0.0467\n",
      "Train Epoch: 033 Batch: 00027/00094 | Loss: 0.1360 | EWC Loss: 0.0234 | CE Loss: 0.1126\n",
      "Train Epoch: 033 Batch: 00028/00094 | Loss: 0.0836 | EWC Loss: 0.0234 | CE Loss: 0.0601\n",
      "Train Epoch: 033 Batch: 00029/00094 | Loss: 0.1318 | EWC Loss: 0.0234 | CE Loss: 0.1085\n",
      "Train Epoch: 033 Batch: 00030/00094 | Loss: 0.0796 | EWC Loss: 0.0234 | CE Loss: 0.0562\n",
      "Train Epoch: 033 Batch: 00031/00094 | Loss: 0.2446 | EWC Loss: 0.0233 | CE Loss: 0.2213\n",
      "Train Epoch: 033 Batch: 00032/00094 | Loss: 0.0866 | EWC Loss: 0.0234 | CE Loss: 0.0633\n",
      "Train Epoch: 033 Batch: 00033/00094 | Loss: 0.0550 | EWC Loss: 0.0234 | CE Loss: 0.0316\n",
      "Train Epoch: 033 Batch: 00034/00094 | Loss: 0.0556 | EWC Loss: 0.0234 | CE Loss: 0.0322\n",
      "Train Epoch: 033 Batch: 00035/00094 | Loss: 0.0634 | EWC Loss: 0.0234 | CE Loss: 0.0400\n",
      "Train Epoch: 033 Batch: 00036/00094 | Loss: 0.0519 | EWC Loss: 0.0234 | CE Loss: 0.0285\n",
      "Train Epoch: 033 Batch: 00037/00094 | Loss: 0.0611 | EWC Loss: 0.0234 | CE Loss: 0.0377\n",
      "Train Epoch: 033 Batch: 00038/00094 | Loss: 0.1101 | EWC Loss: 0.0234 | CE Loss: 0.0867\n",
      "Train Epoch: 033 Batch: 00039/00094 | Loss: 0.0681 | EWC Loss: 0.0234 | CE Loss: 0.0447\n",
      "Train Epoch: 033 Batch: 00040/00094 | Loss: 0.0646 | EWC Loss: 0.0234 | CE Loss: 0.0413\n",
      "Train Epoch: 033 Batch: 00041/00094 | Loss: 0.1206 | EWC Loss: 0.0234 | CE Loss: 0.0972\n",
      "Train Epoch: 033 Batch: 00042/00094 | Loss: 0.0953 | EWC Loss: 0.0234 | CE Loss: 0.0719\n",
      "Train Epoch: 033 Batch: 00043/00094 | Loss: 0.0764 | EWC Loss: 0.0233 | CE Loss: 0.0530\n",
      "Train Epoch: 033 Batch: 00044/00094 | Loss: 0.0600 | EWC Loss: 0.0233 | CE Loss: 0.0366\n",
      "Train Epoch: 033 Batch: 00045/00094 | Loss: 0.0372 | EWC Loss: 0.0233 | CE Loss: 0.0139\n",
      "Train Epoch: 033 Batch: 00046/00094 | Loss: 0.1061 | EWC Loss: 0.0233 | CE Loss: 0.0827\n",
      "Train Epoch: 033 Batch: 00047/00094 | Loss: 0.0731 | EWC Loss: 0.0234 | CE Loss: 0.0497\n",
      "Train Epoch: 033 Batch: 00048/00094 | Loss: 0.1224 | EWC Loss: 0.0233 | CE Loss: 0.0991\n",
      "Train Epoch: 033 Batch: 00049/00094 | Loss: 0.0575 | EWC Loss: 0.0234 | CE Loss: 0.0342\n",
      "Train Epoch: 033 Batch: 00050/00094 | Loss: 0.0806 | EWC Loss: 0.0234 | CE Loss: 0.0572\n",
      "Train Epoch: 033 Batch: 00051/00094 | Loss: 0.0833 | EWC Loss: 0.0234 | CE Loss: 0.0598\n",
      "Train Epoch: 033 Batch: 00052/00094 | Loss: 0.0629 | EWC Loss: 0.0235 | CE Loss: 0.0394\n",
      "Train Epoch: 033 Batch: 00053/00094 | Loss: 0.0762 | EWC Loss: 0.0235 | CE Loss: 0.0527\n",
      "Train Epoch: 033 Batch: 00054/00094 | Loss: 0.0613 | EWC Loss: 0.0235 | CE Loss: 0.0378\n",
      "Train Epoch: 033 Batch: 00055/00094 | Loss: 0.0584 | EWC Loss: 0.0235 | CE Loss: 0.0349\n",
      "Train Epoch: 033 Batch: 00056/00094 | Loss: 0.0617 | EWC Loss: 0.0235 | CE Loss: 0.0382\n",
      "Train Epoch: 033 Batch: 00057/00094 | Loss: 0.0698 | EWC Loss: 0.0235 | CE Loss: 0.0463\n",
      "Train Epoch: 033 Batch: 00058/00094 | Loss: 0.1501 | EWC Loss: 0.0235 | CE Loss: 0.1265\n",
      "Train Epoch: 033 Batch: 00059/00094 | Loss: 0.0965 | EWC Loss: 0.0235 | CE Loss: 0.0730\n",
      "Train Epoch: 033 Batch: 00060/00094 | Loss: 0.0924 | EWC Loss: 0.0235 | CE Loss: 0.0689\n",
      "Train Epoch: 033 Batch: 00061/00094 | Loss: 0.0746 | EWC Loss: 0.0235 | CE Loss: 0.0511\n",
      "Train Epoch: 033 Batch: 00062/00094 | Loss: 0.1230 | EWC Loss: 0.0235 | CE Loss: 0.0995\n",
      "Train Epoch: 033 Batch: 00063/00094 | Loss: 0.0746 | EWC Loss: 0.0235 | CE Loss: 0.0511\n",
      "Train Epoch: 033 Batch: 00064/00094 | Loss: 0.1452 | EWC Loss: 0.0235 | CE Loss: 0.1216\n",
      "Train Epoch: 033 Batch: 00065/00094 | Loss: 0.0995 | EWC Loss: 0.0235 | CE Loss: 0.0760\n",
      "Train Epoch: 033 Batch: 00066/00094 | Loss: 0.1833 | EWC Loss: 0.0235 | CE Loss: 0.1598\n",
      "Train Epoch: 033 Batch: 00067/00094 | Loss: 0.0549 | EWC Loss: 0.0235 | CE Loss: 0.0313\n",
      "Train Epoch: 033 Batch: 00068/00094 | Loss: 0.1064 | EWC Loss: 0.0235 | CE Loss: 0.0828\n",
      "Train Epoch: 033 Batch: 00069/00094 | Loss: 0.1154 | EWC Loss: 0.0235 | CE Loss: 0.0919\n",
      "Train Epoch: 033 Batch: 00070/00094 | Loss: 0.1217 | EWC Loss: 0.0235 | CE Loss: 0.0982\n",
      "Train Epoch: 033 Batch: 00071/00094 | Loss: 0.0820 | EWC Loss: 0.0235 | CE Loss: 0.0585\n",
      "Train Epoch: 033 Batch: 00072/00094 | Loss: 0.0557 | EWC Loss: 0.0235 | CE Loss: 0.0322\n",
      "Train Epoch: 033 Batch: 00073/00094 | Loss: 0.0731 | EWC Loss: 0.0235 | CE Loss: 0.0496\n",
      "Train Epoch: 033 Batch: 00074/00094 | Loss: 0.0914 | EWC Loss: 0.0235 | CE Loss: 0.0680\n",
      "Train Epoch: 033 Batch: 00075/00094 | Loss: 0.0713 | EWC Loss: 0.0235 | CE Loss: 0.0479\n",
      "Train Epoch: 033 Batch: 00076/00094 | Loss: 0.0718 | EWC Loss: 0.0235 | CE Loss: 0.0484\n",
      "Train Epoch: 033 Batch: 00077/00094 | Loss: 0.0777 | EWC Loss: 0.0234 | CE Loss: 0.0543\n",
      "Train Epoch: 033 Batch: 00078/00094 | Loss: 0.0575 | EWC Loss: 0.0234 | CE Loss: 0.0341\n",
      "Train Epoch: 033 Batch: 00079/00094 | Loss: 0.0739 | EWC Loss: 0.0234 | CE Loss: 0.0505\n",
      "Train Epoch: 033 Batch: 00080/00094 | Loss: 0.0827 | EWC Loss: 0.0233 | CE Loss: 0.0594\n",
      "Train Epoch: 033 Batch: 00081/00094 | Loss: 0.0551 | EWC Loss: 0.0233 | CE Loss: 0.0318\n",
      "Train Epoch: 033 Batch: 00082/00094 | Loss: 0.0743 | EWC Loss: 0.0233 | CE Loss: 0.0510\n",
      "Train Epoch: 033 Batch: 00083/00094 | Loss: 0.1736 | EWC Loss: 0.0233 | CE Loss: 0.1503\n",
      "Train Epoch: 033 Batch: 00084/00094 | Loss: 0.1000 | EWC Loss: 0.0233 | CE Loss: 0.0767\n",
      "Train Epoch: 033 Batch: 00085/00094 | Loss: 0.0688 | EWC Loss: 0.0233 | CE Loss: 0.0455\n",
      "Train Epoch: 033 Batch: 00086/00094 | Loss: 0.1007 | EWC Loss: 0.0233 | CE Loss: 0.0774\n",
      "Train Epoch: 033 Batch: 00087/00094 | Loss: 0.0708 | EWC Loss: 0.0233 | CE Loss: 0.0475\n",
      "Train Epoch: 033 Batch: 00088/00094 | Loss: 0.1179 | EWC Loss: 0.0233 | CE Loss: 0.0946\n",
      "Train Epoch: 033 Batch: 00089/00094 | Loss: 0.0665 | EWC Loss: 0.0233 | CE Loss: 0.0432\n",
      "Train Epoch: 033 Batch: 00090/00094 | Loss: 0.0684 | EWC Loss: 0.0233 | CE Loss: 0.0451\n",
      "Train Epoch: 033 Batch: 00091/00094 | Loss: 0.1423 | EWC Loss: 0.0233 | CE Loss: 0.1190\n",
      "Train Epoch: 033 Batch: 00092/00094 | Loss: 0.0509 | EWC Loss: 0.0232 | CE Loss: 0.0277\n",
      "Train Epoch: 033 Batch: 00093/00094 | Loss: 0.1260 | EWC Loss: 0.0232 | CE Loss: 0.1028\n",
      "Train Epoch: 033 Batch: 00094/00094 | Loss: 0.1389 | EWC Loss: 0.0233 | CE Loss: 0.1157\n",
      "Train Epoch: 032 |Acc 97.48333 | Loss: 0.09161 | Task Loss 0.06821 | EWC Loss: 0.02340\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0510 | acc:98.6000\n",
      "[VAL Acc] Target: 98.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7273 | acc:49.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2309 | acc:62.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 62.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5300 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5419 | acc:56.5831\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.58%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7163 | acc:76.6174\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5628 | acc:79.5455\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 79.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1742 | acc:54.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 54.37%\n",
      "[VAL Acc] Avg 65.69%\n",
      "\n",
      "\n",
      "---------- Starting epoch 33 ----------\n",
      "Train Epoch: 034 Batch: 00001/00094 | Loss: 0.0697 | EWC Loss: 0.0233 | CE Loss: 0.0465\n",
      "Train Epoch: 034 Batch: 00002/00094 | Loss: 0.0662 | EWC Loss: 0.0233 | CE Loss: 0.0429\n",
      "Train Epoch: 034 Batch: 00003/00094 | Loss: 0.1158 | EWC Loss: 0.0233 | CE Loss: 0.0925\n",
      "Train Epoch: 034 Batch: 00004/00094 | Loss: 0.1721 | EWC Loss: 0.0235 | CE Loss: 0.1486\n",
      "Train Epoch: 034 Batch: 00005/00094 | Loss: 0.0658 | EWC Loss: 0.0234 | CE Loss: 0.0424\n",
      "Train Epoch: 034 Batch: 00006/00094 | Loss: 0.0633 | EWC Loss: 0.0234 | CE Loss: 0.0399\n",
      "Train Epoch: 034 Batch: 00007/00094 | Loss: 0.0560 | EWC Loss: 0.0234 | CE Loss: 0.0326\n",
      "Train Epoch: 034 Batch: 00008/00094 | Loss: 0.0847 | EWC Loss: 0.0233 | CE Loss: 0.0614\n",
      "Train Epoch: 034 Batch: 00009/00094 | Loss: 0.1111 | EWC Loss: 0.0233 | CE Loss: 0.0878\n",
      "Train Epoch: 034 Batch: 00010/00094 | Loss: 0.0571 | EWC Loss: 0.0233 | CE Loss: 0.0338\n",
      "Train Epoch: 034 Batch: 00011/00094 | Loss: 0.0952 | EWC Loss: 0.0233 | CE Loss: 0.0719\n",
      "Train Epoch: 034 Batch: 00012/00094 | Loss: 0.0580 | EWC Loss: 0.0234 | CE Loss: 0.0346\n",
      "Train Epoch: 034 Batch: 00013/00094 | Loss: 0.0874 | EWC Loss: 0.0234 | CE Loss: 0.0640\n",
      "Train Epoch: 034 Batch: 00014/00094 | Loss: 0.0537 | EWC Loss: 0.0233 | CE Loss: 0.0304\n",
      "Train Epoch: 034 Batch: 00015/00094 | Loss: 0.0826 | EWC Loss: 0.0233 | CE Loss: 0.0593\n",
      "Train Epoch: 034 Batch: 00016/00094 | Loss: 0.0782 | EWC Loss: 0.0233 | CE Loss: 0.0549\n",
      "Train Epoch: 034 Batch: 00017/00094 | Loss: 0.0755 | EWC Loss: 0.0233 | CE Loss: 0.0522\n",
      "Train Epoch: 034 Batch: 00018/00094 | Loss: 0.0385 | EWC Loss: 0.0233 | CE Loss: 0.0152\n",
      "Train Epoch: 034 Batch: 00019/00094 | Loss: 0.0843 | EWC Loss: 0.0233 | CE Loss: 0.0610\n",
      "Train Epoch: 034 Batch: 00020/00094 | Loss: 0.1373 | EWC Loss: 0.0233 | CE Loss: 0.1140\n",
      "Train Epoch: 034 Batch: 00021/00094 | Loss: 0.0721 | EWC Loss: 0.0232 | CE Loss: 0.0489\n",
      "Train Epoch: 034 Batch: 00022/00094 | Loss: 0.0420 | EWC Loss: 0.0233 | CE Loss: 0.0187\n",
      "Train Epoch: 034 Batch: 00023/00094 | Loss: 0.1213 | EWC Loss: 0.0233 | CE Loss: 0.0981\n",
      "Train Epoch: 034 Batch: 00024/00094 | Loss: 0.0671 | EWC Loss: 0.0232 | CE Loss: 0.0439\n",
      "Train Epoch: 034 Batch: 00025/00094 | Loss: 0.1571 | EWC Loss: 0.0232 | CE Loss: 0.1339\n",
      "Train Epoch: 034 Batch: 00026/00094 | Loss: 0.0676 | EWC Loss: 0.0231 | CE Loss: 0.0445\n",
      "Train Epoch: 034 Batch: 00027/00094 | Loss: 0.0354 | EWC Loss: 0.0230 | CE Loss: 0.0124\n",
      "Train Epoch: 034 Batch: 00028/00094 | Loss: 0.0792 | EWC Loss: 0.0230 | CE Loss: 0.0562\n",
      "Train Epoch: 034 Batch: 00029/00094 | Loss: 0.0520 | EWC Loss: 0.0230 | CE Loss: 0.0290\n",
      "Train Epoch: 034 Batch: 00030/00094 | Loss: 0.0811 | EWC Loss: 0.0230 | CE Loss: 0.0580\n",
      "Train Epoch: 034 Batch: 00031/00094 | Loss: 0.1175 | EWC Loss: 0.0231 | CE Loss: 0.0945\n",
      "Train Epoch: 034 Batch: 00032/00094 | Loss: 0.1232 | EWC Loss: 0.0231 | CE Loss: 0.1001\n",
      "Train Epoch: 034 Batch: 00033/00094 | Loss: 0.0817 | EWC Loss: 0.0231 | CE Loss: 0.0586\n",
      "Train Epoch: 034 Batch: 00034/00094 | Loss: 0.1441 | EWC Loss: 0.0231 | CE Loss: 0.1210\n",
      "Train Epoch: 034 Batch: 00035/00094 | Loss: 0.1267 | EWC Loss: 0.0232 | CE Loss: 0.1035\n",
      "Train Epoch: 034 Batch: 00036/00094 | Loss: 0.0612 | EWC Loss: 0.0232 | CE Loss: 0.0380\n",
      "Train Epoch: 034 Batch: 00037/00094 | Loss: 0.1984 | EWC Loss: 0.0231 | CE Loss: 0.1753\n",
      "Train Epoch: 034 Batch: 00038/00094 | Loss: 0.0440 | EWC Loss: 0.0231 | CE Loss: 0.0209\n",
      "Train Epoch: 034 Batch: 00039/00094 | Loss: 0.0405 | EWC Loss: 0.0231 | CE Loss: 0.0173\n",
      "Train Epoch: 034 Batch: 00040/00094 | Loss: 0.1113 | EWC Loss: 0.0231 | CE Loss: 0.0882\n",
      "Train Epoch: 034 Batch: 00041/00094 | Loss: 0.1272 | EWC Loss: 0.0231 | CE Loss: 0.1041\n",
      "Train Epoch: 034 Batch: 00042/00094 | Loss: 0.0409 | EWC Loss: 0.0231 | CE Loss: 0.0179\n",
      "Train Epoch: 034 Batch: 00043/00094 | Loss: 0.0516 | EWC Loss: 0.0231 | CE Loss: 0.0285\n",
      "Train Epoch: 034 Batch: 00044/00094 | Loss: 0.1150 | EWC Loss: 0.0230 | CE Loss: 0.0919\n",
      "Train Epoch: 034 Batch: 00045/00094 | Loss: 0.0578 | EWC Loss: 0.0231 | CE Loss: 0.0347\n",
      "Train Epoch: 034 Batch: 00046/00094 | Loss: 0.0467 | EWC Loss: 0.0230 | CE Loss: 0.0237\n",
      "Train Epoch: 034 Batch: 00047/00094 | Loss: 0.0799 | EWC Loss: 0.0230 | CE Loss: 0.0569\n",
      "Train Epoch: 034 Batch: 00048/00094 | Loss: 0.0551 | EWC Loss: 0.0229 | CE Loss: 0.0322\n",
      "Train Epoch: 034 Batch: 00049/00094 | Loss: 0.0466 | EWC Loss: 0.0229 | CE Loss: 0.0236\n",
      "Train Epoch: 034 Batch: 00050/00094 | Loss: 0.0452 | EWC Loss: 0.0229 | CE Loss: 0.0223\n",
      "Train Epoch: 034 Batch: 00051/00094 | Loss: 0.1078 | EWC Loss: 0.0229 | CE Loss: 0.0849\n",
      "Train Epoch: 034 Batch: 00052/00094 | Loss: 0.0534 | EWC Loss: 0.0229 | CE Loss: 0.0305\n",
      "Train Epoch: 034 Batch: 00053/00094 | Loss: 0.1667 | EWC Loss: 0.0229 | CE Loss: 0.1437\n",
      "Train Epoch: 034 Batch: 00054/00094 | Loss: 0.0786 | EWC Loss: 0.0229 | CE Loss: 0.0557\n",
      "Train Epoch: 034 Batch: 00055/00094 | Loss: 0.0891 | EWC Loss: 0.0229 | CE Loss: 0.0661\n",
      "Train Epoch: 034 Batch: 00056/00094 | Loss: 0.0391 | EWC Loss: 0.0229 | CE Loss: 0.0161\n",
      "Train Epoch: 034 Batch: 00057/00094 | Loss: 0.1586 | EWC Loss: 0.0229 | CE Loss: 0.1357\n",
      "Train Epoch: 034 Batch: 00058/00094 | Loss: 0.0569 | EWC Loss: 0.0229 | CE Loss: 0.0340\n",
      "Train Epoch: 034 Batch: 00059/00094 | Loss: 0.1037 | EWC Loss: 0.0229 | CE Loss: 0.0807\n",
      "Train Epoch: 034 Batch: 00060/00094 | Loss: 0.0743 | EWC Loss: 0.0229 | CE Loss: 0.0514\n",
      "Train Epoch: 034 Batch: 00061/00094 | Loss: 0.0789 | EWC Loss: 0.0229 | CE Loss: 0.0560\n",
      "Train Epoch: 034 Batch: 00062/00094 | Loss: 0.0474 | EWC Loss: 0.0229 | CE Loss: 0.0245\n",
      "Train Epoch: 034 Batch: 00063/00094 | Loss: 0.1187 | EWC Loss: 0.0229 | CE Loss: 0.0958\n",
      "Train Epoch: 034 Batch: 00064/00094 | Loss: 0.0516 | EWC Loss: 0.0229 | CE Loss: 0.0287\n",
      "Train Epoch: 034 Batch: 00065/00094 | Loss: 0.0508 | EWC Loss: 0.0229 | CE Loss: 0.0279\n",
      "Train Epoch: 034 Batch: 00066/00094 | Loss: 0.0783 | EWC Loss: 0.0228 | CE Loss: 0.0555\n",
      "Train Epoch: 034 Batch: 00067/00094 | Loss: 0.0368 | EWC Loss: 0.0228 | CE Loss: 0.0139\n",
      "Train Epoch: 034 Batch: 00068/00094 | Loss: 0.2244 | EWC Loss: 0.0228 | CE Loss: 0.2015\n",
      "Train Epoch: 034 Batch: 00069/00094 | Loss: 0.0435 | EWC Loss: 0.0229 | CE Loss: 0.0206\n",
      "Train Epoch: 034 Batch: 00070/00094 | Loss: 0.0744 | EWC Loss: 0.0229 | CE Loss: 0.0515\n",
      "Train Epoch: 034 Batch: 00071/00094 | Loss: 0.1989 | EWC Loss: 0.0229 | CE Loss: 0.1760\n",
      "Train Epoch: 034 Batch: 00072/00094 | Loss: 0.1073 | EWC Loss: 0.0230 | CE Loss: 0.0843\n",
      "Train Epoch: 034 Batch: 00073/00094 | Loss: 0.1449 | EWC Loss: 0.0230 | CE Loss: 0.1220\n",
      "Train Epoch: 034 Batch: 00074/00094 | Loss: 0.0816 | EWC Loss: 0.0230 | CE Loss: 0.0585\n",
      "Train Epoch: 034 Batch: 00075/00094 | Loss: 0.0656 | EWC Loss: 0.0230 | CE Loss: 0.0426\n",
      "Train Epoch: 034 Batch: 00076/00094 | Loss: 0.0949 | EWC Loss: 0.0230 | CE Loss: 0.0719\n",
      "Train Epoch: 034 Batch: 00077/00094 | Loss: 0.0858 | EWC Loss: 0.0230 | CE Loss: 0.0628\n",
      "Train Epoch: 034 Batch: 00078/00094 | Loss: 0.1267 | EWC Loss: 0.0231 | CE Loss: 0.1037\n",
      "Train Epoch: 034 Batch: 00079/00094 | Loss: 0.0419 | EWC Loss: 0.0233 | CE Loss: 0.0187\n",
      "Train Epoch: 034 Batch: 00080/00094 | Loss: 0.0531 | EWC Loss: 0.0232 | CE Loss: 0.0298\n",
      "Train Epoch: 034 Batch: 00081/00094 | Loss: 0.0965 | EWC Loss: 0.0232 | CE Loss: 0.0733\n",
      "Train Epoch: 034 Batch: 00082/00094 | Loss: 0.0862 | EWC Loss: 0.0232 | CE Loss: 0.0631\n",
      "Train Epoch: 034 Batch: 00083/00094 | Loss: 0.0753 | EWC Loss: 0.0232 | CE Loss: 0.0521\n",
      "Train Epoch: 034 Batch: 00084/00094 | Loss: 0.1105 | EWC Loss: 0.0232 | CE Loss: 0.0873\n",
      "Train Epoch: 034 Batch: 00085/00094 | Loss: 0.0980 | EWC Loss: 0.0232 | CE Loss: 0.0748\n",
      "Train Epoch: 034 Batch: 00086/00094 | Loss: 0.0551 | EWC Loss: 0.0233 | CE Loss: 0.0319\n",
      "Train Epoch: 034 Batch: 00087/00094 | Loss: 0.1147 | EWC Loss: 0.0233 | CE Loss: 0.0914\n",
      "Train Epoch: 034 Batch: 00088/00094 | Loss: 0.0371 | EWC Loss: 0.0233 | CE Loss: 0.0138\n",
      "Train Epoch: 034 Batch: 00089/00094 | Loss: 0.0734 | EWC Loss: 0.0233 | CE Loss: 0.0501\n",
      "Train Epoch: 034 Batch: 00090/00094 | Loss: 0.1094 | EWC Loss: 0.0233 | CE Loss: 0.0861\n",
      "Train Epoch: 034 Batch: 00091/00094 | Loss: 0.0840 | EWC Loss: 0.0233 | CE Loss: 0.0607\n",
      "Train Epoch: 034 Batch: 00092/00094 | Loss: 0.1395 | EWC Loss: 0.0232 | CE Loss: 0.1162\n",
      "Train Epoch: 034 Batch: 00093/00094 | Loss: 0.1255 | EWC Loss: 0.0232 | CE Loss: 0.1023\n",
      "Train Epoch: 034 Batch: 00094/00094 | Loss: 0.0765 | EWC Loss: 0.0232 | CE Loss: 0.0533\n",
      "Train Epoch: 033 |Acc 97.88333 | Loss: 0.08678 | Task Loss 0.06366 | EWC Loss: 0.02312\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0679 | acc:97.5500\n",
      "[VAL Acc] Target: 97.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7152 | acc:49.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2831 | acc:60.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 60.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5995 | acc:46.7557\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6288 | acc:54.8197\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6163 | acc:77.7264\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 77.73%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.5830 | acc:79.7806\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 79.78%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.2117 | acc:55.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.94%\n",
      "[VAL Acc] Avg 65.29%\n",
      "\n",
      "\n",
      "---------- Starting epoch 34 ----------\n",
      "Train Epoch: 035 Batch: 00001/00094 | Loss: 0.0728 | EWC Loss: 0.0232 | CE Loss: 0.0496\n",
      "Train Epoch: 035 Batch: 00002/00094 | Loss: 0.0466 | EWC Loss: 0.0233 | CE Loss: 0.0233\n",
      "Train Epoch: 035 Batch: 00003/00094 | Loss: 0.0562 | EWC Loss: 0.0233 | CE Loss: 0.0329\n",
      "Train Epoch: 035 Batch: 00004/00094 | Loss: 0.0520 | EWC Loss: 0.0233 | CE Loss: 0.0287\n",
      "Train Epoch: 035 Batch: 00005/00094 | Loss: 0.0533 | EWC Loss: 0.0234 | CE Loss: 0.0300\n",
      "Train Epoch: 035 Batch: 00006/00094 | Loss: 0.0731 | EWC Loss: 0.0234 | CE Loss: 0.0497\n",
      "Train Epoch: 035 Batch: 00007/00094 | Loss: 0.0796 | EWC Loss: 0.0234 | CE Loss: 0.0562\n",
      "Train Epoch: 035 Batch: 00008/00094 | Loss: 0.1019 | EWC Loss: 0.0235 | CE Loss: 0.0784\n",
      "Train Epoch: 035 Batch: 00009/00094 | Loss: 0.0781 | EWC Loss: 0.0236 | CE Loss: 0.0545\n",
      "Train Epoch: 035 Batch: 00010/00094 | Loss: 0.0565 | EWC Loss: 0.0238 | CE Loss: 0.0328\n",
      "Train Epoch: 035 Batch: 00011/00094 | Loss: 0.1131 | EWC Loss: 0.0237 | CE Loss: 0.0893\n",
      "Train Epoch: 035 Batch: 00012/00094 | Loss: 0.0521 | EWC Loss: 0.0236 | CE Loss: 0.0285\n",
      "Train Epoch: 035 Batch: 00013/00094 | Loss: 0.1030 | EWC Loss: 0.0235 | CE Loss: 0.0794\n",
      "Train Epoch: 035 Batch: 00014/00094 | Loss: 0.2064 | EWC Loss: 0.0237 | CE Loss: 0.1828\n",
      "Train Epoch: 035 Batch: 00015/00094 | Loss: 0.0414 | EWC Loss: 0.0242 | CE Loss: 0.0172\n",
      "Train Epoch: 035 Batch: 00016/00094 | Loss: 0.0501 | EWC Loss: 0.0243 | CE Loss: 0.0259\n",
      "Train Epoch: 035 Batch: 00017/00094 | Loss: 0.1023 | EWC Loss: 0.0242 | CE Loss: 0.0781\n",
      "Train Epoch: 035 Batch: 00018/00094 | Loss: 0.1154 | EWC Loss: 0.0242 | CE Loss: 0.0912\n",
      "Train Epoch: 035 Batch: 00019/00094 | Loss: 0.0546 | EWC Loss: 0.0242 | CE Loss: 0.0304\n",
      "Train Epoch: 035 Batch: 00020/00094 | Loss: 0.0790 | EWC Loss: 0.0241 | CE Loss: 0.0549\n",
      "Train Epoch: 035 Batch: 00021/00094 | Loss: 0.1017 | EWC Loss: 0.0242 | CE Loss: 0.0776\n",
      "Train Epoch: 035 Batch: 00022/00094 | Loss: 0.1163 | EWC Loss: 0.0242 | CE Loss: 0.0922\n",
      "Train Epoch: 035 Batch: 00023/00094 | Loss: 0.0715 | EWC Loss: 0.0241 | CE Loss: 0.0474\n",
      "Train Epoch: 035 Batch: 00024/00094 | Loss: 0.0550 | EWC Loss: 0.0242 | CE Loss: 0.0308\n",
      "Train Epoch: 035 Batch: 00025/00094 | Loss: 0.1002 | EWC Loss: 0.0241 | CE Loss: 0.0761\n",
      "Train Epoch: 035 Batch: 00026/00094 | Loss: 0.0939 | EWC Loss: 0.0241 | CE Loss: 0.0697\n",
      "Train Epoch: 035 Batch: 00027/00094 | Loss: 0.1340 | EWC Loss: 0.0244 | CE Loss: 0.1097\n",
      "Train Epoch: 035 Batch: 00028/00094 | Loss: 0.1345 | EWC Loss: 0.0242 | CE Loss: 0.1103\n",
      "Train Epoch: 035 Batch: 00029/00094 | Loss: 0.0902 | EWC Loss: 0.0243 | CE Loss: 0.0659\n",
      "Train Epoch: 035 Batch: 00030/00094 | Loss: 0.0875 | EWC Loss: 0.0242 | CE Loss: 0.0633\n",
      "Train Epoch: 035 Batch: 00031/00094 | Loss: 0.1382 | EWC Loss: 0.0241 | CE Loss: 0.1141\n",
      "Train Epoch: 035 Batch: 00032/00094 | Loss: 0.0661 | EWC Loss: 0.0241 | CE Loss: 0.0420\n",
      "Train Epoch: 035 Batch: 00033/00094 | Loss: 0.0990 | EWC Loss: 0.0241 | CE Loss: 0.0749\n",
      "Train Epoch: 035 Batch: 00034/00094 | Loss: 0.0551 | EWC Loss: 0.0241 | CE Loss: 0.0310\n",
      "Train Epoch: 035 Batch: 00035/00094 | Loss: 0.0756 | EWC Loss: 0.0241 | CE Loss: 0.0515\n",
      "Train Epoch: 035 Batch: 00036/00094 | Loss: 0.0672 | EWC Loss: 0.0240 | CE Loss: 0.0432\n",
      "Train Epoch: 035 Batch: 00037/00094 | Loss: 0.0806 | EWC Loss: 0.0239 | CE Loss: 0.0566\n",
      "Train Epoch: 035 Batch: 00038/00094 | Loss: 0.1354 | EWC Loss: 0.0241 | CE Loss: 0.1113\n",
      "Train Epoch: 035 Batch: 00039/00094 | Loss: 0.0657 | EWC Loss: 0.0240 | CE Loss: 0.0417\n",
      "Train Epoch: 035 Batch: 00040/00094 | Loss: 0.1022 | EWC Loss: 0.0240 | CE Loss: 0.0782\n",
      "Train Epoch: 035 Batch: 00041/00094 | Loss: 0.1391 | EWC Loss: 0.0239 | CE Loss: 0.1151\n",
      "Train Epoch: 035 Batch: 00042/00094 | Loss: 0.0917 | EWC Loss: 0.0239 | CE Loss: 0.0678\n",
      "Train Epoch: 035 Batch: 00043/00094 | Loss: 0.0853 | EWC Loss: 0.0238 | CE Loss: 0.0614\n",
      "Train Epoch: 035 Batch: 00044/00094 | Loss: 0.0525 | EWC Loss: 0.0238 | CE Loss: 0.0287\n",
      "Train Epoch: 035 Batch: 00045/00094 | Loss: 0.0689 | EWC Loss: 0.0237 | CE Loss: 0.0452\n",
      "Train Epoch: 035 Batch: 00046/00094 | Loss: 0.0598 | EWC Loss: 0.0237 | CE Loss: 0.0362\n",
      "Train Epoch: 035 Batch: 00047/00094 | Loss: 0.1193 | EWC Loss: 0.0236 | CE Loss: 0.0957\n",
      "Train Epoch: 035 Batch: 00048/00094 | Loss: 0.0748 | EWC Loss: 0.0237 | CE Loss: 0.0511\n",
      "Train Epoch: 035 Batch: 00049/00094 | Loss: 0.1241 | EWC Loss: 0.0236 | CE Loss: 0.1006\n",
      "Train Epoch: 035 Batch: 00050/00094 | Loss: 0.0889 | EWC Loss: 0.0237 | CE Loss: 0.0653\n",
      "Train Epoch: 035 Batch: 00051/00094 | Loss: 0.0468 | EWC Loss: 0.0237 | CE Loss: 0.0231\n",
      "Train Epoch: 035 Batch: 00052/00094 | Loss: 0.1980 | EWC Loss: 0.0236 | CE Loss: 0.1743\n",
      "Train Epoch: 035 Batch: 00053/00094 | Loss: 0.0507 | EWC Loss: 0.0233 | CE Loss: 0.0274\n",
      "Train Epoch: 035 Batch: 00054/00094 | Loss: 0.0985 | EWC Loss: 0.0232 | CE Loss: 0.0753\n",
      "Train Epoch: 035 Batch: 00055/00094 | Loss: 0.0605 | EWC Loss: 0.0232 | CE Loss: 0.0373\n",
      "Train Epoch: 035 Batch: 00056/00094 | Loss: 0.0830 | EWC Loss: 0.0231 | CE Loss: 0.0599\n",
      "Train Epoch: 035 Batch: 00057/00094 | Loss: 0.0422 | EWC Loss: 0.0231 | CE Loss: 0.0191\n",
      "Train Epoch: 035 Batch: 00058/00094 | Loss: 0.1149 | EWC Loss: 0.0231 | CE Loss: 0.0918\n",
      "Train Epoch: 035 Batch: 00059/00094 | Loss: 0.1168 | EWC Loss: 0.0231 | CE Loss: 0.0937\n",
      "Train Epoch: 035 Batch: 00060/00094 | Loss: 0.1352 | EWC Loss: 0.0232 | CE Loss: 0.1121\n",
      "Train Epoch: 035 Batch: 00061/00094 | Loss: 0.1340 | EWC Loss: 0.0231 | CE Loss: 0.1109\n",
      "Train Epoch: 035 Batch: 00062/00094 | Loss: 0.0669 | EWC Loss: 0.0232 | CE Loss: 0.0437\n",
      "Train Epoch: 035 Batch: 00063/00094 | Loss: 0.0574 | EWC Loss: 0.0233 | CE Loss: 0.0341\n",
      "Train Epoch: 035 Batch: 00064/00094 | Loss: 0.0888 | EWC Loss: 0.0233 | CE Loss: 0.0655\n",
      "Train Epoch: 035 Batch: 00065/00094 | Loss: 0.1569 | EWC Loss: 0.0234 | CE Loss: 0.1335\n",
      "Train Epoch: 035 Batch: 00066/00094 | Loss: 0.1019 | EWC Loss: 0.0237 | CE Loss: 0.0783\n",
      "Train Epoch: 035 Batch: 00067/00094 | Loss: 0.0922 | EWC Loss: 0.0236 | CE Loss: 0.0685\n",
      "Train Epoch: 035 Batch: 00068/00094 | Loss: 0.0975 | EWC Loss: 0.0238 | CE Loss: 0.0737\n",
      "Train Epoch: 035 Batch: 00069/00094 | Loss: 0.0449 | EWC Loss: 0.0239 | CE Loss: 0.0210\n",
      "Train Epoch: 035 Batch: 00070/00094 | Loss: 0.0374 | EWC Loss: 0.0238 | CE Loss: 0.0135\n",
      "Train Epoch: 035 Batch: 00071/00094 | Loss: 0.1152 | EWC Loss: 0.0238 | CE Loss: 0.0914\n",
      "Train Epoch: 035 Batch: 00072/00094 | Loss: 0.0969 | EWC Loss: 0.0239 | CE Loss: 0.0730\n",
      "Train Epoch: 035 Batch: 00073/00094 | Loss: 0.1155 | EWC Loss: 0.0239 | CE Loss: 0.0916\n",
      "Train Epoch: 035 Batch: 00074/00094 | Loss: 0.1226 | EWC Loss: 0.0239 | CE Loss: 0.0987\n",
      "Train Epoch: 035 Batch: 00075/00094 | Loss: 0.0414 | EWC Loss: 0.0240 | CE Loss: 0.0174\n",
      "Train Epoch: 035 Batch: 00076/00094 | Loss: 0.1286 | EWC Loss: 0.0240 | CE Loss: 0.1046\n",
      "Train Epoch: 035 Batch: 00077/00094 | Loss: 0.0769 | EWC Loss: 0.0238 | CE Loss: 0.0531\n",
      "Train Epoch: 035 Batch: 00078/00094 | Loss: 0.1752 | EWC Loss: 0.0236 | CE Loss: 0.1516\n",
      "Train Epoch: 035 Batch: 00079/00094 | Loss: 0.0867 | EWC Loss: 0.0236 | CE Loss: 0.0632\n",
      "Train Epoch: 035 Batch: 00080/00094 | Loss: 0.0571 | EWC Loss: 0.0237 | CE Loss: 0.0334\n",
      "Train Epoch: 035 Batch: 00081/00094 | Loss: 0.0656 | EWC Loss: 0.0237 | CE Loss: 0.0419\n",
      "Train Epoch: 035 Batch: 00082/00094 | Loss: 0.0886 | EWC Loss: 0.0238 | CE Loss: 0.0648\n",
      "Train Epoch: 035 Batch: 00083/00094 | Loss: 0.0740 | EWC Loss: 0.0239 | CE Loss: 0.0501\n",
      "Train Epoch: 035 Batch: 00084/00094 | Loss: 0.0438 | EWC Loss: 0.0237 | CE Loss: 0.0201\n",
      "Train Epoch: 035 Batch: 00085/00094 | Loss: 0.0644 | EWC Loss: 0.0237 | CE Loss: 0.0408\n",
      "Train Epoch: 035 Batch: 00086/00094 | Loss: 0.1276 | EWC Loss: 0.0235 | CE Loss: 0.1040\n",
      "Train Epoch: 035 Batch: 00087/00094 | Loss: 0.0708 | EWC Loss: 0.0235 | CE Loss: 0.0473\n",
      "Train Epoch: 035 Batch: 00088/00094 | Loss: 0.0636 | EWC Loss: 0.0234 | CE Loss: 0.0401\n",
      "Train Epoch: 035 Batch: 00089/00094 | Loss: 0.0916 | EWC Loss: 0.0234 | CE Loss: 0.0682\n",
      "Train Epoch: 035 Batch: 00090/00094 | Loss: 0.1167 | EWC Loss: 0.0234 | CE Loss: 0.0933\n",
      "Train Epoch: 035 Batch: 00091/00094 | Loss: 0.0577 | EWC Loss: 0.0233 | CE Loss: 0.0344\n",
      "Train Epoch: 035 Batch: 00092/00094 | Loss: 0.0616 | EWC Loss: 0.0233 | CE Loss: 0.0384\n",
      "Train Epoch: 035 Batch: 00093/00094 | Loss: 0.1062 | EWC Loss: 0.0233 | CE Loss: 0.0829\n",
      "Train Epoch: 035 Batch: 00094/00094 | Loss: 0.0933 | EWC Loss: 0.0230 | CE Loss: 0.0703\n",
      "Train Epoch: 034 |Acc 97.61667 | Loss: 0.08860 | Task Loss 0.06489 | EWC Loss: 0.02371\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0588 | acc:98.1000\n",
      "[VAL Acc] Target: 98.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.6761 | acc:49.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.2072 | acc:62.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 62.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.4966 | acc:49.8092\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4784 | acc:57.0533\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6628 | acc:77.8189\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 77.82%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.4931 | acc:81.1912\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 81.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1583 | acc:55.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.50%\n",
      "[VAL Acc] Avg 66.42%\n",
      "\n",
      "\n",
      "---------- Starting epoch 35 ----------\n",
      "Train Epoch: 036 Batch: 00001/00094 | Loss: 0.0573 | EWC Loss: 0.0231 | CE Loss: 0.0343\n",
      "Train Epoch: 036 Batch: 00002/00094 | Loss: 0.0810 | EWC Loss: 0.0232 | CE Loss: 0.0578\n",
      "Train Epoch: 036 Batch: 00003/00094 | Loss: 0.0785 | EWC Loss: 0.0232 | CE Loss: 0.0553\n",
      "Train Epoch: 036 Batch: 00004/00094 | Loss: 0.1182 | EWC Loss: 0.0231 | CE Loss: 0.0950\n",
      "Train Epoch: 036 Batch: 00005/00094 | Loss: 0.0599 | EWC Loss: 0.0232 | CE Loss: 0.0367\n",
      "Train Epoch: 036 Batch: 00006/00094 | Loss: 0.1298 | EWC Loss: 0.0235 | CE Loss: 0.1064\n",
      "Train Epoch: 036 Batch: 00007/00094 | Loss: 0.1045 | EWC Loss: 0.0234 | CE Loss: 0.0810\n",
      "Train Epoch: 036 Batch: 00008/00094 | Loss: 0.0683 | EWC Loss: 0.0235 | CE Loss: 0.0448\n",
      "Train Epoch: 036 Batch: 00009/00094 | Loss: 0.1066 | EWC Loss: 0.0233 | CE Loss: 0.0834\n",
      "Train Epoch: 036 Batch: 00010/00094 | Loss: 0.0766 | EWC Loss: 0.0232 | CE Loss: 0.0534\n",
      "Train Epoch: 036 Batch: 00011/00094 | Loss: 0.0958 | EWC Loss: 0.0232 | CE Loss: 0.0726\n",
      "Train Epoch: 036 Batch: 00012/00094 | Loss: 0.0525 | EWC Loss: 0.0231 | CE Loss: 0.0294\n",
      "Train Epoch: 036 Batch: 00013/00094 | Loss: 0.0853 | EWC Loss: 0.0230 | CE Loss: 0.0623\n",
      "Train Epoch: 036 Batch: 00014/00094 | Loss: 0.0612 | EWC Loss: 0.0231 | CE Loss: 0.0381\n",
      "Train Epoch: 036 Batch: 00015/00094 | Loss: 0.0631 | EWC Loss: 0.0231 | CE Loss: 0.0400\n",
      "Train Epoch: 036 Batch: 00016/00094 | Loss: 0.0743 | EWC Loss: 0.0231 | CE Loss: 0.0512\n",
      "Train Epoch: 036 Batch: 00017/00094 | Loss: 0.0663 | EWC Loss: 0.0231 | CE Loss: 0.0432\n",
      "Train Epoch: 036 Batch: 00018/00094 | Loss: 0.1506 | EWC Loss: 0.0229 | CE Loss: 0.1277\n",
      "Train Epoch: 036 Batch: 00019/00094 | Loss: 0.0690 | EWC Loss: 0.0229 | CE Loss: 0.0460\n",
      "Train Epoch: 036 Batch: 00020/00094 | Loss: 0.0411 | EWC Loss: 0.0228 | CE Loss: 0.0182\n",
      "Train Epoch: 036 Batch: 00021/00094 | Loss: 0.0723 | EWC Loss: 0.0228 | CE Loss: 0.0495\n",
      "Train Epoch: 036 Batch: 00022/00094 | Loss: 0.1045 | EWC Loss: 0.0227 | CE Loss: 0.0818\n",
      "Train Epoch: 036 Batch: 00023/00094 | Loss: 0.1534 | EWC Loss: 0.0225 | CE Loss: 0.1309\n",
      "Train Epoch: 036 Batch: 00024/00094 | Loss: 0.0493 | EWC Loss: 0.0224 | CE Loss: 0.0268\n",
      "Train Epoch: 036 Batch: 00025/00094 | Loss: 0.0505 | EWC Loss: 0.0224 | CE Loss: 0.0281\n",
      "Train Epoch: 036 Batch: 00026/00094 | Loss: 0.1090 | EWC Loss: 0.0224 | CE Loss: 0.0867\n",
      "Train Epoch: 036 Batch: 00027/00094 | Loss: 0.0574 | EWC Loss: 0.0225 | CE Loss: 0.0348\n",
      "Train Epoch: 036 Batch: 00028/00094 | Loss: 0.0794 | EWC Loss: 0.0225 | CE Loss: 0.0569\n",
      "Train Epoch: 036 Batch: 00029/00094 | Loss: 0.0687 | EWC Loss: 0.0225 | CE Loss: 0.0462\n",
      "Train Epoch: 036 Batch: 00030/00094 | Loss: 0.0485 | EWC Loss: 0.0225 | CE Loss: 0.0260\n",
      "Train Epoch: 036 Batch: 00031/00094 | Loss: 0.0639 | EWC Loss: 0.0226 | CE Loss: 0.0413\n",
      "Train Epoch: 036 Batch: 00032/00094 | Loss: 0.0884 | EWC Loss: 0.0226 | CE Loss: 0.0658\n",
      "Train Epoch: 036 Batch: 00033/00094 | Loss: 0.1494 | EWC Loss: 0.0226 | CE Loss: 0.1268\n",
      "Train Epoch: 036 Batch: 00034/00094 | Loss: 0.0573 | EWC Loss: 0.0225 | CE Loss: 0.0347\n",
      "Train Epoch: 036 Batch: 00035/00094 | Loss: 0.0876 | EWC Loss: 0.0226 | CE Loss: 0.0650\n",
      "Train Epoch: 036 Batch: 00036/00094 | Loss: 0.1149 | EWC Loss: 0.0227 | CE Loss: 0.0922\n",
      "Train Epoch: 036 Batch: 00037/00094 | Loss: 0.1766 | EWC Loss: 0.0227 | CE Loss: 0.1539\n",
      "Train Epoch: 036 Batch: 00038/00094 | Loss: 0.0979 | EWC Loss: 0.0226 | CE Loss: 0.0753\n",
      "Train Epoch: 036 Batch: 00039/00094 | Loss: 0.1195 | EWC Loss: 0.0226 | CE Loss: 0.0969\n",
      "Train Epoch: 036 Batch: 00040/00094 | Loss: 0.0515 | EWC Loss: 0.0226 | CE Loss: 0.0289\n",
      "Train Epoch: 036 Batch: 00041/00094 | Loss: 0.1894 | EWC Loss: 0.0226 | CE Loss: 0.1668\n",
      "Train Epoch: 036 Batch: 00042/00094 | Loss: 0.0554 | EWC Loss: 0.0227 | CE Loss: 0.0328\n",
      "Train Epoch: 036 Batch: 00043/00094 | Loss: 0.0627 | EWC Loss: 0.0225 | CE Loss: 0.0401\n",
      "Train Epoch: 036 Batch: 00044/00094 | Loss: 0.0677 | EWC Loss: 0.0224 | CE Loss: 0.0452\n",
      "Train Epoch: 036 Batch: 00045/00094 | Loss: 0.0953 | EWC Loss: 0.0224 | CE Loss: 0.0729\n",
      "Train Epoch: 036 Batch: 00046/00094 | Loss: 0.0828 | EWC Loss: 0.0224 | CE Loss: 0.0604\n",
      "Train Epoch: 036 Batch: 00047/00094 | Loss: 0.1096 | EWC Loss: 0.0226 | CE Loss: 0.0870\n",
      "Train Epoch: 036 Batch: 00048/00094 | Loss: 0.0426 | EWC Loss: 0.0226 | CE Loss: 0.0200\n",
      "Train Epoch: 036 Batch: 00049/00094 | Loss: 0.0834 | EWC Loss: 0.0226 | CE Loss: 0.0608\n",
      "Train Epoch: 036 Batch: 00050/00094 | Loss: 0.0692 | EWC Loss: 0.0226 | CE Loss: 0.0467\n",
      "Train Epoch: 036 Batch: 00051/00094 | Loss: 0.0985 | EWC Loss: 0.0226 | CE Loss: 0.0759\n",
      "Train Epoch: 036 Batch: 00052/00094 | Loss: 0.1368 | EWC Loss: 0.0225 | CE Loss: 0.1143\n",
      "Train Epoch: 036 Batch: 00053/00094 | Loss: 0.1284 | EWC Loss: 0.0224 | CE Loss: 0.1060\n",
      "Train Epoch: 036 Batch: 00054/00094 | Loss: 0.0794 | EWC Loss: 0.0224 | CE Loss: 0.0570\n",
      "Train Epoch: 036 Batch: 00055/00094 | Loss: 0.0471 | EWC Loss: 0.0225 | CE Loss: 0.0246\n",
      "Train Epoch: 036 Batch: 00056/00094 | Loss: 0.0637 | EWC Loss: 0.0224 | CE Loss: 0.0413\n",
      "Train Epoch: 036 Batch: 00057/00094 | Loss: 0.0703 | EWC Loss: 0.0223 | CE Loss: 0.0479\n",
      "Train Epoch: 036 Batch: 00058/00094 | Loss: 0.0986 | EWC Loss: 0.0222 | CE Loss: 0.0763\n",
      "Train Epoch: 036 Batch: 00059/00094 | Loss: 0.1855 | EWC Loss: 0.0222 | CE Loss: 0.1633\n",
      "Train Epoch: 036 Batch: 00060/00094 | Loss: 0.0705 | EWC Loss: 0.0221 | CE Loss: 0.0484\n",
      "Train Epoch: 036 Batch: 00061/00094 | Loss: 0.1680 | EWC Loss: 0.0220 | CE Loss: 0.1460\n",
      "Train Epoch: 036 Batch: 00062/00094 | Loss: 0.1217 | EWC Loss: 0.0221 | CE Loss: 0.0996\n",
      "Train Epoch: 036 Batch: 00063/00094 | Loss: 0.0655 | EWC Loss: 0.0223 | CE Loss: 0.0431\n",
      "Train Epoch: 036 Batch: 00064/00094 | Loss: 0.0492 | EWC Loss: 0.0224 | CE Loss: 0.0269\n",
      "Train Epoch: 036 Batch: 00065/00094 | Loss: 0.0641 | EWC Loss: 0.0224 | CE Loss: 0.0418\n",
      "Train Epoch: 036 Batch: 00066/00094 | Loss: 0.0619 | EWC Loss: 0.0224 | CE Loss: 0.0395\n",
      "Train Epoch: 036 Batch: 00067/00094 | Loss: 0.0888 | EWC Loss: 0.0224 | CE Loss: 0.0664\n",
      "Train Epoch: 036 Batch: 00068/00094 | Loss: 0.1317 | EWC Loss: 0.0224 | CE Loss: 0.1093\n",
      "Train Epoch: 036 Batch: 00069/00094 | Loss: 0.0522 | EWC Loss: 0.0230 | CE Loss: 0.0292\n",
      "Train Epoch: 036 Batch: 00070/00094 | Loss: 0.2017 | EWC Loss: 0.0230 | CE Loss: 0.1787\n",
      "Train Epoch: 036 Batch: 00071/00094 | Loss: 0.0683 | EWC Loss: 0.0231 | CE Loss: 0.0452\n",
      "Train Epoch: 036 Batch: 00072/00094 | Loss: 0.0794 | EWC Loss: 0.0232 | CE Loss: 0.0562\n",
      "Train Epoch: 036 Batch: 00073/00094 | Loss: 0.1257 | EWC Loss: 0.0231 | CE Loss: 0.1025\n",
      "Train Epoch: 036 Batch: 00074/00094 | Loss: 0.1207 | EWC Loss: 0.0230 | CE Loss: 0.0977\n",
      "Train Epoch: 036 Batch: 00075/00094 | Loss: 0.1343 | EWC Loss: 0.0231 | CE Loss: 0.1112\n",
      "Train Epoch: 036 Batch: 00076/00094 | Loss: 0.0769 | EWC Loss: 0.0232 | CE Loss: 0.0537\n",
      "Train Epoch: 036 Batch: 00077/00094 | Loss: 0.0924 | EWC Loss: 0.0232 | CE Loss: 0.0692\n",
      "Train Epoch: 036 Batch: 00078/00094 | Loss: 0.1143 | EWC Loss: 0.0233 | CE Loss: 0.0910\n",
      "Train Epoch: 036 Batch: 00079/00094 | Loss: 0.1294 | EWC Loss: 0.0234 | CE Loss: 0.1060\n",
      "Train Epoch: 036 Batch: 00080/00094 | Loss: 0.1034 | EWC Loss: 0.0235 | CE Loss: 0.0799\n",
      "Train Epoch: 036 Batch: 00081/00094 | Loss: 0.0991 | EWC Loss: 0.0234 | CE Loss: 0.0756\n",
      "Train Epoch: 036 Batch: 00082/00094 | Loss: 0.1284 | EWC Loss: 0.0235 | CE Loss: 0.1049\n",
      "Train Epoch: 036 Batch: 00083/00094 | Loss: 0.0961 | EWC Loss: 0.0236 | CE Loss: 0.0725\n",
      "Train Epoch: 036 Batch: 00084/00094 | Loss: 0.1005 | EWC Loss: 0.0234 | CE Loss: 0.0771\n",
      "Train Epoch: 036 Batch: 00085/00094 | Loss: 0.1506 | EWC Loss: 0.0233 | CE Loss: 0.1273\n",
      "Train Epoch: 036 Batch: 00086/00094 | Loss: 0.1111 | EWC Loss: 0.0234 | CE Loss: 0.0877\n",
      "Train Epoch: 036 Batch: 00087/00094 | Loss: 0.2648 | EWC Loss: 0.0236 | CE Loss: 0.2412\n",
      "Train Epoch: 036 Batch: 00088/00094 | Loss: 0.0864 | EWC Loss: 0.0235 | CE Loss: 0.0630\n",
      "Train Epoch: 036 Batch: 00089/00094 | Loss: 0.0519 | EWC Loss: 0.0234 | CE Loss: 0.0284\n",
      "Train Epoch: 036 Batch: 00090/00094 | Loss: 0.1487 | EWC Loss: 0.0234 | CE Loss: 0.1254\n",
      "Train Epoch: 036 Batch: 00091/00094 | Loss: 0.0815 | EWC Loss: 0.0234 | CE Loss: 0.0582\n",
      "Train Epoch: 036 Batch: 00092/00094 | Loss: 0.1017 | EWC Loss: 0.0233 | CE Loss: 0.0784\n",
      "Train Epoch: 036 Batch: 00093/00094 | Loss: 0.1446 | EWC Loss: 0.0233 | CE Loss: 0.1213\n",
      "Train Epoch: 036 Batch: 00094/00094 | Loss: 0.0520 | EWC Loss: 0.0232 | CE Loss: 0.0288\n",
      "Train Epoch: 035 |Acc 97.60000 | Loss: 0.09514 | Task Loss 0.07230 | EWC Loss: 0.02285\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0568 | acc:97.7500\n",
      "[VAL Acc] Target: 97.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8504 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.3489 | acc:59.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 59.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.5716 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.7731 | acc:55.0157\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6881 | acc:76.3401\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6662 | acc:77.3903\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 77.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.2572 | acc:53.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 53.31%\n",
      "[VAL Acc] Avg 64.50%\n",
      "Early stopping ...\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : tewcmn_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/a3ca79664a1a41159054f5243e8208d8\u001b[0m\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/biggan_val_acc [36]        : (59.25, 64.875)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/crn_val_acc [36]           : (74.17711598746082, 81.26959247648902)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/cyclegan_val_acc [36]      : (44.847328244274806, 64.31297709923665)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/faceforensics_val_acc [36] : (61.82994454713494, 78.46580406654344)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/gaugan_val_acc [36]        : (49.1, 54.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/imle_val_acc [36]          : (54.2319749216301, 63.40125391849529)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/wild_val_acc [36]          : (53.31249999999999, 60.62499999999999)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/target_val_acc [36]                                             : (88.6, 98.6)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/train_acc [36]                                                  : (79.56666666666666, 97.88333333333334)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/val_acc [36]                                                    : (64.50064148800988, 67.12470298009112)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss [36]                                                    : (0.08607008476602904, 0.44676757645619875)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_ewc [36]                                                : (0.01070701420193221, 0.02475099160553927)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_main [36]                                               : (0.062396315302937586, 0.43606056225426654)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start_acc                                                           : 58.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : tewcmn_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size   : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config_file  : ewc_model_config.conf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stop   : 35\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs       : 250\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flip         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     importance   : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr           : 0.005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_schedule  : cosine\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_gpu      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resolution   : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     sample_batch : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (1.02 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1 (9.28 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\", \"biggan\", \"cyclegan\", \"imle\", \"faceforensics\", \"crn\", \"wild\"] \n",
    "TARGET_DATASET = \"diffusionshort\"\n",
    "NETWORK = \"MobileNet2\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/EWC_mn/cddb_easy\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/EWC_mn/diff\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "COMET_NAME = \"tewcmn_diff\"\n",
    "\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/ewc_train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='ewc_model_config.conf', network='ResNet18', input_model='../KD-AIGC-Detection/checkpoints/EWC_r18/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild', output_dir='../KD-AIGC-Detection/checkpoints/EWC_r18/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild', target_dataset='../KD-AIGC-Detection/datasets/custom/diffusionshort', use_comet=True, comet_name='tewc18_diff', num_gpu='0', epochs='250', early_stop='35', batch_size='64', resolution='128', lr_schedule='cosine', lr='0.005', importance='10', sample_batch='4', flip='False')\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/12966a43cddf4d31ab58b15a05dece0b\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/imle\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/crn\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/wild\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/train\n",
      "Dataset available in target_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan / ../KD-AIGC-Detection/datasets/custom/biggan / ../KD-AIGC-Detection/datasets/custom/cyclegan / ../KD-AIGC-Detection/datasets/custom/imle / ../KD-AIGC-Detection/datasets/custom/faceforensics / ../KD-AIGC-Detection/datasets/custom/crn / ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Loading ResNet18 from ../KD-AIGC-Detection/checkpoints/EWC_r18/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild/model_best_accuracy.pth\n",
      "Loaded\n",
      "Apply Cosine learning rate schedule\n",
      "/home/fra/miniconda3/envs/paper/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:1.7216 | acc:52.7000\n",
      "Start Target Validation ACC: 52.70%\n",
      "Start training in 250 epochs\n",
      "\n",
      "\n",
      "---------- Starting epoch 0 ----------\n",
      "Train Epoch: 000 Batch: 00001/00094 | Loss: 1.8534 | EWC Loss: 0.0000 | CE Loss: 1.8534\n",
      "Train Epoch: 000 Batch: 00002/00094 | Loss: 1.2025 | EWC Loss: 0.0032 | CE Loss: 1.1993\n",
      "Train Epoch: 000 Batch: 00003/00094 | Loss: 1.1942 | EWC Loss: 0.0058 | CE Loss: 1.1883\n",
      "Train Epoch: 000 Batch: 00004/00094 | Loss: 1.2374 | EWC Loss: 0.0087 | CE Loss: 1.2287\n",
      "Train Epoch: 000 Batch: 00005/00094 | Loss: 0.9573 | EWC Loss: 0.0108 | CE Loss: 0.9464\n",
      "Train Epoch: 000 Batch: 00006/00094 | Loss: 0.7056 | EWC Loss: 0.0141 | CE Loss: 0.6916\n",
      "Train Epoch: 000 Batch: 00007/00094 | Loss: 1.0927 | EWC Loss: 0.0157 | CE Loss: 1.0771\n",
      "Train Epoch: 000 Batch: 00008/00094 | Loss: 0.5807 | EWC Loss: 0.0173 | CE Loss: 0.5634\n",
      "Train Epoch: 000 Batch: 00009/00094 | Loss: 0.6587 | EWC Loss: 0.0173 | CE Loss: 0.6414\n",
      "Train Epoch: 000 Batch: 00010/00094 | Loss: 0.5453 | EWC Loss: 0.0177 | CE Loss: 0.5276\n",
      "Train Epoch: 000 Batch: 00011/00094 | Loss: 0.6228 | EWC Loss: 0.0190 | CE Loss: 0.6038\n",
      "Train Epoch: 000 Batch: 00012/00094 | Loss: 0.9734 | EWC Loss: 0.0195 | CE Loss: 0.9539\n",
      "Train Epoch: 000 Batch: 00013/00094 | Loss: 0.4855 | EWC Loss: 0.0229 | CE Loss: 0.4626\n",
      "Train Epoch: 000 Batch: 00014/00094 | Loss: 0.6588 | EWC Loss: 0.0226 | CE Loss: 0.6361\n",
      "Train Epoch: 000 Batch: 00015/00094 | Loss: 0.8266 | EWC Loss: 0.0229 | CE Loss: 0.8037\n",
      "Train Epoch: 000 Batch: 00016/00094 | Loss: 0.7679 | EWC Loss: 0.0234 | CE Loss: 0.7445\n",
      "Train Epoch: 000 Batch: 00017/00094 | Loss: 0.5316 | EWC Loss: 0.0257 | CE Loss: 0.5060\n",
      "Train Epoch: 000 Batch: 00018/00094 | Loss: 0.6769 | EWC Loss: 0.0246 | CE Loss: 0.6523\n",
      "Train Epoch: 000 Batch: 00019/00094 | Loss: 0.5448 | EWC Loss: 0.0256 | CE Loss: 0.5192\n",
      "Train Epoch: 000 Batch: 00020/00094 | Loss: 0.6727 | EWC Loss: 0.0254 | CE Loss: 0.6472\n",
      "Train Epoch: 000 Batch: 00021/00094 | Loss: 0.6484 | EWC Loss: 0.0260 | CE Loss: 0.6225\n",
      "Train Epoch: 000 Batch: 00022/00094 | Loss: 0.6510 | EWC Loss: 0.0268 | CE Loss: 0.6243\n",
      "Train Epoch: 000 Batch: 00023/00094 | Loss: 0.6346 | EWC Loss: 0.0277 | CE Loss: 0.6069\n",
      "Train Epoch: 000 Batch: 00024/00094 | Loss: 0.3605 | EWC Loss: 0.0278 | CE Loss: 0.3326\n",
      "Train Epoch: 000 Batch: 00025/00094 | Loss: 0.7479 | EWC Loss: 0.0274 | CE Loss: 0.7205\n",
      "Train Epoch: 000 Batch: 00026/00094 | Loss: 0.4577 | EWC Loss: 0.0277 | CE Loss: 0.4300\n",
      "Train Epoch: 000 Batch: 00027/00094 | Loss: 0.6680 | EWC Loss: 0.0287 | CE Loss: 0.6394\n",
      "Train Epoch: 000 Batch: 00028/00094 | Loss: 0.5326 | EWC Loss: 0.0295 | CE Loss: 0.5032\n",
      "Train Epoch: 000 Batch: 00029/00094 | Loss: 0.3648 | EWC Loss: 0.0297 | CE Loss: 0.3351\n",
      "Train Epoch: 000 Batch: 00030/00094 | Loss: 0.5433 | EWC Loss: 0.0294 | CE Loss: 0.5138\n",
      "Train Epoch: 000 Batch: 00031/00094 | Loss: 0.5091 | EWC Loss: 0.0295 | CE Loss: 0.4796\n",
      "Train Epoch: 000 Batch: 00032/00094 | Loss: 0.3864 | EWC Loss: 0.0304 | CE Loss: 0.3560\n",
      "Train Epoch: 000 Batch: 00033/00094 | Loss: 0.4996 | EWC Loss: 0.0304 | CE Loss: 0.4692\n",
      "Train Epoch: 000 Batch: 00034/00094 | Loss: 0.5657 | EWC Loss: 0.0298 | CE Loss: 0.5359\n",
      "Train Epoch: 000 Batch: 00035/00094 | Loss: 0.4555 | EWC Loss: 0.0300 | CE Loss: 0.4255\n",
      "Train Epoch: 000 Batch: 00036/00094 | Loss: 0.6254 | EWC Loss: 0.0301 | CE Loss: 0.5952\n",
      "Train Epoch: 000 Batch: 00037/00094 | Loss: 0.6306 | EWC Loss: 0.0308 | CE Loss: 0.5998\n",
      "Train Epoch: 000 Batch: 00038/00094 | Loss: 0.3903 | EWC Loss: 0.0313 | CE Loss: 0.3590\n",
      "Train Epoch: 000 Batch: 00039/00094 | Loss: 0.4184 | EWC Loss: 0.0308 | CE Loss: 0.3876\n",
      "Train Epoch: 000 Batch: 00040/00094 | Loss: 0.4922 | EWC Loss: 0.0319 | CE Loss: 0.4603\n",
      "Train Epoch: 000 Batch: 00041/00094 | Loss: 0.4229 | EWC Loss: 0.0304 | CE Loss: 0.3925\n",
      "Train Epoch: 000 Batch: 00042/00094 | Loss: 0.3890 | EWC Loss: 0.0297 | CE Loss: 0.3592\n",
      "Train Epoch: 000 Batch: 00043/00094 | Loss: 0.4194 | EWC Loss: 0.0298 | CE Loss: 0.3896\n",
      "Train Epoch: 000 Batch: 00044/00094 | Loss: 0.2759 | EWC Loss: 0.0301 | CE Loss: 0.2458\n",
      "Train Epoch: 000 Batch: 00045/00094 | Loss: 0.3824 | EWC Loss: 0.0301 | CE Loss: 0.3524\n",
      "Train Epoch: 000 Batch: 00046/00094 | Loss: 0.3695 | EWC Loss: 0.0292 | CE Loss: 0.3403\n",
      "Train Epoch: 000 Batch: 00047/00094 | Loss: 0.2927 | EWC Loss: 0.0293 | CE Loss: 0.2633\n",
      "Train Epoch: 000 Batch: 00048/00094 | Loss: 0.3621 | EWC Loss: 0.0293 | CE Loss: 0.3328\n",
      "Train Epoch: 000 Batch: 00049/00094 | Loss: 0.5955 | EWC Loss: 0.0293 | CE Loss: 0.5662\n",
      "Train Epoch: 000 Batch: 00050/00094 | Loss: 0.4167 | EWC Loss: 0.0298 | CE Loss: 0.3870\n",
      "Train Epoch: 000 Batch: 00051/00094 | Loss: 0.4978 | EWC Loss: 0.0300 | CE Loss: 0.4679\n",
      "Train Epoch: 000 Batch: 00052/00094 | Loss: 0.3173 | EWC Loss: 0.0307 | CE Loss: 0.2866\n",
      "Train Epoch: 000 Batch: 00053/00094 | Loss: 0.2498 | EWC Loss: 0.0306 | CE Loss: 0.2192\n",
      "Train Epoch: 000 Batch: 00054/00094 | Loss: 0.4458 | EWC Loss: 0.0305 | CE Loss: 0.4154\n",
      "Train Epoch: 000 Batch: 00055/00094 | Loss: 0.2982 | EWC Loss: 0.0306 | CE Loss: 0.2676\n",
      "Train Epoch: 000 Batch: 00056/00094 | Loss: 0.3267 | EWC Loss: 0.0304 | CE Loss: 0.2964\n",
      "Train Epoch: 000 Batch: 00057/00094 | Loss: 0.5237 | EWC Loss: 0.0305 | CE Loss: 0.4932\n",
      "Train Epoch: 000 Batch: 00058/00094 | Loss: 0.3781 | EWC Loss: 0.0309 | CE Loss: 0.3472\n",
      "Train Epoch: 000 Batch: 00059/00094 | Loss: 0.3483 | EWC Loss: 0.0310 | CE Loss: 0.3173\n",
      "Train Epoch: 000 Batch: 00060/00094 | Loss: 0.3034 | EWC Loss: 0.0309 | CE Loss: 0.2725\n",
      "Train Epoch: 000 Batch: 00061/00094 | Loss: 0.2945 | EWC Loss: 0.0310 | CE Loss: 0.2635\n",
      "Train Epoch: 000 Batch: 00062/00094 | Loss: 0.2370 | EWC Loss: 0.0314 | CE Loss: 0.2056\n",
      "Train Epoch: 000 Batch: 00063/00094 | Loss: 0.2914 | EWC Loss: 0.0315 | CE Loss: 0.2599\n",
      "Train Epoch: 000 Batch: 00064/00094 | Loss: 0.3894 | EWC Loss: 0.0312 | CE Loss: 0.3582\n",
      "Train Epoch: 000 Batch: 00065/00094 | Loss: 0.4230 | EWC Loss: 0.0315 | CE Loss: 0.3916\n",
      "Train Epoch: 000 Batch: 00066/00094 | Loss: 0.4321 | EWC Loss: 0.0316 | CE Loss: 0.4005\n",
      "Train Epoch: 000 Batch: 00067/00094 | Loss: 0.2806 | EWC Loss: 0.0319 | CE Loss: 0.2486\n",
      "Train Epoch: 000 Batch: 00068/00094 | Loss: 0.4426 | EWC Loss: 0.0326 | CE Loss: 0.4100\n",
      "Train Epoch: 000 Batch: 00069/00094 | Loss: 0.2559 | EWC Loss: 0.0325 | CE Loss: 0.2235\n",
      "Train Epoch: 000 Batch: 00070/00094 | Loss: 0.2737 | EWC Loss: 0.0314 | CE Loss: 0.2423\n",
      "Train Epoch: 000 Batch: 00071/00094 | Loss: 0.1731 | EWC Loss: 0.0318 | CE Loss: 0.1413\n",
      "Train Epoch: 000 Batch: 00072/00094 | Loss: 0.2046 | EWC Loss: 0.0317 | CE Loss: 0.1729\n",
      "Train Epoch: 000 Batch: 00073/00094 | Loss: 0.3792 | EWC Loss: 0.0317 | CE Loss: 0.3475\n",
      "Train Epoch: 000 Batch: 00074/00094 | Loss: 0.2197 | EWC Loss: 0.0320 | CE Loss: 0.1876\n",
      "Train Epoch: 000 Batch: 00075/00094 | Loss: 0.3976 | EWC Loss: 0.0319 | CE Loss: 0.3657\n",
      "Train Epoch: 000 Batch: 00076/00094 | Loss: 0.1868 | EWC Loss: 0.0313 | CE Loss: 0.1555\n",
      "Train Epoch: 000 Batch: 00077/00094 | Loss: 0.2396 | EWC Loss: 0.0312 | CE Loss: 0.2084\n",
      "Train Epoch: 000 Batch: 00078/00094 | Loss: 0.3568 | EWC Loss: 0.0309 | CE Loss: 0.3258\n",
      "Train Epoch: 000 Batch: 00079/00094 | Loss: 0.2761 | EWC Loss: 0.0310 | CE Loss: 0.2452\n",
      "Train Epoch: 000 Batch: 00080/00094 | Loss: 0.2771 | EWC Loss: 0.0308 | CE Loss: 0.2463\n",
      "Train Epoch: 000 Batch: 00081/00094 | Loss: 0.2065 | EWC Loss: 0.0310 | CE Loss: 0.1755\n",
      "Train Epoch: 000 Batch: 00082/00094 | Loss: 0.2606 | EWC Loss: 0.0309 | CE Loss: 0.2297\n",
      "Train Epoch: 000 Batch: 00083/00094 | Loss: 0.3034 | EWC Loss: 0.0316 | CE Loss: 0.2719\n",
      "Train Epoch: 000 Batch: 00084/00094 | Loss: 0.3877 | EWC Loss: 0.0313 | CE Loss: 0.3563\n",
      "Train Epoch: 000 Batch: 00085/00094 | Loss: 0.3386 | EWC Loss: 0.0309 | CE Loss: 0.3077\n",
      "Train Epoch: 000 Batch: 00086/00094 | Loss: 0.4820 | EWC Loss: 0.0304 | CE Loss: 0.4516\n",
      "Train Epoch: 000 Batch: 00087/00094 | Loss: 0.1959 | EWC Loss: 0.0312 | CE Loss: 0.1647\n",
      "Train Epoch: 000 Batch: 00088/00094 | Loss: 0.3760 | EWC Loss: 0.0310 | CE Loss: 0.3449\n",
      "Train Epoch: 000 Batch: 00089/00094 | Loss: 0.2930 | EWC Loss: 0.0312 | CE Loss: 0.2618\n",
      "Train Epoch: 000 Batch: 00090/00094 | Loss: 0.2446 | EWC Loss: 0.0312 | CE Loss: 0.2135\n",
      "Train Epoch: 000 Batch: 00091/00094 | Loss: 0.3243 | EWC Loss: 0.0309 | CE Loss: 0.2934\n",
      "Train Epoch: 000 Batch: 00092/00094 | Loss: 0.3374 | EWC Loss: 0.0307 | CE Loss: 0.3067\n",
      "Train Epoch: 000 Batch: 00093/00094 | Loss: 0.3213 | EWC Loss: 0.0307 | CE Loss: 0.2907\n",
      "Train Epoch: 000 Batch: 00094/00094 | Loss: 0.5028 | EWC Loss: 0.0309 | CE Loss: 0.4719\n",
      "Train Epoch: 000 |Acc 80.60000 | Loss: 0.48501 | Task Loss 0.45740 | EWC Loss: 0.02761\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2792 | acc:88.6500\n",
      "[VAL Acc] Target: 88.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.3635 | acc:50.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 50.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.6991 | acc:52.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.4642 | acc:52.4809\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 52.48%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.0036 | acc:62.4608\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 62.46%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5300 | acc:76.0628\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.06%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8061 | acc:68.2602\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.26%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9038 | acc:66.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 66.69%\n",
      "[VAL Acc] Avg 64.70%\n",
      "VAL Acc improve from 0.00% to 64.70%\n",
      "Save best model\n",
      "\n",
      "\n",
      "---------- Starting epoch 1 ----------\n",
      "Train Epoch: 001 Batch: 00001/00094 | Loss: 0.2886 | EWC Loss: 0.0317 | CE Loss: 0.2570\n",
      "Train Epoch: 001 Batch: 00002/00094 | Loss: 0.2897 | EWC Loss: 0.0316 | CE Loss: 0.2580\n",
      "Train Epoch: 001 Batch: 00003/00094 | Loss: 0.1591 | EWC Loss: 0.0315 | CE Loss: 0.1276\n",
      "Train Epoch: 001 Batch: 00004/00094 | Loss: 0.2580 | EWC Loss: 0.0313 | CE Loss: 0.2267\n",
      "Train Epoch: 001 Batch: 00005/00094 | Loss: 0.2906 | EWC Loss: 0.0309 | CE Loss: 0.2597\n",
      "Train Epoch: 001 Batch: 00006/00094 | Loss: 0.3006 | EWC Loss: 0.0311 | CE Loss: 0.2695\n",
      "Train Epoch: 001 Batch: 00007/00094 | Loss: 0.2683 | EWC Loss: 0.0314 | CE Loss: 0.2369\n",
      "Train Epoch: 001 Batch: 00008/00094 | Loss: 0.4002 | EWC Loss: 0.0315 | CE Loss: 0.3688\n",
      "Train Epoch: 001 Batch: 00009/00094 | Loss: 0.2131 | EWC Loss: 0.0318 | CE Loss: 0.1813\n",
      "Train Epoch: 001 Batch: 00010/00094 | Loss: 0.2085 | EWC Loss: 0.0312 | CE Loss: 0.1773\n",
      "Train Epoch: 001 Batch: 00011/00094 | Loss: 0.2481 | EWC Loss: 0.0309 | CE Loss: 0.2171\n",
      "Train Epoch: 001 Batch: 00012/00094 | Loss: 0.4218 | EWC Loss: 0.0307 | CE Loss: 0.3910\n",
      "Train Epoch: 001 Batch: 00013/00094 | Loss: 0.3109 | EWC Loss: 0.0316 | CE Loss: 0.2793\n",
      "Train Epoch: 001 Batch: 00014/00094 | Loss: 0.4146 | EWC Loss: 0.0319 | CE Loss: 0.3826\n",
      "Train Epoch: 001 Batch: 00015/00094 | Loss: 0.2204 | EWC Loss: 0.0322 | CE Loss: 0.1882\n",
      "Train Epoch: 001 Batch: 00016/00094 | Loss: 0.2266 | EWC Loss: 0.0319 | CE Loss: 0.1947\n",
      "Train Epoch: 001 Batch: 00017/00094 | Loss: 0.1986 | EWC Loss: 0.0317 | CE Loss: 0.1669\n",
      "Train Epoch: 001 Batch: 00018/00094 | Loss: 0.3690 | EWC Loss: 0.0312 | CE Loss: 0.3378\n",
      "Train Epoch: 001 Batch: 00019/00094 | Loss: 0.3248 | EWC Loss: 0.0316 | CE Loss: 0.2932\n",
      "Train Epoch: 001 Batch: 00020/00094 | Loss: 0.2334 | EWC Loss: 0.0316 | CE Loss: 0.2018\n",
      "Train Epoch: 001 Batch: 00021/00094 | Loss: 0.2420 | EWC Loss: 0.0316 | CE Loss: 0.2104\n",
      "Train Epoch: 001 Batch: 00022/00094 | Loss: 0.2494 | EWC Loss: 0.0315 | CE Loss: 0.2178\n",
      "Train Epoch: 001 Batch: 00023/00094 | Loss: 0.1977 | EWC Loss: 0.0316 | CE Loss: 0.1662\n",
      "Train Epoch: 001 Batch: 00024/00094 | Loss: 0.3605 | EWC Loss: 0.0314 | CE Loss: 0.3291\n",
      "Train Epoch: 001 Batch: 00025/00094 | Loss: 0.2374 | EWC Loss: 0.0320 | CE Loss: 0.2054\n",
      "Train Epoch: 001 Batch: 00026/00094 | Loss: 0.4260 | EWC Loss: 0.0314 | CE Loss: 0.3947\n",
      "Train Epoch: 001 Batch: 00027/00094 | Loss: 0.3070 | EWC Loss: 0.0312 | CE Loss: 0.2758\n",
      "Train Epoch: 001 Batch: 00028/00094 | Loss: 0.3088 | EWC Loss: 0.0313 | CE Loss: 0.2776\n",
      "Train Epoch: 001 Batch: 00029/00094 | Loss: 0.1927 | EWC Loss: 0.0318 | CE Loss: 0.1609\n",
      "Train Epoch: 001 Batch: 00030/00094 | Loss: 0.1660 | EWC Loss: 0.0318 | CE Loss: 0.1341\n",
      "Train Epoch: 001 Batch: 00031/00094 | Loss: 0.4392 | EWC Loss: 0.0319 | CE Loss: 0.4073\n",
      "Train Epoch: 001 Batch: 00032/00094 | Loss: 0.2161 | EWC Loss: 0.0324 | CE Loss: 0.1837\n",
      "Train Epoch: 001 Batch: 00033/00094 | Loss: 0.3887 | EWC Loss: 0.0317 | CE Loss: 0.3570\n",
      "Train Epoch: 001 Batch: 00034/00094 | Loss: 0.2580 | EWC Loss: 0.0321 | CE Loss: 0.2258\n",
      "Train Epoch: 001 Batch: 00035/00094 | Loss: 0.2329 | EWC Loss: 0.0322 | CE Loss: 0.2008\n",
      "Train Epoch: 001 Batch: 00036/00094 | Loss: 0.2784 | EWC Loss: 0.0321 | CE Loss: 0.2463\n",
      "Train Epoch: 001 Batch: 00037/00094 | Loss: 0.2765 | EWC Loss: 0.0327 | CE Loss: 0.2438\n",
      "Train Epoch: 001 Batch: 00038/00094 | Loss: 0.3416 | EWC Loss: 0.0335 | CE Loss: 0.3081\n",
      "Train Epoch: 001 Batch: 00039/00094 | Loss: 0.2185 | EWC Loss: 0.0334 | CE Loss: 0.1851\n",
      "Train Epoch: 001 Batch: 00040/00094 | Loss: 0.1952 | EWC Loss: 0.0332 | CE Loss: 0.1620\n",
      "Train Epoch: 001 Batch: 00041/00094 | Loss: 0.1979 | EWC Loss: 0.0330 | CE Loss: 0.1649\n",
      "Train Epoch: 001 Batch: 00042/00094 | Loss: 0.2177 | EWC Loss: 0.0329 | CE Loss: 0.1847\n",
      "Train Epoch: 001 Batch: 00043/00094 | Loss: 0.2598 | EWC Loss: 0.0330 | CE Loss: 0.2269\n",
      "Train Epoch: 001 Batch: 00044/00094 | Loss: 0.3445 | EWC Loss: 0.0330 | CE Loss: 0.3115\n",
      "Train Epoch: 001 Batch: 00045/00094 | Loss: 0.2529 | EWC Loss: 0.0325 | CE Loss: 0.2205\n",
      "Train Epoch: 001 Batch: 00046/00094 | Loss: 0.2163 | EWC Loss: 0.0326 | CE Loss: 0.1837\n",
      "Train Epoch: 001 Batch: 00047/00094 | Loss: 0.2812 | EWC Loss: 0.0329 | CE Loss: 0.2483\n",
      "Train Epoch: 001 Batch: 00048/00094 | Loss: 0.2287 | EWC Loss: 0.0336 | CE Loss: 0.1951\n",
      "Train Epoch: 001 Batch: 00049/00094 | Loss: 0.1547 | EWC Loss: 0.0340 | CE Loss: 0.1207\n",
      "Train Epoch: 001 Batch: 00050/00094 | Loss: 0.1550 | EWC Loss: 0.0333 | CE Loss: 0.1217\n",
      "Train Epoch: 001 Batch: 00051/00094 | Loss: 0.4237 | EWC Loss: 0.0332 | CE Loss: 0.3905\n",
      "Train Epoch: 001 Batch: 00052/00094 | Loss: 0.2595 | EWC Loss: 0.0329 | CE Loss: 0.2266\n",
      "Train Epoch: 001 Batch: 00053/00094 | Loss: 0.2317 | EWC Loss: 0.0330 | CE Loss: 0.1987\n",
      "Train Epoch: 001 Batch: 00054/00094 | Loss: 0.2646 | EWC Loss: 0.0331 | CE Loss: 0.2315\n",
      "Train Epoch: 001 Batch: 00055/00094 | Loss: 0.2672 | EWC Loss: 0.0330 | CE Loss: 0.2342\n",
      "Train Epoch: 001 Batch: 00056/00094 | Loss: 0.2980 | EWC Loss: 0.0334 | CE Loss: 0.2647\n",
      "Train Epoch: 001 Batch: 00057/00094 | Loss: 0.1969 | EWC Loss: 0.0331 | CE Loss: 0.1637\n",
      "Train Epoch: 001 Batch: 00058/00094 | Loss: 0.3049 | EWC Loss: 0.0330 | CE Loss: 0.2719\n",
      "Train Epoch: 001 Batch: 00059/00094 | Loss: 0.1725 | EWC Loss: 0.0330 | CE Loss: 0.1395\n",
      "Train Epoch: 001 Batch: 00060/00094 | Loss: 0.2560 | EWC Loss: 0.0328 | CE Loss: 0.2232\n",
      "Train Epoch: 001 Batch: 00061/00094 | Loss: 0.2513 | EWC Loss: 0.0332 | CE Loss: 0.2181\n",
      "Train Epoch: 001 Batch: 00062/00094 | Loss: 0.1832 | EWC Loss: 0.0335 | CE Loss: 0.1496\n",
      "Train Epoch: 001 Batch: 00063/00094 | Loss: 0.1975 | EWC Loss: 0.0343 | CE Loss: 0.1633\n",
      "Train Epoch: 001 Batch: 00064/00094 | Loss: 0.3353 | EWC Loss: 0.0347 | CE Loss: 0.3006\n",
      "Train Epoch: 001 Batch: 00065/00094 | Loss: 0.1589 | EWC Loss: 0.0358 | CE Loss: 0.1230\n",
      "Train Epoch: 001 Batch: 00066/00094 | Loss: 0.2716 | EWC Loss: 0.0351 | CE Loss: 0.2365\n",
      "Train Epoch: 001 Batch: 00067/00094 | Loss: 0.1404 | EWC Loss: 0.0343 | CE Loss: 0.1061\n",
      "Train Epoch: 001 Batch: 00068/00094 | Loss: 0.2177 | EWC Loss: 0.0339 | CE Loss: 0.1838\n",
      "Train Epoch: 001 Batch: 00069/00094 | Loss: 0.2402 | EWC Loss: 0.0338 | CE Loss: 0.2064\n",
      "Train Epoch: 001 Batch: 00070/00094 | Loss: 0.2000 | EWC Loss: 0.0337 | CE Loss: 0.1663\n",
      "Train Epoch: 001 Batch: 00071/00094 | Loss: 0.2000 | EWC Loss: 0.0330 | CE Loss: 0.1670\n",
      "Train Epoch: 001 Batch: 00072/00094 | Loss: 0.3602 | EWC Loss: 0.0331 | CE Loss: 0.3271\n",
      "Train Epoch: 001 Batch: 00073/00094 | Loss: 0.3224 | EWC Loss: 0.0329 | CE Loss: 0.2896\n",
      "Train Epoch: 001 Batch: 00074/00094 | Loss: 0.3210 | EWC Loss: 0.0332 | CE Loss: 0.2878\n",
      "Train Epoch: 001 Batch: 00075/00094 | Loss: 0.1606 | EWC Loss: 0.0331 | CE Loss: 0.1275\n",
      "Train Epoch: 001 Batch: 00076/00094 | Loss: 0.2390 | EWC Loss: 0.0332 | CE Loss: 0.2058\n",
      "Train Epoch: 001 Batch: 00077/00094 | Loss: 0.2391 | EWC Loss: 0.0327 | CE Loss: 0.2063\n",
      "Train Epoch: 001 Batch: 00078/00094 | Loss: 0.1590 | EWC Loss: 0.0329 | CE Loss: 0.1261\n",
      "Train Epoch: 001 Batch: 00079/00094 | Loss: 0.1534 | EWC Loss: 0.0327 | CE Loss: 0.1207\n",
      "Train Epoch: 001 Batch: 00080/00094 | Loss: 0.1813 | EWC Loss: 0.0327 | CE Loss: 0.1486\n",
      "Train Epoch: 001 Batch: 00081/00094 | Loss: 0.3454 | EWC Loss: 0.0325 | CE Loss: 0.3128\n",
      "Train Epoch: 001 Batch: 00082/00094 | Loss: 0.2231 | EWC Loss: 0.0327 | CE Loss: 0.1904\n",
      "Train Epoch: 001 Batch: 00083/00094 | Loss: 0.2470 | EWC Loss: 0.0324 | CE Loss: 0.2146\n",
      "Train Epoch: 001 Batch: 00084/00094 | Loss: 0.2571 | EWC Loss: 0.0328 | CE Loss: 0.2243\n",
      "Train Epoch: 001 Batch: 00085/00094 | Loss: 0.1902 | EWC Loss: 0.0329 | CE Loss: 0.1573\n",
      "Train Epoch: 001 Batch: 00086/00094 | Loss: 0.2371 | EWC Loss: 0.0327 | CE Loss: 0.2044\n",
      "Train Epoch: 001 Batch: 00087/00094 | Loss: 0.2480 | EWC Loss: 0.0325 | CE Loss: 0.2155\n",
      "Train Epoch: 001 Batch: 00088/00094 | Loss: 0.2419 | EWC Loss: 0.0330 | CE Loss: 0.2089\n",
      "Train Epoch: 001 Batch: 00089/00094 | Loss: 0.1782 | EWC Loss: 0.0328 | CE Loss: 0.1454\n",
      "Train Epoch: 001 Batch: 00090/00094 | Loss: 0.1302 | EWC Loss: 0.0329 | CE Loss: 0.0973\n",
      "Train Epoch: 001 Batch: 00091/00094 | Loss: 0.2092 | EWC Loss: 0.0327 | CE Loss: 0.1765\n",
      "Train Epoch: 001 Batch: 00092/00094 | Loss: 0.2181 | EWC Loss: 0.0324 | CE Loss: 0.1857\n",
      "Train Epoch: 001 Batch: 00093/00094 | Loss: 0.3409 | EWC Loss: 0.0325 | CE Loss: 0.3083\n",
      "Train Epoch: 001 Batch: 00094/00094 | Loss: 0.2066 | EWC Loss: 0.0328 | CE Loss: 0.1739\n",
      "Train Epoch: 001 |Acc 91.03333 | Loss: 0.25496 | Task Loss 0.22240 | EWC Loss: 0.03256\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1781 | acc:92.6500\n",
      "[VAL Acc] Target: 92.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.5669 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.6310 | acc:54.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.6820 | acc:50.9542\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4674 | acc:57.1708\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.17%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5188 | acc:77.6340\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 77.63%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1393 | acc:63.9498\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8693 | acc:66.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 66.62%\n",
      "[VAL Acc] Avg 64.01%\n",
      "\n",
      "\n",
      "---------- Starting epoch 2 ----------\n",
      "Train Epoch: 002 Batch: 00001/00094 | Loss: 0.2348 | EWC Loss: 0.0333 | CE Loss: 0.2014\n",
      "Train Epoch: 002 Batch: 00002/00094 | Loss: 0.2698 | EWC Loss: 0.0336 | CE Loss: 0.2362\n",
      "Train Epoch: 002 Batch: 00003/00094 | Loss: 0.1471 | EWC Loss: 0.0331 | CE Loss: 0.1140\n",
      "Train Epoch: 002 Batch: 00004/00094 | Loss: 0.3546 | EWC Loss: 0.0330 | CE Loss: 0.3216\n",
      "Train Epoch: 002 Batch: 00005/00094 | Loss: 0.2011 | EWC Loss: 0.0330 | CE Loss: 0.1681\n",
      "Train Epoch: 002 Batch: 00006/00094 | Loss: 0.2056 | EWC Loss: 0.0326 | CE Loss: 0.1730\n",
      "Train Epoch: 002 Batch: 00007/00094 | Loss: 0.2549 | EWC Loss: 0.0325 | CE Loss: 0.2224\n",
      "Train Epoch: 002 Batch: 00008/00094 | Loss: 0.1849 | EWC Loss: 0.0321 | CE Loss: 0.1528\n",
      "Train Epoch: 002 Batch: 00009/00094 | Loss: 0.2334 | EWC Loss: 0.0320 | CE Loss: 0.2014\n",
      "Train Epoch: 002 Batch: 00010/00094 | Loss: 0.2256 | EWC Loss: 0.0324 | CE Loss: 0.1932\n",
      "Train Epoch: 002 Batch: 00011/00094 | Loss: 0.1918 | EWC Loss: 0.0323 | CE Loss: 0.1594\n",
      "Train Epoch: 002 Batch: 00012/00094 | Loss: 0.2899 | EWC Loss: 0.0322 | CE Loss: 0.2577\n",
      "Train Epoch: 002 Batch: 00013/00094 | Loss: 0.1754 | EWC Loss: 0.0321 | CE Loss: 0.1432\n",
      "Train Epoch: 002 Batch: 00014/00094 | Loss: 0.1614 | EWC Loss: 0.0321 | CE Loss: 0.1294\n",
      "Train Epoch: 002 Batch: 00015/00094 | Loss: 0.1438 | EWC Loss: 0.0319 | CE Loss: 0.1119\n",
      "Train Epoch: 002 Batch: 00016/00094 | Loss: 0.2643 | EWC Loss: 0.0319 | CE Loss: 0.2324\n",
      "Train Epoch: 002 Batch: 00017/00094 | Loss: 0.2984 | EWC Loss: 0.0321 | CE Loss: 0.2663\n",
      "Train Epoch: 002 Batch: 00018/00094 | Loss: 0.2027 | EWC Loss: 0.0326 | CE Loss: 0.1701\n",
      "Train Epoch: 002 Batch: 00019/00094 | Loss: 0.1571 | EWC Loss: 0.0323 | CE Loss: 0.1248\n",
      "Train Epoch: 002 Batch: 00020/00094 | Loss: 0.1795 | EWC Loss: 0.0321 | CE Loss: 0.1474\n",
      "Train Epoch: 002 Batch: 00021/00094 | Loss: 0.3137 | EWC Loss: 0.0321 | CE Loss: 0.2816\n",
      "Train Epoch: 002 Batch: 00022/00094 | Loss: 0.1861 | EWC Loss: 0.0325 | CE Loss: 0.1536\n",
      "Train Epoch: 002 Batch: 00023/00094 | Loss: 0.1683 | EWC Loss: 0.0324 | CE Loss: 0.1358\n",
      "Train Epoch: 002 Batch: 00024/00094 | Loss: 0.1529 | EWC Loss: 0.0323 | CE Loss: 0.1206\n",
      "Train Epoch: 002 Batch: 00025/00094 | Loss: 0.2461 | EWC Loss: 0.0321 | CE Loss: 0.2140\n",
      "Train Epoch: 002 Batch: 00026/00094 | Loss: 0.2458 | EWC Loss: 0.0319 | CE Loss: 0.2138\n",
      "Train Epoch: 002 Batch: 00027/00094 | Loss: 0.1349 | EWC Loss: 0.0321 | CE Loss: 0.1028\n",
      "Train Epoch: 002 Batch: 00028/00094 | Loss: 0.2104 | EWC Loss: 0.0320 | CE Loss: 0.1783\n",
      "Train Epoch: 002 Batch: 00029/00094 | Loss: 0.1350 | EWC Loss: 0.0320 | CE Loss: 0.1030\n",
      "Train Epoch: 002 Batch: 00030/00094 | Loss: 0.1639 | EWC Loss: 0.0318 | CE Loss: 0.1321\n",
      "Train Epoch: 002 Batch: 00031/00094 | Loss: 0.1627 | EWC Loss: 0.0319 | CE Loss: 0.1308\n",
      "Train Epoch: 002 Batch: 00032/00094 | Loss: 0.2694 | EWC Loss: 0.0318 | CE Loss: 0.2376\n",
      "Train Epoch: 002 Batch: 00033/00094 | Loss: 0.1768 | EWC Loss: 0.0317 | CE Loss: 0.1452\n",
      "Train Epoch: 002 Batch: 00034/00094 | Loss: 0.4419 | EWC Loss: 0.0318 | CE Loss: 0.4100\n",
      "Train Epoch: 002 Batch: 00035/00094 | Loss: 0.1497 | EWC Loss: 0.0319 | CE Loss: 0.1178\n",
      "Train Epoch: 002 Batch: 00036/00094 | Loss: 0.3554 | EWC Loss: 0.0319 | CE Loss: 0.3235\n",
      "Train Epoch: 002 Batch: 00037/00094 | Loss: 0.2073 | EWC Loss: 0.0321 | CE Loss: 0.1752\n",
      "Train Epoch: 002 Batch: 00038/00094 | Loss: 0.1977 | EWC Loss: 0.0320 | CE Loss: 0.1657\n",
      "Train Epoch: 002 Batch: 00039/00094 | Loss: 0.1451 | EWC Loss: 0.0319 | CE Loss: 0.1132\n",
      "Train Epoch: 002 Batch: 00040/00094 | Loss: 0.2818 | EWC Loss: 0.0318 | CE Loss: 0.2500\n",
      "Train Epoch: 002 Batch: 00041/00094 | Loss: 0.2027 | EWC Loss: 0.0318 | CE Loss: 0.1708\n",
      "Train Epoch: 002 Batch: 00042/00094 | Loss: 0.2700 | EWC Loss: 0.0321 | CE Loss: 0.2379\n",
      "Train Epoch: 002 Batch: 00043/00094 | Loss: 0.3634 | EWC Loss: 0.0324 | CE Loss: 0.3309\n",
      "Train Epoch: 002 Batch: 00044/00094 | Loss: 0.1388 | EWC Loss: 0.0325 | CE Loss: 0.1063\n",
      "Train Epoch: 002 Batch: 00045/00094 | Loss: 0.2516 | EWC Loss: 0.0322 | CE Loss: 0.2194\n",
      "Train Epoch: 002 Batch: 00046/00094 | Loss: 0.2495 | EWC Loss: 0.0321 | CE Loss: 0.2174\n",
      "Train Epoch: 002 Batch: 00047/00094 | Loss: 0.2059 | EWC Loss: 0.0325 | CE Loss: 0.1735\n",
      "Train Epoch: 002 Batch: 00048/00094 | Loss: 0.0940 | EWC Loss: 0.0321 | CE Loss: 0.0619\n",
      "Train Epoch: 002 Batch: 00049/00094 | Loss: 0.1889 | EWC Loss: 0.0320 | CE Loss: 0.1570\n",
      "Train Epoch: 002 Batch: 00050/00094 | Loss: 0.2191 | EWC Loss: 0.0320 | CE Loss: 0.1871\n",
      "Train Epoch: 002 Batch: 00051/00094 | Loss: 0.2920 | EWC Loss: 0.0322 | CE Loss: 0.2599\n",
      "Train Epoch: 002 Batch: 00052/00094 | Loss: 0.2534 | EWC Loss: 0.0321 | CE Loss: 0.2213\n",
      "Train Epoch: 002 Batch: 00053/00094 | Loss: 0.2609 | EWC Loss: 0.0325 | CE Loss: 0.2284\n",
      "Train Epoch: 002 Batch: 00054/00094 | Loss: 0.1987 | EWC Loss: 0.0330 | CE Loss: 0.1657\n",
      "Train Epoch: 002 Batch: 00055/00094 | Loss: 0.2791 | EWC Loss: 0.0325 | CE Loss: 0.2466\n",
      "Train Epoch: 002 Batch: 00056/00094 | Loss: 0.1143 | EWC Loss: 0.0330 | CE Loss: 0.0813\n",
      "Train Epoch: 002 Batch: 00057/00094 | Loss: 0.1336 | EWC Loss: 0.0328 | CE Loss: 0.1007\n",
      "Train Epoch: 002 Batch: 00058/00094 | Loss: 0.2539 | EWC Loss: 0.0326 | CE Loss: 0.2212\n",
      "Train Epoch: 002 Batch: 00059/00094 | Loss: 0.3011 | EWC Loss: 0.0327 | CE Loss: 0.2684\n",
      "Train Epoch: 002 Batch: 00060/00094 | Loss: 0.2412 | EWC Loss: 0.0325 | CE Loss: 0.2087\n",
      "Train Epoch: 002 Batch: 00061/00094 | Loss: 0.1816 | EWC Loss: 0.0328 | CE Loss: 0.1487\n",
      "Train Epoch: 002 Batch: 00062/00094 | Loss: 0.1050 | EWC Loss: 0.0328 | CE Loss: 0.0722\n",
      "Train Epoch: 002 Batch: 00063/00094 | Loss: 0.1767 | EWC Loss: 0.0326 | CE Loss: 0.1441\n",
      "Train Epoch: 002 Batch: 00064/00094 | Loss: 0.1222 | EWC Loss: 0.0324 | CE Loss: 0.0898\n",
      "Train Epoch: 002 Batch: 00065/00094 | Loss: 0.1837 | EWC Loss: 0.0323 | CE Loss: 0.1514\n",
      "Train Epoch: 002 Batch: 00066/00094 | Loss: 0.2403 | EWC Loss: 0.0322 | CE Loss: 0.2081\n",
      "Train Epoch: 002 Batch: 00067/00094 | Loss: 0.1776 | EWC Loss: 0.0322 | CE Loss: 0.1453\n",
      "Train Epoch: 002 Batch: 00068/00094 | Loss: 0.1471 | EWC Loss: 0.0323 | CE Loss: 0.1148\n",
      "Train Epoch: 002 Batch: 00069/00094 | Loss: 0.1541 | EWC Loss: 0.0324 | CE Loss: 0.1217\n",
      "Train Epoch: 002 Batch: 00070/00094 | Loss: 0.1785 | EWC Loss: 0.0323 | CE Loss: 0.1462\n",
      "Train Epoch: 002 Batch: 00071/00094 | Loss: 0.2087 | EWC Loss: 0.0322 | CE Loss: 0.1765\n",
      "Train Epoch: 002 Batch: 00072/00094 | Loss: 0.1964 | EWC Loss: 0.0325 | CE Loss: 0.1638\n",
      "Train Epoch: 002 Batch: 00073/00094 | Loss: 0.2520 | EWC Loss: 0.0326 | CE Loss: 0.2195\n",
      "Train Epoch: 002 Batch: 00074/00094 | Loss: 0.2436 | EWC Loss: 0.0328 | CE Loss: 0.2108\n",
      "Train Epoch: 002 Batch: 00075/00094 | Loss: 0.2168 | EWC Loss: 0.0326 | CE Loss: 0.1842\n",
      "Train Epoch: 002 Batch: 00076/00094 | Loss: 0.2757 | EWC Loss: 0.0330 | CE Loss: 0.2427\n",
      "Train Epoch: 002 Batch: 00077/00094 | Loss: 0.1749 | EWC Loss: 0.0328 | CE Loss: 0.1421\n",
      "Train Epoch: 002 Batch: 00078/00094 | Loss: 0.2013 | EWC Loss: 0.0325 | CE Loss: 0.1688\n",
      "Train Epoch: 002 Batch: 00079/00094 | Loss: 0.2621 | EWC Loss: 0.0325 | CE Loss: 0.2296\n",
      "Train Epoch: 002 Batch: 00080/00094 | Loss: 0.2915 | EWC Loss: 0.0326 | CE Loss: 0.2589\n",
      "Train Epoch: 002 Batch: 00081/00094 | Loss: 0.1808 | EWC Loss: 0.0328 | CE Loss: 0.1480\n",
      "Train Epoch: 002 Batch: 00082/00094 | Loss: 0.1433 | EWC Loss: 0.0328 | CE Loss: 0.1105\n",
      "Train Epoch: 002 Batch: 00083/00094 | Loss: 0.1697 | EWC Loss: 0.0326 | CE Loss: 0.1370\n",
      "Train Epoch: 002 Batch: 00084/00094 | Loss: 0.1131 | EWC Loss: 0.0328 | CE Loss: 0.0804\n",
      "Train Epoch: 002 Batch: 00085/00094 | Loss: 0.1502 | EWC Loss: 0.0328 | CE Loss: 0.1175\n",
      "Train Epoch: 002 Batch: 00086/00094 | Loss: 0.1831 | EWC Loss: 0.0325 | CE Loss: 0.1506\n",
      "Train Epoch: 002 Batch: 00087/00094 | Loss: 0.1630 | EWC Loss: 0.0324 | CE Loss: 0.1306\n",
      "Train Epoch: 002 Batch: 00088/00094 | Loss: 0.2310 | EWC Loss: 0.0329 | CE Loss: 0.1981\n",
      "Train Epoch: 002 Batch: 00089/00094 | Loss: 0.1299 | EWC Loss: 0.0328 | CE Loss: 0.0972\n",
      "Train Epoch: 002 Batch: 00090/00094 | Loss: 0.1729 | EWC Loss: 0.0327 | CE Loss: 0.1402\n",
      "Train Epoch: 002 Batch: 00091/00094 | Loss: 0.2415 | EWC Loss: 0.0329 | CE Loss: 0.2086\n",
      "Train Epoch: 002 Batch: 00092/00094 | Loss: 0.3183 | EWC Loss: 0.0331 | CE Loss: 0.2852\n",
      "Train Epoch: 002 Batch: 00093/00094 | Loss: 0.2891 | EWC Loss: 0.0330 | CE Loss: 0.2561\n",
      "Train Epoch: 002 Batch: 00094/00094 | Loss: 0.4203 | EWC Loss: 0.0335 | CE Loss: 0.3868\n",
      "Train Epoch: 002 |Acc 92.76667 | Loss: 0.21413 | Task Loss 0.18172 | EWC Loss: 0.03242\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1698 | acc:93.9000\n",
      "[VAL Acc] Target: 93.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.4573 | acc:49.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.6767 | acc:53.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.7144 | acc:49.6183\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 49.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.3872 | acc:57.1708\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.17%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4168 | acc:83.0869\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 83.09%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0199 | acc:65.7915\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8325 | acc:66.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 66.50%\n",
      "[VAL Acc] Avg 64.86%\n",
      "VAL Acc improve from 64.70% to 64.86%\n",
      "Save best model\n",
      "\n",
      "\n",
      "---------- Starting epoch 3 ----------\n",
      "Train Epoch: 003 Batch: 00001/00094 | Loss: 0.1615 | EWC Loss: 0.0340 | CE Loss: 0.1275\n",
      "Train Epoch: 003 Batch: 00002/00094 | Loss: 0.2219 | EWC Loss: 0.0339 | CE Loss: 0.1880\n",
      "Train Epoch: 003 Batch: 00003/00094 | Loss: 0.2101 | EWC Loss: 0.0340 | CE Loss: 0.1761\n",
      "Train Epoch: 003 Batch: 00004/00094 | Loss: 0.2819 | EWC Loss: 0.0337 | CE Loss: 0.2483\n",
      "Train Epoch: 003 Batch: 00005/00094 | Loss: 0.0964 | EWC Loss: 0.0340 | CE Loss: 0.0624\n",
      "Train Epoch: 003 Batch: 00006/00094 | Loss: 0.3100 | EWC Loss: 0.0339 | CE Loss: 0.2761\n",
      "Train Epoch: 003 Batch: 00007/00094 | Loss: 0.2658 | EWC Loss: 0.0340 | CE Loss: 0.2318\n",
      "Train Epoch: 003 Batch: 00008/00094 | Loss: 0.2342 | EWC Loss: 0.0338 | CE Loss: 0.2004\n",
      "Train Epoch: 003 Batch: 00009/00094 | Loss: 0.2399 | EWC Loss: 0.0340 | CE Loss: 0.2060\n",
      "Train Epoch: 003 Batch: 00010/00094 | Loss: 0.2059 | EWC Loss: 0.0339 | CE Loss: 0.1720\n",
      "Train Epoch: 003 Batch: 00011/00094 | Loss: 0.0860 | EWC Loss: 0.0337 | CE Loss: 0.0524\n",
      "Train Epoch: 003 Batch: 00012/00094 | Loss: 0.2122 | EWC Loss: 0.0336 | CE Loss: 0.1785\n",
      "Train Epoch: 003 Batch: 00013/00094 | Loss: 0.1863 | EWC Loss: 0.0335 | CE Loss: 0.1528\n",
      "Train Epoch: 003 Batch: 00014/00094 | Loss: 0.2835 | EWC Loss: 0.0334 | CE Loss: 0.2501\n",
      "Train Epoch: 003 Batch: 00015/00094 | Loss: 0.2407 | EWC Loss: 0.0336 | CE Loss: 0.2072\n",
      "Train Epoch: 003 Batch: 00016/00094 | Loss: 0.1863 | EWC Loss: 0.0335 | CE Loss: 0.1527\n",
      "Train Epoch: 003 Batch: 00017/00094 | Loss: 0.1998 | EWC Loss: 0.0336 | CE Loss: 0.1662\n",
      "Train Epoch: 003 Batch: 00018/00094 | Loss: 0.2412 | EWC Loss: 0.0336 | CE Loss: 0.2076\n",
      "Train Epoch: 003 Batch: 00019/00094 | Loss: 0.3425 | EWC Loss: 0.0337 | CE Loss: 0.3088\n",
      "Train Epoch: 003 Batch: 00020/00094 | Loss: 0.1826 | EWC Loss: 0.0336 | CE Loss: 0.1491\n",
      "Train Epoch: 003 Batch: 00021/00094 | Loss: 0.2238 | EWC Loss: 0.0337 | CE Loss: 0.1901\n",
      "Train Epoch: 003 Batch: 00022/00094 | Loss: 0.1334 | EWC Loss: 0.0338 | CE Loss: 0.0996\n",
      "Train Epoch: 003 Batch: 00023/00094 | Loss: 0.1233 | EWC Loss: 0.0337 | CE Loss: 0.0895\n",
      "Train Epoch: 003 Batch: 00024/00094 | Loss: 0.1823 | EWC Loss: 0.0335 | CE Loss: 0.1488\n",
      "Train Epoch: 003 Batch: 00025/00094 | Loss: 0.1592 | EWC Loss: 0.0334 | CE Loss: 0.1258\n",
      "Train Epoch: 003 Batch: 00026/00094 | Loss: 0.1341 | EWC Loss: 0.0333 | CE Loss: 0.1008\n",
      "Train Epoch: 003 Batch: 00027/00094 | Loss: 0.0795 | EWC Loss: 0.0334 | CE Loss: 0.0462\n",
      "Train Epoch: 003 Batch: 00028/00094 | Loss: 0.1944 | EWC Loss: 0.0332 | CE Loss: 0.1612\n",
      "Train Epoch: 003 Batch: 00029/00094 | Loss: 0.1692 | EWC Loss: 0.0330 | CE Loss: 0.1362\n",
      "Train Epoch: 003 Batch: 00030/00094 | Loss: 0.2095 | EWC Loss: 0.0331 | CE Loss: 0.1764\n",
      "Train Epoch: 003 Batch: 00031/00094 | Loss: 0.2137 | EWC Loss: 0.0329 | CE Loss: 0.1808\n",
      "Train Epoch: 003 Batch: 00032/00094 | Loss: 0.1349 | EWC Loss: 0.0328 | CE Loss: 0.1021\n",
      "Train Epoch: 003 Batch: 00033/00094 | Loss: 0.1333 | EWC Loss: 0.0328 | CE Loss: 0.1004\n",
      "Train Epoch: 003 Batch: 00034/00094 | Loss: 0.1521 | EWC Loss: 0.0327 | CE Loss: 0.1194\n",
      "Train Epoch: 003 Batch: 00035/00094 | Loss: 0.1969 | EWC Loss: 0.0329 | CE Loss: 0.1640\n",
      "Train Epoch: 003 Batch: 00036/00094 | Loss: 0.2368 | EWC Loss: 0.0331 | CE Loss: 0.2038\n",
      "Train Epoch: 003 Batch: 00037/00094 | Loss: 0.2932 | EWC Loss: 0.0333 | CE Loss: 0.2598\n",
      "Train Epoch: 003 Batch: 00038/00094 | Loss: 0.1134 | EWC Loss: 0.0332 | CE Loss: 0.0802\n",
      "Train Epoch: 003 Batch: 00039/00094 | Loss: 0.2614 | EWC Loss: 0.0333 | CE Loss: 0.2281\n",
      "Train Epoch: 003 Batch: 00040/00094 | Loss: 0.1597 | EWC Loss: 0.0334 | CE Loss: 0.1263\n",
      "Train Epoch: 003 Batch: 00041/00094 | Loss: 0.1674 | EWC Loss: 0.0333 | CE Loss: 0.1340\n",
      "Train Epoch: 003 Batch: 00042/00094 | Loss: 0.1521 | EWC Loss: 0.0334 | CE Loss: 0.1187\n",
      "Train Epoch: 003 Batch: 00043/00094 | Loss: 0.0777 | EWC Loss: 0.0331 | CE Loss: 0.0446\n",
      "Train Epoch: 003 Batch: 00044/00094 | Loss: 0.1496 | EWC Loss: 0.0330 | CE Loss: 0.1166\n",
      "Train Epoch: 003 Batch: 00045/00094 | Loss: 0.3148 | EWC Loss: 0.0331 | CE Loss: 0.2817\n",
      "Train Epoch: 003 Batch: 00046/00094 | Loss: 0.1957 | EWC Loss: 0.0332 | CE Loss: 0.1625\n",
      "Train Epoch: 003 Batch: 00047/00094 | Loss: 0.1441 | EWC Loss: 0.0333 | CE Loss: 0.1108\n",
      "Train Epoch: 003 Batch: 00048/00094 | Loss: 0.1643 | EWC Loss: 0.0331 | CE Loss: 0.1312\n",
      "Train Epoch: 003 Batch: 00049/00094 | Loss: 0.1369 | EWC Loss: 0.0332 | CE Loss: 0.1038\n",
      "Train Epoch: 003 Batch: 00050/00094 | Loss: 0.2207 | EWC Loss: 0.0330 | CE Loss: 0.1877\n",
      "Train Epoch: 003 Batch: 00051/00094 | Loss: 0.1851 | EWC Loss: 0.0329 | CE Loss: 0.1523\n",
      "Train Epoch: 003 Batch: 00052/00094 | Loss: 0.1224 | EWC Loss: 0.0327 | CE Loss: 0.0897\n",
      "Train Epoch: 003 Batch: 00053/00094 | Loss: 0.2561 | EWC Loss: 0.0325 | CE Loss: 0.2236\n",
      "Train Epoch: 003 Batch: 00054/00094 | Loss: 0.2970 | EWC Loss: 0.0326 | CE Loss: 0.2644\n",
      "Train Epoch: 003 Batch: 00055/00094 | Loss: 0.2069 | EWC Loss: 0.0329 | CE Loss: 0.1740\n",
      "Train Epoch: 003 Batch: 00056/00094 | Loss: 0.3016 | EWC Loss: 0.0330 | CE Loss: 0.2687\n",
      "Train Epoch: 003 Batch: 00057/00094 | Loss: 0.1496 | EWC Loss: 0.0328 | CE Loss: 0.1168\n",
      "Train Epoch: 003 Batch: 00058/00094 | Loss: 0.2413 | EWC Loss: 0.0328 | CE Loss: 0.2085\n",
      "Train Epoch: 003 Batch: 00059/00094 | Loss: 0.1139 | EWC Loss: 0.0332 | CE Loss: 0.0807\n",
      "Train Epoch: 003 Batch: 00060/00094 | Loss: 0.2943 | EWC Loss: 0.0330 | CE Loss: 0.2613\n",
      "Train Epoch: 003 Batch: 00061/00094 | Loss: 0.2292 | EWC Loss: 0.0336 | CE Loss: 0.1956\n",
      "Train Epoch: 003 Batch: 00062/00094 | Loss: 0.1571 | EWC Loss: 0.0336 | CE Loss: 0.1235\n",
      "Train Epoch: 003 Batch: 00063/00094 | Loss: 0.1746 | EWC Loss: 0.0334 | CE Loss: 0.1412\n",
      "Train Epoch: 003 Batch: 00064/00094 | Loss: 0.2518 | EWC Loss: 0.0335 | CE Loss: 0.2184\n",
      "Train Epoch: 003 Batch: 00065/00094 | Loss: 0.2287 | EWC Loss: 0.0335 | CE Loss: 0.1952\n",
      "Train Epoch: 003 Batch: 00066/00094 | Loss: 0.2946 | EWC Loss: 0.0341 | CE Loss: 0.2605\n",
      "Train Epoch: 003 Batch: 00067/00094 | Loss: 0.1517 | EWC Loss: 0.0341 | CE Loss: 0.1176\n",
      "Train Epoch: 003 Batch: 00068/00094 | Loss: 0.1535 | EWC Loss: 0.0339 | CE Loss: 0.1196\n",
      "Train Epoch: 003 Batch: 00069/00094 | Loss: 0.1847 | EWC Loss: 0.0338 | CE Loss: 0.1508\n",
      "Train Epoch: 003 Batch: 00070/00094 | Loss: 0.1500 | EWC Loss: 0.0337 | CE Loss: 0.1163\n",
      "Train Epoch: 003 Batch: 00071/00094 | Loss: 0.1128 | EWC Loss: 0.0337 | CE Loss: 0.0791\n",
      "Train Epoch: 003 Batch: 00072/00094 | Loss: 0.1331 | EWC Loss: 0.0336 | CE Loss: 0.0994\n",
      "Train Epoch: 003 Batch: 00073/00094 | Loss: 0.1502 | EWC Loss: 0.0335 | CE Loss: 0.1167\n",
      "Train Epoch: 003 Batch: 00074/00094 | Loss: 0.1467 | EWC Loss: 0.0335 | CE Loss: 0.1133\n",
      "Train Epoch: 003 Batch: 00075/00094 | Loss: 0.1440 | EWC Loss: 0.0335 | CE Loss: 0.1104\n",
      "Train Epoch: 003 Batch: 00076/00094 | Loss: 0.1524 | EWC Loss: 0.0336 | CE Loss: 0.1188\n",
      "Train Epoch: 003 Batch: 00077/00094 | Loss: 0.2067 | EWC Loss: 0.0335 | CE Loss: 0.1732\n",
      "Train Epoch: 003 Batch: 00078/00094 | Loss: 0.2136 | EWC Loss: 0.0335 | CE Loss: 0.1802\n",
      "Train Epoch: 003 Batch: 00079/00094 | Loss: 0.1348 | EWC Loss: 0.0343 | CE Loss: 0.1005\n",
      "Train Epoch: 003 Batch: 00080/00094 | Loss: 0.2293 | EWC Loss: 0.0340 | CE Loss: 0.1953\n",
      "Train Epoch: 003 Batch: 00081/00094 | Loss: 0.0949 | EWC Loss: 0.0340 | CE Loss: 0.0608\n",
      "Train Epoch: 003 Batch: 00082/00094 | Loss: 0.2408 | EWC Loss: 0.0339 | CE Loss: 0.2070\n",
      "Train Epoch: 003 Batch: 00083/00094 | Loss: 0.2453 | EWC Loss: 0.0339 | CE Loss: 0.2114\n",
      "Train Epoch: 003 Batch: 00084/00094 | Loss: 0.1090 | EWC Loss: 0.0340 | CE Loss: 0.0749\n",
      "Train Epoch: 003 Batch: 00085/00094 | Loss: 0.1292 | EWC Loss: 0.0338 | CE Loss: 0.0955\n",
      "Train Epoch: 003 Batch: 00086/00094 | Loss: 0.1457 | EWC Loss: 0.0339 | CE Loss: 0.1118\n",
      "Train Epoch: 003 Batch: 00087/00094 | Loss: 0.1356 | EWC Loss: 0.0336 | CE Loss: 0.1020\n",
      "Train Epoch: 003 Batch: 00088/00094 | Loss: 0.2425 | EWC Loss: 0.0333 | CE Loss: 0.2092\n",
      "Train Epoch: 003 Batch: 00089/00094 | Loss: 0.1572 | EWC Loss: 0.0333 | CE Loss: 0.1239\n",
      "Train Epoch: 003 Batch: 00090/00094 | Loss: 0.1521 | EWC Loss: 0.0332 | CE Loss: 0.1189\n",
      "Train Epoch: 003 Batch: 00091/00094 | Loss: 0.1433 | EWC Loss: 0.0330 | CE Loss: 0.1103\n",
      "Train Epoch: 003 Batch: 00092/00094 | Loss: 0.1746 | EWC Loss: 0.0332 | CE Loss: 0.1414\n",
      "Train Epoch: 003 Batch: 00093/00094 | Loss: 0.2137 | EWC Loss: 0.0329 | CE Loss: 0.1808\n",
      "Train Epoch: 003 Batch: 00094/00094 | Loss: 0.2620 | EWC Loss: 0.0329 | CE Loss: 0.2292\n",
      "Train Epoch: 003 |Acc 94.28333 | Loss: 0.18968 | Task Loss 0.15625 | EWC Loss: 0.03343\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1266 | acc:95.5000\n",
      "[VAL Acc] Target: 95.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8013 | acc:49.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8694 | acc:53.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 53.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.0294 | acc:45.6107\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6419 | acc:55.5643\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5252 | acc:78.2810\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1439 | acc:64.6944\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8554 | acc:65.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 65.38%\n",
      "[VAL Acc] Avg 63.47%\n",
      "\n",
      "\n",
      "---------- Starting epoch 4 ----------\n",
      "Train Epoch: 004 Batch: 00001/00094 | Loss: 0.1380 | EWC Loss: 0.0327 | CE Loss: 0.1053\n",
      "Train Epoch: 004 Batch: 00002/00094 | Loss: 0.2059 | EWC Loss: 0.0327 | CE Loss: 0.1733\n",
      "Train Epoch: 004 Batch: 00003/00094 | Loss: 0.1766 | EWC Loss: 0.0328 | CE Loss: 0.1438\n",
      "Train Epoch: 004 Batch: 00004/00094 | Loss: 0.2927 | EWC Loss: 0.0328 | CE Loss: 0.2599\n",
      "Train Epoch: 004 Batch: 00005/00094 | Loss: 0.1322 | EWC Loss: 0.0328 | CE Loss: 0.0994\n",
      "Train Epoch: 004 Batch: 00006/00094 | Loss: 0.1687 | EWC Loss: 0.0327 | CE Loss: 0.1360\n",
      "Train Epoch: 004 Batch: 00007/00094 | Loss: 0.1647 | EWC Loss: 0.0326 | CE Loss: 0.1321\n",
      "Train Epoch: 004 Batch: 00008/00094 | Loss: 0.1738 | EWC Loss: 0.0325 | CE Loss: 0.1413\n",
      "Train Epoch: 004 Batch: 00009/00094 | Loss: 0.1079 | EWC Loss: 0.0325 | CE Loss: 0.0754\n",
      "Train Epoch: 004 Batch: 00010/00094 | Loss: 0.1399 | EWC Loss: 0.0324 | CE Loss: 0.1075\n",
      "Train Epoch: 004 Batch: 00011/00094 | Loss: 0.2767 | EWC Loss: 0.0323 | CE Loss: 0.2444\n",
      "Train Epoch: 004 Batch: 00012/00094 | Loss: 0.1102 | EWC Loss: 0.0324 | CE Loss: 0.0778\n",
      "Train Epoch: 004 Batch: 00013/00094 | Loss: 0.1776 | EWC Loss: 0.0323 | CE Loss: 0.1454\n",
      "Train Epoch: 004 Batch: 00014/00094 | Loss: 0.1613 | EWC Loss: 0.0323 | CE Loss: 0.1291\n",
      "Train Epoch: 004 Batch: 00015/00094 | Loss: 0.2009 | EWC Loss: 0.0322 | CE Loss: 0.1687\n",
      "Train Epoch: 004 Batch: 00016/00094 | Loss: 0.0914 | EWC Loss: 0.0322 | CE Loss: 0.0592\n",
      "Train Epoch: 004 Batch: 00017/00094 | Loss: 0.2369 | EWC Loss: 0.0321 | CE Loss: 0.2048\n",
      "Train Epoch: 004 Batch: 00018/00094 | Loss: 0.1698 | EWC Loss: 0.0318 | CE Loss: 0.1380\n",
      "Train Epoch: 004 Batch: 00019/00094 | Loss: 0.2188 | EWC Loss: 0.0319 | CE Loss: 0.1869\n",
      "Train Epoch: 004 Batch: 00020/00094 | Loss: 0.1672 | EWC Loss: 0.0318 | CE Loss: 0.1354\n",
      "Train Epoch: 004 Batch: 00021/00094 | Loss: 0.1758 | EWC Loss: 0.0317 | CE Loss: 0.1441\n",
      "Train Epoch: 004 Batch: 00022/00094 | Loss: 0.1676 | EWC Loss: 0.0318 | CE Loss: 0.1358\n",
      "Train Epoch: 004 Batch: 00023/00094 | Loss: 0.2239 | EWC Loss: 0.0317 | CE Loss: 0.1922\n",
      "Train Epoch: 004 Batch: 00024/00094 | Loss: 0.1326 | EWC Loss: 0.0318 | CE Loss: 0.1007\n",
      "Train Epoch: 004 Batch: 00025/00094 | Loss: 0.2432 | EWC Loss: 0.0318 | CE Loss: 0.2114\n",
      "Train Epoch: 004 Batch: 00026/00094 | Loss: 0.1800 | EWC Loss: 0.0320 | CE Loss: 0.1481\n",
      "Train Epoch: 004 Batch: 00027/00094 | Loss: 0.1994 | EWC Loss: 0.0322 | CE Loss: 0.1672\n",
      "Train Epoch: 004 Batch: 00028/00094 | Loss: 0.1913 | EWC Loss: 0.0322 | CE Loss: 0.1591\n",
      "Train Epoch: 004 Batch: 00029/00094 | Loss: 0.1432 | EWC Loss: 0.0322 | CE Loss: 0.1110\n",
      "Train Epoch: 004 Batch: 00030/00094 | Loss: 0.2604 | EWC Loss: 0.0321 | CE Loss: 0.2282\n",
      "Train Epoch: 004 Batch: 00031/00094 | Loss: 0.1685 | EWC Loss: 0.0322 | CE Loss: 0.1363\n",
      "Train Epoch: 004 Batch: 00032/00094 | Loss: 0.2065 | EWC Loss: 0.0321 | CE Loss: 0.1744\n",
      "Train Epoch: 004 Batch: 00033/00094 | Loss: 0.1414 | EWC Loss: 0.0321 | CE Loss: 0.1093\n",
      "Train Epoch: 004 Batch: 00034/00094 | Loss: 0.2634 | EWC Loss: 0.0321 | CE Loss: 0.2313\n",
      "Train Epoch: 004 Batch: 00035/00094 | Loss: 0.1167 | EWC Loss: 0.0322 | CE Loss: 0.0846\n",
      "Train Epoch: 004 Batch: 00036/00094 | Loss: 0.0769 | EWC Loss: 0.0320 | CE Loss: 0.0449\n",
      "Train Epoch: 004 Batch: 00037/00094 | Loss: 0.1522 | EWC Loss: 0.0320 | CE Loss: 0.1203\n",
      "Train Epoch: 004 Batch: 00038/00094 | Loss: 0.2322 | EWC Loss: 0.0322 | CE Loss: 0.2000\n",
      "Train Epoch: 004 Batch: 00039/00094 | Loss: 0.0857 | EWC Loss: 0.0320 | CE Loss: 0.0537\n",
      "Train Epoch: 004 Batch: 00040/00094 | Loss: 0.1184 | EWC Loss: 0.0319 | CE Loss: 0.0865\n",
      "Train Epoch: 004 Batch: 00041/00094 | Loss: 0.1381 | EWC Loss: 0.0319 | CE Loss: 0.1062\n",
      "Train Epoch: 004 Batch: 00042/00094 | Loss: 0.1315 | EWC Loss: 0.0319 | CE Loss: 0.0996\n",
      "Train Epoch: 004 Batch: 00043/00094 | Loss: 0.1959 | EWC Loss: 0.0319 | CE Loss: 0.1641\n",
      "Train Epoch: 004 Batch: 00044/00094 | Loss: 0.2309 | EWC Loss: 0.0320 | CE Loss: 0.1989\n",
      "Train Epoch: 004 Batch: 00045/00094 | Loss: 0.1340 | EWC Loss: 0.0323 | CE Loss: 0.1017\n",
      "Train Epoch: 004 Batch: 00046/00094 | Loss: 0.1100 | EWC Loss: 0.0320 | CE Loss: 0.0780\n",
      "Train Epoch: 004 Batch: 00047/00094 | Loss: 0.1765 | EWC Loss: 0.0319 | CE Loss: 0.1446\n",
      "Train Epoch: 004 Batch: 00048/00094 | Loss: 0.1738 | EWC Loss: 0.0320 | CE Loss: 0.1418\n",
      "Train Epoch: 004 Batch: 00049/00094 | Loss: 0.1607 | EWC Loss: 0.0321 | CE Loss: 0.1286\n",
      "Train Epoch: 004 Batch: 00050/00094 | Loss: 0.1678 | EWC Loss: 0.0322 | CE Loss: 0.1355\n",
      "Train Epoch: 004 Batch: 00051/00094 | Loss: 0.1323 | EWC Loss: 0.0323 | CE Loss: 0.1000\n",
      "Train Epoch: 004 Batch: 00052/00094 | Loss: 0.2705 | EWC Loss: 0.0324 | CE Loss: 0.2381\n",
      "Train Epoch: 004 Batch: 00053/00094 | Loss: 0.1039 | EWC Loss: 0.0325 | CE Loss: 0.0714\n",
      "Train Epoch: 004 Batch: 00054/00094 | Loss: 0.2227 | EWC Loss: 0.0324 | CE Loss: 0.1903\n",
      "Train Epoch: 004 Batch: 00055/00094 | Loss: 0.1229 | EWC Loss: 0.0322 | CE Loss: 0.0908\n",
      "Train Epoch: 004 Batch: 00056/00094 | Loss: 0.1220 | EWC Loss: 0.0322 | CE Loss: 0.0899\n",
      "Train Epoch: 004 Batch: 00057/00094 | Loss: 0.0916 | EWC Loss: 0.0322 | CE Loss: 0.0595\n",
      "Train Epoch: 004 Batch: 00058/00094 | Loss: 0.1699 | EWC Loss: 0.0321 | CE Loss: 0.1378\n",
      "Train Epoch: 004 Batch: 00059/00094 | Loss: 0.1137 | EWC Loss: 0.0321 | CE Loss: 0.0815\n",
      "Train Epoch: 004 Batch: 00060/00094 | Loss: 0.0997 | EWC Loss: 0.0321 | CE Loss: 0.0676\n",
      "Train Epoch: 004 Batch: 00061/00094 | Loss: 0.1395 | EWC Loss: 0.0320 | CE Loss: 0.1075\n",
      "Train Epoch: 004 Batch: 00062/00094 | Loss: 0.1988 | EWC Loss: 0.0321 | CE Loss: 0.1667\n",
      "Train Epoch: 004 Batch: 00063/00094 | Loss: 0.0948 | EWC Loss: 0.0319 | CE Loss: 0.0630\n",
      "Train Epoch: 004 Batch: 00064/00094 | Loss: 0.1430 | EWC Loss: 0.0318 | CE Loss: 0.1112\n",
      "Train Epoch: 004 Batch: 00065/00094 | Loss: 0.1031 | EWC Loss: 0.0317 | CE Loss: 0.0714\n",
      "Train Epoch: 004 Batch: 00066/00094 | Loss: 0.1628 | EWC Loss: 0.0317 | CE Loss: 0.1311\n",
      "Train Epoch: 004 Batch: 00067/00094 | Loss: 0.0976 | EWC Loss: 0.0316 | CE Loss: 0.0660\n",
      "Train Epoch: 004 Batch: 00068/00094 | Loss: 0.1185 | EWC Loss: 0.0316 | CE Loss: 0.0869\n",
      "Train Epoch: 004 Batch: 00069/00094 | Loss: 0.1671 | EWC Loss: 0.0316 | CE Loss: 0.1355\n",
      "Train Epoch: 004 Batch: 00070/00094 | Loss: 0.1541 | EWC Loss: 0.0316 | CE Loss: 0.1225\n",
      "Train Epoch: 004 Batch: 00071/00094 | Loss: 0.1635 | EWC Loss: 0.0316 | CE Loss: 0.1319\n",
      "Train Epoch: 004 Batch: 00072/00094 | Loss: 0.2077 | EWC Loss: 0.0315 | CE Loss: 0.1762\n",
      "Train Epoch: 004 Batch: 00073/00094 | Loss: 0.0922 | EWC Loss: 0.0316 | CE Loss: 0.0607\n",
      "Train Epoch: 004 Batch: 00074/00094 | Loss: 0.2217 | EWC Loss: 0.0315 | CE Loss: 0.1902\n",
      "Train Epoch: 004 Batch: 00075/00094 | Loss: 0.1485 | EWC Loss: 0.0314 | CE Loss: 0.1171\n",
      "Train Epoch: 004 Batch: 00076/00094 | Loss: 0.1044 | EWC Loss: 0.0317 | CE Loss: 0.0728\n",
      "Train Epoch: 004 Batch: 00077/00094 | Loss: 0.2489 | EWC Loss: 0.0316 | CE Loss: 0.2173\n",
      "Train Epoch: 004 Batch: 00078/00094 | Loss: 0.1876 | EWC Loss: 0.0316 | CE Loss: 0.1561\n",
      "Train Epoch: 004 Batch: 00079/00094 | Loss: 0.1780 | EWC Loss: 0.0315 | CE Loss: 0.1465\n",
      "Train Epoch: 004 Batch: 00080/00094 | Loss: 0.1872 | EWC Loss: 0.0313 | CE Loss: 0.1559\n",
      "Train Epoch: 004 Batch: 00081/00094 | Loss: 0.1933 | EWC Loss: 0.0315 | CE Loss: 0.1618\n",
      "Train Epoch: 004 Batch: 00082/00094 | Loss: 0.1085 | EWC Loss: 0.0316 | CE Loss: 0.0769\n",
      "Train Epoch: 004 Batch: 00083/00094 | Loss: 0.2525 | EWC Loss: 0.0316 | CE Loss: 0.2209\n",
      "Train Epoch: 004 Batch: 00084/00094 | Loss: 0.1062 | EWC Loss: 0.0315 | CE Loss: 0.0747\n",
      "Train Epoch: 004 Batch: 00085/00094 | Loss: 0.0982 | EWC Loss: 0.0316 | CE Loss: 0.0665\n",
      "Train Epoch: 004 Batch: 00086/00094 | Loss: 0.1187 | EWC Loss: 0.0316 | CE Loss: 0.0871\n",
      "Train Epoch: 004 Batch: 00087/00094 | Loss: 0.2015 | EWC Loss: 0.0316 | CE Loss: 0.1699\n",
      "Train Epoch: 004 Batch: 00088/00094 | Loss: 0.2024 | EWC Loss: 0.0319 | CE Loss: 0.1706\n",
      "Train Epoch: 004 Batch: 00089/00094 | Loss: 0.2173 | EWC Loss: 0.0318 | CE Loss: 0.1856\n",
      "Train Epoch: 004 Batch: 00090/00094 | Loss: 0.2777 | EWC Loss: 0.0317 | CE Loss: 0.2460\n",
      "Train Epoch: 004 Batch: 00091/00094 | Loss: 0.1275 | EWC Loss: 0.0317 | CE Loss: 0.0958\n",
      "Train Epoch: 004 Batch: 00092/00094 | Loss: 0.1609 | EWC Loss: 0.0316 | CE Loss: 0.1294\n",
      "Train Epoch: 004 Batch: 00093/00094 | Loss: 0.1672 | EWC Loss: 0.0315 | CE Loss: 0.1357\n",
      "Train Epoch: 004 Batch: 00094/00094 | Loss: 0.1299 | EWC Loss: 0.0316 | CE Loss: 0.0983\n",
      "Train Epoch: 004 |Acc 94.70000 | Loss: 0.16532 | Task Loss 0.13335 | EWC Loss: 0.03197\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1300 | acc:95.0500\n",
      "[VAL Acc] Target: 95.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8935 | acc:48.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9508 | acc:52.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.0257 | acc:46.1832\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.7382 | acc:54.1144\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4915 | acc:79.8521\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 79.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.2335 | acc:64.9295\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.93%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9069 | acc:64.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.56%\n",
      "[VAL Acc] Avg 63.22%\n",
      "\n",
      "\n",
      "---------- Starting epoch 5 ----------\n",
      "Train Epoch: 005 Batch: 00001/00094 | Loss: 0.0976 | EWC Loss: 0.0315 | CE Loss: 0.0661\n",
      "Train Epoch: 005 Batch: 00002/00094 | Loss: 0.1474 | EWC Loss: 0.0314 | CE Loss: 0.1160\n",
      "Train Epoch: 005 Batch: 00003/00094 | Loss: 0.1609 | EWC Loss: 0.0314 | CE Loss: 0.1294\n",
      "Train Epoch: 005 Batch: 00004/00094 | Loss: 0.1021 | EWC Loss: 0.0314 | CE Loss: 0.0707\n",
      "Train Epoch: 005 Batch: 00005/00094 | Loss: 0.0636 | EWC Loss: 0.0314 | CE Loss: 0.0322\n",
      "Train Epoch: 005 Batch: 00006/00094 | Loss: 0.0603 | EWC Loss: 0.0313 | CE Loss: 0.0291\n",
      "Train Epoch: 005 Batch: 00007/00094 | Loss: 0.1724 | EWC Loss: 0.0312 | CE Loss: 0.1412\n",
      "Train Epoch: 005 Batch: 00008/00094 | Loss: 0.1302 | EWC Loss: 0.0311 | CE Loss: 0.0991\n",
      "Train Epoch: 005 Batch: 00009/00094 | Loss: 0.1174 | EWC Loss: 0.0312 | CE Loss: 0.0862\n",
      "Train Epoch: 005 Batch: 00010/00094 | Loss: 0.1021 | EWC Loss: 0.0312 | CE Loss: 0.0710\n",
      "Train Epoch: 005 Batch: 00011/00094 | Loss: 0.1142 | EWC Loss: 0.0312 | CE Loss: 0.0830\n",
      "Train Epoch: 005 Batch: 00012/00094 | Loss: 0.1438 | EWC Loss: 0.0310 | CE Loss: 0.1128\n",
      "Train Epoch: 005 Batch: 00013/00094 | Loss: 0.2423 | EWC Loss: 0.0310 | CE Loss: 0.2113\n",
      "Train Epoch: 005 Batch: 00014/00094 | Loss: 0.0893 | EWC Loss: 0.0309 | CE Loss: 0.0585\n",
      "Train Epoch: 005 Batch: 00015/00094 | Loss: 0.1631 | EWC Loss: 0.0308 | CE Loss: 0.1323\n",
      "Train Epoch: 005 Batch: 00016/00094 | Loss: 0.3423 | EWC Loss: 0.0309 | CE Loss: 0.3114\n",
      "Train Epoch: 005 Batch: 00017/00094 | Loss: 0.1729 | EWC Loss: 0.0311 | CE Loss: 0.1418\n",
      "Train Epoch: 005 Batch: 00018/00094 | Loss: 0.2204 | EWC Loss: 0.0312 | CE Loss: 0.1891\n",
      "Train Epoch: 005 Batch: 00019/00094 | Loss: 0.1509 | EWC Loss: 0.0312 | CE Loss: 0.1197\n",
      "Train Epoch: 005 Batch: 00020/00094 | Loss: 0.2557 | EWC Loss: 0.0312 | CE Loss: 0.2245\n",
      "Train Epoch: 005 Batch: 00021/00094 | Loss: 0.1393 | EWC Loss: 0.0314 | CE Loss: 0.1079\n",
      "Train Epoch: 005 Batch: 00022/00094 | Loss: 0.1102 | EWC Loss: 0.0314 | CE Loss: 0.0788\n",
      "Train Epoch: 005 Batch: 00023/00094 | Loss: 0.1601 | EWC Loss: 0.0313 | CE Loss: 0.1288\n",
      "Train Epoch: 005 Batch: 00024/00094 | Loss: 0.0737 | EWC Loss: 0.0315 | CE Loss: 0.0422\n",
      "Train Epoch: 005 Batch: 00025/00094 | Loss: 0.0779 | EWC Loss: 0.0314 | CE Loss: 0.0465\n",
      "Train Epoch: 005 Batch: 00026/00094 | Loss: 0.0962 | EWC Loss: 0.0314 | CE Loss: 0.0649\n",
      "Train Epoch: 005 Batch: 00027/00094 | Loss: 0.0791 | EWC Loss: 0.0313 | CE Loss: 0.0478\n",
      "Train Epoch: 005 Batch: 00028/00094 | Loss: 0.1442 | EWC Loss: 0.0314 | CE Loss: 0.1128\n",
      "Train Epoch: 005 Batch: 00029/00094 | Loss: 0.1465 | EWC Loss: 0.0313 | CE Loss: 0.1153\n",
      "Train Epoch: 005 Batch: 00030/00094 | Loss: 0.0607 | EWC Loss: 0.0313 | CE Loss: 0.0294\n",
      "Train Epoch: 005 Batch: 00031/00094 | Loss: 0.2080 | EWC Loss: 0.0312 | CE Loss: 0.1768\n",
      "Train Epoch: 005 Batch: 00032/00094 | Loss: 0.3414 | EWC Loss: 0.0311 | CE Loss: 0.3103\n",
      "Train Epoch: 005 Batch: 00033/00094 | Loss: 0.1992 | EWC Loss: 0.0311 | CE Loss: 0.1681\n",
      "Train Epoch: 005 Batch: 00034/00094 | Loss: 0.1482 | EWC Loss: 0.0311 | CE Loss: 0.1171\n",
      "Train Epoch: 005 Batch: 00035/00094 | Loss: 0.1044 | EWC Loss: 0.0312 | CE Loss: 0.0732\n",
      "Train Epoch: 005 Batch: 00036/00094 | Loss: 0.1676 | EWC Loss: 0.0312 | CE Loss: 0.1365\n",
      "Train Epoch: 005 Batch: 00037/00094 | Loss: 0.1996 | EWC Loss: 0.0311 | CE Loss: 0.1685\n",
      "Train Epoch: 005 Batch: 00038/00094 | Loss: 0.0872 | EWC Loss: 0.0312 | CE Loss: 0.0560\n",
      "Train Epoch: 005 Batch: 00039/00094 | Loss: 0.0941 | EWC Loss: 0.0312 | CE Loss: 0.0630\n",
      "Train Epoch: 005 Batch: 00040/00094 | Loss: 0.1404 | EWC Loss: 0.0312 | CE Loss: 0.1093\n",
      "Train Epoch: 005 Batch: 00041/00094 | Loss: 0.1008 | EWC Loss: 0.0312 | CE Loss: 0.0696\n",
      "Train Epoch: 005 Batch: 00042/00094 | Loss: 0.1243 | EWC Loss: 0.0311 | CE Loss: 0.0931\n",
      "Train Epoch: 005 Batch: 00043/00094 | Loss: 0.1275 | EWC Loss: 0.0312 | CE Loss: 0.0964\n",
      "Train Epoch: 005 Batch: 00044/00094 | Loss: 0.0868 | EWC Loss: 0.0311 | CE Loss: 0.0557\n",
      "Train Epoch: 005 Batch: 00045/00094 | Loss: 0.2341 | EWC Loss: 0.0310 | CE Loss: 0.2031\n",
      "Train Epoch: 005 Batch: 00046/00094 | Loss: 0.0813 | EWC Loss: 0.0312 | CE Loss: 0.0501\n",
      "Train Epoch: 005 Batch: 00047/00094 | Loss: 0.2059 | EWC Loss: 0.0313 | CE Loss: 0.1746\n",
      "Train Epoch: 005 Batch: 00048/00094 | Loss: 0.1290 | EWC Loss: 0.0309 | CE Loss: 0.0981\n",
      "Train Epoch: 005 Batch: 00049/00094 | Loss: 0.1857 | EWC Loss: 0.0308 | CE Loss: 0.1549\n",
      "Train Epoch: 005 Batch: 00050/00094 | Loss: 0.0852 | EWC Loss: 0.0308 | CE Loss: 0.0544\n",
      "Train Epoch: 005 Batch: 00051/00094 | Loss: 0.1465 | EWC Loss: 0.0308 | CE Loss: 0.1157\n",
      "Train Epoch: 005 Batch: 00052/00094 | Loss: 0.2266 | EWC Loss: 0.0307 | CE Loss: 0.1958\n",
      "Train Epoch: 005 Batch: 00053/00094 | Loss: 0.1506 | EWC Loss: 0.0309 | CE Loss: 0.1197\n",
      "Train Epoch: 005 Batch: 00054/00094 | Loss: 0.1609 | EWC Loss: 0.0308 | CE Loss: 0.1301\n",
      "Train Epoch: 005 Batch: 00055/00094 | Loss: 0.1513 | EWC Loss: 0.0309 | CE Loss: 0.1204\n",
      "Train Epoch: 005 Batch: 00056/00094 | Loss: 0.2190 | EWC Loss: 0.0307 | CE Loss: 0.1883\n",
      "Train Epoch: 005 Batch: 00057/00094 | Loss: 0.1655 | EWC Loss: 0.0306 | CE Loss: 0.1349\n",
      "Train Epoch: 005 Batch: 00058/00094 | Loss: 0.1021 | EWC Loss: 0.0307 | CE Loss: 0.0715\n",
      "Train Epoch: 005 Batch: 00059/00094 | Loss: 0.1309 | EWC Loss: 0.0306 | CE Loss: 0.1003\n",
      "Train Epoch: 005 Batch: 00060/00094 | Loss: 0.2831 | EWC Loss: 0.0306 | CE Loss: 0.2525\n",
      "Train Epoch: 005 Batch: 00061/00094 | Loss: 0.2460 | EWC Loss: 0.0305 | CE Loss: 0.2155\n",
      "Train Epoch: 005 Batch: 00062/00094 | Loss: 0.1851 | EWC Loss: 0.0306 | CE Loss: 0.1544\n",
      "Train Epoch: 005 Batch: 00063/00094 | Loss: 0.1968 | EWC Loss: 0.0308 | CE Loss: 0.1660\n",
      "Train Epoch: 005 Batch: 00064/00094 | Loss: 0.0891 | EWC Loss: 0.0308 | CE Loss: 0.0584\n",
      "Train Epoch: 005 Batch: 00065/00094 | Loss: 0.1620 | EWC Loss: 0.0307 | CE Loss: 0.1313\n",
      "Train Epoch: 005 Batch: 00066/00094 | Loss: 0.2100 | EWC Loss: 0.0307 | CE Loss: 0.1794\n",
      "Train Epoch: 005 Batch: 00067/00094 | Loss: 0.1216 | EWC Loss: 0.0306 | CE Loss: 0.0909\n",
      "Train Epoch: 005 Batch: 00068/00094 | Loss: 0.1842 | EWC Loss: 0.0306 | CE Loss: 0.1535\n",
      "Train Epoch: 005 Batch: 00069/00094 | Loss: 0.1242 | EWC Loss: 0.0306 | CE Loss: 0.0936\n",
      "Train Epoch: 005 Batch: 00070/00094 | Loss: 0.1589 | EWC Loss: 0.0305 | CE Loss: 0.1284\n",
      "Train Epoch: 005 Batch: 00071/00094 | Loss: 0.0742 | EWC Loss: 0.0307 | CE Loss: 0.0435\n",
      "Train Epoch: 005 Batch: 00072/00094 | Loss: 0.1600 | EWC Loss: 0.0306 | CE Loss: 0.1294\n",
      "Train Epoch: 005 Batch: 00073/00094 | Loss: 0.2199 | EWC Loss: 0.0307 | CE Loss: 0.1892\n",
      "Train Epoch: 005 Batch: 00074/00094 | Loss: 0.0901 | EWC Loss: 0.0306 | CE Loss: 0.0596\n",
      "Train Epoch: 005 Batch: 00075/00094 | Loss: 0.0620 | EWC Loss: 0.0305 | CE Loss: 0.0315\n",
      "Train Epoch: 005 Batch: 00076/00094 | Loss: 0.1576 | EWC Loss: 0.0304 | CE Loss: 0.1272\n",
      "Train Epoch: 005 Batch: 00077/00094 | Loss: 0.2179 | EWC Loss: 0.0304 | CE Loss: 0.1875\n",
      "Train Epoch: 005 Batch: 00078/00094 | Loss: 0.1299 | EWC Loss: 0.0303 | CE Loss: 0.0996\n",
      "Train Epoch: 005 Batch: 00079/00094 | Loss: 0.2019 | EWC Loss: 0.0303 | CE Loss: 0.1716\n",
      "Train Epoch: 005 Batch: 00080/00094 | Loss: 0.1525 | EWC Loss: 0.0304 | CE Loss: 0.1221\n",
      "Train Epoch: 005 Batch: 00081/00094 | Loss: 0.2178 | EWC Loss: 0.0303 | CE Loss: 0.1876\n",
      "Train Epoch: 005 Batch: 00082/00094 | Loss: 0.1679 | EWC Loss: 0.0303 | CE Loss: 0.1376\n",
      "Train Epoch: 005 Batch: 00083/00094 | Loss: 0.1205 | EWC Loss: 0.0303 | CE Loss: 0.0902\n",
      "Train Epoch: 005 Batch: 00084/00094 | Loss: 0.1281 | EWC Loss: 0.0302 | CE Loss: 0.0979\n",
      "Train Epoch: 005 Batch: 00085/00094 | Loss: 0.0599 | EWC Loss: 0.0302 | CE Loss: 0.0298\n",
      "Train Epoch: 005 Batch: 00086/00094 | Loss: 0.1379 | EWC Loss: 0.0302 | CE Loss: 0.1077\n",
      "Train Epoch: 005 Batch: 00087/00094 | Loss: 0.1033 | EWC Loss: 0.0302 | CE Loss: 0.0731\n",
      "Train Epoch: 005 Batch: 00088/00094 | Loss: 0.1123 | EWC Loss: 0.0303 | CE Loss: 0.0820\n",
      "Train Epoch: 005 Batch: 00089/00094 | Loss: 0.0910 | EWC Loss: 0.0303 | CE Loss: 0.0606\n",
      "Train Epoch: 005 Batch: 00090/00094 | Loss: 0.2114 | EWC Loss: 0.0303 | CE Loss: 0.1811\n",
      "Train Epoch: 005 Batch: 00091/00094 | Loss: 0.1299 | EWC Loss: 0.0303 | CE Loss: 0.0996\n",
      "Train Epoch: 005 Batch: 00092/00094 | Loss: 0.1971 | EWC Loss: 0.0304 | CE Loss: 0.1667\n",
      "Train Epoch: 005 Batch: 00093/00094 | Loss: 0.1001 | EWC Loss: 0.0306 | CE Loss: 0.0695\n",
      "Train Epoch: 005 Batch: 00094/00094 | Loss: 0.1091 | EWC Loss: 0.0305 | CE Loss: 0.0786\n",
      "Train Epoch: 005 |Acc 95.13333 | Loss: 0.14845 | Task Loss 0.11758 | EWC Loss: 0.03087\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1089 | acc:95.7000\n",
      "[VAL Acc] Target: 95.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7504 | acc:48.4000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.7296 | acc:54.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.7857 | acc:46.1832\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.7126 | acc:54.4279\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6896 | acc:72.4584\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.46%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.2525 | acc:64.3025\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9451 | acc:64.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.25%\n",
      "[VAL Acc] Avg 62.56%\n",
      "\n",
      "\n",
      "---------- Starting epoch 6 ----------\n",
      "Train Epoch: 006 Batch: 00001/00094 | Loss: 0.1461 | EWC Loss: 0.0306 | CE Loss: 0.1155\n",
      "Train Epoch: 006 Batch: 00002/00094 | Loss: 0.1897 | EWC Loss: 0.0308 | CE Loss: 0.1589\n",
      "Train Epoch: 006 Batch: 00003/00094 | Loss: 0.1254 | EWC Loss: 0.0308 | CE Loss: 0.0946\n",
      "Train Epoch: 006 Batch: 00004/00094 | Loss: 0.1060 | EWC Loss: 0.0307 | CE Loss: 0.0752\n",
      "Train Epoch: 006 Batch: 00005/00094 | Loss: 0.2088 | EWC Loss: 0.0306 | CE Loss: 0.1782\n",
      "Train Epoch: 006 Batch: 00006/00094 | Loss: 0.1550 | EWC Loss: 0.0307 | CE Loss: 0.1243\n",
      "Train Epoch: 006 Batch: 00007/00094 | Loss: 0.0965 | EWC Loss: 0.0307 | CE Loss: 0.0658\n",
      "Train Epoch: 006 Batch: 00008/00094 | Loss: 0.2256 | EWC Loss: 0.0307 | CE Loss: 0.1949\n",
      "Train Epoch: 006 Batch: 00009/00094 | Loss: 0.1921 | EWC Loss: 0.0308 | CE Loss: 0.1613\n",
      "Train Epoch: 006 Batch: 00010/00094 | Loss: 0.2227 | EWC Loss: 0.0307 | CE Loss: 0.1920\n",
      "Train Epoch: 006 Batch: 00011/00094 | Loss: 0.2217 | EWC Loss: 0.0308 | CE Loss: 0.1909\n",
      "Train Epoch: 006 Batch: 00012/00094 | Loss: 0.1066 | EWC Loss: 0.0308 | CE Loss: 0.0758\n",
      "Train Epoch: 006 Batch: 00013/00094 | Loss: 0.1133 | EWC Loss: 0.0308 | CE Loss: 0.0826\n",
      "Train Epoch: 006 Batch: 00014/00094 | Loss: 0.1276 | EWC Loss: 0.0308 | CE Loss: 0.0968\n",
      "Train Epoch: 006 Batch: 00015/00094 | Loss: 0.1920 | EWC Loss: 0.0307 | CE Loss: 0.1613\n",
      "Train Epoch: 006 Batch: 00016/00094 | Loss: 0.1194 | EWC Loss: 0.0307 | CE Loss: 0.0886\n",
      "Train Epoch: 006 Batch: 00017/00094 | Loss: 0.1081 | EWC Loss: 0.0307 | CE Loss: 0.0774\n",
      "Train Epoch: 006 Batch: 00018/00094 | Loss: 0.1516 | EWC Loss: 0.0308 | CE Loss: 0.1209\n",
      "Train Epoch: 006 Batch: 00019/00094 | Loss: 0.1637 | EWC Loss: 0.0307 | CE Loss: 0.1330\n",
      "Train Epoch: 006 Batch: 00020/00094 | Loss: 0.0964 | EWC Loss: 0.0306 | CE Loss: 0.0658\n",
      "Train Epoch: 006 Batch: 00021/00094 | Loss: 0.1126 | EWC Loss: 0.0307 | CE Loss: 0.0819\n",
      "Train Epoch: 006 Batch: 00022/00094 | Loss: 0.1464 | EWC Loss: 0.0306 | CE Loss: 0.1157\n",
      "Train Epoch: 006 Batch: 00023/00094 | Loss: 0.1893 | EWC Loss: 0.0307 | CE Loss: 0.1587\n",
      "Train Epoch: 006 Batch: 00024/00094 | Loss: 0.1147 | EWC Loss: 0.0307 | CE Loss: 0.0841\n",
      "Train Epoch: 006 Batch: 00025/00094 | Loss: 0.3061 | EWC Loss: 0.0307 | CE Loss: 0.2754\n",
      "Train Epoch: 006 Batch: 00026/00094 | Loss: 0.1104 | EWC Loss: 0.0306 | CE Loss: 0.0798\n",
      "Train Epoch: 006 Batch: 00027/00094 | Loss: 0.1713 | EWC Loss: 0.0306 | CE Loss: 0.1407\n",
      "Train Epoch: 006 Batch: 00028/00094 | Loss: 0.1733 | EWC Loss: 0.0306 | CE Loss: 0.1427\n",
      "Train Epoch: 006 Batch: 00029/00094 | Loss: 0.1110 | EWC Loss: 0.0306 | CE Loss: 0.0804\n",
      "Train Epoch: 006 Batch: 00030/00094 | Loss: 0.0958 | EWC Loss: 0.0306 | CE Loss: 0.0652\n",
      "Train Epoch: 006 Batch: 00031/00094 | Loss: 0.2415 | EWC Loss: 0.0305 | CE Loss: 0.2110\n",
      "Train Epoch: 006 Batch: 00032/00094 | Loss: 0.3402 | EWC Loss: 0.0306 | CE Loss: 0.3096\n",
      "Train Epoch: 006 Batch: 00033/00094 | Loss: 0.1056 | EWC Loss: 0.0307 | CE Loss: 0.0749\n",
      "Train Epoch: 006 Batch: 00034/00094 | Loss: 0.1210 | EWC Loss: 0.0307 | CE Loss: 0.0903\n",
      "Train Epoch: 006 Batch: 00035/00094 | Loss: 0.2129 | EWC Loss: 0.0307 | CE Loss: 0.1822\n",
      "Train Epoch: 006 Batch: 00036/00094 | Loss: 0.1100 | EWC Loss: 0.0307 | CE Loss: 0.0794\n",
      "Train Epoch: 006 Batch: 00037/00094 | Loss: 0.1303 | EWC Loss: 0.0307 | CE Loss: 0.0995\n",
      "Train Epoch: 006 Batch: 00038/00094 | Loss: 0.1429 | EWC Loss: 0.0307 | CE Loss: 0.1122\n",
      "Train Epoch: 006 Batch: 00039/00094 | Loss: 0.2897 | EWC Loss: 0.0307 | CE Loss: 0.2589\n",
      "Train Epoch: 006 Batch: 00040/00094 | Loss: 0.1150 | EWC Loss: 0.0307 | CE Loss: 0.0842\n",
      "Train Epoch: 006 Batch: 00041/00094 | Loss: 0.2176 | EWC Loss: 0.0307 | CE Loss: 0.1869\n",
      "Train Epoch: 006 Batch: 00042/00094 | Loss: 0.1126 | EWC Loss: 0.0307 | CE Loss: 0.0819\n",
      "Train Epoch: 006 Batch: 00043/00094 | Loss: 0.2866 | EWC Loss: 0.0307 | CE Loss: 0.2558\n",
      "Train Epoch: 006 Batch: 00044/00094 | Loss: 0.0795 | EWC Loss: 0.0310 | CE Loss: 0.0485\n",
      "Train Epoch: 006 Batch: 00045/00094 | Loss: 0.1729 | EWC Loss: 0.0310 | CE Loss: 0.1420\n",
      "Train Epoch: 006 Batch: 00046/00094 | Loss: 0.1693 | EWC Loss: 0.0308 | CE Loss: 0.1385\n",
      "Train Epoch: 006 Batch: 00047/00094 | Loss: 0.1671 | EWC Loss: 0.0308 | CE Loss: 0.1364\n",
      "Train Epoch: 006 Batch: 00048/00094 | Loss: 0.0693 | EWC Loss: 0.0307 | CE Loss: 0.0386\n",
      "Train Epoch: 006 Batch: 00049/00094 | Loss: 0.1004 | EWC Loss: 0.0307 | CE Loss: 0.0697\n",
      "Train Epoch: 006 Batch: 00050/00094 | Loss: 0.0726 | EWC Loss: 0.0306 | CE Loss: 0.0420\n",
      "Train Epoch: 006 Batch: 00051/00094 | Loss: 0.1351 | EWC Loss: 0.0305 | CE Loss: 0.1046\n",
      "Train Epoch: 006 Batch: 00052/00094 | Loss: 0.0946 | EWC Loss: 0.0304 | CE Loss: 0.0642\n",
      "Train Epoch: 006 Batch: 00053/00094 | Loss: 0.1690 | EWC Loss: 0.0304 | CE Loss: 0.1386\n",
      "Train Epoch: 006 Batch: 00054/00094 | Loss: 0.1082 | EWC Loss: 0.0304 | CE Loss: 0.0777\n",
      "Train Epoch: 006 Batch: 00055/00094 | Loss: 0.0960 | EWC Loss: 0.0305 | CE Loss: 0.0655\n",
      "Train Epoch: 006 Batch: 00056/00094 | Loss: 0.1030 | EWC Loss: 0.0305 | CE Loss: 0.0725\n",
      "Train Epoch: 006 Batch: 00057/00094 | Loss: 0.1446 | EWC Loss: 0.0305 | CE Loss: 0.1141\n",
      "Train Epoch: 006 Batch: 00058/00094 | Loss: 0.1166 | EWC Loss: 0.0305 | CE Loss: 0.0862\n",
      "Train Epoch: 006 Batch: 00059/00094 | Loss: 0.2686 | EWC Loss: 0.0304 | CE Loss: 0.2382\n",
      "Train Epoch: 006 Batch: 00060/00094 | Loss: 0.1290 | EWC Loss: 0.0305 | CE Loss: 0.0986\n",
      "Train Epoch: 006 Batch: 00061/00094 | Loss: 0.3253 | EWC Loss: 0.0305 | CE Loss: 0.2948\n",
      "Train Epoch: 006 Batch: 00062/00094 | Loss: 0.1068 | EWC Loss: 0.0306 | CE Loss: 0.0762\n",
      "Train Epoch: 006 Batch: 00063/00094 | Loss: 0.1790 | EWC Loss: 0.0306 | CE Loss: 0.1484\n",
      "Train Epoch: 006 Batch: 00064/00094 | Loss: 0.2264 | EWC Loss: 0.0304 | CE Loss: 0.1960\n",
      "Train Epoch: 006 Batch: 00065/00094 | Loss: 0.0642 | EWC Loss: 0.0303 | CE Loss: 0.0339\n",
      "Train Epoch: 006 Batch: 00066/00094 | Loss: 0.1110 | EWC Loss: 0.0303 | CE Loss: 0.0807\n",
      "Train Epoch: 006 Batch: 00067/00094 | Loss: 0.1037 | EWC Loss: 0.0303 | CE Loss: 0.0734\n",
      "Train Epoch: 006 Batch: 00068/00094 | Loss: 0.1411 | EWC Loss: 0.0303 | CE Loss: 0.1109\n",
      "Train Epoch: 006 Batch: 00069/00094 | Loss: 0.1105 | EWC Loss: 0.0303 | CE Loss: 0.0803\n",
      "Train Epoch: 006 Batch: 00070/00094 | Loss: 0.1072 | EWC Loss: 0.0303 | CE Loss: 0.0769\n",
      "Train Epoch: 006 Batch: 00071/00094 | Loss: 0.2902 | EWC Loss: 0.0303 | CE Loss: 0.2599\n",
      "Train Epoch: 006 Batch: 00072/00094 | Loss: 0.0962 | EWC Loss: 0.0303 | CE Loss: 0.0659\n",
      "Train Epoch: 006 Batch: 00073/00094 | Loss: 0.1688 | EWC Loss: 0.0303 | CE Loss: 0.1386\n",
      "Train Epoch: 006 Batch: 00074/00094 | Loss: 0.1748 | EWC Loss: 0.0303 | CE Loss: 0.1446\n",
      "Train Epoch: 006 Batch: 00075/00094 | Loss: 0.0649 | EWC Loss: 0.0303 | CE Loss: 0.0346\n",
      "Train Epoch: 006 Batch: 00076/00094 | Loss: 0.0948 | EWC Loss: 0.0303 | CE Loss: 0.0646\n",
      "Train Epoch: 006 Batch: 00077/00094 | Loss: 0.0778 | EWC Loss: 0.0303 | CE Loss: 0.0475\n",
      "Train Epoch: 006 Batch: 00078/00094 | Loss: 0.1191 | EWC Loss: 0.0303 | CE Loss: 0.0888\n",
      "Train Epoch: 006 Batch: 00079/00094 | Loss: 0.1566 | EWC Loss: 0.0303 | CE Loss: 0.1263\n",
      "Train Epoch: 006 Batch: 00080/00094 | Loss: 0.1863 | EWC Loss: 0.0303 | CE Loss: 0.1560\n",
      "Train Epoch: 006 Batch: 00081/00094 | Loss: 0.1198 | EWC Loss: 0.0303 | CE Loss: 0.0896\n",
      "Train Epoch: 006 Batch: 00082/00094 | Loss: 0.1596 | EWC Loss: 0.0303 | CE Loss: 0.1293\n",
      "Train Epoch: 006 Batch: 00083/00094 | Loss: 0.0792 | EWC Loss: 0.0303 | CE Loss: 0.0489\n",
      "Train Epoch: 006 Batch: 00084/00094 | Loss: 0.1065 | EWC Loss: 0.0303 | CE Loss: 0.0762\n",
      "Train Epoch: 006 Batch: 00085/00094 | Loss: 0.1479 | EWC Loss: 0.0303 | CE Loss: 0.1176\n",
      "Train Epoch: 006 Batch: 00086/00094 | Loss: 0.1135 | EWC Loss: 0.0303 | CE Loss: 0.0832\n",
      "Train Epoch: 006 Batch: 00087/00094 | Loss: 0.1010 | EWC Loss: 0.0303 | CE Loss: 0.0707\n",
      "Train Epoch: 006 Batch: 00088/00094 | Loss: 0.2122 | EWC Loss: 0.0303 | CE Loss: 0.1819\n",
      "Train Epoch: 006 Batch: 00089/00094 | Loss: 0.1636 | EWC Loss: 0.0304 | CE Loss: 0.1332\n",
      "Train Epoch: 006 Batch: 00090/00094 | Loss: 0.1361 | EWC Loss: 0.0304 | CE Loss: 0.1057\n",
      "Train Epoch: 006 Batch: 00091/00094 | Loss: 0.2225 | EWC Loss: 0.0303 | CE Loss: 0.1922\n",
      "Train Epoch: 006 Batch: 00092/00094 | Loss: 0.0831 | EWC Loss: 0.0303 | CE Loss: 0.0528\n",
      "Train Epoch: 006 Batch: 00093/00094 | Loss: 0.1155 | EWC Loss: 0.0302 | CE Loss: 0.0853\n",
      "Train Epoch: 006 Batch: 00094/00094 | Loss: 0.0915 | EWC Loss: 0.0302 | CE Loss: 0.0613\n",
      "Train Epoch: 006 |Acc 95.46667 | Loss: 0.14870 | Task Loss 0.11816 | EWC Loss: 0.03053\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1000 | acc:96.1500\n",
      "[VAL Acc] Target: 96.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.5925 | acc:48.1000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.6334 | acc:54.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8803 | acc:46.3740\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4750 | acc:57.2100\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.21%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4917 | acc:79.3900\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 79.39%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9150 | acc:69.3966\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8718 | acc:64.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.12%\n",
      "[VAL Acc] Avg 64.41%\n",
      "\n",
      "\n",
      "---------- Starting epoch 7 ----------\n",
      "Train Epoch: 007 Batch: 00001/00094 | Loss: 0.1709 | EWC Loss: 0.0301 | CE Loss: 0.1408\n",
      "Train Epoch: 007 Batch: 00002/00094 | Loss: 0.1551 | EWC Loss: 0.0302 | CE Loss: 0.1250\n",
      "Train Epoch: 007 Batch: 00003/00094 | Loss: 0.1837 | EWC Loss: 0.0301 | CE Loss: 0.1536\n",
      "Train Epoch: 007 Batch: 00004/00094 | Loss: 0.2441 | EWC Loss: 0.0303 | CE Loss: 0.2139\n",
      "Train Epoch: 007 Batch: 00005/00094 | Loss: 0.1224 | EWC Loss: 0.0302 | CE Loss: 0.0922\n",
      "Train Epoch: 007 Batch: 00006/00094 | Loss: 0.0871 | EWC Loss: 0.0302 | CE Loss: 0.0568\n",
      "Train Epoch: 007 Batch: 00007/00094 | Loss: 0.1068 | EWC Loss: 0.0302 | CE Loss: 0.0766\n",
      "Train Epoch: 007 Batch: 00008/00094 | Loss: 0.1278 | EWC Loss: 0.0302 | CE Loss: 0.0976\n",
      "Train Epoch: 007 Batch: 00009/00094 | Loss: 0.1271 | EWC Loss: 0.0302 | CE Loss: 0.0969\n",
      "Train Epoch: 007 Batch: 00010/00094 | Loss: 0.1883 | EWC Loss: 0.0301 | CE Loss: 0.1582\n",
      "Train Epoch: 007 Batch: 00011/00094 | Loss: 0.2080 | EWC Loss: 0.0301 | CE Loss: 0.1779\n",
      "Train Epoch: 007 Batch: 00012/00094 | Loss: 0.1876 | EWC Loss: 0.0302 | CE Loss: 0.1574\n",
      "Train Epoch: 007 Batch: 00013/00094 | Loss: 0.0800 | EWC Loss: 0.0302 | CE Loss: 0.0498\n",
      "Train Epoch: 007 Batch: 00014/00094 | Loss: 0.0935 | EWC Loss: 0.0302 | CE Loss: 0.0633\n",
      "Train Epoch: 007 Batch: 00015/00094 | Loss: 0.2101 | EWC Loss: 0.0302 | CE Loss: 0.1799\n",
      "Train Epoch: 007 Batch: 00016/00094 | Loss: 0.0777 | EWC Loss: 0.0302 | CE Loss: 0.0475\n",
      "Train Epoch: 007 Batch: 00017/00094 | Loss: 0.2428 | EWC Loss: 0.0302 | CE Loss: 0.2126\n",
      "Train Epoch: 007 Batch: 00018/00094 | Loss: 0.0870 | EWC Loss: 0.0302 | CE Loss: 0.0568\n",
      "Train Epoch: 007 Batch: 00019/00094 | Loss: 0.0799 | EWC Loss: 0.0302 | CE Loss: 0.0498\n",
      "Train Epoch: 007 Batch: 00020/00094 | Loss: 0.1841 | EWC Loss: 0.0302 | CE Loss: 0.1539\n",
      "Train Epoch: 007 Batch: 00021/00094 | Loss: 0.1083 | EWC Loss: 0.0302 | CE Loss: 0.0781\n",
      "Train Epoch: 007 Batch: 00022/00094 | Loss: 0.0779 | EWC Loss: 0.0302 | CE Loss: 0.0477\n",
      "Train Epoch: 007 Batch: 00023/00094 | Loss: 0.1165 | EWC Loss: 0.0302 | CE Loss: 0.0863\n",
      "Train Epoch: 007 Batch: 00024/00094 | Loss: 0.0980 | EWC Loss: 0.0302 | CE Loss: 0.0678\n",
      "Train Epoch: 007 Batch: 00025/00094 | Loss: 0.2903 | EWC Loss: 0.0302 | CE Loss: 0.2601\n",
      "Train Epoch: 007 Batch: 00026/00094 | Loss: 0.0856 | EWC Loss: 0.0302 | CE Loss: 0.0554\n",
      "Train Epoch: 007 Batch: 00027/00094 | Loss: 0.1272 | EWC Loss: 0.0302 | CE Loss: 0.0970\n",
      "Train Epoch: 007 Batch: 00028/00094 | Loss: 0.0953 | EWC Loss: 0.0302 | CE Loss: 0.0651\n",
      "Train Epoch: 007 Batch: 00029/00094 | Loss: 0.1456 | EWC Loss: 0.0302 | CE Loss: 0.1154\n",
      "Train Epoch: 007 Batch: 00030/00094 | Loss: 0.2463 | EWC Loss: 0.0301 | CE Loss: 0.2162\n",
      "Train Epoch: 007 Batch: 00031/00094 | Loss: 0.1306 | EWC Loss: 0.0302 | CE Loss: 0.1005\n",
      "Train Epoch: 007 Batch: 00032/00094 | Loss: 0.1347 | EWC Loss: 0.0302 | CE Loss: 0.1045\n",
      "Train Epoch: 007 Batch: 00033/00094 | Loss: 0.0614 | EWC Loss: 0.0302 | CE Loss: 0.0311\n",
      "Train Epoch: 007 Batch: 00034/00094 | Loss: 0.1127 | EWC Loss: 0.0302 | CE Loss: 0.0825\n",
      "Train Epoch: 007 Batch: 00035/00094 | Loss: 0.1188 | EWC Loss: 0.0302 | CE Loss: 0.0886\n",
      "Train Epoch: 007 Batch: 00036/00094 | Loss: 0.1535 | EWC Loss: 0.0302 | CE Loss: 0.1234\n",
      "Train Epoch: 007 Batch: 00037/00094 | Loss: 0.1178 | EWC Loss: 0.0302 | CE Loss: 0.0876\n",
      "Train Epoch: 007 Batch: 00038/00094 | Loss: 0.1381 | EWC Loss: 0.0301 | CE Loss: 0.1080\n",
      "Train Epoch: 007 Batch: 00039/00094 | Loss: 0.1200 | EWC Loss: 0.0301 | CE Loss: 0.0899\n",
      "Train Epoch: 007 Batch: 00040/00094 | Loss: 0.1506 | EWC Loss: 0.0301 | CE Loss: 0.1204\n",
      "Train Epoch: 007 Batch: 00041/00094 | Loss: 0.1160 | EWC Loss: 0.0301 | CE Loss: 0.0859\n",
      "Train Epoch: 007 Batch: 00042/00094 | Loss: 0.1258 | EWC Loss: 0.0301 | CE Loss: 0.0957\n",
      "Train Epoch: 007 Batch: 00043/00094 | Loss: 0.1847 | EWC Loss: 0.0301 | CE Loss: 0.1546\n",
      "Train Epoch: 007 Batch: 00044/00094 | Loss: 0.0701 | EWC Loss: 0.0301 | CE Loss: 0.0400\n",
      "Train Epoch: 007 Batch: 00045/00094 | Loss: 0.1276 | EWC Loss: 0.0301 | CE Loss: 0.0975\n",
      "Train Epoch: 007 Batch: 00046/00094 | Loss: 0.1490 | EWC Loss: 0.0301 | CE Loss: 0.1189\n",
      "Train Epoch: 007 Batch: 00047/00094 | Loss: 0.0977 | EWC Loss: 0.0301 | CE Loss: 0.0676\n",
      "Train Epoch: 007 Batch: 00048/00094 | Loss: 0.0717 | EWC Loss: 0.0301 | CE Loss: 0.0416\n",
      "Train Epoch: 007 Batch: 00049/00094 | Loss: 0.1596 | EWC Loss: 0.0301 | CE Loss: 0.1296\n",
      "Train Epoch: 007 Batch: 00050/00094 | Loss: 0.1153 | EWC Loss: 0.0300 | CE Loss: 0.0852\n",
      "Train Epoch: 007 Batch: 00051/00094 | Loss: 0.0746 | EWC Loss: 0.0300 | CE Loss: 0.0446\n",
      "Train Epoch: 007 Batch: 00052/00094 | Loss: 0.1657 | EWC Loss: 0.0301 | CE Loss: 0.1357\n",
      "Train Epoch: 007 Batch: 00053/00094 | Loss: 0.1132 | EWC Loss: 0.0300 | CE Loss: 0.0831\n",
      "Train Epoch: 007 Batch: 00054/00094 | Loss: 0.1157 | EWC Loss: 0.0300 | CE Loss: 0.0857\n",
      "Train Epoch: 007 Batch: 00055/00094 | Loss: 0.1348 | EWC Loss: 0.0301 | CE Loss: 0.1048\n",
      "Train Epoch: 007 Batch: 00056/00094 | Loss: 0.0920 | EWC Loss: 0.0300 | CE Loss: 0.0619\n",
      "Train Epoch: 007 Batch: 00057/00094 | Loss: 0.1754 | EWC Loss: 0.0300 | CE Loss: 0.1454\n",
      "Train Epoch: 007 Batch: 00058/00094 | Loss: 0.1525 | EWC Loss: 0.0300 | CE Loss: 0.1225\n",
      "Train Epoch: 007 Batch: 00059/00094 | Loss: 0.1194 | EWC Loss: 0.0300 | CE Loss: 0.0894\n",
      "Train Epoch: 007 Batch: 00060/00094 | Loss: 0.1337 | EWC Loss: 0.0300 | CE Loss: 0.1037\n",
      "Train Epoch: 007 Batch: 00061/00094 | Loss: 0.0958 | EWC Loss: 0.0300 | CE Loss: 0.0658\n",
      "Train Epoch: 007 Batch: 00062/00094 | Loss: 0.0911 | EWC Loss: 0.0300 | CE Loss: 0.0611\n",
      "Train Epoch: 007 Batch: 00063/00094 | Loss: 0.1386 | EWC Loss: 0.0300 | CE Loss: 0.1086\n",
      "Train Epoch: 007 Batch: 00064/00094 | Loss: 0.0909 | EWC Loss: 0.0300 | CE Loss: 0.0610\n",
      "Train Epoch: 007 Batch: 00065/00094 | Loss: 0.1323 | EWC Loss: 0.0300 | CE Loss: 0.1023\n",
      "Train Epoch: 007 Batch: 00066/00094 | Loss: 0.1034 | EWC Loss: 0.0300 | CE Loss: 0.0734\n",
      "Train Epoch: 007 Batch: 00067/00094 | Loss: 0.0778 | EWC Loss: 0.0300 | CE Loss: 0.0479\n",
      "Train Epoch: 007 Batch: 00068/00094 | Loss: 0.1027 | EWC Loss: 0.0300 | CE Loss: 0.0727\n",
      "Train Epoch: 007 Batch: 00069/00094 | Loss: 0.1526 | EWC Loss: 0.0300 | CE Loss: 0.1226\n",
      "Train Epoch: 007 Batch: 00070/00094 | Loss: 0.0968 | EWC Loss: 0.0300 | CE Loss: 0.0668\n",
      "Train Epoch: 007 Batch: 00071/00094 | Loss: 0.3084 | EWC Loss: 0.0300 | CE Loss: 0.2784\n",
      "Train Epoch: 007 Batch: 00072/00094 | Loss: 0.1368 | EWC Loss: 0.0300 | CE Loss: 0.1067\n",
      "Train Epoch: 007 Batch: 00073/00094 | Loss: 0.1570 | EWC Loss: 0.0300 | CE Loss: 0.1270\n",
      "Train Epoch: 007 Batch: 00074/00094 | Loss: 0.1269 | EWC Loss: 0.0300 | CE Loss: 0.0969\n",
      "Train Epoch: 007 Batch: 00075/00094 | Loss: 0.1629 | EWC Loss: 0.0300 | CE Loss: 0.1329\n",
      "Train Epoch: 007 Batch: 00076/00094 | Loss: 0.1115 | EWC Loss: 0.0300 | CE Loss: 0.0815\n",
      "Train Epoch: 007 Batch: 00077/00094 | Loss: 0.1540 | EWC Loss: 0.0300 | CE Loss: 0.1241\n",
      "Train Epoch: 007 Batch: 00078/00094 | Loss: 0.2227 | EWC Loss: 0.0300 | CE Loss: 0.1928\n",
      "Train Epoch: 007 Batch: 00079/00094 | Loss: 0.2171 | EWC Loss: 0.0300 | CE Loss: 0.1871\n",
      "Train Epoch: 007 Batch: 00080/00094 | Loss: 0.0664 | EWC Loss: 0.0299 | CE Loss: 0.0365\n",
      "Train Epoch: 007 Batch: 00081/00094 | Loss: 0.1359 | EWC Loss: 0.0299 | CE Loss: 0.1060\n",
      "Train Epoch: 007 Batch: 00082/00094 | Loss: 0.2768 | EWC Loss: 0.0299 | CE Loss: 0.2468\n",
      "Train Epoch: 007 Batch: 00083/00094 | Loss: 0.1171 | EWC Loss: 0.0300 | CE Loss: 0.0871\n",
      "Train Epoch: 007 Batch: 00084/00094 | Loss: 0.1063 | EWC Loss: 0.0300 | CE Loss: 0.0763\n",
      "Train Epoch: 007 Batch: 00085/00094 | Loss: 0.1467 | EWC Loss: 0.0300 | CE Loss: 0.1166\n",
      "Train Epoch: 007 Batch: 00086/00094 | Loss: 0.2141 | EWC Loss: 0.0300 | CE Loss: 0.1841\n",
      "Train Epoch: 007 Batch: 00087/00094 | Loss: 0.1904 | EWC Loss: 0.0300 | CE Loss: 0.1604\n",
      "Train Epoch: 007 Batch: 00088/00094 | Loss: 0.1613 | EWC Loss: 0.0300 | CE Loss: 0.1313\n",
      "Train Epoch: 007 Batch: 00089/00094 | Loss: 0.2026 | EWC Loss: 0.0300 | CE Loss: 0.1726\n",
      "Train Epoch: 007 Batch: 00090/00094 | Loss: 0.1762 | EWC Loss: 0.0300 | CE Loss: 0.1463\n",
      "Train Epoch: 007 Batch: 00091/00094 | Loss: 0.0860 | EWC Loss: 0.0300 | CE Loss: 0.0560\n",
      "Train Epoch: 007 Batch: 00092/00094 | Loss: 0.2442 | EWC Loss: 0.0300 | CE Loss: 0.2142\n",
      "Train Epoch: 007 Batch: 00093/00094 | Loss: 0.1745 | EWC Loss: 0.0300 | CE Loss: 0.1445\n",
      "Train Epoch: 007 Batch: 00094/00094 | Loss: 0.1026 | EWC Loss: 0.0300 | CE Loss: 0.0727\n",
      "Train Epoch: 007 |Acc 95.70000 | Loss: 0.14009 | Task Loss 0.11001 | EWC Loss: 0.03008\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1150 | acc:95.6500\n",
      "[VAL Acc] Target: 95.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.9066 | acc:48.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9364 | acc:50.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9431 | acc:44.6565\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.66%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.8368 | acc:54.9373\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.94%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6174 | acc:75.8780\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1834 | acc:66.0658\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9099 | acc:64.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.88%\n",
      "[VAL Acc] Avg 62.62%\n",
      "\n",
      "\n",
      "---------- Starting epoch 8 ----------\n",
      "Train Epoch: 008 Batch: 00001/00094 | Loss: 0.1341 | EWC Loss: 0.0299 | CE Loss: 0.1041\n",
      "Train Epoch: 008 Batch: 00002/00094 | Loss: 0.1572 | EWC Loss: 0.0299 | CE Loss: 0.1273\n",
      "Train Epoch: 008 Batch: 00003/00094 | Loss: 0.1582 | EWC Loss: 0.0299 | CE Loss: 0.1283\n",
      "Train Epoch: 008 Batch: 00004/00094 | Loss: 0.1000 | EWC Loss: 0.0300 | CE Loss: 0.0700\n",
      "Train Epoch: 008 Batch: 00005/00094 | Loss: 0.0829 | EWC Loss: 0.0300 | CE Loss: 0.0529\n",
      "Train Epoch: 008 Batch: 00006/00094 | Loss: 0.1755 | EWC Loss: 0.0300 | CE Loss: 0.1456\n",
      "Train Epoch: 008 Batch: 00007/00094 | Loss: 0.1236 | EWC Loss: 0.0299 | CE Loss: 0.0937\n",
      "Train Epoch: 008 Batch: 00008/00094 | Loss: 0.1417 | EWC Loss: 0.0299 | CE Loss: 0.1118\n",
      "Train Epoch: 008 Batch: 00009/00094 | Loss: 0.1910 | EWC Loss: 0.0299 | CE Loss: 0.1611\n",
      "Train Epoch: 008 Batch: 00010/00094 | Loss: 0.2461 | EWC Loss: 0.0299 | CE Loss: 0.2162\n",
      "Train Epoch: 008 Batch: 00011/00094 | Loss: 0.1384 | EWC Loss: 0.0300 | CE Loss: 0.1084\n",
      "Train Epoch: 008 Batch: 00012/00094 | Loss: 0.1557 | EWC Loss: 0.0300 | CE Loss: 0.1257\n",
      "Train Epoch: 008 Batch: 00013/00094 | Loss: 0.1115 | EWC Loss: 0.0299 | CE Loss: 0.0816\n",
      "Train Epoch: 008 Batch: 00014/00094 | Loss: 0.1068 | EWC Loss: 0.0299 | CE Loss: 0.0768\n",
      "Train Epoch: 008 Batch: 00015/00094 | Loss: 0.1748 | EWC Loss: 0.0299 | CE Loss: 0.1449\n",
      "Train Epoch: 008 Batch: 00016/00094 | Loss: 0.0960 | EWC Loss: 0.0299 | CE Loss: 0.0660\n",
      "Train Epoch: 008 Batch: 00017/00094 | Loss: 0.1296 | EWC Loss: 0.0299 | CE Loss: 0.0997\n",
      "Train Epoch: 008 Batch: 00018/00094 | Loss: 0.0615 | EWC Loss: 0.0299 | CE Loss: 0.0315\n",
      "Train Epoch: 008 Batch: 00019/00094 | Loss: 0.0604 | EWC Loss: 0.0299 | CE Loss: 0.0304\n",
      "Train Epoch: 008 Batch: 00020/00094 | Loss: 0.1282 | EWC Loss: 0.0299 | CE Loss: 0.0983\n",
      "Train Epoch: 008 Batch: 00021/00094 | Loss: 0.0848 | EWC Loss: 0.0299 | CE Loss: 0.0548\n",
      "Train Epoch: 008 Batch: 00022/00094 | Loss: 0.0849 | EWC Loss: 0.0299 | CE Loss: 0.0550\n",
      "Train Epoch: 008 Batch: 00023/00094 | Loss: 0.0917 | EWC Loss: 0.0299 | CE Loss: 0.0618\n",
      "Train Epoch: 008 Batch: 00024/00094 | Loss: 0.0859 | EWC Loss: 0.0299 | CE Loss: 0.0560\n",
      "Train Epoch: 008 Batch: 00025/00094 | Loss: 0.0856 | EWC Loss: 0.0299 | CE Loss: 0.0557\n",
      "Train Epoch: 008 Batch: 00026/00094 | Loss: 0.1033 | EWC Loss: 0.0299 | CE Loss: 0.0734\n",
      "Train Epoch: 008 Batch: 00027/00094 | Loss: 0.1680 | EWC Loss: 0.0299 | CE Loss: 0.1381\n",
      "Train Epoch: 008 Batch: 00028/00094 | Loss: 0.1361 | EWC Loss: 0.0299 | CE Loss: 0.1062\n",
      "Train Epoch: 008 Batch: 00029/00094 | Loss: 0.1884 | EWC Loss: 0.0299 | CE Loss: 0.1585\n",
      "Train Epoch: 008 Batch: 00030/00094 | Loss: 0.1191 | EWC Loss: 0.0299 | CE Loss: 0.0892\n",
      "Train Epoch: 008 Batch: 00031/00094 | Loss: 0.1673 | EWC Loss: 0.0299 | CE Loss: 0.1374\n",
      "Train Epoch: 008 Batch: 00032/00094 | Loss: 0.1533 | EWC Loss: 0.0299 | CE Loss: 0.1234\n",
      "Train Epoch: 008 Batch: 00033/00094 | Loss: 0.0926 | EWC Loss: 0.0299 | CE Loss: 0.0627\n",
      "Train Epoch: 008 Batch: 00034/00094 | Loss: 0.2140 | EWC Loss: 0.0299 | CE Loss: 0.1841\n",
      "Train Epoch: 008 Batch: 00035/00094 | Loss: 0.1552 | EWC Loss: 0.0299 | CE Loss: 0.1253\n",
      "Train Epoch: 008 Batch: 00036/00094 | Loss: 0.2137 | EWC Loss: 0.0299 | CE Loss: 0.1838\n",
      "Train Epoch: 008 Batch: 00037/00094 | Loss: 0.1294 | EWC Loss: 0.0299 | CE Loss: 0.0995\n",
      "Train Epoch: 008 Batch: 00038/00094 | Loss: 0.1769 | EWC Loss: 0.0299 | CE Loss: 0.1470\n",
      "Train Epoch: 008 Batch: 00039/00094 | Loss: 0.1225 | EWC Loss: 0.0299 | CE Loss: 0.0926\n",
      "Train Epoch: 008 Batch: 00040/00094 | Loss: 0.0973 | EWC Loss: 0.0299 | CE Loss: 0.0673\n",
      "Train Epoch: 008 Batch: 00041/00094 | Loss: 0.1268 | EWC Loss: 0.0299 | CE Loss: 0.0969\n",
      "Train Epoch: 008 Batch: 00042/00094 | Loss: 0.1915 | EWC Loss: 0.0299 | CE Loss: 0.1616\n",
      "Train Epoch: 008 Batch: 00043/00094 | Loss: 0.2546 | EWC Loss: 0.0299 | CE Loss: 0.2246\n",
      "Train Epoch: 008 Batch: 00044/00094 | Loss: 0.0568 | EWC Loss: 0.0299 | CE Loss: 0.0268\n",
      "Train Epoch: 008 Batch: 00045/00094 | Loss: 0.1493 | EWC Loss: 0.0299 | CE Loss: 0.1194\n",
      "Train Epoch: 008 Batch: 00046/00094 | Loss: 0.1007 | EWC Loss: 0.0300 | CE Loss: 0.0707\n",
      "Train Epoch: 008 Batch: 00047/00094 | Loss: 0.0782 | EWC Loss: 0.0300 | CE Loss: 0.0483\n",
      "Train Epoch: 008 Batch: 00048/00094 | Loss: 0.1362 | EWC Loss: 0.0299 | CE Loss: 0.1063\n",
      "Train Epoch: 008 Batch: 00049/00094 | Loss: 0.0993 | EWC Loss: 0.0300 | CE Loss: 0.0693\n",
      "Train Epoch: 008 Batch: 00050/00094 | Loss: 0.3168 | EWC Loss: 0.0299 | CE Loss: 0.2869\n",
      "Train Epoch: 008 Batch: 00051/00094 | Loss: 0.1230 | EWC Loss: 0.0299 | CE Loss: 0.0931\n",
      "Train Epoch: 008 Batch: 00052/00094 | Loss: 0.0978 | EWC Loss: 0.0299 | CE Loss: 0.0679\n",
      "Train Epoch: 008 Batch: 00053/00094 | Loss: 0.1692 | EWC Loss: 0.0299 | CE Loss: 0.1393\n",
      "Train Epoch: 008 Batch: 00054/00094 | Loss: 0.0787 | EWC Loss: 0.0299 | CE Loss: 0.0488\n",
      "Train Epoch: 008 Batch: 00055/00094 | Loss: 0.0938 | EWC Loss: 0.0299 | CE Loss: 0.0639\n",
      "Train Epoch: 008 Batch: 00056/00094 | Loss: 0.1089 | EWC Loss: 0.0299 | CE Loss: 0.0790\n",
      "Train Epoch: 008 Batch: 00057/00094 | Loss: 0.2352 | EWC Loss: 0.0299 | CE Loss: 0.2054\n",
      "Train Epoch: 008 Batch: 00058/00094 | Loss: 0.0967 | EWC Loss: 0.0298 | CE Loss: 0.0669\n",
      "Train Epoch: 008 Batch: 00059/00094 | Loss: 0.0836 | EWC Loss: 0.0298 | CE Loss: 0.0538\n",
      "Train Epoch: 008 Batch: 00060/00094 | Loss: 0.0991 | EWC Loss: 0.0298 | CE Loss: 0.0693\n",
      "Train Epoch: 008 Batch: 00061/00094 | Loss: 0.1149 | EWC Loss: 0.0298 | CE Loss: 0.0852\n",
      "Train Epoch: 008 Batch: 00062/00094 | Loss: 0.1481 | EWC Loss: 0.0298 | CE Loss: 0.1183\n",
      "Train Epoch: 008 Batch: 00063/00094 | Loss: 0.2038 | EWC Loss: 0.0298 | CE Loss: 0.1741\n",
      "Train Epoch: 008 Batch: 00064/00094 | Loss: 0.1829 | EWC Loss: 0.0298 | CE Loss: 0.1531\n",
      "Train Epoch: 008 Batch: 00065/00094 | Loss: 0.2881 | EWC Loss: 0.0298 | CE Loss: 0.2583\n",
      "Train Epoch: 008 Batch: 00066/00094 | Loss: 0.1607 | EWC Loss: 0.0298 | CE Loss: 0.1309\n",
      "Train Epoch: 008 Batch: 00067/00094 | Loss: 0.1572 | EWC Loss: 0.0298 | CE Loss: 0.1274\n",
      "Train Epoch: 008 Batch: 00068/00094 | Loss: 0.1042 | EWC Loss: 0.0298 | CE Loss: 0.0744\n",
      "Train Epoch: 008 Batch: 00069/00094 | Loss: 0.0765 | EWC Loss: 0.0298 | CE Loss: 0.0467\n",
      "Train Epoch: 008 Batch: 00070/00094 | Loss: 0.0883 | EWC Loss: 0.0298 | CE Loss: 0.0585\n",
      "Train Epoch: 008 Batch: 00071/00094 | Loss: 0.0924 | EWC Loss: 0.0298 | CE Loss: 0.0627\n",
      "Train Epoch: 008 Batch: 00072/00094 | Loss: 0.0932 | EWC Loss: 0.0298 | CE Loss: 0.0635\n",
      "Train Epoch: 008 Batch: 00073/00094 | Loss: 0.1771 | EWC Loss: 0.0298 | CE Loss: 0.1474\n",
      "Train Epoch: 008 Batch: 00074/00094 | Loss: 0.1948 | EWC Loss: 0.0298 | CE Loss: 0.1650\n",
      "Train Epoch: 008 Batch: 00075/00094 | Loss: 0.1154 | EWC Loss: 0.0298 | CE Loss: 0.0856\n",
      "Train Epoch: 008 Batch: 00076/00094 | Loss: 0.1246 | EWC Loss: 0.0298 | CE Loss: 0.0949\n",
      "Train Epoch: 008 Batch: 00077/00094 | Loss: 0.0767 | EWC Loss: 0.0298 | CE Loss: 0.0469\n",
      "Train Epoch: 008 Batch: 00078/00094 | Loss: 0.2249 | EWC Loss: 0.0298 | CE Loss: 0.1951\n",
      "Train Epoch: 008 Batch: 00079/00094 | Loss: 0.1247 | EWC Loss: 0.0297 | CE Loss: 0.0950\n",
      "Train Epoch: 008 Batch: 00080/00094 | Loss: 0.1547 | EWC Loss: 0.0297 | CE Loss: 0.1250\n",
      "Train Epoch: 008 Batch: 00081/00094 | Loss: 0.1303 | EWC Loss: 0.0297 | CE Loss: 0.1005\n",
      "Train Epoch: 008 Batch: 00082/00094 | Loss: 0.0565 | EWC Loss: 0.0297 | CE Loss: 0.0267\n",
      "Train Epoch: 008 Batch: 00083/00094 | Loss: 0.1132 | EWC Loss: 0.0297 | CE Loss: 0.0835\n",
      "Train Epoch: 008 Batch: 00084/00094 | Loss: 0.2940 | EWC Loss: 0.0297 | CE Loss: 0.2643\n",
      "Train Epoch: 008 Batch: 00085/00094 | Loss: 0.1072 | EWC Loss: 0.0297 | CE Loss: 0.0775\n",
      "Train Epoch: 008 Batch: 00086/00094 | Loss: 0.2671 | EWC Loss: 0.0297 | CE Loss: 0.2374\n",
      "Train Epoch: 008 Batch: 00087/00094 | Loss: 0.0968 | EWC Loss: 0.0297 | CE Loss: 0.0671\n",
      "Train Epoch: 008 Batch: 00088/00094 | Loss: 0.0991 | EWC Loss: 0.0297 | CE Loss: 0.0694\n",
      "Train Epoch: 008 Batch: 00089/00094 | Loss: 0.1430 | EWC Loss: 0.0297 | CE Loss: 0.1133\n",
      "Train Epoch: 008 Batch: 00090/00094 | Loss: 0.0773 | EWC Loss: 0.0297 | CE Loss: 0.0476\n",
      "Train Epoch: 008 Batch: 00091/00094 | Loss: 0.0967 | EWC Loss: 0.0297 | CE Loss: 0.0670\n",
      "Train Epoch: 008 Batch: 00092/00094 | Loss: 0.1724 | EWC Loss: 0.0297 | CE Loss: 0.1427\n",
      "Train Epoch: 008 Batch: 00093/00094 | Loss: 0.1325 | EWC Loss: 0.0297 | CE Loss: 0.1028\n",
      "Train Epoch: 008 Batch: 00094/00094 | Loss: 0.1287 | EWC Loss: 0.0297 | CE Loss: 0.0990\n",
      "Train Epoch: 008 |Acc 96.00000 | Loss: 0.13678 | Task Loss 0.10693 | EWC Loss: 0.02986\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0851 | acc:96.6500\n",
      "[VAL Acc] Target: 96.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8077 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9716 | acc:51.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9549 | acc:44.2748\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6705 | acc:55.3683\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6135 | acc:76.8946\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1350 | acc:66.8887\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9326 | acc:64.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.31%\n",
      "[VAL Acc] Avg 63.10%\n",
      "\n",
      "\n",
      "---------- Starting epoch 9 ----------\n",
      "Train Epoch: 009 Batch: 00001/00094 | Loss: 0.1449 | EWC Loss: 0.0297 | CE Loss: 0.1152\n",
      "Train Epoch: 009 Batch: 00002/00094 | Loss: 0.1182 | EWC Loss: 0.0297 | CE Loss: 0.0885\n",
      "Train Epoch: 009 Batch: 00003/00094 | Loss: 0.1523 | EWC Loss: 0.0297 | CE Loss: 0.1226\n",
      "Train Epoch: 009 Batch: 00004/00094 | Loss: 0.1360 | EWC Loss: 0.0297 | CE Loss: 0.1063\n",
      "Train Epoch: 009 Batch: 00005/00094 | Loss: 0.0715 | EWC Loss: 0.0297 | CE Loss: 0.0418\n",
      "Train Epoch: 009 Batch: 00006/00094 | Loss: 0.1074 | EWC Loss: 0.0297 | CE Loss: 0.0777\n",
      "Train Epoch: 009 Batch: 00007/00094 | Loss: 0.1914 | EWC Loss: 0.0297 | CE Loss: 0.1617\n",
      "Train Epoch: 009 Batch: 00008/00094 | Loss: 0.1950 | EWC Loss: 0.0297 | CE Loss: 0.1653\n",
      "Train Epoch: 009 Batch: 00009/00094 | Loss: 0.0831 | EWC Loss: 0.0297 | CE Loss: 0.0534\n",
      "Train Epoch: 009 Batch: 00010/00094 | Loss: 0.0850 | EWC Loss: 0.0297 | CE Loss: 0.0553\n",
      "Train Epoch: 009 Batch: 00011/00094 | Loss: 0.1248 | EWC Loss: 0.0297 | CE Loss: 0.0951\n",
      "Train Epoch: 009 Batch: 00012/00094 | Loss: 0.1562 | EWC Loss: 0.0297 | CE Loss: 0.1265\n",
      "Train Epoch: 009 Batch: 00013/00094 | Loss: 0.1445 | EWC Loss: 0.0297 | CE Loss: 0.1148\n",
      "Train Epoch: 009 Batch: 00014/00094 | Loss: 0.1101 | EWC Loss: 0.0297 | CE Loss: 0.0804\n",
      "Train Epoch: 009 Batch: 00015/00094 | Loss: 0.1367 | EWC Loss: 0.0297 | CE Loss: 0.1070\n",
      "Train Epoch: 009 Batch: 00016/00094 | Loss: 0.1188 | EWC Loss: 0.0297 | CE Loss: 0.0891\n",
      "Train Epoch: 009 Batch: 00017/00094 | Loss: 0.0943 | EWC Loss: 0.0297 | CE Loss: 0.0646\n",
      "Train Epoch: 009 Batch: 00018/00094 | Loss: 0.2385 | EWC Loss: 0.0297 | CE Loss: 0.2088\n",
      "Train Epoch: 009 Batch: 00019/00094 | Loss: 0.0933 | EWC Loss: 0.0297 | CE Loss: 0.0636\n",
      "Train Epoch: 009 Batch: 00020/00094 | Loss: 0.1656 | EWC Loss: 0.0297 | CE Loss: 0.1359\n",
      "Train Epoch: 009 Batch: 00021/00094 | Loss: 0.1044 | EWC Loss: 0.0297 | CE Loss: 0.0747\n",
      "Train Epoch: 009 Batch: 00022/00094 | Loss: 0.1165 | EWC Loss: 0.0297 | CE Loss: 0.0868\n",
      "Train Epoch: 009 Batch: 00023/00094 | Loss: 0.1751 | EWC Loss: 0.0297 | CE Loss: 0.1454\n",
      "Train Epoch: 009 Batch: 00024/00094 | Loss: 0.1466 | EWC Loss: 0.0297 | CE Loss: 0.1169\n",
      "Train Epoch: 009 Batch: 00025/00094 | Loss: 0.0690 | EWC Loss: 0.0297 | CE Loss: 0.0393\n",
      "Train Epoch: 009 Batch: 00026/00094 | Loss: 0.1328 | EWC Loss: 0.0297 | CE Loss: 0.1031\n",
      "Train Epoch: 009 Batch: 00027/00094 | Loss: 0.0701 | EWC Loss: 0.0297 | CE Loss: 0.0404\n",
      "Train Epoch: 009 Batch: 00028/00094 | Loss: 0.0977 | EWC Loss: 0.0297 | CE Loss: 0.0681\n",
      "Train Epoch: 009 Batch: 00029/00094 | Loss: 0.1538 | EWC Loss: 0.0297 | CE Loss: 0.1242\n",
      "Train Epoch: 009 Batch: 00030/00094 | Loss: 0.0875 | EWC Loss: 0.0297 | CE Loss: 0.0579\n",
      "Train Epoch: 009 Batch: 00031/00094 | Loss: 0.1123 | EWC Loss: 0.0297 | CE Loss: 0.0826\n",
      "Train Epoch: 009 Batch: 00032/00094 | Loss: 0.1819 | EWC Loss: 0.0297 | CE Loss: 0.1523\n",
      "Train Epoch: 009 Batch: 00033/00094 | Loss: 0.1015 | EWC Loss: 0.0297 | CE Loss: 0.0718\n",
      "Train Epoch: 009 Batch: 00034/00094 | Loss: 0.1209 | EWC Loss: 0.0297 | CE Loss: 0.0912\n",
      "Train Epoch: 009 Batch: 00035/00094 | Loss: 0.1794 | EWC Loss: 0.0297 | CE Loss: 0.1497\n",
      "Train Epoch: 009 Batch: 00036/00094 | Loss: 0.1149 | EWC Loss: 0.0297 | CE Loss: 0.0852\n",
      "Train Epoch: 009 Batch: 00037/00094 | Loss: 0.2652 | EWC Loss: 0.0297 | CE Loss: 0.2355\n",
      "Train Epoch: 009 Batch: 00038/00094 | Loss: 0.1398 | EWC Loss: 0.0297 | CE Loss: 0.1101\n",
      "Train Epoch: 009 Batch: 00039/00094 | Loss: 0.1294 | EWC Loss: 0.0297 | CE Loss: 0.0998\n",
      "Train Epoch: 009 Batch: 00040/00094 | Loss: 0.1208 | EWC Loss: 0.0297 | CE Loss: 0.0911\n",
      "Train Epoch: 009 Batch: 00041/00094 | Loss: 0.1065 | EWC Loss: 0.0297 | CE Loss: 0.0769\n",
      "Train Epoch: 009 Batch: 00042/00094 | Loss: 0.1088 | EWC Loss: 0.0297 | CE Loss: 0.0791\n",
      "Train Epoch: 009 Batch: 00043/00094 | Loss: 0.1702 | EWC Loss: 0.0297 | CE Loss: 0.1405\n",
      "Train Epoch: 009 Batch: 00044/00094 | Loss: 0.1665 | EWC Loss: 0.0297 | CE Loss: 0.1368\n",
      "Train Epoch: 009 Batch: 00045/00094 | Loss: 0.1205 | EWC Loss: 0.0297 | CE Loss: 0.0909\n",
      "Train Epoch: 009 Batch: 00046/00094 | Loss: 0.0978 | EWC Loss: 0.0297 | CE Loss: 0.0681\n",
      "Train Epoch: 009 Batch: 00047/00094 | Loss: 0.0787 | EWC Loss: 0.0297 | CE Loss: 0.0491\n",
      "Train Epoch: 009 Batch: 00048/00094 | Loss: 0.1563 | EWC Loss: 0.0297 | CE Loss: 0.1267\n",
      "Train Epoch: 009 Batch: 00049/00094 | Loss: 0.0867 | EWC Loss: 0.0297 | CE Loss: 0.0571\n",
      "Train Epoch: 009 Batch: 00050/00094 | Loss: 0.0939 | EWC Loss: 0.0297 | CE Loss: 0.0642\n",
      "Train Epoch: 009 Batch: 00051/00094 | Loss: 0.0763 | EWC Loss: 0.0296 | CE Loss: 0.0466\n",
      "Train Epoch: 009 Batch: 00052/00094 | Loss: 0.1166 | EWC Loss: 0.0296 | CE Loss: 0.0870\n",
      "Train Epoch: 009 Batch: 00053/00094 | Loss: 0.1305 | EWC Loss: 0.0297 | CE Loss: 0.1008\n",
      "Train Epoch: 009 Batch: 00054/00094 | Loss: 0.2935 | EWC Loss: 0.0297 | CE Loss: 0.2639\n",
      "Train Epoch: 009 Batch: 00055/00094 | Loss: 0.1678 | EWC Loss: 0.0297 | CE Loss: 0.1382\n",
      "Train Epoch: 009 Batch: 00056/00094 | Loss: 0.0929 | EWC Loss: 0.0297 | CE Loss: 0.0632\n",
      "Train Epoch: 009 Batch: 00057/00094 | Loss: 0.1724 | EWC Loss: 0.0297 | CE Loss: 0.1428\n",
      "Train Epoch: 009 Batch: 00058/00094 | Loss: 0.2125 | EWC Loss: 0.0297 | CE Loss: 0.1829\n",
      "Train Epoch: 009 Batch: 00059/00094 | Loss: 0.1845 | EWC Loss: 0.0297 | CE Loss: 0.1548\n",
      "Train Epoch: 009 Batch: 00060/00094 | Loss: 0.0798 | EWC Loss: 0.0297 | CE Loss: 0.0502\n",
      "Train Epoch: 009 Batch: 00061/00094 | Loss: 0.2037 | EWC Loss: 0.0297 | CE Loss: 0.1740\n",
      "Train Epoch: 009 Batch: 00062/00094 | Loss: 0.2090 | EWC Loss: 0.0297 | CE Loss: 0.1793\n",
      "Train Epoch: 009 Batch: 00063/00094 | Loss: 0.0980 | EWC Loss: 0.0297 | CE Loss: 0.0683\n",
      "Train Epoch: 009 Batch: 00064/00094 | Loss: 0.2753 | EWC Loss: 0.0297 | CE Loss: 0.2456\n",
      "Train Epoch: 009 Batch: 00065/00094 | Loss: 0.0926 | EWC Loss: 0.0297 | CE Loss: 0.0629\n",
      "Train Epoch: 009 Batch: 00066/00094 | Loss: 0.1279 | EWC Loss: 0.0297 | CE Loss: 0.0983\n",
      "Train Epoch: 009 Batch: 00067/00094 | Loss: 0.1516 | EWC Loss: 0.0297 | CE Loss: 0.1219\n",
      "Train Epoch: 009 Batch: 00068/00094 | Loss: 0.2021 | EWC Loss: 0.0297 | CE Loss: 0.1724\n",
      "Train Epoch: 009 Batch: 00069/00094 | Loss: 0.0711 | EWC Loss: 0.0297 | CE Loss: 0.0415\n",
      "Train Epoch: 009 Batch: 00070/00094 | Loss: 0.1631 | EWC Loss: 0.0297 | CE Loss: 0.1334\n",
      "Train Epoch: 009 Batch: 00071/00094 | Loss: 0.0572 | EWC Loss: 0.0297 | CE Loss: 0.0275\n",
      "Train Epoch: 009 Batch: 00072/00094 | Loss: 0.1083 | EWC Loss: 0.0297 | CE Loss: 0.0786\n",
      "Train Epoch: 009 Batch: 00073/00094 | Loss: 0.2980 | EWC Loss: 0.0297 | CE Loss: 0.2683\n",
      "Train Epoch: 009 Batch: 00074/00094 | Loss: 0.1678 | EWC Loss: 0.0297 | CE Loss: 0.1381\n",
      "Train Epoch: 009 Batch: 00075/00094 | Loss: 0.1133 | EWC Loss: 0.0297 | CE Loss: 0.0837\n",
      "Train Epoch: 009 Batch: 00076/00094 | Loss: 0.1253 | EWC Loss: 0.0297 | CE Loss: 0.0956\n",
      "Train Epoch: 009 Batch: 00077/00094 | Loss: 0.2376 | EWC Loss: 0.0297 | CE Loss: 0.2080\n",
      "Train Epoch: 009 Batch: 00078/00094 | Loss: 0.1538 | EWC Loss: 0.0297 | CE Loss: 0.1241\n",
      "Train Epoch: 009 Batch: 00079/00094 | Loss: 0.0753 | EWC Loss: 0.0297 | CE Loss: 0.0456\n",
      "Train Epoch: 009 Batch: 00080/00094 | Loss: 0.0977 | EWC Loss: 0.0297 | CE Loss: 0.0680\n",
      "Train Epoch: 009 Batch: 00081/00094 | Loss: 0.1851 | EWC Loss: 0.0297 | CE Loss: 0.1554\n",
      "Train Epoch: 009 Batch: 00082/00094 | Loss: 0.1244 | EWC Loss: 0.0297 | CE Loss: 0.0947\n",
      "Train Epoch: 009 Batch: 00083/00094 | Loss: 0.1249 | EWC Loss: 0.0297 | CE Loss: 0.0952\n",
      "Train Epoch: 009 Batch: 00084/00094 | Loss: 0.1032 | EWC Loss: 0.0297 | CE Loss: 0.0734\n",
      "Train Epoch: 009 Batch: 00085/00094 | Loss: 0.1008 | EWC Loss: 0.0297 | CE Loss: 0.0711\n",
      "Train Epoch: 009 Batch: 00086/00094 | Loss: 0.1227 | EWC Loss: 0.0297 | CE Loss: 0.0930\n",
      "Train Epoch: 009 Batch: 00087/00094 | Loss: 0.1374 | EWC Loss: 0.0297 | CE Loss: 0.1077\n",
      "Train Epoch: 009 Batch: 00088/00094 | Loss: 0.1043 | EWC Loss: 0.0297 | CE Loss: 0.0746\n",
      "Train Epoch: 009 Batch: 00089/00094 | Loss: 0.1326 | EWC Loss: 0.0297 | CE Loss: 0.1029\n",
      "Train Epoch: 009 Batch: 00090/00094 | Loss: 0.2067 | EWC Loss: 0.0297 | CE Loss: 0.1770\n",
      "Train Epoch: 009 Batch: 00091/00094 | Loss: 0.1171 | EWC Loss: 0.0297 | CE Loss: 0.0874\n",
      "Train Epoch: 009 Batch: 00092/00094 | Loss: 0.1024 | EWC Loss: 0.0297 | CE Loss: 0.0727\n",
      "Train Epoch: 009 Batch: 00093/00094 | Loss: 0.1298 | EWC Loss: 0.0297 | CE Loss: 0.1001\n",
      "Train Epoch: 009 Batch: 00094/00094 | Loss: 0.1176 | EWC Loss: 0.0297 | CE Loss: 0.0879\n",
      "Train Epoch: 009 |Acc 95.81667 | Loss: 0.13657 | Task Loss 0.10689 | EWC Loss: 0.02968\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0954 | acc:96.2000\n",
      "[VAL Acc] Target: 96.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7154 | acc:48.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.7905 | acc:52.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8974 | acc:45.8015\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5631 | acc:57.3276\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7059 | acc:74.9538\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 74.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0506 | acc:67.1630\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9732 | acc:64.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.44%\n",
      "[VAL Acc] Avg 63.33%\n",
      "\n",
      "\n",
      "---------- Starting epoch 10 ----------\n",
      "Train Epoch: 010 Batch: 00001/00094 | Loss: 0.0692 | EWC Loss: 0.0297 | CE Loss: 0.0395\n",
      "Train Epoch: 010 Batch: 00002/00094 | Loss: 0.1090 | EWC Loss: 0.0297 | CE Loss: 0.0793\n",
      "Train Epoch: 010 Batch: 00003/00094 | Loss: 0.1545 | EWC Loss: 0.0297 | CE Loss: 0.1248\n",
      "Train Epoch: 010 Batch: 00004/00094 | Loss: 0.1259 | EWC Loss: 0.0297 | CE Loss: 0.0962\n",
      "Train Epoch: 010 Batch: 00005/00094 | Loss: 0.0887 | EWC Loss: 0.0297 | CE Loss: 0.0590\n",
      "Train Epoch: 010 Batch: 00006/00094 | Loss: 0.1911 | EWC Loss: 0.0297 | CE Loss: 0.1615\n",
      "Train Epoch: 010 Batch: 00007/00094 | Loss: 0.1964 | EWC Loss: 0.0297 | CE Loss: 0.1667\n",
      "Train Epoch: 010 Batch: 00008/00094 | Loss: 0.1027 | EWC Loss: 0.0297 | CE Loss: 0.0730\n",
      "Train Epoch: 010 Batch: 00009/00094 | Loss: 0.1103 | EWC Loss: 0.0297 | CE Loss: 0.0806\n",
      "Train Epoch: 010 Batch: 00010/00094 | Loss: 0.1121 | EWC Loss: 0.0297 | CE Loss: 0.0824\n",
      "Train Epoch: 010 Batch: 00011/00094 | Loss: 0.0705 | EWC Loss: 0.0297 | CE Loss: 0.0409\n",
      "Train Epoch: 010 Batch: 00012/00094 | Loss: 0.2795 | EWC Loss: 0.0297 | CE Loss: 0.2499\n",
      "Train Epoch: 010 Batch: 00013/00094 | Loss: 0.1189 | EWC Loss: 0.0297 | CE Loss: 0.0892\n",
      "Train Epoch: 010 Batch: 00014/00094 | Loss: 0.1387 | EWC Loss: 0.0297 | CE Loss: 0.1090\n",
      "Train Epoch: 010 Batch: 00015/00094 | Loss: 0.1077 | EWC Loss: 0.0297 | CE Loss: 0.0780\n",
      "Train Epoch: 010 Batch: 00016/00094 | Loss: 0.0803 | EWC Loss: 0.0297 | CE Loss: 0.0506\n",
      "Train Epoch: 010 Batch: 00017/00094 | Loss: 0.1771 | EWC Loss: 0.0297 | CE Loss: 0.1474\n",
      "Train Epoch: 010 Batch: 00018/00094 | Loss: 0.1019 | EWC Loss: 0.0297 | CE Loss: 0.0722\n",
      "Train Epoch: 010 Batch: 00019/00094 | Loss: 0.1600 | EWC Loss: 0.0297 | CE Loss: 0.1303\n",
      "Train Epoch: 010 Batch: 00020/00094 | Loss: 0.0752 | EWC Loss: 0.0297 | CE Loss: 0.0455\n",
      "Train Epoch: 010 Batch: 00021/00094 | Loss: 0.1445 | EWC Loss: 0.0297 | CE Loss: 0.1148\n",
      "Train Epoch: 010 Batch: 00022/00094 | Loss: 0.0996 | EWC Loss: 0.0297 | CE Loss: 0.0699\n",
      "Train Epoch: 010 Batch: 00023/00094 | Loss: 0.2055 | EWC Loss: 0.0297 | CE Loss: 0.1759\n",
      "Train Epoch: 010 Batch: 00024/00094 | Loss: 0.0924 | EWC Loss: 0.0297 | CE Loss: 0.0627\n",
      "Train Epoch: 010 Batch: 00025/00094 | Loss: 0.0952 | EWC Loss: 0.0297 | CE Loss: 0.0655\n",
      "Train Epoch: 010 Batch: 00026/00094 | Loss: 0.0652 | EWC Loss: 0.0297 | CE Loss: 0.0355\n",
      "Train Epoch: 010 Batch: 00027/00094 | Loss: 0.1137 | EWC Loss: 0.0297 | CE Loss: 0.0840\n",
      "Train Epoch: 010 Batch: 00028/00094 | Loss: 0.0989 | EWC Loss: 0.0297 | CE Loss: 0.0692\n",
      "Train Epoch: 010 Batch: 00029/00094 | Loss: 0.1372 | EWC Loss: 0.0297 | CE Loss: 0.1075\n",
      "Train Epoch: 010 Batch: 00030/00094 | Loss: 0.2007 | EWC Loss: 0.0297 | CE Loss: 0.1710\n",
      "Train Epoch: 010 Batch: 00031/00094 | Loss: 0.1562 | EWC Loss: 0.0297 | CE Loss: 0.1265\n",
      "Train Epoch: 010 Batch: 00032/00094 | Loss: 0.1327 | EWC Loss: 0.0297 | CE Loss: 0.1031\n",
      "Train Epoch: 010 Batch: 00033/00094 | Loss: 0.1535 | EWC Loss: 0.0297 | CE Loss: 0.1239\n",
      "Train Epoch: 010 Batch: 00034/00094 | Loss: 0.1669 | EWC Loss: 0.0297 | CE Loss: 0.1373\n",
      "Train Epoch: 010 Batch: 00035/00094 | Loss: 0.1298 | EWC Loss: 0.0297 | CE Loss: 0.1002\n",
      "Train Epoch: 010 Batch: 00036/00094 | Loss: 0.1082 | EWC Loss: 0.0297 | CE Loss: 0.0785\n",
      "Train Epoch: 010 Batch: 00037/00094 | Loss: 0.1533 | EWC Loss: 0.0297 | CE Loss: 0.1236\n",
      "Train Epoch: 010 Batch: 00038/00094 | Loss: 0.1497 | EWC Loss: 0.0297 | CE Loss: 0.1200\n",
      "Train Epoch: 010 Batch: 00039/00094 | Loss: 0.0963 | EWC Loss: 0.0297 | CE Loss: 0.0666\n",
      "Train Epoch: 010 Batch: 00040/00094 | Loss: 0.0693 | EWC Loss: 0.0297 | CE Loss: 0.0396\n",
      "Train Epoch: 010 Batch: 00041/00094 | Loss: 0.1343 | EWC Loss: 0.0297 | CE Loss: 0.1046\n",
      "Train Epoch: 010 Batch: 00042/00094 | Loss: 0.3094 | EWC Loss: 0.0297 | CE Loss: 0.2797\n",
      "Train Epoch: 010 Batch: 00043/00094 | Loss: 0.2082 | EWC Loss: 0.0297 | CE Loss: 0.1785\n",
      "Train Epoch: 010 Batch: 00044/00094 | Loss: 0.0715 | EWC Loss: 0.0297 | CE Loss: 0.0418\n",
      "Train Epoch: 010 Batch: 00045/00094 | Loss: 0.1260 | EWC Loss: 0.0297 | CE Loss: 0.0963\n",
      "Train Epoch: 010 Batch: 00046/00094 | Loss: 0.1566 | EWC Loss: 0.0297 | CE Loss: 0.1269\n",
      "Train Epoch: 010 Batch: 00047/00094 | Loss: 0.1121 | EWC Loss: 0.0297 | CE Loss: 0.0824\n",
      "Train Epoch: 010 Batch: 00048/00094 | Loss: 0.1019 | EWC Loss: 0.0297 | CE Loss: 0.0722\n",
      "Train Epoch: 010 Batch: 00049/00094 | Loss: 0.1400 | EWC Loss: 0.0297 | CE Loss: 0.1103\n",
      "Train Epoch: 010 Batch: 00050/00094 | Loss: 0.1399 | EWC Loss: 0.0297 | CE Loss: 0.1102\n",
      "Train Epoch: 010 Batch: 00051/00094 | Loss: 0.1516 | EWC Loss: 0.0297 | CE Loss: 0.1219\n",
      "Train Epoch: 010 Batch: 00052/00094 | Loss: 0.1390 | EWC Loss: 0.0297 | CE Loss: 0.1093\n",
      "Train Epoch: 010 Batch: 00053/00094 | Loss: 0.1149 | EWC Loss: 0.0297 | CE Loss: 0.0853\n",
      "Train Epoch: 010 Batch: 00054/00094 | Loss: 0.1551 | EWC Loss: 0.0297 | CE Loss: 0.1254\n",
      "Train Epoch: 010 Batch: 00055/00094 | Loss: 0.0807 | EWC Loss: 0.0297 | CE Loss: 0.0510\n",
      "Train Epoch: 010 Batch: 00056/00094 | Loss: 0.2008 | EWC Loss: 0.0297 | CE Loss: 0.1712\n",
      "Train Epoch: 010 Batch: 00057/00094 | Loss: 0.0772 | EWC Loss: 0.0297 | CE Loss: 0.0475\n",
      "Train Epoch: 010 Batch: 00058/00094 | Loss: 0.1270 | EWC Loss: 0.0297 | CE Loss: 0.0973\n",
      "Train Epoch: 010 Batch: 00059/00094 | Loss: 0.2068 | EWC Loss: 0.0297 | CE Loss: 0.1772\n",
      "Train Epoch: 010 Batch: 00060/00094 | Loss: 0.0920 | EWC Loss: 0.0297 | CE Loss: 0.0624\n",
      "Train Epoch: 010 Batch: 00061/00094 | Loss: 0.1416 | EWC Loss: 0.0297 | CE Loss: 0.1119\n",
      "Train Epoch: 010 Batch: 00062/00094 | Loss: 0.0792 | EWC Loss: 0.0297 | CE Loss: 0.0495\n",
      "Train Epoch: 010 Batch: 00063/00094 | Loss: 0.1928 | EWC Loss: 0.0297 | CE Loss: 0.1631\n",
      "Train Epoch: 010 Batch: 00064/00094 | Loss: 0.1374 | EWC Loss: 0.0297 | CE Loss: 0.1077\n",
      "Train Epoch: 010 Batch: 00065/00094 | Loss: 0.1394 | EWC Loss: 0.0297 | CE Loss: 0.1098\n",
      "Train Epoch: 010 Batch: 00066/00094 | Loss: 0.0846 | EWC Loss: 0.0297 | CE Loss: 0.0549\n",
      "Train Epoch: 010 Batch: 00067/00094 | Loss: 0.1412 | EWC Loss: 0.0297 | CE Loss: 0.1115\n",
      "Train Epoch: 010 Batch: 00068/00094 | Loss: 0.2668 | EWC Loss: 0.0297 | CE Loss: 0.2371\n",
      "Train Epoch: 010 Batch: 00069/00094 | Loss: 0.2054 | EWC Loss: 0.0297 | CE Loss: 0.1757\n",
      "Train Epoch: 010 Batch: 00070/00094 | Loss: 0.0853 | EWC Loss: 0.0297 | CE Loss: 0.0556\n",
      "Train Epoch: 010 Batch: 00071/00094 | Loss: 0.2616 | EWC Loss: 0.0297 | CE Loss: 0.2319\n",
      "Train Epoch: 010 Batch: 00072/00094 | Loss: 0.1002 | EWC Loss: 0.0297 | CE Loss: 0.0705\n",
      "Train Epoch: 010 Batch: 00073/00094 | Loss: 0.1317 | EWC Loss: 0.0297 | CE Loss: 0.1020\n",
      "Train Epoch: 010 Batch: 00074/00094 | Loss: 0.0764 | EWC Loss: 0.0297 | CE Loss: 0.0467\n",
      "Train Epoch: 010 Batch: 00075/00094 | Loss: 0.1438 | EWC Loss: 0.0297 | CE Loss: 0.1142\n",
      "Train Epoch: 010 Batch: 00076/00094 | Loss: 0.1300 | EWC Loss: 0.0297 | CE Loss: 0.1003\n",
      "Train Epoch: 010 Batch: 00077/00094 | Loss: 0.1903 | EWC Loss: 0.0297 | CE Loss: 0.1607\n",
      "Train Epoch: 010 Batch: 00078/00094 | Loss: 0.0906 | EWC Loss: 0.0297 | CE Loss: 0.0609\n",
      "Train Epoch: 010 Batch: 00079/00094 | Loss: 0.1546 | EWC Loss: 0.0297 | CE Loss: 0.1249\n",
      "Train Epoch: 010 Batch: 00080/00094 | Loss: 0.1250 | EWC Loss: 0.0297 | CE Loss: 0.0954\n",
      "Train Epoch: 010 Batch: 00081/00094 | Loss: 0.1944 | EWC Loss: 0.0297 | CE Loss: 0.1647\n",
      "Train Epoch: 010 Batch: 00082/00094 | Loss: 0.1560 | EWC Loss: 0.0297 | CE Loss: 0.1263\n",
      "Train Epoch: 010 Batch: 00083/00094 | Loss: 0.0910 | EWC Loss: 0.0297 | CE Loss: 0.0613\n",
      "Train Epoch: 010 Batch: 00084/00094 | Loss: 0.0805 | EWC Loss: 0.0297 | CE Loss: 0.0508\n",
      "Train Epoch: 010 Batch: 00085/00094 | Loss: 0.1405 | EWC Loss: 0.0297 | CE Loss: 0.1109\n",
      "Train Epoch: 010 Batch: 00086/00094 | Loss: 0.1333 | EWC Loss: 0.0297 | CE Loss: 0.1036\n",
      "Train Epoch: 010 Batch: 00087/00094 | Loss: 0.1109 | EWC Loss: 0.0297 | CE Loss: 0.0812\n",
      "Train Epoch: 010 Batch: 00088/00094 | Loss: 0.1400 | EWC Loss: 0.0297 | CE Loss: 0.1103\n",
      "Train Epoch: 010 Batch: 00089/00094 | Loss: 0.1106 | EWC Loss: 0.0297 | CE Loss: 0.0809\n",
      "Train Epoch: 010 Batch: 00090/00094 | Loss: 0.1642 | EWC Loss: 0.0297 | CE Loss: 0.1345\n",
      "Train Epoch: 010 Batch: 00091/00094 | Loss: 0.1394 | EWC Loss: 0.0297 | CE Loss: 0.1097\n",
      "Train Epoch: 010 Batch: 00092/00094 | Loss: 0.0875 | EWC Loss: 0.0297 | CE Loss: 0.0578\n",
      "Train Epoch: 010 Batch: 00093/00094 | Loss: 0.1372 | EWC Loss: 0.0297 | CE Loss: 0.1075\n",
      "Train Epoch: 010 Batch: 00094/00094 | Loss: 0.0979 | EWC Loss: 0.0297 | CE Loss: 0.0683\n",
      "Train Epoch: 010 |Acc 96.10000 | Loss: 0.13452 | Task Loss 0.10484 | EWC Loss: 0.02968\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1066 | acc:96.0500\n",
      "[VAL Acc] Target: 96.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7850 | acc:48.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.7752 | acc:54.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8428 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6275 | acc:56.2696\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5851 | acc:76.4325\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.43%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0675 | acc:67.7116\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9162 | acc:63.9375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 63.94%\n",
      "[VAL Acc] Avg 63.58%\n",
      "\n",
      "\n",
      "---------- Starting epoch 11 ----------\n",
      "Train Epoch: 011 Batch: 00001/00094 | Loss: 0.1532 | EWC Loss: 0.0297 | CE Loss: 0.1236\n",
      "Train Epoch: 011 Batch: 00002/00094 | Loss: 0.2429 | EWC Loss: 0.0297 | CE Loss: 0.2132\n",
      "Train Epoch: 011 Batch: 00003/00094 | Loss: 0.1990 | EWC Loss: 0.0297 | CE Loss: 0.1693\n",
      "Train Epoch: 011 Batch: 00004/00094 | Loss: 0.1386 | EWC Loss: 0.0297 | CE Loss: 0.1089\n",
      "Train Epoch: 011 Batch: 00005/00094 | Loss: 0.1661 | EWC Loss: 0.0297 | CE Loss: 0.1364\n",
      "Train Epoch: 011 Batch: 00006/00094 | Loss: 0.1820 | EWC Loss: 0.0297 | CE Loss: 0.1523\n",
      "Train Epoch: 011 Batch: 00007/00094 | Loss: 0.0954 | EWC Loss: 0.0297 | CE Loss: 0.0657\n",
      "Train Epoch: 011 Batch: 00008/00094 | Loss: 0.1549 | EWC Loss: 0.0297 | CE Loss: 0.1252\n",
      "Train Epoch: 011 Batch: 00009/00094 | Loss: 0.1816 | EWC Loss: 0.0297 | CE Loss: 0.1520\n",
      "Train Epoch: 011 Batch: 00010/00094 | Loss: 0.0772 | EWC Loss: 0.0297 | CE Loss: 0.0476\n",
      "Train Epoch: 011 Batch: 00011/00094 | Loss: 0.1254 | EWC Loss: 0.0297 | CE Loss: 0.0957\n",
      "Train Epoch: 011 Batch: 00012/00094 | Loss: 0.1370 | EWC Loss: 0.0297 | CE Loss: 0.1073\n",
      "Train Epoch: 011 Batch: 00013/00094 | Loss: 0.3108 | EWC Loss: 0.0297 | CE Loss: 0.2812\n",
      "Train Epoch: 011 Batch: 00014/00094 | Loss: 0.1379 | EWC Loss: 0.0297 | CE Loss: 0.1082\n",
      "Train Epoch: 011 Batch: 00015/00094 | Loss: 0.1617 | EWC Loss: 0.0297 | CE Loss: 0.1321\n",
      "Train Epoch: 011 Batch: 00016/00094 | Loss: 0.1627 | EWC Loss: 0.0297 | CE Loss: 0.1330\n",
      "Train Epoch: 011 Batch: 00017/00094 | Loss: 0.1403 | EWC Loss: 0.0297 | CE Loss: 0.1107\n",
      "Train Epoch: 011 Batch: 00018/00094 | Loss: 0.0918 | EWC Loss: 0.0297 | CE Loss: 0.0622\n",
      "Train Epoch: 011 Batch: 00019/00094 | Loss: 0.1579 | EWC Loss: 0.0297 | CE Loss: 0.1283\n",
      "Train Epoch: 011 Batch: 00020/00094 | Loss: 0.1223 | EWC Loss: 0.0297 | CE Loss: 0.0926\n",
      "Train Epoch: 011 Batch: 00021/00094 | Loss: 0.1881 | EWC Loss: 0.0297 | CE Loss: 0.1585\n",
      "Train Epoch: 011 Batch: 00022/00094 | Loss: 0.1279 | EWC Loss: 0.0297 | CE Loss: 0.0982\n",
      "Train Epoch: 011 Batch: 00023/00094 | Loss: 0.1010 | EWC Loss: 0.0297 | CE Loss: 0.0713\n",
      "Train Epoch: 011 Batch: 00024/00094 | Loss: 0.1143 | EWC Loss: 0.0297 | CE Loss: 0.0846\n",
      "Train Epoch: 011 Batch: 00025/00094 | Loss: 0.1802 | EWC Loss: 0.0297 | CE Loss: 0.1505\n",
      "Train Epoch: 011 Batch: 00026/00094 | Loss: 0.0843 | EWC Loss: 0.0297 | CE Loss: 0.0546\n",
      "Train Epoch: 011 Batch: 00027/00094 | Loss: 0.1267 | EWC Loss: 0.0297 | CE Loss: 0.0970\n",
      "Train Epoch: 011 Batch: 00028/00094 | Loss: 0.1316 | EWC Loss: 0.0297 | CE Loss: 0.1020\n",
      "Train Epoch: 011 Batch: 00029/00094 | Loss: 0.1246 | EWC Loss: 0.0297 | CE Loss: 0.0949\n",
      "Train Epoch: 011 Batch: 00030/00094 | Loss: 0.1310 | EWC Loss: 0.0297 | CE Loss: 0.1014\n",
      "Train Epoch: 011 Batch: 00031/00094 | Loss: 0.0522 | EWC Loss: 0.0296 | CE Loss: 0.0226\n",
      "Train Epoch: 011 Batch: 00032/00094 | Loss: 0.0806 | EWC Loss: 0.0296 | CE Loss: 0.0510\n",
      "Train Epoch: 011 Batch: 00033/00094 | Loss: 0.1328 | EWC Loss: 0.0296 | CE Loss: 0.1032\n",
      "Train Epoch: 011 Batch: 00034/00094 | Loss: 0.0741 | EWC Loss: 0.0296 | CE Loss: 0.0444\n",
      "Train Epoch: 011 Batch: 00035/00094 | Loss: 0.1715 | EWC Loss: 0.0296 | CE Loss: 0.1419\n",
      "Train Epoch: 011 Batch: 00036/00094 | Loss: 0.1053 | EWC Loss: 0.0297 | CE Loss: 0.0756\n",
      "Train Epoch: 011 Batch: 00037/00094 | Loss: 0.0856 | EWC Loss: 0.0296 | CE Loss: 0.0559\n",
      "Train Epoch: 011 Batch: 00038/00094 | Loss: 0.1669 | EWC Loss: 0.0296 | CE Loss: 0.1372\n",
      "Train Epoch: 011 Batch: 00039/00094 | Loss: 0.1521 | EWC Loss: 0.0296 | CE Loss: 0.1224\n",
      "Train Epoch: 011 Batch: 00040/00094 | Loss: 0.1027 | EWC Loss: 0.0296 | CE Loss: 0.0730\n",
      "Train Epoch: 011 Batch: 00041/00094 | Loss: 0.1298 | EWC Loss: 0.0296 | CE Loss: 0.1001\n",
      "Train Epoch: 011 Batch: 00042/00094 | Loss: 0.0979 | EWC Loss: 0.0296 | CE Loss: 0.0682\n",
      "Train Epoch: 011 Batch: 00043/00094 | Loss: 0.1743 | EWC Loss: 0.0296 | CE Loss: 0.1446\n",
      "Train Epoch: 011 Batch: 00044/00094 | Loss: 0.1060 | EWC Loss: 0.0296 | CE Loss: 0.0764\n",
      "Train Epoch: 011 Batch: 00045/00094 | Loss: 0.0685 | EWC Loss: 0.0296 | CE Loss: 0.0389\n",
      "Train Epoch: 011 Batch: 00046/00094 | Loss: 0.1930 | EWC Loss: 0.0296 | CE Loss: 0.1634\n",
      "Train Epoch: 011 Batch: 00047/00094 | Loss: 0.1536 | EWC Loss: 0.0296 | CE Loss: 0.1239\n",
      "Train Epoch: 011 Batch: 00048/00094 | Loss: 0.0531 | EWC Loss: 0.0296 | CE Loss: 0.0235\n",
      "Train Epoch: 011 Batch: 00049/00094 | Loss: 0.1040 | EWC Loss: 0.0296 | CE Loss: 0.0744\n",
      "Train Epoch: 011 Batch: 00050/00094 | Loss: 0.1131 | EWC Loss: 0.0296 | CE Loss: 0.0835\n",
      "Train Epoch: 011 Batch: 00051/00094 | Loss: 0.1335 | EWC Loss: 0.0296 | CE Loss: 0.1039\n",
      "Train Epoch: 011 Batch: 00052/00094 | Loss: 0.1102 | EWC Loss: 0.0296 | CE Loss: 0.0806\n",
      "Train Epoch: 011 Batch: 00053/00094 | Loss: 0.1503 | EWC Loss: 0.0296 | CE Loss: 0.1207\n",
      "Train Epoch: 011 Batch: 00054/00094 | Loss: 0.1839 | EWC Loss: 0.0296 | CE Loss: 0.1543\n",
      "Train Epoch: 011 Batch: 00055/00094 | Loss: 0.1274 | EWC Loss: 0.0296 | CE Loss: 0.0978\n",
      "Train Epoch: 011 Batch: 00056/00094 | Loss: 0.1756 | EWC Loss: 0.0296 | CE Loss: 0.1459\n",
      "Train Epoch: 011 Batch: 00057/00094 | Loss: 0.0997 | EWC Loss: 0.0296 | CE Loss: 0.0700\n",
      "Train Epoch: 011 Batch: 00058/00094 | Loss: 0.1003 | EWC Loss: 0.0296 | CE Loss: 0.0707\n",
      "Train Epoch: 011 Batch: 00059/00094 | Loss: 0.1019 | EWC Loss: 0.0296 | CE Loss: 0.0723\n",
      "Train Epoch: 011 Batch: 00060/00094 | Loss: 0.0990 | EWC Loss: 0.0296 | CE Loss: 0.0694\n",
      "Train Epoch: 011 Batch: 00061/00094 | Loss: 0.1283 | EWC Loss: 0.0296 | CE Loss: 0.0987\n",
      "Train Epoch: 011 Batch: 00062/00094 | Loss: 0.1279 | EWC Loss: 0.0296 | CE Loss: 0.0982\n",
      "Train Epoch: 011 Batch: 00063/00094 | Loss: 0.0776 | EWC Loss: 0.0296 | CE Loss: 0.0480\n",
      "Train Epoch: 011 Batch: 00064/00094 | Loss: 0.0650 | EWC Loss: 0.0296 | CE Loss: 0.0354\n",
      "Train Epoch: 011 Batch: 00065/00094 | Loss: 0.0786 | EWC Loss: 0.0296 | CE Loss: 0.0490\n",
      "Train Epoch: 011 Batch: 00066/00094 | Loss: 0.1067 | EWC Loss: 0.0296 | CE Loss: 0.0771\n",
      "Train Epoch: 011 Batch: 00067/00094 | Loss: 0.1402 | EWC Loss: 0.0296 | CE Loss: 0.1106\n",
      "Train Epoch: 011 Batch: 00068/00094 | Loss: 0.1446 | EWC Loss: 0.0296 | CE Loss: 0.1149\n",
      "Train Epoch: 011 Batch: 00069/00094 | Loss: 0.0855 | EWC Loss: 0.0296 | CE Loss: 0.0559\n",
      "Train Epoch: 011 Batch: 00070/00094 | Loss: 0.1009 | EWC Loss: 0.0296 | CE Loss: 0.0713\n",
      "Train Epoch: 011 Batch: 00071/00094 | Loss: 0.0639 | EWC Loss: 0.0296 | CE Loss: 0.0343\n",
      "Train Epoch: 011 Batch: 00072/00094 | Loss: 0.1135 | EWC Loss: 0.0296 | CE Loss: 0.0839\n",
      "Train Epoch: 011 Batch: 00073/00094 | Loss: 0.1541 | EWC Loss: 0.0296 | CE Loss: 0.1245\n",
      "Train Epoch: 011 Batch: 00074/00094 | Loss: 0.1904 | EWC Loss: 0.0296 | CE Loss: 0.1608\n",
      "Train Epoch: 011 Batch: 00075/00094 | Loss: 0.1605 | EWC Loss: 0.0296 | CE Loss: 0.1309\n",
      "Train Epoch: 011 Batch: 00076/00094 | Loss: 0.0887 | EWC Loss: 0.0296 | CE Loss: 0.0590\n",
      "Train Epoch: 011 Batch: 00077/00094 | Loss: 0.1106 | EWC Loss: 0.0296 | CE Loss: 0.0809\n",
      "Train Epoch: 011 Batch: 00078/00094 | Loss: 0.1134 | EWC Loss: 0.0296 | CE Loss: 0.0838\n",
      "Train Epoch: 011 Batch: 00079/00094 | Loss: 0.1212 | EWC Loss: 0.0296 | CE Loss: 0.0915\n",
      "Train Epoch: 011 Batch: 00080/00094 | Loss: 0.0984 | EWC Loss: 0.0296 | CE Loss: 0.0688\n",
      "Train Epoch: 011 Batch: 00081/00094 | Loss: 0.2272 | EWC Loss: 0.0296 | CE Loss: 0.1976\n",
      "Train Epoch: 011 Batch: 00082/00094 | Loss: 0.1110 | EWC Loss: 0.0296 | CE Loss: 0.0814\n",
      "Train Epoch: 011 Batch: 00083/00094 | Loss: 0.0765 | EWC Loss: 0.0296 | CE Loss: 0.0469\n",
      "Train Epoch: 011 Batch: 00084/00094 | Loss: 0.1511 | EWC Loss: 0.0296 | CE Loss: 0.1215\n",
      "Train Epoch: 011 Batch: 00085/00094 | Loss: 0.0781 | EWC Loss: 0.0296 | CE Loss: 0.0485\n",
      "Train Epoch: 011 Batch: 00086/00094 | Loss: 0.0792 | EWC Loss: 0.0296 | CE Loss: 0.0496\n",
      "Train Epoch: 011 Batch: 00087/00094 | Loss: 0.1517 | EWC Loss: 0.0296 | CE Loss: 0.1221\n",
      "Train Epoch: 011 Batch: 00088/00094 | Loss: 0.1280 | EWC Loss: 0.0296 | CE Loss: 0.0984\n",
      "Train Epoch: 011 Batch: 00089/00094 | Loss: 0.1585 | EWC Loss: 0.0296 | CE Loss: 0.1289\n",
      "Train Epoch: 011 Batch: 00090/00094 | Loss: 0.2194 | EWC Loss: 0.0296 | CE Loss: 0.1898\n",
      "Train Epoch: 011 Batch: 00091/00094 | Loss: 0.1318 | EWC Loss: 0.0296 | CE Loss: 0.1022\n",
      "Train Epoch: 011 Batch: 00092/00094 | Loss: 0.2286 | EWC Loss: 0.0296 | CE Loss: 0.1990\n",
      "Train Epoch: 011 Batch: 00093/00094 | Loss: 0.0970 | EWC Loss: 0.0296 | CE Loss: 0.0674\n",
      "Train Epoch: 011 Batch: 00094/00094 | Loss: 0.2081 | EWC Loss: 0.0296 | CE Loss: 0.1785\n",
      "Train Epoch: 011 |Acc 96.25000 | Loss: 0.13153 | Task Loss 0.10189 | EWC Loss: 0.02964\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1038 | acc:96.1500\n",
      "[VAL Acc] Target: 96.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8492 | acc:48.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9610 | acc:51.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8527 | acc:48.2824\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 48.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6300 | acc:56.5047\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5807 | acc:75.5083\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1309 | acc:66.5361\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9127 | acc:64.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.88%\n",
      "[VAL Acc] Avg 63.55%\n",
      "\n",
      "\n",
      "---------- Starting epoch 12 ----------\n",
      "Train Epoch: 012 Batch: 00001/00094 | Loss: 0.1508 | EWC Loss: 0.0296 | CE Loss: 0.1212\n",
      "Train Epoch: 012 Batch: 00002/00094 | Loss: 0.1511 | EWC Loss: 0.0296 | CE Loss: 0.1215\n",
      "Train Epoch: 012 Batch: 00003/00094 | Loss: 0.1060 | EWC Loss: 0.0296 | CE Loss: 0.0764\n",
      "Train Epoch: 012 Batch: 00004/00094 | Loss: 0.0990 | EWC Loss: 0.0296 | CE Loss: 0.0694\n",
      "Train Epoch: 012 Batch: 00005/00094 | Loss: 0.1720 | EWC Loss: 0.0296 | CE Loss: 0.1424\n",
      "Train Epoch: 012 Batch: 00006/00094 | Loss: 0.1261 | EWC Loss: 0.0296 | CE Loss: 0.0965\n",
      "Train Epoch: 012 Batch: 00007/00094 | Loss: 0.1178 | EWC Loss: 0.0296 | CE Loss: 0.0881\n",
      "Train Epoch: 012 Batch: 00008/00094 | Loss: 0.0721 | EWC Loss: 0.0296 | CE Loss: 0.0425\n",
      "Train Epoch: 012 Batch: 00009/00094 | Loss: 0.0722 | EWC Loss: 0.0296 | CE Loss: 0.0426\n",
      "Train Epoch: 012 Batch: 00010/00094 | Loss: 0.1228 | EWC Loss: 0.0296 | CE Loss: 0.0932\n",
      "Train Epoch: 012 Batch: 00011/00094 | Loss: 0.1216 | EWC Loss: 0.0296 | CE Loss: 0.0920\n",
      "Train Epoch: 012 Batch: 00012/00094 | Loss: 0.0619 | EWC Loss: 0.0296 | CE Loss: 0.0323\n",
      "Train Epoch: 012 Batch: 00013/00094 | Loss: 0.1038 | EWC Loss: 0.0296 | CE Loss: 0.0741\n",
      "Train Epoch: 012 Batch: 00014/00094 | Loss: 0.1198 | EWC Loss: 0.0296 | CE Loss: 0.0902\n",
      "Train Epoch: 012 Batch: 00015/00094 | Loss: 0.1056 | EWC Loss: 0.0296 | CE Loss: 0.0760\n",
      "Train Epoch: 012 Batch: 00016/00094 | Loss: 0.0889 | EWC Loss: 0.0296 | CE Loss: 0.0593\n",
      "Train Epoch: 012 Batch: 00017/00094 | Loss: 0.0828 | EWC Loss: 0.0296 | CE Loss: 0.0532\n",
      "Train Epoch: 012 Batch: 00018/00094 | Loss: 0.1337 | EWC Loss: 0.0296 | CE Loss: 0.1041\n",
      "Train Epoch: 012 Batch: 00019/00094 | Loss: 0.2642 | EWC Loss: 0.0296 | CE Loss: 0.2346\n",
      "Train Epoch: 012 Batch: 00020/00094 | Loss: 0.1158 | EWC Loss: 0.0296 | CE Loss: 0.0863\n",
      "Train Epoch: 012 Batch: 00021/00094 | Loss: 0.1055 | EWC Loss: 0.0296 | CE Loss: 0.0760\n",
      "Train Epoch: 012 Batch: 00022/00094 | Loss: 0.1428 | EWC Loss: 0.0296 | CE Loss: 0.1132\n",
      "Train Epoch: 012 Batch: 00023/00094 | Loss: 0.0889 | EWC Loss: 0.0296 | CE Loss: 0.0593\n",
      "Train Epoch: 012 Batch: 00024/00094 | Loss: 0.1798 | EWC Loss: 0.0296 | CE Loss: 0.1502\n",
      "Train Epoch: 012 Batch: 00025/00094 | Loss: 0.1048 | EWC Loss: 0.0296 | CE Loss: 0.0752\n",
      "Train Epoch: 012 Batch: 00026/00094 | Loss: 0.1195 | EWC Loss: 0.0296 | CE Loss: 0.0900\n",
      "Train Epoch: 012 Batch: 00027/00094 | Loss: 0.1414 | EWC Loss: 0.0296 | CE Loss: 0.1119\n",
      "Train Epoch: 012 Batch: 00028/00094 | Loss: 0.0785 | EWC Loss: 0.0295 | CE Loss: 0.0489\n",
      "Train Epoch: 012 Batch: 00029/00094 | Loss: 0.1504 | EWC Loss: 0.0295 | CE Loss: 0.1209\n",
      "Train Epoch: 012 Batch: 00030/00094 | Loss: 0.1158 | EWC Loss: 0.0295 | CE Loss: 0.0862\n",
      "Train Epoch: 012 Batch: 00031/00094 | Loss: 0.1243 | EWC Loss: 0.0295 | CE Loss: 0.0947\n",
      "Train Epoch: 012 Batch: 00032/00094 | Loss: 0.1446 | EWC Loss: 0.0295 | CE Loss: 0.1151\n",
      "Train Epoch: 012 Batch: 00033/00094 | Loss: 0.1256 | EWC Loss: 0.0296 | CE Loss: 0.0961\n",
      "Train Epoch: 012 Batch: 00034/00094 | Loss: 0.0725 | EWC Loss: 0.0296 | CE Loss: 0.0429\n",
      "Train Epoch: 012 Batch: 00035/00094 | Loss: 0.1579 | EWC Loss: 0.0296 | CE Loss: 0.1284\n",
      "Train Epoch: 012 Batch: 00036/00094 | Loss: 0.0952 | EWC Loss: 0.0296 | CE Loss: 0.0656\n",
      "Train Epoch: 012 Batch: 00037/00094 | Loss: 0.0615 | EWC Loss: 0.0296 | CE Loss: 0.0319\n",
      "Train Epoch: 012 Batch: 00038/00094 | Loss: 0.1614 | EWC Loss: 0.0296 | CE Loss: 0.1319\n",
      "Train Epoch: 012 Batch: 00039/00094 | Loss: 0.1462 | EWC Loss: 0.0296 | CE Loss: 0.1167\n",
      "Train Epoch: 012 Batch: 00040/00094 | Loss: 0.2421 | EWC Loss: 0.0296 | CE Loss: 0.2125\n",
      "Train Epoch: 012 Batch: 00041/00094 | Loss: 0.0751 | EWC Loss: 0.0296 | CE Loss: 0.0455\n",
      "Train Epoch: 012 Batch: 00042/00094 | Loss: 0.1178 | EWC Loss: 0.0295 | CE Loss: 0.0883\n",
      "Train Epoch: 012 Batch: 00043/00094 | Loss: 0.0945 | EWC Loss: 0.0295 | CE Loss: 0.0650\n",
      "Train Epoch: 012 Batch: 00044/00094 | Loss: 0.1586 | EWC Loss: 0.0295 | CE Loss: 0.1291\n",
      "Train Epoch: 012 Batch: 00045/00094 | Loss: 0.1288 | EWC Loss: 0.0296 | CE Loss: 0.0993\n",
      "Train Epoch: 012 Batch: 00046/00094 | Loss: 0.1285 | EWC Loss: 0.0296 | CE Loss: 0.0990\n",
      "Train Epoch: 012 Batch: 00047/00094 | Loss: 0.0768 | EWC Loss: 0.0296 | CE Loss: 0.0472\n",
      "Train Epoch: 012 Batch: 00048/00094 | Loss: 0.0744 | EWC Loss: 0.0296 | CE Loss: 0.0448\n",
      "Train Epoch: 012 Batch: 00049/00094 | Loss: 0.0736 | EWC Loss: 0.0296 | CE Loss: 0.0440\n",
      "Train Epoch: 012 Batch: 00050/00094 | Loss: 0.1565 | EWC Loss: 0.0295 | CE Loss: 0.1270\n",
      "Train Epoch: 012 Batch: 00051/00094 | Loss: 0.0870 | EWC Loss: 0.0295 | CE Loss: 0.0574\n",
      "Train Epoch: 012 Batch: 00052/00094 | Loss: 0.0969 | EWC Loss: 0.0295 | CE Loss: 0.0673\n",
      "Train Epoch: 012 Batch: 00053/00094 | Loss: 0.2445 | EWC Loss: 0.0295 | CE Loss: 0.2150\n",
      "Train Epoch: 012 Batch: 00054/00094 | Loss: 0.1230 | EWC Loss: 0.0295 | CE Loss: 0.0934\n",
      "Train Epoch: 012 Batch: 00055/00094 | Loss: 0.0932 | EWC Loss: 0.0295 | CE Loss: 0.0636\n",
      "Train Epoch: 012 Batch: 00056/00094 | Loss: 0.1805 | EWC Loss: 0.0295 | CE Loss: 0.1510\n",
      "Train Epoch: 012 Batch: 00057/00094 | Loss: 0.1069 | EWC Loss: 0.0296 | CE Loss: 0.0774\n",
      "Train Epoch: 012 Batch: 00058/00094 | Loss: 0.1003 | EWC Loss: 0.0296 | CE Loss: 0.0708\n",
      "Train Epoch: 012 Batch: 00059/00094 | Loss: 0.1112 | EWC Loss: 0.0295 | CE Loss: 0.0817\n",
      "Train Epoch: 012 Batch: 00060/00094 | Loss: 0.1903 | EWC Loss: 0.0295 | CE Loss: 0.1608\n",
      "Train Epoch: 012 Batch: 00061/00094 | Loss: 0.1764 | EWC Loss: 0.0295 | CE Loss: 0.1469\n",
      "Train Epoch: 012 Batch: 00062/00094 | Loss: 0.0725 | EWC Loss: 0.0296 | CE Loss: 0.0430\n",
      "Train Epoch: 012 Batch: 00063/00094 | Loss: 0.1017 | EWC Loss: 0.0296 | CE Loss: 0.0722\n",
      "Train Epoch: 012 Batch: 00064/00094 | Loss: 0.1732 | EWC Loss: 0.0296 | CE Loss: 0.1436\n",
      "Train Epoch: 012 Batch: 00065/00094 | Loss: 0.1257 | EWC Loss: 0.0296 | CE Loss: 0.0961\n",
      "Train Epoch: 012 Batch: 00066/00094 | Loss: 0.1842 | EWC Loss: 0.0296 | CE Loss: 0.1546\n",
      "Train Epoch: 012 Batch: 00067/00094 | Loss: 0.1800 | EWC Loss: 0.0296 | CE Loss: 0.1504\n",
      "Train Epoch: 012 Batch: 00068/00094 | Loss: 0.1943 | EWC Loss: 0.0296 | CE Loss: 0.1647\n",
      "Train Epoch: 012 Batch: 00069/00094 | Loss: 0.1286 | EWC Loss: 0.0296 | CE Loss: 0.0990\n",
      "Train Epoch: 012 Batch: 00070/00094 | Loss: 0.2537 | EWC Loss: 0.0296 | CE Loss: 0.2241\n",
      "Train Epoch: 012 Batch: 00071/00094 | Loss: 0.1692 | EWC Loss: 0.0296 | CE Loss: 0.1396\n",
      "Train Epoch: 012 Batch: 00072/00094 | Loss: 0.1236 | EWC Loss: 0.0296 | CE Loss: 0.0940\n",
      "Train Epoch: 012 Batch: 00073/00094 | Loss: 0.1329 | EWC Loss: 0.0296 | CE Loss: 0.1033\n",
      "Train Epoch: 012 Batch: 00074/00094 | Loss: 0.1736 | EWC Loss: 0.0296 | CE Loss: 0.1439\n",
      "Train Epoch: 012 Batch: 00075/00094 | Loss: 0.1825 | EWC Loss: 0.0296 | CE Loss: 0.1529\n",
      "Train Epoch: 012 Batch: 00076/00094 | Loss: 0.0877 | EWC Loss: 0.0296 | CE Loss: 0.0581\n",
      "Train Epoch: 012 Batch: 00077/00094 | Loss: 0.1157 | EWC Loss: 0.0296 | CE Loss: 0.0861\n",
      "Train Epoch: 012 Batch: 00078/00094 | Loss: 0.0987 | EWC Loss: 0.0296 | CE Loss: 0.0691\n",
      "Train Epoch: 012 Batch: 00079/00094 | Loss: 0.1599 | EWC Loss: 0.0297 | CE Loss: 0.1302\n",
      "Train Epoch: 012 Batch: 00080/00094 | Loss: 0.1857 | EWC Loss: 0.0296 | CE Loss: 0.1561\n",
      "Train Epoch: 012 Batch: 00081/00094 | Loss: 0.0613 | EWC Loss: 0.0296 | CE Loss: 0.0316\n",
      "Train Epoch: 012 Batch: 00082/00094 | Loss: 0.2183 | EWC Loss: 0.0296 | CE Loss: 0.1886\n",
      "Train Epoch: 012 Batch: 00083/00094 | Loss: 0.0679 | EWC Loss: 0.0296 | CE Loss: 0.0383\n",
      "Train Epoch: 012 Batch: 00084/00094 | Loss: 0.0725 | EWC Loss: 0.0296 | CE Loss: 0.0429\n",
      "Train Epoch: 012 Batch: 00085/00094 | Loss: 0.1221 | EWC Loss: 0.0296 | CE Loss: 0.0925\n",
      "Train Epoch: 012 Batch: 00086/00094 | Loss: 0.1716 | EWC Loss: 0.0296 | CE Loss: 0.1420\n",
      "Train Epoch: 012 Batch: 00087/00094 | Loss: 0.1439 | EWC Loss: 0.0296 | CE Loss: 0.1143\n",
      "Train Epoch: 012 Batch: 00088/00094 | Loss: 0.1101 | EWC Loss: 0.0296 | CE Loss: 0.0805\n",
      "Train Epoch: 012 Batch: 00089/00094 | Loss: 0.1451 | EWC Loss: 0.0296 | CE Loss: 0.1155\n",
      "Train Epoch: 012 Batch: 00090/00094 | Loss: 0.1854 | EWC Loss: 0.0296 | CE Loss: 0.1558\n",
      "Train Epoch: 012 Batch: 00091/00094 | Loss: 0.1891 | EWC Loss: 0.0296 | CE Loss: 0.1594\n",
      "Train Epoch: 012 Batch: 00092/00094 | Loss: 0.1356 | EWC Loss: 0.0296 | CE Loss: 0.1060\n",
      "Train Epoch: 012 Batch: 00093/00094 | Loss: 0.0859 | EWC Loss: 0.0296 | CE Loss: 0.0563\n",
      "Train Epoch: 012 Batch: 00094/00094 | Loss: 0.1662 | EWC Loss: 0.0296 | CE Loss: 0.1366\n",
      "Train Epoch: 012 |Acc 96.16667 | Loss: 0.13043 | Task Loss 0.10084 | EWC Loss: 0.02958\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0921 | acc:96.8500\n",
      "[VAL Acc] Target: 96.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8708 | acc:48.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8774 | acc:51.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8463 | acc:47.9008\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.8461 | acc:54.1144\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5998 | acc:76.2477\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.2116 | acc:64.4984\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9189 | acc:63.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 63.75%\n",
      "[VAL Acc] Avg 62.84%\n",
      "\n",
      "\n",
      "---------- Starting epoch 13 ----------\n",
      "Train Epoch: 013 Batch: 00001/00094 | Loss: 0.1006 | EWC Loss: 0.0296 | CE Loss: 0.0710\n",
      "Train Epoch: 013 Batch: 00002/00094 | Loss: 0.1409 | EWC Loss: 0.0296 | CE Loss: 0.1113\n",
      "Train Epoch: 013 Batch: 00003/00094 | Loss: 0.0784 | EWC Loss: 0.0296 | CE Loss: 0.0488\n",
      "Train Epoch: 013 Batch: 00004/00094 | Loss: 0.1572 | EWC Loss: 0.0296 | CE Loss: 0.1276\n",
      "Train Epoch: 013 Batch: 00005/00094 | Loss: 0.1183 | EWC Loss: 0.0296 | CE Loss: 0.0887\n",
      "Train Epoch: 013 Batch: 00006/00094 | Loss: 0.0736 | EWC Loss: 0.0296 | CE Loss: 0.0440\n",
      "Train Epoch: 013 Batch: 00007/00094 | Loss: 0.1632 | EWC Loss: 0.0296 | CE Loss: 0.1336\n",
      "Train Epoch: 013 Batch: 00008/00094 | Loss: 0.1704 | EWC Loss: 0.0297 | CE Loss: 0.1408\n",
      "Train Epoch: 013 Batch: 00009/00094 | Loss: 0.1114 | EWC Loss: 0.0297 | CE Loss: 0.0817\n",
      "Train Epoch: 013 Batch: 00010/00094 | Loss: 0.0950 | EWC Loss: 0.0297 | CE Loss: 0.0653\n",
      "Train Epoch: 013 Batch: 00011/00094 | Loss: 0.3567 | EWC Loss: 0.0297 | CE Loss: 0.3270\n",
      "Train Epoch: 013 Batch: 00012/00094 | Loss: 0.1332 | EWC Loss: 0.0299 | CE Loss: 0.1033\n",
      "Train Epoch: 013 Batch: 00013/00094 | Loss: 0.0962 | EWC Loss: 0.0298 | CE Loss: 0.0663\n",
      "Train Epoch: 013 Batch: 00014/00094 | Loss: 0.1782 | EWC Loss: 0.0298 | CE Loss: 0.1483\n",
      "Train Epoch: 013 Batch: 00015/00094 | Loss: 0.2109 | EWC Loss: 0.0298 | CE Loss: 0.1811\n",
      "Train Epoch: 013 Batch: 00016/00094 | Loss: 0.1520 | EWC Loss: 0.0299 | CE Loss: 0.1221\n",
      "Train Epoch: 013 Batch: 00017/00094 | Loss: 0.1503 | EWC Loss: 0.0299 | CE Loss: 0.1204\n",
      "Train Epoch: 013 Batch: 00018/00094 | Loss: 0.1857 | EWC Loss: 0.0299 | CE Loss: 0.1558\n",
      "Train Epoch: 013 Batch: 00019/00094 | Loss: 0.0878 | EWC Loss: 0.0299 | CE Loss: 0.0579\n",
      "Train Epoch: 013 Batch: 00020/00094 | Loss: 0.1685 | EWC Loss: 0.0298 | CE Loss: 0.1386\n",
      "Train Epoch: 013 Batch: 00021/00094 | Loss: 0.1301 | EWC Loss: 0.0298 | CE Loss: 0.1003\n",
      "Train Epoch: 013 Batch: 00022/00094 | Loss: 0.1692 | EWC Loss: 0.0298 | CE Loss: 0.1394\n",
      "Train Epoch: 013 Batch: 00023/00094 | Loss: 0.1108 | EWC Loss: 0.0298 | CE Loss: 0.0810\n",
      "Train Epoch: 013 Batch: 00024/00094 | Loss: 0.3966 | EWC Loss: 0.0298 | CE Loss: 0.3668\n",
      "Train Epoch: 013 Batch: 00025/00094 | Loss: 0.1248 | EWC Loss: 0.0298 | CE Loss: 0.0949\n",
      "Train Epoch: 013 Batch: 00026/00094 | Loss: 0.1917 | EWC Loss: 0.0298 | CE Loss: 0.1619\n",
      "Train Epoch: 013 Batch: 00027/00094 | Loss: 0.1446 | EWC Loss: 0.0298 | CE Loss: 0.1148\n",
      "Train Epoch: 013 Batch: 00028/00094 | Loss: 0.1511 | EWC Loss: 0.0298 | CE Loss: 0.1213\n",
      "Train Epoch: 013 Batch: 00029/00094 | Loss: 0.2128 | EWC Loss: 0.0299 | CE Loss: 0.1829\n",
      "Train Epoch: 013 Batch: 00030/00094 | Loss: 0.1290 | EWC Loss: 0.0298 | CE Loss: 0.0992\n",
      "Train Epoch: 013 Batch: 00031/00094 | Loss: 0.1095 | EWC Loss: 0.0298 | CE Loss: 0.0798\n",
      "Train Epoch: 013 Batch: 00032/00094 | Loss: 0.1840 | EWC Loss: 0.0297 | CE Loss: 0.1543\n",
      "Train Epoch: 013 Batch: 00033/00094 | Loss: 0.1122 | EWC Loss: 0.0297 | CE Loss: 0.0825\n",
      "Train Epoch: 013 Batch: 00034/00094 | Loss: 0.1360 | EWC Loss: 0.0297 | CE Loss: 0.1063\n",
      "Train Epoch: 013 Batch: 00035/00094 | Loss: 0.1325 | EWC Loss: 0.0297 | CE Loss: 0.1028\n",
      "Train Epoch: 013 Batch: 00036/00094 | Loss: 0.2137 | EWC Loss: 0.0297 | CE Loss: 0.1839\n",
      "Train Epoch: 013 Batch: 00037/00094 | Loss: 0.0726 | EWC Loss: 0.0297 | CE Loss: 0.0429\n",
      "Train Epoch: 013 Batch: 00038/00094 | Loss: 0.2272 | EWC Loss: 0.0297 | CE Loss: 0.1975\n",
      "Train Epoch: 013 Batch: 00039/00094 | Loss: 0.1900 | EWC Loss: 0.0297 | CE Loss: 0.1603\n",
      "Train Epoch: 013 Batch: 00040/00094 | Loss: 0.1408 | EWC Loss: 0.0297 | CE Loss: 0.1111\n",
      "Train Epoch: 013 Batch: 00041/00094 | Loss: 0.1915 | EWC Loss: 0.0297 | CE Loss: 0.1618\n",
      "Train Epoch: 013 Batch: 00042/00094 | Loss: 0.1969 | EWC Loss: 0.0298 | CE Loss: 0.1671\n",
      "Train Epoch: 013 Batch: 00043/00094 | Loss: 0.1112 | EWC Loss: 0.0297 | CE Loss: 0.0815\n",
      "Train Epoch: 013 Batch: 00044/00094 | Loss: 0.0889 | EWC Loss: 0.0297 | CE Loss: 0.0592\n",
      "Train Epoch: 013 Batch: 00045/00094 | Loss: 0.2270 | EWC Loss: 0.0297 | CE Loss: 0.1973\n",
      "Train Epoch: 013 Batch: 00046/00094 | Loss: 0.0851 | EWC Loss: 0.0297 | CE Loss: 0.0554\n",
      "Train Epoch: 013 Batch: 00047/00094 | Loss: 0.1270 | EWC Loss: 0.0297 | CE Loss: 0.0973\n",
      "Train Epoch: 013 Batch: 00048/00094 | Loss: 0.2313 | EWC Loss: 0.0298 | CE Loss: 0.2016\n",
      "Train Epoch: 013 Batch: 00049/00094 | Loss: 0.0932 | EWC Loss: 0.0298 | CE Loss: 0.0634\n",
      "Train Epoch: 013 Batch: 00050/00094 | Loss: 0.1415 | EWC Loss: 0.0299 | CE Loss: 0.1116\n",
      "Train Epoch: 013 Batch: 00051/00094 | Loss: 0.1565 | EWC Loss: 0.0298 | CE Loss: 0.1267\n",
      "Train Epoch: 013 Batch: 00052/00094 | Loss: 0.1500 | EWC Loss: 0.0298 | CE Loss: 0.1202\n",
      "Train Epoch: 013 Batch: 00053/00094 | Loss: 0.1411 | EWC Loss: 0.0298 | CE Loss: 0.1113\n",
      "Train Epoch: 013 Batch: 00054/00094 | Loss: 0.1811 | EWC Loss: 0.0298 | CE Loss: 0.1513\n",
      "Train Epoch: 013 Batch: 00055/00094 | Loss: 0.1026 | EWC Loss: 0.0299 | CE Loss: 0.0727\n",
      "Train Epoch: 013 Batch: 00056/00094 | Loss: 0.1153 | EWC Loss: 0.0299 | CE Loss: 0.0854\n",
      "Train Epoch: 013 Batch: 00057/00094 | Loss: 0.0720 | EWC Loss: 0.0299 | CE Loss: 0.0421\n",
      "Train Epoch: 013 Batch: 00058/00094 | Loss: 0.1262 | EWC Loss: 0.0299 | CE Loss: 0.0964\n",
      "Train Epoch: 013 Batch: 00059/00094 | Loss: 0.0907 | EWC Loss: 0.0298 | CE Loss: 0.0608\n",
      "Train Epoch: 013 Batch: 00060/00094 | Loss: 0.0792 | EWC Loss: 0.0298 | CE Loss: 0.0494\n",
      "Train Epoch: 013 Batch: 00061/00094 | Loss: 0.1464 | EWC Loss: 0.0299 | CE Loss: 0.1165\n",
      "Train Epoch: 013 Batch: 00062/00094 | Loss: 0.0647 | EWC Loss: 0.0298 | CE Loss: 0.0348\n",
      "Train Epoch: 013 Batch: 00063/00094 | Loss: 0.1184 | EWC Loss: 0.0298 | CE Loss: 0.0886\n",
      "Train Epoch: 013 Batch: 00064/00094 | Loss: 0.1779 | EWC Loss: 0.0297 | CE Loss: 0.1481\n",
      "Train Epoch: 013 Batch: 00065/00094 | Loss: 0.1243 | EWC Loss: 0.0297 | CE Loss: 0.0946\n",
      "Train Epoch: 013 Batch: 00066/00094 | Loss: 0.1163 | EWC Loss: 0.0297 | CE Loss: 0.0865\n",
      "Train Epoch: 013 Batch: 00067/00094 | Loss: 0.1392 | EWC Loss: 0.0297 | CE Loss: 0.1094\n",
      "Train Epoch: 013 Batch: 00068/00094 | Loss: 0.1312 | EWC Loss: 0.0297 | CE Loss: 0.1014\n",
      "Train Epoch: 013 Batch: 00069/00094 | Loss: 0.1162 | EWC Loss: 0.0297 | CE Loss: 0.0865\n",
      "Train Epoch: 013 Batch: 00070/00094 | Loss: 0.2119 | EWC Loss: 0.0297 | CE Loss: 0.1822\n",
      "Train Epoch: 013 Batch: 00071/00094 | Loss: 0.1285 | EWC Loss: 0.0297 | CE Loss: 0.0988\n",
      "Train Epoch: 013 Batch: 00072/00094 | Loss: 0.0919 | EWC Loss: 0.0297 | CE Loss: 0.0622\n",
      "Train Epoch: 013 Batch: 00073/00094 | Loss: 0.5585 | EWC Loss: 0.0297 | CE Loss: 0.5288\n",
      "Train Epoch: 013 Batch: 00074/00094 | Loss: 0.2092 | EWC Loss: 0.0297 | CE Loss: 0.1795\n",
      "Train Epoch: 013 Batch: 00075/00094 | Loss: 0.1036 | EWC Loss: 0.0297 | CE Loss: 0.0739\n",
      "Train Epoch: 013 Batch: 00076/00094 | Loss: 0.0946 | EWC Loss: 0.0297 | CE Loss: 0.0649\n",
      "Train Epoch: 013 Batch: 00077/00094 | Loss: 0.1466 | EWC Loss: 0.0296 | CE Loss: 0.1170\n",
      "Train Epoch: 013 Batch: 00078/00094 | Loss: 0.1297 | EWC Loss: 0.0297 | CE Loss: 0.1000\n",
      "Train Epoch: 013 Batch: 00079/00094 | Loss: 0.1482 | EWC Loss: 0.0297 | CE Loss: 0.1185\n",
      "Train Epoch: 013 Batch: 00080/00094 | Loss: 0.1177 | EWC Loss: 0.0297 | CE Loss: 0.0880\n",
      "Train Epoch: 013 Batch: 00081/00094 | Loss: 0.0657 | EWC Loss: 0.0298 | CE Loss: 0.0359\n",
      "Train Epoch: 013 Batch: 00082/00094 | Loss: 0.1066 | EWC Loss: 0.0298 | CE Loss: 0.0768\n",
      "Train Epoch: 013 Batch: 00083/00094 | Loss: 0.0857 | EWC Loss: 0.0297 | CE Loss: 0.0559\n",
      "Train Epoch: 013 Batch: 00084/00094 | Loss: 0.1858 | EWC Loss: 0.0297 | CE Loss: 0.1561\n",
      "Train Epoch: 013 Batch: 00085/00094 | Loss: 0.0664 | EWC Loss: 0.0297 | CE Loss: 0.0367\n",
      "Train Epoch: 013 Batch: 00086/00094 | Loss: 0.1013 | EWC Loss: 0.0297 | CE Loss: 0.0716\n",
      "Train Epoch: 013 Batch: 00087/00094 | Loss: 0.1425 | EWC Loss: 0.0297 | CE Loss: 0.1128\n",
      "Train Epoch: 013 Batch: 00088/00094 | Loss: 0.1218 | EWC Loss: 0.0297 | CE Loss: 0.0921\n",
      "Train Epoch: 013 Batch: 00089/00094 | Loss: 0.0980 | EWC Loss: 0.0297 | CE Loss: 0.0683\n",
      "Train Epoch: 013 Batch: 00090/00094 | Loss: 0.1194 | EWC Loss: 0.0297 | CE Loss: 0.0898\n",
      "Train Epoch: 013 Batch: 00091/00094 | Loss: 0.1098 | EWC Loss: 0.0297 | CE Loss: 0.0801\n",
      "Train Epoch: 013 Batch: 00092/00094 | Loss: 0.1605 | EWC Loss: 0.0297 | CE Loss: 0.1308\n",
      "Train Epoch: 013 Batch: 00093/00094 | Loss: 0.0539 | EWC Loss: 0.0297 | CE Loss: 0.0241\n",
      "Train Epoch: 013 Batch: 00094/00094 | Loss: 0.0514 | EWC Loss: 0.0297 | CE Loss: 0.0217\n",
      "Train Epoch: 013 |Acc 95.63333 | Loss: 0.14319 | Task Loss 0.11344 | EWC Loss: 0.02975\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0837 | acc:96.9000\n",
      "[VAL Acc] Target: 96.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8518 | acc:48.7000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.70%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9902 | acc:51.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8877 | acc:46.3740\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.7819 | acc:54.5455\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4730 | acc:80.6839\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 80.68%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.2677 | acc:64.2633\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.26%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.8784 | acc:64.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.19%\n",
      "[VAL Acc] Avg 63.39%\n",
      "\n",
      "\n",
      "---------- Starting epoch 14 ----------\n",
      "Train Epoch: 014 Batch: 00001/00094 | Loss: 0.1448 | EWC Loss: 0.0297 | CE Loss: 0.1151\n",
      "Train Epoch: 014 Batch: 00002/00094 | Loss: 0.1033 | EWC Loss: 0.0297 | CE Loss: 0.0736\n",
      "Train Epoch: 014 Batch: 00003/00094 | Loss: 0.0976 | EWC Loss: 0.0296 | CE Loss: 0.0680\n",
      "Train Epoch: 014 Batch: 00004/00094 | Loss: 0.1218 | EWC Loss: 0.0295 | CE Loss: 0.0922\n",
      "Train Epoch: 014 Batch: 00005/00094 | Loss: 0.1465 | EWC Loss: 0.0296 | CE Loss: 0.1169\n",
      "Train Epoch: 014 Batch: 00006/00094 | Loss: 0.0954 | EWC Loss: 0.0297 | CE Loss: 0.0657\n",
      "Train Epoch: 014 Batch: 00007/00094 | Loss: 0.2663 | EWC Loss: 0.0297 | CE Loss: 0.2366\n",
      "Train Epoch: 014 Batch: 00008/00094 | Loss: 0.0836 | EWC Loss: 0.0299 | CE Loss: 0.0538\n",
      "Train Epoch: 014 Batch: 00009/00094 | Loss: 0.1167 | EWC Loss: 0.0299 | CE Loss: 0.0868\n",
      "Train Epoch: 014 Batch: 00010/00094 | Loss: 0.1150 | EWC Loss: 0.0298 | CE Loss: 0.0852\n",
      "Train Epoch: 014 Batch: 00011/00094 | Loss: 0.1140 | EWC Loss: 0.0298 | CE Loss: 0.0842\n",
      "Train Epoch: 014 Batch: 00012/00094 | Loss: 0.1633 | EWC Loss: 0.0298 | CE Loss: 0.1335\n",
      "Train Epoch: 014 Batch: 00013/00094 | Loss: 0.1356 | EWC Loss: 0.0297 | CE Loss: 0.1059\n",
      "Train Epoch: 014 Batch: 00014/00094 | Loss: 0.0865 | EWC Loss: 0.0297 | CE Loss: 0.0568\n",
      "Train Epoch: 014 Batch: 00015/00094 | Loss: 0.0694 | EWC Loss: 0.0297 | CE Loss: 0.0397\n",
      "Train Epoch: 014 Batch: 00016/00094 | Loss: 0.0633 | EWC Loss: 0.0296 | CE Loss: 0.0337\n",
      "Train Epoch: 014 Batch: 00017/00094 | Loss: 0.1460 | EWC Loss: 0.0297 | CE Loss: 0.1164\n",
      "Train Epoch: 014 Batch: 00018/00094 | Loss: 0.0865 | EWC Loss: 0.0296 | CE Loss: 0.0568\n",
      "Train Epoch: 014 Batch: 00019/00094 | Loss: 0.0772 | EWC Loss: 0.0296 | CE Loss: 0.0476\n",
      "Train Epoch: 014 Batch: 00020/00094 | Loss: 0.1712 | EWC Loss: 0.0296 | CE Loss: 0.1417\n",
      "Train Epoch: 014 Batch: 00021/00094 | Loss: 0.1790 | EWC Loss: 0.0296 | CE Loss: 0.1495\n",
      "Train Epoch: 014 Batch: 00022/00094 | Loss: 0.0974 | EWC Loss: 0.0297 | CE Loss: 0.0677\n",
      "Train Epoch: 014 Batch: 00023/00094 | Loss: 0.0856 | EWC Loss: 0.0297 | CE Loss: 0.0560\n",
      "Train Epoch: 014 Batch: 00024/00094 | Loss: 0.1002 | EWC Loss: 0.0296 | CE Loss: 0.0706\n",
      "Train Epoch: 014 Batch: 00025/00094 | Loss: 0.1932 | EWC Loss: 0.0296 | CE Loss: 0.1636\n",
      "Train Epoch: 014 Batch: 00026/00094 | Loss: 0.1052 | EWC Loss: 0.0296 | CE Loss: 0.0756\n",
      "Train Epoch: 014 Batch: 00027/00094 | Loss: 0.1490 | EWC Loss: 0.0295 | CE Loss: 0.1195\n",
      "Train Epoch: 014 Batch: 00028/00094 | Loss: 0.1024 | EWC Loss: 0.0297 | CE Loss: 0.0727\n",
      "Train Epoch: 014 Batch: 00029/00094 | Loss: 0.0905 | EWC Loss: 0.0297 | CE Loss: 0.0608\n",
      "Train Epoch: 014 Batch: 00030/00094 | Loss: 0.1359 | EWC Loss: 0.0297 | CE Loss: 0.1063\n",
      "Train Epoch: 014 Batch: 00031/00094 | Loss: 0.1743 | EWC Loss: 0.0296 | CE Loss: 0.1447\n",
      "Train Epoch: 014 Batch: 00032/00094 | Loss: 0.1353 | EWC Loss: 0.0295 | CE Loss: 0.1058\n",
      "Train Epoch: 014 Batch: 00033/00094 | Loss: 0.0566 | EWC Loss: 0.0294 | CE Loss: 0.0272\n",
      "Train Epoch: 014 Batch: 00034/00094 | Loss: 0.1165 | EWC Loss: 0.0294 | CE Loss: 0.0871\n",
      "Train Epoch: 014 Batch: 00035/00094 | Loss: 0.1173 | EWC Loss: 0.0294 | CE Loss: 0.0880\n",
      "Train Epoch: 014 Batch: 00036/00094 | Loss: 0.2070 | EWC Loss: 0.0295 | CE Loss: 0.1775\n",
      "Train Epoch: 014 Batch: 00037/00094 | Loss: 0.1125 | EWC Loss: 0.0296 | CE Loss: 0.0829\n",
      "Train Epoch: 014 Batch: 00038/00094 | Loss: 0.0818 | EWC Loss: 0.0295 | CE Loss: 0.0523\n",
      "Train Epoch: 014 Batch: 00039/00094 | Loss: 0.1582 | EWC Loss: 0.0294 | CE Loss: 0.1287\n",
      "Train Epoch: 014 Batch: 00040/00094 | Loss: 0.1436 | EWC Loss: 0.0295 | CE Loss: 0.1141\n",
      "Train Epoch: 014 Batch: 00041/00094 | Loss: 0.0870 | EWC Loss: 0.0294 | CE Loss: 0.0576\n",
      "Train Epoch: 014 Batch: 00042/00094 | Loss: 0.2579 | EWC Loss: 0.0293 | CE Loss: 0.2286\n",
      "Train Epoch: 014 Batch: 00043/00094 | Loss: 0.1364 | EWC Loss: 0.0294 | CE Loss: 0.1070\n",
      "Train Epoch: 014 Batch: 00044/00094 | Loss: 0.2234 | EWC Loss: 0.0295 | CE Loss: 0.1940\n",
      "Train Epoch: 014 Batch: 00045/00094 | Loss: 0.1166 | EWC Loss: 0.0295 | CE Loss: 0.0871\n",
      "Train Epoch: 014 Batch: 00046/00094 | Loss: 0.1009 | EWC Loss: 0.0295 | CE Loss: 0.0714\n",
      "Train Epoch: 014 Batch: 00047/00094 | Loss: 0.1452 | EWC Loss: 0.0295 | CE Loss: 0.1157\n",
      "Train Epoch: 014 Batch: 00048/00094 | Loss: 0.0872 | EWC Loss: 0.0294 | CE Loss: 0.0578\n",
      "Train Epoch: 014 Batch: 00049/00094 | Loss: 0.1015 | EWC Loss: 0.0294 | CE Loss: 0.0721\n",
      "Train Epoch: 014 Batch: 00050/00094 | Loss: 0.1304 | EWC Loss: 0.0293 | CE Loss: 0.1011\n",
      "Train Epoch: 014 Batch: 00051/00094 | Loss: 0.1414 | EWC Loss: 0.0293 | CE Loss: 0.1120\n",
      "Train Epoch: 014 Batch: 00052/00094 | Loss: 0.0666 | EWC Loss: 0.0294 | CE Loss: 0.0372\n",
      "Train Epoch: 014 Batch: 00053/00094 | Loss: 0.1937 | EWC Loss: 0.0294 | CE Loss: 0.1643\n",
      "Train Epoch: 014 Batch: 00054/00094 | Loss: 0.1120 | EWC Loss: 0.0293 | CE Loss: 0.0827\n",
      "Train Epoch: 014 Batch: 00055/00094 | Loss: 0.0934 | EWC Loss: 0.0294 | CE Loss: 0.0640\n",
      "Train Epoch: 014 Batch: 00056/00094 | Loss: 0.1153 | EWC Loss: 0.0293 | CE Loss: 0.0860\n",
      "Train Epoch: 014 Batch: 00057/00094 | Loss: 0.2326 | EWC Loss: 0.0293 | CE Loss: 0.2033\n",
      "Train Epoch: 014 Batch: 00058/00094 | Loss: 0.1529 | EWC Loss: 0.0294 | CE Loss: 0.1236\n",
      "Train Epoch: 014 Batch: 00059/00094 | Loss: 0.1454 | EWC Loss: 0.0295 | CE Loss: 0.1159\n",
      "Train Epoch: 014 Batch: 00060/00094 | Loss: 0.1375 | EWC Loss: 0.0292 | CE Loss: 0.1082\n",
      "Train Epoch: 014 Batch: 00061/00094 | Loss: 0.1283 | EWC Loss: 0.0292 | CE Loss: 0.0991\n",
      "Train Epoch: 014 Batch: 00062/00094 | Loss: 0.0836 | EWC Loss: 0.0292 | CE Loss: 0.0544\n",
      "Train Epoch: 014 Batch: 00063/00094 | Loss: 0.1204 | EWC Loss: 0.0291 | CE Loss: 0.0912\n",
      "Train Epoch: 014 Batch: 00064/00094 | Loss: 0.1886 | EWC Loss: 0.0291 | CE Loss: 0.1595\n",
      "Train Epoch: 014 Batch: 00065/00094 | Loss: 0.0908 | EWC Loss: 0.0293 | CE Loss: 0.0615\n",
      "Train Epoch: 014 Batch: 00066/00094 | Loss: 0.1002 | EWC Loss: 0.0293 | CE Loss: 0.0709\n",
      "Train Epoch: 014 Batch: 00067/00094 | Loss: 0.2522 | EWC Loss: 0.0293 | CE Loss: 0.2229\n",
      "Train Epoch: 014 Batch: 00068/00094 | Loss: 0.1338 | EWC Loss: 0.0293 | CE Loss: 0.1045\n",
      "Train Epoch: 014 Batch: 00069/00094 | Loss: 0.1164 | EWC Loss: 0.0292 | CE Loss: 0.0872\n",
      "Train Epoch: 014 Batch: 00070/00094 | Loss: 0.1409 | EWC Loss: 0.0293 | CE Loss: 0.1116\n",
      "Train Epoch: 014 Batch: 00071/00094 | Loss: 0.1702 | EWC Loss: 0.0293 | CE Loss: 0.1409\n",
      "Train Epoch: 014 Batch: 00072/00094 | Loss: 0.1631 | EWC Loss: 0.0293 | CE Loss: 0.1338\n",
      "Train Epoch: 014 Batch: 00073/00094 | Loss: 0.1085 | EWC Loss: 0.0293 | CE Loss: 0.0792\n",
      "Train Epoch: 014 Batch: 00074/00094 | Loss: 0.0879 | EWC Loss: 0.0293 | CE Loss: 0.0586\n",
      "Train Epoch: 014 Batch: 00075/00094 | Loss: 0.1509 | EWC Loss: 0.0292 | CE Loss: 0.1217\n",
      "Train Epoch: 014 Batch: 00076/00094 | Loss: 0.1050 | EWC Loss: 0.0293 | CE Loss: 0.0757\n",
      "Train Epoch: 014 Batch: 00077/00094 | Loss: 0.1264 | EWC Loss: 0.0293 | CE Loss: 0.0970\n",
      "Train Epoch: 014 Batch: 00078/00094 | Loss: 0.0727 | EWC Loss: 0.0293 | CE Loss: 0.0434\n",
      "Train Epoch: 014 Batch: 00079/00094 | Loss: 0.1164 | EWC Loss: 0.0293 | CE Loss: 0.0871\n",
      "Train Epoch: 014 Batch: 00080/00094 | Loss: 0.1048 | EWC Loss: 0.0293 | CE Loss: 0.0755\n",
      "Train Epoch: 014 Batch: 00081/00094 | Loss: 0.0985 | EWC Loss: 0.0293 | CE Loss: 0.0692\n",
      "Train Epoch: 014 Batch: 00082/00094 | Loss: 0.0668 | EWC Loss: 0.0293 | CE Loss: 0.0375\n",
      "Train Epoch: 014 Batch: 00083/00094 | Loss: 0.1032 | EWC Loss: 0.0292 | CE Loss: 0.0740\n",
      "Train Epoch: 014 Batch: 00084/00094 | Loss: 0.1294 | EWC Loss: 0.0293 | CE Loss: 0.1001\n",
      "Train Epoch: 014 Batch: 00085/00094 | Loss: 0.1920 | EWC Loss: 0.0293 | CE Loss: 0.1627\n",
      "Train Epoch: 014 Batch: 00086/00094 | Loss: 0.1769 | EWC Loss: 0.0293 | CE Loss: 0.1476\n",
      "Train Epoch: 014 Batch: 00087/00094 | Loss: 0.1740 | EWC Loss: 0.0293 | CE Loss: 0.1448\n",
      "Train Epoch: 014 Batch: 00088/00094 | Loss: 0.1272 | EWC Loss: 0.0294 | CE Loss: 0.0977\n",
      "Train Epoch: 014 Batch: 00089/00094 | Loss: 0.0607 | EWC Loss: 0.0294 | CE Loss: 0.0313\n",
      "Train Epoch: 014 Batch: 00090/00094 | Loss: 0.0850 | EWC Loss: 0.0293 | CE Loss: 0.0557\n",
      "Train Epoch: 014 Batch: 00091/00094 | Loss: 0.1279 | EWC Loss: 0.0293 | CE Loss: 0.0985\n",
      "Train Epoch: 014 Batch: 00092/00094 | Loss: 0.1612 | EWC Loss: 0.0293 | CE Loss: 0.1319\n",
      "Train Epoch: 014 Batch: 00093/00094 | Loss: 0.1176 | EWC Loss: 0.0293 | CE Loss: 0.0883\n",
      "Train Epoch: 014 Batch: 00094/00094 | Loss: 0.1637 | EWC Loss: 0.0293 | CE Loss: 0.1343\n",
      "Train Epoch: 014 |Acc 96.30000 | Loss: 0.12838 | Task Loss 0.09893 | EWC Loss: 0.02945\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1027 | acc:95.9500\n",
      "[VAL Acc] Target: 95.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.6610 | acc:48.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8874 | acc:52.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9475 | acc:43.3206\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.32%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4540 | acc:58.3072\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5790 | acc:78.2810\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9441 | acc:70.3762\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9717 | acc:63.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 63.81%\n",
      "[VAL Acc] Avg 63.93%\n",
      "\n",
      "\n",
      "---------- Starting epoch 15 ----------\n",
      "Train Epoch: 015 Batch: 00001/00094 | Loss: 0.0784 | EWC Loss: 0.0295 | CE Loss: 0.0490\n",
      "Train Epoch: 015 Batch: 00002/00094 | Loss: 0.1380 | EWC Loss: 0.0294 | CE Loss: 0.1085\n",
      "Train Epoch: 015 Batch: 00003/00094 | Loss: 0.0841 | EWC Loss: 0.0293 | CE Loss: 0.0548\n",
      "Train Epoch: 015 Batch: 00004/00094 | Loss: 0.0731 | EWC Loss: 0.0293 | CE Loss: 0.0438\n",
      "Train Epoch: 015 Batch: 00005/00094 | Loss: 0.0906 | EWC Loss: 0.0293 | CE Loss: 0.0613\n",
      "Train Epoch: 015 Batch: 00006/00094 | Loss: 0.1867 | EWC Loss: 0.0294 | CE Loss: 0.1574\n",
      "Train Epoch: 015 Batch: 00007/00094 | Loss: 0.1187 | EWC Loss: 0.0293 | CE Loss: 0.0893\n",
      "Train Epoch: 015 Batch: 00008/00094 | Loss: 0.0660 | EWC Loss: 0.0294 | CE Loss: 0.0367\n",
      "Train Epoch: 015 Batch: 00009/00094 | Loss: 0.1621 | EWC Loss: 0.0293 | CE Loss: 0.1328\n",
      "Train Epoch: 015 Batch: 00010/00094 | Loss: 0.1638 | EWC Loss: 0.0295 | CE Loss: 0.1343\n",
      "Train Epoch: 015 Batch: 00011/00094 | Loss: 0.1615 | EWC Loss: 0.0296 | CE Loss: 0.1319\n",
      "Train Epoch: 015 Batch: 00012/00094 | Loss: 0.1303 | EWC Loss: 0.0296 | CE Loss: 0.1007\n",
      "Train Epoch: 015 Batch: 00013/00094 | Loss: 0.1480 | EWC Loss: 0.0297 | CE Loss: 0.1183\n",
      "Train Epoch: 015 Batch: 00014/00094 | Loss: 0.0843 | EWC Loss: 0.0298 | CE Loss: 0.0544\n",
      "Train Epoch: 015 Batch: 00015/00094 | Loss: 0.0838 | EWC Loss: 0.0298 | CE Loss: 0.0540\n",
      "Train Epoch: 015 Batch: 00016/00094 | Loss: 0.0618 | EWC Loss: 0.0298 | CE Loss: 0.0319\n",
      "Train Epoch: 015 Batch: 00017/00094 | Loss: 0.0937 | EWC Loss: 0.0298 | CE Loss: 0.0639\n",
      "Train Epoch: 015 Batch: 00018/00094 | Loss: 0.2363 | EWC Loss: 0.0297 | CE Loss: 0.2065\n",
      "Train Epoch: 015 Batch: 00019/00094 | Loss: 0.1105 | EWC Loss: 0.0298 | CE Loss: 0.0807\n",
      "Train Epoch: 015 Batch: 00020/00094 | Loss: 0.0608 | EWC Loss: 0.0298 | CE Loss: 0.0310\n",
      "Train Epoch: 015 Batch: 00021/00094 | Loss: 0.1732 | EWC Loss: 0.0297 | CE Loss: 0.1435\n",
      "Train Epoch: 015 Batch: 00022/00094 | Loss: 0.1932 | EWC Loss: 0.0299 | CE Loss: 0.1633\n",
      "Train Epoch: 015 Batch: 00023/00094 | Loss: 0.1291 | EWC Loss: 0.0298 | CE Loss: 0.0993\n",
      "Train Epoch: 015 Batch: 00024/00094 | Loss: 0.1682 | EWC Loss: 0.0297 | CE Loss: 0.1385\n",
      "Train Epoch: 015 Batch: 00025/00094 | Loss: 0.2039 | EWC Loss: 0.0296 | CE Loss: 0.1743\n",
      "Train Epoch: 015 Batch: 00026/00094 | Loss: 0.1553 | EWC Loss: 0.0296 | CE Loss: 0.1258\n",
      "Train Epoch: 015 Batch: 00027/00094 | Loss: 0.0803 | EWC Loss: 0.0297 | CE Loss: 0.0506\n",
      "Train Epoch: 015 Batch: 00028/00094 | Loss: 0.0951 | EWC Loss: 0.0297 | CE Loss: 0.0655\n",
      "Train Epoch: 015 Batch: 00029/00094 | Loss: 0.1126 | EWC Loss: 0.0296 | CE Loss: 0.0831\n",
      "Train Epoch: 015 Batch: 00030/00094 | Loss: 0.1042 | EWC Loss: 0.0295 | CE Loss: 0.0747\n",
      "Train Epoch: 015 Batch: 00031/00094 | Loss: 0.1307 | EWC Loss: 0.0295 | CE Loss: 0.1012\n",
      "Train Epoch: 015 Batch: 00032/00094 | Loss: 0.0877 | EWC Loss: 0.0295 | CE Loss: 0.0582\n",
      "Train Epoch: 015 Batch: 00033/00094 | Loss: 0.1822 | EWC Loss: 0.0295 | CE Loss: 0.1527\n",
      "Train Epoch: 015 Batch: 00034/00094 | Loss: 0.1253 | EWC Loss: 0.0295 | CE Loss: 0.0958\n",
      "Train Epoch: 015 Batch: 00035/00094 | Loss: 0.1724 | EWC Loss: 0.0295 | CE Loss: 0.1430\n",
      "Train Epoch: 015 Batch: 00036/00094 | Loss: 0.0863 | EWC Loss: 0.0295 | CE Loss: 0.0567\n",
      "Train Epoch: 015 Batch: 00037/00094 | Loss: 0.1852 | EWC Loss: 0.0295 | CE Loss: 0.1557\n",
      "Train Epoch: 015 Batch: 00038/00094 | Loss: 0.1723 | EWC Loss: 0.0298 | CE Loss: 0.1425\n",
      "Train Epoch: 015 Batch: 00039/00094 | Loss: 0.1122 | EWC Loss: 0.0300 | CE Loss: 0.0822\n",
      "Train Epoch: 015 Batch: 00040/00094 | Loss: 0.3467 | EWC Loss: 0.0299 | CE Loss: 0.3168\n",
      "Train Epoch: 015 Batch: 00041/00094 | Loss: 0.0912 | EWC Loss: 0.0299 | CE Loss: 0.0613\n",
      "Train Epoch: 015 Batch: 00042/00094 | Loss: 0.3024 | EWC Loss: 0.0299 | CE Loss: 0.2725\n",
      "Train Epoch: 015 Batch: 00043/00094 | Loss: 0.1618 | EWC Loss: 0.0299 | CE Loss: 0.1319\n",
      "Train Epoch: 015 Batch: 00044/00094 | Loss: 0.2057 | EWC Loss: 0.0300 | CE Loss: 0.1757\n",
      "Train Epoch: 015 Batch: 00045/00094 | Loss: 0.1002 | EWC Loss: 0.0299 | CE Loss: 0.0703\n",
      "Train Epoch: 015 Batch: 00046/00094 | Loss: 0.0807 | EWC Loss: 0.0299 | CE Loss: 0.0508\n",
      "Train Epoch: 015 Batch: 00047/00094 | Loss: 0.0722 | EWC Loss: 0.0299 | CE Loss: 0.0423\n",
      "Train Epoch: 015 Batch: 00048/00094 | Loss: 0.1279 | EWC Loss: 0.0299 | CE Loss: 0.0980\n",
      "Train Epoch: 015 Batch: 00049/00094 | Loss: 0.0936 | EWC Loss: 0.0299 | CE Loss: 0.0637\n",
      "Train Epoch: 015 Batch: 00050/00094 | Loss: 0.2098 | EWC Loss: 0.0299 | CE Loss: 0.1799\n",
      "Train Epoch: 015 Batch: 00051/00094 | Loss: 0.1768 | EWC Loss: 0.0300 | CE Loss: 0.1468\n",
      "Train Epoch: 015 Batch: 00052/00094 | Loss: 0.1029 | EWC Loss: 0.0300 | CE Loss: 0.0729\n",
      "Train Epoch: 015 Batch: 00053/00094 | Loss: 0.1018 | EWC Loss: 0.0300 | CE Loss: 0.0718\n",
      "Train Epoch: 015 Batch: 00054/00094 | Loss: 0.1609 | EWC Loss: 0.0301 | CE Loss: 0.1308\n",
      "Train Epoch: 015 Batch: 00055/00094 | Loss: 0.2277 | EWC Loss: 0.0301 | CE Loss: 0.1977\n",
      "Train Epoch: 015 Batch: 00056/00094 | Loss: 0.1183 | EWC Loss: 0.0304 | CE Loss: 0.0880\n",
      "Train Epoch: 015 Batch: 00057/00094 | Loss: 0.1667 | EWC Loss: 0.0303 | CE Loss: 0.1364\n",
      "Train Epoch: 015 Batch: 00058/00094 | Loss: 0.1006 | EWC Loss: 0.0303 | CE Loss: 0.0703\n",
      "Train Epoch: 015 Batch: 00059/00094 | Loss: 0.0992 | EWC Loss: 0.0303 | CE Loss: 0.0689\n",
      "Train Epoch: 015 Batch: 00060/00094 | Loss: 0.1292 | EWC Loss: 0.0303 | CE Loss: 0.0990\n",
      "Train Epoch: 015 Batch: 00061/00094 | Loss: 0.1607 | EWC Loss: 0.0303 | CE Loss: 0.1304\n",
      "Train Epoch: 015 Batch: 00062/00094 | Loss: 0.1562 | EWC Loss: 0.0302 | CE Loss: 0.1260\n",
      "Train Epoch: 015 Batch: 00063/00094 | Loss: 0.1227 | EWC Loss: 0.0302 | CE Loss: 0.0925\n",
      "Train Epoch: 015 Batch: 00064/00094 | Loss: 0.1056 | EWC Loss: 0.0303 | CE Loss: 0.0754\n",
      "Train Epoch: 015 Batch: 00065/00094 | Loss: 0.1585 | EWC Loss: 0.0302 | CE Loss: 0.1283\n",
      "Train Epoch: 015 Batch: 00066/00094 | Loss: 0.2700 | EWC Loss: 0.0302 | CE Loss: 0.2398\n",
      "Train Epoch: 015 Batch: 00067/00094 | Loss: 0.1350 | EWC Loss: 0.0301 | CE Loss: 0.1049\n",
      "Train Epoch: 015 Batch: 00068/00094 | Loss: 0.0930 | EWC Loss: 0.0302 | CE Loss: 0.0627\n",
      "Train Epoch: 015 Batch: 00069/00094 | Loss: 0.1486 | EWC Loss: 0.0303 | CE Loss: 0.1183\n",
      "Train Epoch: 015 Batch: 00070/00094 | Loss: 0.1597 | EWC Loss: 0.0301 | CE Loss: 0.1296\n",
      "Train Epoch: 015 Batch: 00071/00094 | Loss: 0.1025 | EWC Loss: 0.0301 | CE Loss: 0.0724\n",
      "Train Epoch: 015 Batch: 00072/00094 | Loss: 0.0977 | EWC Loss: 0.0301 | CE Loss: 0.0677\n",
      "Train Epoch: 015 Batch: 00073/00094 | Loss: 0.2285 | EWC Loss: 0.0300 | CE Loss: 0.1984\n",
      "Train Epoch: 015 Batch: 00074/00094 | Loss: 0.0811 | EWC Loss: 0.0302 | CE Loss: 0.0509\n",
      "Train Epoch: 015 Batch: 00075/00094 | Loss: 0.1840 | EWC Loss: 0.0301 | CE Loss: 0.1539\n",
      "Train Epoch: 015 Batch: 00076/00094 | Loss: 0.1273 | EWC Loss: 0.0299 | CE Loss: 0.0973\n",
      "Train Epoch: 015 Batch: 00077/00094 | Loss: 0.1161 | EWC Loss: 0.0302 | CE Loss: 0.0859\n",
      "Train Epoch: 015 Batch: 00078/00094 | Loss: 0.0942 | EWC Loss: 0.0301 | CE Loss: 0.0641\n",
      "Train Epoch: 015 Batch: 00079/00094 | Loss: 0.0970 | EWC Loss: 0.0300 | CE Loss: 0.0670\n",
      "Train Epoch: 015 Batch: 00080/00094 | Loss: 0.1172 | EWC Loss: 0.0300 | CE Loss: 0.0871\n",
      "Train Epoch: 015 Batch: 00081/00094 | Loss: 0.0823 | EWC Loss: 0.0300 | CE Loss: 0.0523\n",
      "Train Epoch: 015 Batch: 00082/00094 | Loss: 0.1828 | EWC Loss: 0.0300 | CE Loss: 0.1528\n",
      "Train Epoch: 015 Batch: 00083/00094 | Loss: 0.3096 | EWC Loss: 0.0300 | CE Loss: 0.2796\n",
      "Train Epoch: 015 Batch: 00084/00094 | Loss: 0.2070 | EWC Loss: 0.0301 | CE Loss: 0.1769\n",
      "Train Epoch: 015 Batch: 00085/00094 | Loss: 0.0627 | EWC Loss: 0.0301 | CE Loss: 0.0325\n",
      "Train Epoch: 015 Batch: 00086/00094 | Loss: 0.0888 | EWC Loss: 0.0301 | CE Loss: 0.0587\n",
      "Train Epoch: 015 Batch: 00087/00094 | Loss: 0.0867 | EWC Loss: 0.0300 | CE Loss: 0.0566\n",
      "Train Epoch: 015 Batch: 00088/00094 | Loss: 0.1705 | EWC Loss: 0.0299 | CE Loss: 0.1405\n",
      "Train Epoch: 015 Batch: 00089/00094 | Loss: 0.0551 | EWC Loss: 0.0300 | CE Loss: 0.0250\n",
      "Train Epoch: 015 Batch: 00090/00094 | Loss: 0.1186 | EWC Loss: 0.0300 | CE Loss: 0.0886\n",
      "Train Epoch: 015 Batch: 00091/00094 | Loss: 0.1088 | EWC Loss: 0.0299 | CE Loss: 0.0789\n",
      "Train Epoch: 015 Batch: 00092/00094 | Loss: 0.1727 | EWC Loss: 0.0300 | CE Loss: 0.1428\n",
      "Train Epoch: 015 Batch: 00093/00094 | Loss: 0.1292 | EWC Loss: 0.0299 | CE Loss: 0.0993\n",
      "Train Epoch: 015 Batch: 00094/00094 | Loss: 0.0611 | EWC Loss: 0.0299 | CE Loss: 0.0312\n",
      "Train Epoch: 015 |Acc 95.70000 | Loss: 0.13585 | Task Loss 0.10598 | EWC Loss: 0.02987\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0997 | acc:95.7500\n",
      "[VAL Acc] Target: 95.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7774 | acc:48.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.7887 | acc:52.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9595 | acc:44.8473\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5424 | acc:58.2288\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4778 | acc:80.2218\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 80.22%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9180 | acc:70.6505\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9609 | acc:64.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 64.50%\n",
      "[VAL Acc] Avg 64.49%\n",
      "\n",
      "\n",
      "---------- Starting epoch 16 ----------\n",
      "Train Epoch: 016 Batch: 00001/00094 | Loss: 0.1248 | EWC Loss: 0.0298 | CE Loss: 0.0950\n",
      "Train Epoch: 016 Batch: 00002/00094 | Loss: 0.2620 | EWC Loss: 0.0297 | CE Loss: 0.2322\n",
      "Train Epoch: 016 Batch: 00003/00094 | Loss: 0.0631 | EWC Loss: 0.0298 | CE Loss: 0.0333\n",
      "Train Epoch: 016 Batch: 00004/00094 | Loss: 0.1869 | EWC Loss: 0.0297 | CE Loss: 0.1572\n",
      "Train Epoch: 016 Batch: 00005/00094 | Loss: 0.1321 | EWC Loss: 0.0299 | CE Loss: 0.1021\n",
      "Train Epoch: 016 Batch: 00006/00094 | Loss: 0.1314 | EWC Loss: 0.0299 | CE Loss: 0.1014\n",
      "Train Epoch: 016 Batch: 00007/00094 | Loss: 0.1911 | EWC Loss: 0.0301 | CE Loss: 0.1610\n",
      "Train Epoch: 016 Batch: 00008/00094 | Loss: 0.1452 | EWC Loss: 0.0305 | CE Loss: 0.1147\n",
      "Train Epoch: 016 Batch: 00009/00094 | Loss: 0.1333 | EWC Loss: 0.0303 | CE Loss: 0.1030\n",
      "Train Epoch: 016 Batch: 00010/00094 | Loss: 0.1300 | EWC Loss: 0.0302 | CE Loss: 0.0998\n",
      "Train Epoch: 016 Batch: 00011/00094 | Loss: 0.1906 | EWC Loss: 0.0303 | CE Loss: 0.1603\n",
      "Train Epoch: 016 Batch: 00012/00094 | Loss: 0.0885 | EWC Loss: 0.0301 | CE Loss: 0.0584\n",
      "Train Epoch: 016 Batch: 00013/00094 | Loss: 0.0535 | EWC Loss: 0.0300 | CE Loss: 0.0235\n",
      "Train Epoch: 016 Batch: 00014/00094 | Loss: 0.0985 | EWC Loss: 0.0300 | CE Loss: 0.0685\n",
      "Train Epoch: 016 Batch: 00015/00094 | Loss: 0.1062 | EWC Loss: 0.0300 | CE Loss: 0.0762\n",
      "Train Epoch: 016 Batch: 00016/00094 | Loss: 0.1444 | EWC Loss: 0.0299 | CE Loss: 0.1145\n",
      "Train Epoch: 016 Batch: 00017/00094 | Loss: 0.1371 | EWC Loss: 0.0299 | CE Loss: 0.1072\n",
      "Train Epoch: 016 Batch: 00018/00094 | Loss: 0.2193 | EWC Loss: 0.0300 | CE Loss: 0.1892\n",
      "Train Epoch: 016 Batch: 00019/00094 | Loss: 0.1286 | EWC Loss: 0.0302 | CE Loss: 0.0985\n",
      "Train Epoch: 016 Batch: 00020/00094 | Loss: 0.1171 | EWC Loss: 0.0304 | CE Loss: 0.0867\n",
      "Train Epoch: 016 Batch: 00021/00094 | Loss: 0.1794 | EWC Loss: 0.0304 | CE Loss: 0.1490\n",
      "Train Epoch: 016 Batch: 00022/00094 | Loss: 0.1156 | EWC Loss: 0.0305 | CE Loss: 0.0850\n",
      "Train Epoch: 016 Batch: 00023/00094 | Loss: 0.0534 | EWC Loss: 0.0302 | CE Loss: 0.0232\n",
      "Train Epoch: 016 Batch: 00024/00094 | Loss: 0.1424 | EWC Loss: 0.0301 | CE Loss: 0.1123\n",
      "Train Epoch: 016 Batch: 00025/00094 | Loss: 0.1102 | EWC Loss: 0.0300 | CE Loss: 0.0801\n",
      "Train Epoch: 016 Batch: 00026/00094 | Loss: 0.1973 | EWC Loss: 0.0298 | CE Loss: 0.1676\n",
      "Train Epoch: 016 Batch: 00027/00094 | Loss: 0.0778 | EWC Loss: 0.0297 | CE Loss: 0.0480\n",
      "Train Epoch: 016 Batch: 00028/00094 | Loss: 0.0763 | EWC Loss: 0.0297 | CE Loss: 0.0467\n",
      "Train Epoch: 016 Batch: 00029/00094 | Loss: 0.1444 | EWC Loss: 0.0296 | CE Loss: 0.1148\n",
      "Train Epoch: 016 Batch: 00030/00094 | Loss: 0.1067 | EWC Loss: 0.0296 | CE Loss: 0.0771\n",
      "Train Epoch: 016 Batch: 00031/00094 | Loss: 0.1009 | EWC Loss: 0.0296 | CE Loss: 0.0713\n",
      "Train Epoch: 016 Batch: 00032/00094 | Loss: 0.1299 | EWC Loss: 0.0297 | CE Loss: 0.1002\n",
      "Train Epoch: 016 Batch: 00033/00094 | Loss: 0.1335 | EWC Loss: 0.0296 | CE Loss: 0.1039\n",
      "Train Epoch: 016 Batch: 00034/00094 | Loss: 0.1359 | EWC Loss: 0.0297 | CE Loss: 0.1062\n",
      "Train Epoch: 016 Batch: 00035/00094 | Loss: 0.1486 | EWC Loss: 0.0299 | CE Loss: 0.1187\n",
      "Train Epoch: 016 Batch: 00036/00094 | Loss: 0.1144 | EWC Loss: 0.0298 | CE Loss: 0.0846\n",
      "Train Epoch: 016 Batch: 00037/00094 | Loss: 0.1768 | EWC Loss: 0.0298 | CE Loss: 0.1470\n",
      "Train Epoch: 016 Batch: 00038/00094 | Loss: 0.1383 | EWC Loss: 0.0299 | CE Loss: 0.1083\n",
      "Train Epoch: 016 Batch: 00039/00094 | Loss: 0.1000 | EWC Loss: 0.0300 | CE Loss: 0.0700\n",
      "Train Epoch: 016 Batch: 00040/00094 | Loss: 0.2142 | EWC Loss: 0.0300 | CE Loss: 0.1842\n",
      "Train Epoch: 016 Batch: 00041/00094 | Loss: 0.2452 | EWC Loss: 0.0298 | CE Loss: 0.2153\n",
      "Train Epoch: 016 Batch: 00042/00094 | Loss: 0.0884 | EWC Loss: 0.0298 | CE Loss: 0.0586\n",
      "Train Epoch: 016 Batch: 00043/00094 | Loss: 0.0865 | EWC Loss: 0.0296 | CE Loss: 0.0568\n",
      "Train Epoch: 016 Batch: 00044/00094 | Loss: 0.1148 | EWC Loss: 0.0297 | CE Loss: 0.0851\n",
      "Train Epoch: 016 Batch: 00045/00094 | Loss: 0.0963 | EWC Loss: 0.0297 | CE Loss: 0.0667\n",
      "Train Epoch: 016 Batch: 00046/00094 | Loss: 0.0744 | EWC Loss: 0.0296 | CE Loss: 0.0448\n",
      "Train Epoch: 016 Batch: 00047/00094 | Loss: 0.1188 | EWC Loss: 0.0296 | CE Loss: 0.0892\n",
      "Train Epoch: 016 Batch: 00048/00094 | Loss: 0.1139 | EWC Loss: 0.0296 | CE Loss: 0.0843\n",
      "Train Epoch: 016 Batch: 00049/00094 | Loss: 0.2223 | EWC Loss: 0.0297 | CE Loss: 0.1926\n",
      "Train Epoch: 016 Batch: 00050/00094 | Loss: 0.1225 | EWC Loss: 0.0298 | CE Loss: 0.0927\n",
      "Train Epoch: 016 Batch: 00051/00094 | Loss: 0.1520 | EWC Loss: 0.0297 | CE Loss: 0.1223\n",
      "Train Epoch: 016 Batch: 00052/00094 | Loss: 0.2683 | EWC Loss: 0.0297 | CE Loss: 0.2387\n",
      "Train Epoch: 016 Batch: 00053/00094 | Loss: 0.0610 | EWC Loss: 0.0300 | CE Loss: 0.0310\n",
      "Train Epoch: 016 Batch: 00054/00094 | Loss: 0.0780 | EWC Loss: 0.0300 | CE Loss: 0.0480\n",
      "Train Epoch: 016 Batch: 00055/00094 | Loss: 0.0699 | EWC Loss: 0.0299 | CE Loss: 0.0400\n",
      "Train Epoch: 016 Batch: 00056/00094 | Loss: 0.1174 | EWC Loss: 0.0298 | CE Loss: 0.0877\n",
      "Train Epoch: 016 Batch: 00057/00094 | Loss: 0.1165 | EWC Loss: 0.0298 | CE Loss: 0.0867\n",
      "Train Epoch: 016 Batch: 00058/00094 | Loss: 0.0865 | EWC Loss: 0.0298 | CE Loss: 0.0567\n",
      "Train Epoch: 016 Batch: 00059/00094 | Loss: 0.1444 | EWC Loss: 0.0298 | CE Loss: 0.1146\n",
      "Train Epoch: 016 Batch: 00060/00094 | Loss: 0.0826 | EWC Loss: 0.0298 | CE Loss: 0.0529\n",
      "Train Epoch: 016 Batch: 00061/00094 | Loss: 0.0725 | EWC Loss: 0.0297 | CE Loss: 0.0428\n",
      "Train Epoch: 016 Batch: 00062/00094 | Loss: 0.1156 | EWC Loss: 0.0296 | CE Loss: 0.0859\n",
      "Train Epoch: 016 Batch: 00063/00094 | Loss: 0.1701 | EWC Loss: 0.0297 | CE Loss: 0.1404\n",
      "Train Epoch: 016 Batch: 00064/00094 | Loss: 0.1839 | EWC Loss: 0.0296 | CE Loss: 0.1543\n",
      "Train Epoch: 016 Batch: 00065/00094 | Loss: 0.1209 | EWC Loss: 0.0298 | CE Loss: 0.0911\n",
      "Train Epoch: 016 Batch: 00066/00094 | Loss: 0.0876 | EWC Loss: 0.0297 | CE Loss: 0.0580\n",
      "Train Epoch: 016 Batch: 00067/00094 | Loss: 0.1029 | EWC Loss: 0.0296 | CE Loss: 0.0732\n",
      "Train Epoch: 016 Batch: 00068/00094 | Loss: 0.1815 | EWC Loss: 0.0299 | CE Loss: 0.1516\n",
      "Train Epoch: 016 Batch: 00069/00094 | Loss: 0.0688 | EWC Loss: 0.0299 | CE Loss: 0.0389\n",
      "Train Epoch: 016 Batch: 00070/00094 | Loss: 0.0443 | EWC Loss: 0.0298 | CE Loss: 0.0145\n",
      "Train Epoch: 016 Batch: 00071/00094 | Loss: 0.1785 | EWC Loss: 0.0298 | CE Loss: 0.1488\n",
      "Train Epoch: 016 Batch: 00072/00094 | Loss: 0.1166 | EWC Loss: 0.0297 | CE Loss: 0.0869\n",
      "Train Epoch: 016 Batch: 00073/00094 | Loss: 0.2034 | EWC Loss: 0.0298 | CE Loss: 0.1736\n",
      "Train Epoch: 016 Batch: 00074/00094 | Loss: 0.0805 | EWC Loss: 0.0297 | CE Loss: 0.0508\n",
      "Train Epoch: 016 Batch: 00075/00094 | Loss: 0.1014 | EWC Loss: 0.0296 | CE Loss: 0.0718\n",
      "Train Epoch: 016 Batch: 00076/00094 | Loss: 0.2785 | EWC Loss: 0.0295 | CE Loss: 0.2490\n",
      "Train Epoch: 016 Batch: 00077/00094 | Loss: 0.1720 | EWC Loss: 0.0299 | CE Loss: 0.1421\n",
      "Train Epoch: 016 Batch: 00078/00094 | Loss: 0.1990 | EWC Loss: 0.0298 | CE Loss: 0.1691\n",
      "Train Epoch: 016 Batch: 00079/00094 | Loss: 0.0681 | EWC Loss: 0.0300 | CE Loss: 0.0381\n",
      "Train Epoch: 016 Batch: 00080/00094 | Loss: 0.2739 | EWC Loss: 0.0299 | CE Loss: 0.2440\n",
      "Train Epoch: 016 Batch: 00081/00094 | Loss: 0.1895 | EWC Loss: 0.0300 | CE Loss: 0.1596\n",
      "Train Epoch: 016 Batch: 00082/00094 | Loss: 0.1558 | EWC Loss: 0.0299 | CE Loss: 0.1258\n",
      "Train Epoch: 016 Batch: 00083/00094 | Loss: 0.0807 | EWC Loss: 0.0300 | CE Loss: 0.0507\n",
      "Train Epoch: 016 Batch: 00084/00094 | Loss: 0.1894 | EWC Loss: 0.0299 | CE Loss: 0.1595\n",
      "Train Epoch: 016 Batch: 00085/00094 | Loss: 0.1029 | EWC Loss: 0.0300 | CE Loss: 0.0729\n",
      "Train Epoch: 016 Batch: 00086/00094 | Loss: 0.0789 | EWC Loss: 0.0300 | CE Loss: 0.0489\n",
      "Train Epoch: 016 Batch: 00087/00094 | Loss: 0.3735 | EWC Loss: 0.0298 | CE Loss: 0.3437\n",
      "Train Epoch: 016 Batch: 00088/00094 | Loss: 0.0796 | EWC Loss: 0.0306 | CE Loss: 0.0489\n",
      "Train Epoch: 016 Batch: 00089/00094 | Loss: 0.1767 | EWC Loss: 0.0306 | CE Loss: 0.1461\n",
      "Train Epoch: 016 Batch: 00090/00094 | Loss: 0.0860 | EWC Loss: 0.0304 | CE Loss: 0.0556\n",
      "Train Epoch: 016 Batch: 00091/00094 | Loss: 0.1819 | EWC Loss: 0.0303 | CE Loss: 0.1516\n",
      "Train Epoch: 016 Batch: 00092/00094 | Loss: 0.1340 | EWC Loss: 0.0300 | CE Loss: 0.1040\n",
      "Train Epoch: 016 Batch: 00093/00094 | Loss: 0.0720 | EWC Loss: 0.0299 | CE Loss: 0.0421\n",
      "Train Epoch: 016 Batch: 00094/00094 | Loss: 0.0722 | EWC Loss: 0.0298 | CE Loss: 0.0424\n",
      "Train Epoch: 016 |Acc 96.05000 | Loss: 0.13439 | Task Loss 0.10450 | EWC Loss: 0.02989\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0908 | acc:96.5000\n",
      "[VAL Acc] Target: 96.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.9966 | acc:48.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:2.0478 | acc:52.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.1456 | acc:46.5649\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.0705 | acc:54.0752\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4130 | acc:82.9020\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 82.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.3781 | acc:64.3025\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9722 | acc:61.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.81%\n",
      "[VAL Acc] Avg 63.37%\n",
      "\n",
      "\n",
      "---------- Starting epoch 17 ----------\n",
      "Train Epoch: 017 Batch: 00001/00094 | Loss: 0.1626 | EWC Loss: 0.0299 | CE Loss: 0.1327\n",
      "Train Epoch: 017 Batch: 00002/00094 | Loss: 0.0859 | EWC Loss: 0.0301 | CE Loss: 0.0558\n",
      "Train Epoch: 017 Batch: 00003/00094 | Loss: 0.1783 | EWC Loss: 0.0299 | CE Loss: 0.1484\n",
      "Train Epoch: 017 Batch: 00004/00094 | Loss: 0.1116 | EWC Loss: 0.0299 | CE Loss: 0.0817\n",
      "Train Epoch: 017 Batch: 00005/00094 | Loss: 0.0857 | EWC Loss: 0.0299 | CE Loss: 0.0558\n",
      "Train Epoch: 017 Batch: 00006/00094 | Loss: 0.2020 | EWC Loss: 0.0299 | CE Loss: 0.1722\n",
      "Train Epoch: 017 Batch: 00007/00094 | Loss: 0.1992 | EWC Loss: 0.0300 | CE Loss: 0.1692\n",
      "Train Epoch: 017 Batch: 00008/00094 | Loss: 0.1086 | EWC Loss: 0.0302 | CE Loss: 0.0784\n",
      "Train Epoch: 017 Batch: 00009/00094 | Loss: 0.1751 | EWC Loss: 0.0303 | CE Loss: 0.1448\n",
      "Train Epoch: 017 Batch: 00010/00094 | Loss: 0.1068 | EWC Loss: 0.0301 | CE Loss: 0.0767\n",
      "Train Epoch: 017 Batch: 00011/00094 | Loss: 0.0843 | EWC Loss: 0.0302 | CE Loss: 0.0541\n",
      "Train Epoch: 017 Batch: 00012/00094 | Loss: 0.1630 | EWC Loss: 0.0300 | CE Loss: 0.1330\n",
      "Train Epoch: 017 Batch: 00013/00094 | Loss: 0.2110 | EWC Loss: 0.0302 | CE Loss: 0.1808\n",
      "Train Epoch: 017 Batch: 00014/00094 | Loss: 0.1366 | EWC Loss: 0.0302 | CE Loss: 0.1064\n",
      "Train Epoch: 017 Batch: 00015/00094 | Loss: 0.1435 | EWC Loss: 0.0303 | CE Loss: 0.1132\n",
      "Train Epoch: 017 Batch: 00016/00094 | Loss: 0.1039 | EWC Loss: 0.0303 | CE Loss: 0.0736\n",
      "Train Epoch: 017 Batch: 00017/00094 | Loss: 0.2695 | EWC Loss: 0.0305 | CE Loss: 0.2391\n",
      "Train Epoch: 017 Batch: 00018/00094 | Loss: 0.1258 | EWC Loss: 0.0305 | CE Loss: 0.0954\n",
      "Train Epoch: 017 Batch: 00019/00094 | Loss: 0.1431 | EWC Loss: 0.0303 | CE Loss: 0.1128\n",
      "Train Epoch: 017 Batch: 00020/00094 | Loss: 0.1785 | EWC Loss: 0.0302 | CE Loss: 0.1483\n",
      "Train Epoch: 017 Batch: 00021/00094 | Loss: 0.0567 | EWC Loss: 0.0301 | CE Loss: 0.0266\n",
      "Train Epoch: 017 Batch: 00022/00094 | Loss: 0.0798 | EWC Loss: 0.0301 | CE Loss: 0.0496\n",
      "Train Epoch: 017 Batch: 00023/00094 | Loss: 0.1285 | EWC Loss: 0.0301 | CE Loss: 0.0984\n",
      "Train Epoch: 017 Batch: 00024/00094 | Loss: 0.0709 | EWC Loss: 0.0303 | CE Loss: 0.0406\n",
      "Train Epoch: 017 Batch: 00025/00094 | Loss: 0.1533 | EWC Loss: 0.0302 | CE Loss: 0.1231\n",
      "Train Epoch: 017 Batch: 00026/00094 | Loss: 0.1191 | EWC Loss: 0.0303 | CE Loss: 0.0888\n",
      "Train Epoch: 017 Batch: 00027/00094 | Loss: 0.0913 | EWC Loss: 0.0303 | CE Loss: 0.0610\n",
      "Train Epoch: 017 Batch: 00028/00094 | Loss: 0.0942 | EWC Loss: 0.0303 | CE Loss: 0.0639\n",
      "Train Epoch: 017 Batch: 00029/00094 | Loss: 0.0750 | EWC Loss: 0.0303 | CE Loss: 0.0447\n",
      "Train Epoch: 017 Batch: 00030/00094 | Loss: 0.1678 | EWC Loss: 0.0303 | CE Loss: 0.1376\n",
      "Train Epoch: 017 Batch: 00031/00094 | Loss: 0.1425 | EWC Loss: 0.0300 | CE Loss: 0.1125\n",
      "Train Epoch: 017 Batch: 00032/00094 | Loss: 0.1723 | EWC Loss: 0.0301 | CE Loss: 0.1422\n",
      "Train Epoch: 017 Batch: 00033/00094 | Loss: 0.0786 | EWC Loss: 0.0301 | CE Loss: 0.0485\n",
      "Train Epoch: 017 Batch: 00034/00094 | Loss: 0.1407 | EWC Loss: 0.0300 | CE Loss: 0.1107\n",
      "Train Epoch: 017 Batch: 00035/00094 | Loss: 0.1240 | EWC Loss: 0.0300 | CE Loss: 0.0940\n",
      "Train Epoch: 017 Batch: 00036/00094 | Loss: 0.1838 | EWC Loss: 0.0299 | CE Loss: 0.1539\n",
      "Train Epoch: 017 Batch: 00037/00094 | Loss: 0.1624 | EWC Loss: 0.0298 | CE Loss: 0.1326\n",
      "Train Epoch: 017 Batch: 00038/00094 | Loss: 0.2465 | EWC Loss: 0.0298 | CE Loss: 0.2167\n",
      "Train Epoch: 017 Batch: 00039/00094 | Loss: 0.3047 | EWC Loss: 0.0299 | CE Loss: 0.2748\n",
      "Train Epoch: 017 Batch: 00040/00094 | Loss: 0.0940 | EWC Loss: 0.0300 | CE Loss: 0.0640\n",
      "Train Epoch: 017 Batch: 00041/00094 | Loss: 0.0878 | EWC Loss: 0.0300 | CE Loss: 0.0577\n",
      "Train Epoch: 017 Batch: 00042/00094 | Loss: 0.0762 | EWC Loss: 0.0301 | CE Loss: 0.0461\n",
      "Train Epoch: 017 Batch: 00043/00094 | Loss: 0.1615 | EWC Loss: 0.0300 | CE Loss: 0.1315\n",
      "Train Epoch: 017 Batch: 00044/00094 | Loss: 0.1051 | EWC Loss: 0.0302 | CE Loss: 0.0749\n",
      "Train Epoch: 017 Batch: 00045/00094 | Loss: 0.1086 | EWC Loss: 0.0299 | CE Loss: 0.0787\n",
      "Train Epoch: 017 Batch: 00046/00094 | Loss: 0.1663 | EWC Loss: 0.0298 | CE Loss: 0.1364\n",
      "Train Epoch: 017 Batch: 00047/00094 | Loss: 0.1092 | EWC Loss: 0.0301 | CE Loss: 0.0791\n",
      "Train Epoch: 017 Batch: 00048/00094 | Loss: 0.1600 | EWC Loss: 0.0301 | CE Loss: 0.1299\n",
      "Train Epoch: 017 Batch: 00049/00094 | Loss: 0.0978 | EWC Loss: 0.0300 | CE Loss: 0.0678\n",
      "Train Epoch: 017 Batch: 00050/00094 | Loss: 0.1023 | EWC Loss: 0.0299 | CE Loss: 0.0724\n",
      "Train Epoch: 017 Batch: 00051/00094 | Loss: 0.1421 | EWC Loss: 0.0301 | CE Loss: 0.1120\n",
      "Train Epoch: 017 Batch: 00052/00094 | Loss: 0.1528 | EWC Loss: 0.0299 | CE Loss: 0.1229\n",
      "Train Epoch: 017 Batch: 00053/00094 | Loss: 0.1208 | EWC Loss: 0.0298 | CE Loss: 0.0910\n",
      "Train Epoch: 017 Batch: 00054/00094 | Loss: 0.1290 | EWC Loss: 0.0297 | CE Loss: 0.0992\n",
      "Train Epoch: 017 Batch: 00055/00094 | Loss: 0.1498 | EWC Loss: 0.0299 | CE Loss: 0.1199\n",
      "Train Epoch: 017 Batch: 00056/00094 | Loss: 0.0890 | EWC Loss: 0.0300 | CE Loss: 0.0589\n",
      "Train Epoch: 017 Batch: 00057/00094 | Loss: 0.0783 | EWC Loss: 0.0302 | CE Loss: 0.0481\n",
      "Train Epoch: 017 Batch: 00058/00094 | Loss: 0.1180 | EWC Loss: 0.0300 | CE Loss: 0.0880\n",
      "Train Epoch: 017 Batch: 00059/00094 | Loss: 0.1172 | EWC Loss: 0.0300 | CE Loss: 0.0872\n",
      "Train Epoch: 017 Batch: 00060/00094 | Loss: 0.0933 | EWC Loss: 0.0300 | CE Loss: 0.0633\n",
      "Train Epoch: 017 Batch: 00061/00094 | Loss: 0.2377 | EWC Loss: 0.0301 | CE Loss: 0.2076\n",
      "Train Epoch: 017 Batch: 00062/00094 | Loss: 0.1241 | EWC Loss: 0.0299 | CE Loss: 0.0942\n",
      "Train Epoch: 017 Batch: 00063/00094 | Loss: 0.0953 | EWC Loss: 0.0300 | CE Loss: 0.0653\n",
      "Train Epoch: 017 Batch: 00064/00094 | Loss: 0.1291 | EWC Loss: 0.0299 | CE Loss: 0.0991\n",
      "Train Epoch: 017 Batch: 00065/00094 | Loss: 0.1393 | EWC Loss: 0.0299 | CE Loss: 0.1094\n",
      "Train Epoch: 017 Batch: 00066/00094 | Loss: 0.1328 | EWC Loss: 0.0300 | CE Loss: 0.1028\n",
      "Train Epoch: 017 Batch: 00067/00094 | Loss: 0.2801 | EWC Loss: 0.0300 | CE Loss: 0.2501\n",
      "Train Epoch: 017 Batch: 00068/00094 | Loss: 0.2331 | EWC Loss: 0.0316 | CE Loss: 0.2015\n",
      "Train Epoch: 017 Batch: 00069/00094 | Loss: 0.1662 | EWC Loss: 0.0309 | CE Loss: 0.1353\n",
      "Train Epoch: 017 Batch: 00070/00094 | Loss: 0.1361 | EWC Loss: 0.0305 | CE Loss: 0.1056\n",
      "Train Epoch: 017 Batch: 00071/00094 | Loss: 0.0856 | EWC Loss: 0.0304 | CE Loss: 0.0552\n",
      "Train Epoch: 017 Batch: 00072/00094 | Loss: 0.1346 | EWC Loss: 0.0302 | CE Loss: 0.1045\n",
      "Train Epoch: 017 Batch: 00073/00094 | Loss: 0.1475 | EWC Loss: 0.0300 | CE Loss: 0.1176\n",
      "Train Epoch: 017 Batch: 00074/00094 | Loss: 0.1040 | EWC Loss: 0.0302 | CE Loss: 0.0738\n",
      "Train Epoch: 017 Batch: 00075/00094 | Loss: 0.1746 | EWC Loss: 0.0301 | CE Loss: 0.1445\n",
      "Train Epoch: 017 Batch: 00076/00094 | Loss: 0.0769 | EWC Loss: 0.0303 | CE Loss: 0.0466\n",
      "Train Epoch: 017 Batch: 00077/00094 | Loss: 0.0818 | EWC Loss: 0.0302 | CE Loss: 0.0516\n",
      "Train Epoch: 017 Batch: 00078/00094 | Loss: 0.1941 | EWC Loss: 0.0299 | CE Loss: 0.1642\n",
      "Train Epoch: 017 Batch: 00079/00094 | Loss: 0.1241 | EWC Loss: 0.0300 | CE Loss: 0.0941\n",
      "Train Epoch: 017 Batch: 00080/00094 | Loss: 0.1370 | EWC Loss: 0.0302 | CE Loss: 0.1068\n",
      "Train Epoch: 017 Batch: 00081/00094 | Loss: 0.1220 | EWC Loss: 0.0304 | CE Loss: 0.0916\n",
      "Train Epoch: 017 Batch: 00082/00094 | Loss: 0.0934 | EWC Loss: 0.0303 | CE Loss: 0.0631\n",
      "Train Epoch: 017 Batch: 00083/00094 | Loss: 0.1090 | EWC Loss: 0.0304 | CE Loss: 0.0786\n",
      "Train Epoch: 017 Batch: 00084/00094 | Loss: 0.1094 | EWC Loss: 0.0303 | CE Loss: 0.0792\n",
      "Train Epoch: 017 Batch: 00085/00094 | Loss: 0.1537 | EWC Loss: 0.0302 | CE Loss: 0.1235\n",
      "Train Epoch: 017 Batch: 00086/00094 | Loss: 0.1167 | EWC Loss: 0.0302 | CE Loss: 0.0865\n",
      "Train Epoch: 017 Batch: 00087/00094 | Loss: 0.2064 | EWC Loss: 0.0303 | CE Loss: 0.1761\n",
      "Train Epoch: 017 Batch: 00088/00094 | Loss: 0.1883 | EWC Loss: 0.0308 | CE Loss: 0.1575\n",
      "Train Epoch: 017 Batch: 00089/00094 | Loss: 0.0696 | EWC Loss: 0.0311 | CE Loss: 0.0386\n",
      "Train Epoch: 017 Batch: 00090/00094 | Loss: 0.0891 | EWC Loss: 0.0308 | CE Loss: 0.0583\n",
      "Train Epoch: 017 Batch: 00091/00094 | Loss: 0.0998 | EWC Loss: 0.0305 | CE Loss: 0.0693\n",
      "Train Epoch: 017 Batch: 00092/00094 | Loss: 0.1178 | EWC Loss: 0.0305 | CE Loss: 0.0874\n",
      "Train Epoch: 017 Batch: 00093/00094 | Loss: 0.0991 | EWC Loss: 0.0303 | CE Loss: 0.0688\n",
      "Train Epoch: 017 Batch: 00094/00094 | Loss: 0.1297 | EWC Loss: 0.0302 | CE Loss: 0.0995\n",
      "Train Epoch: 017 |Acc 95.86667 | Loss: 0.13473 | Task Loss 0.10456 | EWC Loss: 0.03016\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0964 | acc:96.1500\n",
      "[VAL Acc] Target: 96.15%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7492 | acc:49.3000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9401 | acc:51.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9192 | acc:43.8931\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4066 | acc:59.3652\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5687 | acc:78.0961\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.8926 | acc:71.2774\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.28%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9930 | acc:62.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.81%\n",
      "[VAL Acc] Avg 64.00%\n",
      "\n",
      "\n",
      "---------- Starting epoch 18 ----------\n",
      "Train Epoch: 018 Batch: 00001/00094 | Loss: 0.2244 | EWC Loss: 0.0301 | CE Loss: 0.1943\n",
      "Train Epoch: 018 Batch: 00002/00094 | Loss: 0.0977 | EWC Loss: 0.0297 | CE Loss: 0.0681\n",
      "Train Epoch: 018 Batch: 00003/00094 | Loss: 0.0873 | EWC Loss: 0.0296 | CE Loss: 0.0577\n",
      "Train Epoch: 018 Batch: 00004/00094 | Loss: 0.1146 | EWC Loss: 0.0296 | CE Loss: 0.0850\n",
      "Train Epoch: 018 Batch: 00005/00094 | Loss: 0.1979 | EWC Loss: 0.0297 | CE Loss: 0.1682\n",
      "Train Epoch: 018 Batch: 00006/00094 | Loss: 0.1644 | EWC Loss: 0.0297 | CE Loss: 0.1347\n",
      "Train Epoch: 018 Batch: 00007/00094 | Loss: 0.0512 | EWC Loss: 0.0298 | CE Loss: 0.0213\n",
      "Train Epoch: 018 Batch: 00008/00094 | Loss: 0.1992 | EWC Loss: 0.0297 | CE Loss: 0.1695\n",
      "Train Epoch: 018 Batch: 00009/00094 | Loss: 0.1218 | EWC Loss: 0.0297 | CE Loss: 0.0921\n",
      "Train Epoch: 018 Batch: 00010/00094 | Loss: 0.1119 | EWC Loss: 0.0298 | CE Loss: 0.0821\n",
      "Train Epoch: 018 Batch: 00011/00094 | Loss: 0.0987 | EWC Loss: 0.0297 | CE Loss: 0.0690\n",
      "Train Epoch: 018 Batch: 00012/00094 | Loss: 0.1382 | EWC Loss: 0.0297 | CE Loss: 0.1085\n",
      "Train Epoch: 018 Batch: 00013/00094 | Loss: 0.1238 | EWC Loss: 0.0297 | CE Loss: 0.0942\n",
      "Train Epoch: 018 Batch: 00014/00094 | Loss: 0.1706 | EWC Loss: 0.0297 | CE Loss: 0.1409\n",
      "Train Epoch: 018 Batch: 00015/00094 | Loss: 0.2508 | EWC Loss: 0.0299 | CE Loss: 0.2209\n",
      "Train Epoch: 018 Batch: 00016/00094 | Loss: 0.0753 | EWC Loss: 0.0302 | CE Loss: 0.0451\n",
      "Train Epoch: 018 Batch: 00017/00094 | Loss: 0.1285 | EWC Loss: 0.0302 | CE Loss: 0.0984\n",
      "Train Epoch: 018 Batch: 00018/00094 | Loss: 0.0722 | EWC Loss: 0.0298 | CE Loss: 0.0424\n",
      "Train Epoch: 018 Batch: 00019/00094 | Loss: 0.1567 | EWC Loss: 0.0297 | CE Loss: 0.1270\n",
      "Train Epoch: 018 Batch: 00020/00094 | Loss: 0.0830 | EWC Loss: 0.0298 | CE Loss: 0.0532\n",
      "Train Epoch: 018 Batch: 00021/00094 | Loss: 0.0827 | EWC Loss: 0.0297 | CE Loss: 0.0530\n",
      "Train Epoch: 018 Batch: 00022/00094 | Loss: 0.1833 | EWC Loss: 0.0295 | CE Loss: 0.1538\n",
      "Train Epoch: 018 Batch: 00023/00094 | Loss: 0.1157 | EWC Loss: 0.0296 | CE Loss: 0.0861\n",
      "Train Epoch: 018 Batch: 00024/00094 | Loss: 0.1509 | EWC Loss: 0.0297 | CE Loss: 0.1212\n",
      "Train Epoch: 018 Batch: 00025/00094 | Loss: 0.1176 | EWC Loss: 0.0295 | CE Loss: 0.0881\n",
      "Train Epoch: 018 Batch: 00026/00094 | Loss: 0.1163 | EWC Loss: 0.0296 | CE Loss: 0.0867\n",
      "Train Epoch: 018 Batch: 00027/00094 | Loss: 0.0703 | EWC Loss: 0.0295 | CE Loss: 0.0407\n",
      "Train Epoch: 018 Batch: 00028/00094 | Loss: 0.1664 | EWC Loss: 0.0295 | CE Loss: 0.1368\n",
      "Train Epoch: 018 Batch: 00029/00094 | Loss: 0.0709 | EWC Loss: 0.0295 | CE Loss: 0.0414\n",
      "Train Epoch: 018 Batch: 00030/00094 | Loss: 0.0955 | EWC Loss: 0.0296 | CE Loss: 0.0659\n",
      "Train Epoch: 018 Batch: 00031/00094 | Loss: 0.0740 | EWC Loss: 0.0295 | CE Loss: 0.0445\n",
      "Train Epoch: 018 Batch: 00032/00094 | Loss: 0.1479 | EWC Loss: 0.0295 | CE Loss: 0.1183\n",
      "Train Epoch: 018 Batch: 00033/00094 | Loss: 0.0578 | EWC Loss: 0.0298 | CE Loss: 0.0281\n",
      "Train Epoch: 018 Batch: 00034/00094 | Loss: 0.1005 | EWC Loss: 0.0297 | CE Loss: 0.0708\n",
      "Train Epoch: 018 Batch: 00035/00094 | Loss: 0.1449 | EWC Loss: 0.0297 | CE Loss: 0.1153\n",
      "Train Epoch: 018 Batch: 00036/00094 | Loss: 0.1020 | EWC Loss: 0.0298 | CE Loss: 0.0723\n",
      "Train Epoch: 018 Batch: 00037/00094 | Loss: 0.1487 | EWC Loss: 0.0299 | CE Loss: 0.1188\n",
      "Train Epoch: 018 Batch: 00038/00094 | Loss: 0.1223 | EWC Loss: 0.0301 | CE Loss: 0.0922\n",
      "Train Epoch: 018 Batch: 00039/00094 | Loss: 0.2225 | EWC Loss: 0.0302 | CE Loss: 0.1923\n",
      "Train Epoch: 018 Batch: 00040/00094 | Loss: 0.1429 | EWC Loss: 0.0308 | CE Loss: 0.1121\n",
      "Train Epoch: 018 Batch: 00041/00094 | Loss: 0.1463 | EWC Loss: 0.0305 | CE Loss: 0.1158\n",
      "Train Epoch: 018 Batch: 00042/00094 | Loss: 0.1292 | EWC Loss: 0.0304 | CE Loss: 0.0988\n",
      "Train Epoch: 018 Batch: 00043/00094 | Loss: 0.1305 | EWC Loss: 0.0302 | CE Loss: 0.1003\n",
      "Train Epoch: 018 Batch: 00044/00094 | Loss: 0.1373 | EWC Loss: 0.0301 | CE Loss: 0.1072\n",
      "Train Epoch: 018 Batch: 00045/00094 | Loss: 0.1264 | EWC Loss: 0.0301 | CE Loss: 0.0964\n",
      "Train Epoch: 018 Batch: 00046/00094 | Loss: 0.1910 | EWC Loss: 0.0301 | CE Loss: 0.1608\n",
      "Train Epoch: 018 Batch: 00047/00094 | Loss: 0.1151 | EWC Loss: 0.0303 | CE Loss: 0.0848\n",
      "Train Epoch: 018 Batch: 00048/00094 | Loss: 0.0959 | EWC Loss: 0.0303 | CE Loss: 0.0657\n",
      "Train Epoch: 018 Batch: 00049/00094 | Loss: 0.1375 | EWC Loss: 0.0303 | CE Loss: 0.1072\n",
      "Train Epoch: 018 Batch: 00050/00094 | Loss: 0.1060 | EWC Loss: 0.0306 | CE Loss: 0.0754\n",
      "Train Epoch: 018 Batch: 00051/00094 | Loss: 0.1376 | EWC Loss: 0.0306 | CE Loss: 0.1069\n",
      "Train Epoch: 018 Batch: 00052/00094 | Loss: 0.0899 | EWC Loss: 0.0308 | CE Loss: 0.0592\n",
      "Train Epoch: 018 Batch: 00053/00094 | Loss: 0.0949 | EWC Loss: 0.0306 | CE Loss: 0.0643\n",
      "Train Epoch: 018 Batch: 00054/00094 | Loss: 0.1234 | EWC Loss: 0.0306 | CE Loss: 0.0928\n",
      "Train Epoch: 018 Batch: 00055/00094 | Loss: 0.1630 | EWC Loss: 0.0305 | CE Loss: 0.1325\n",
      "Train Epoch: 018 Batch: 00056/00094 | Loss: 0.0807 | EWC Loss: 0.0307 | CE Loss: 0.0500\n",
      "Train Epoch: 018 Batch: 00057/00094 | Loss: 0.1932 | EWC Loss: 0.0305 | CE Loss: 0.1627\n",
      "Train Epoch: 018 Batch: 00058/00094 | Loss: 0.1434 | EWC Loss: 0.0307 | CE Loss: 0.1127\n",
      "Train Epoch: 018 Batch: 00059/00094 | Loss: 0.1221 | EWC Loss: 0.0306 | CE Loss: 0.0915\n",
      "Train Epoch: 018 Batch: 00060/00094 | Loss: 0.1250 | EWC Loss: 0.0308 | CE Loss: 0.0942\n",
      "Train Epoch: 018 Batch: 00061/00094 | Loss: 0.1220 | EWC Loss: 0.0308 | CE Loss: 0.0913\n",
      "Train Epoch: 018 Batch: 00062/00094 | Loss: 0.1271 | EWC Loss: 0.0306 | CE Loss: 0.0964\n",
      "Train Epoch: 018 Batch: 00063/00094 | Loss: 0.0904 | EWC Loss: 0.0307 | CE Loss: 0.0598\n",
      "Train Epoch: 018 Batch: 00064/00094 | Loss: 0.1134 | EWC Loss: 0.0307 | CE Loss: 0.0827\n",
      "Train Epoch: 018 Batch: 00065/00094 | Loss: 0.0859 | EWC Loss: 0.0304 | CE Loss: 0.0555\n",
      "Train Epoch: 018 Batch: 00066/00094 | Loss: 0.0827 | EWC Loss: 0.0304 | CE Loss: 0.0523\n",
      "Train Epoch: 018 Batch: 00067/00094 | Loss: 0.1124 | EWC Loss: 0.0305 | CE Loss: 0.0819\n",
      "Train Epoch: 018 Batch: 00068/00094 | Loss: 0.1622 | EWC Loss: 0.0305 | CE Loss: 0.1317\n",
      "Train Epoch: 018 Batch: 00069/00094 | Loss: 0.1779 | EWC Loss: 0.0307 | CE Loss: 0.1472\n",
      "Train Epoch: 018 Batch: 00070/00094 | Loss: 0.0923 | EWC Loss: 0.0305 | CE Loss: 0.0619\n",
      "Train Epoch: 018 Batch: 00071/00094 | Loss: 0.0485 | EWC Loss: 0.0304 | CE Loss: 0.0181\n",
      "Train Epoch: 018 Batch: 00072/00094 | Loss: 0.0757 | EWC Loss: 0.0304 | CE Loss: 0.0454\n",
      "Train Epoch: 018 Batch: 00073/00094 | Loss: 0.0864 | EWC Loss: 0.0304 | CE Loss: 0.0560\n",
      "Train Epoch: 018 Batch: 00074/00094 | Loss: 0.1159 | EWC Loss: 0.0304 | CE Loss: 0.0854\n",
      "Train Epoch: 018 Batch: 00075/00094 | Loss: 0.0842 | EWC Loss: 0.0306 | CE Loss: 0.0536\n",
      "Train Epoch: 018 Batch: 00076/00094 | Loss: 0.0695 | EWC Loss: 0.0306 | CE Loss: 0.0390\n",
      "Train Epoch: 018 Batch: 00077/00094 | Loss: 0.1412 | EWC Loss: 0.0305 | CE Loss: 0.1107\n",
      "Train Epoch: 018 Batch: 00078/00094 | Loss: 0.0607 | EWC Loss: 0.0307 | CE Loss: 0.0301\n",
      "Train Epoch: 018 Batch: 00079/00094 | Loss: 0.0506 | EWC Loss: 0.0306 | CE Loss: 0.0199\n",
      "Train Epoch: 018 Batch: 00080/00094 | Loss: 0.2210 | EWC Loss: 0.0304 | CE Loss: 0.1905\n",
      "Train Epoch: 018 Batch: 00081/00094 | Loss: 0.0799 | EWC Loss: 0.0307 | CE Loss: 0.0492\n",
      "Train Epoch: 018 Batch: 00082/00094 | Loss: 0.0733 | EWC Loss: 0.0307 | CE Loss: 0.0425\n",
      "Train Epoch: 018 Batch: 00083/00094 | Loss: 0.0950 | EWC Loss: 0.0306 | CE Loss: 0.0644\n",
      "Train Epoch: 018 Batch: 00084/00094 | Loss: 0.1375 | EWC Loss: 0.0306 | CE Loss: 0.1069\n",
      "Train Epoch: 018 Batch: 00085/00094 | Loss: 0.0869 | EWC Loss: 0.0305 | CE Loss: 0.0564\n",
      "Train Epoch: 018 Batch: 00086/00094 | Loss: 0.1155 | EWC Loss: 0.0303 | CE Loss: 0.0851\n",
      "Train Epoch: 018 Batch: 00087/00094 | Loss: 0.1129 | EWC Loss: 0.0304 | CE Loss: 0.0825\n",
      "Train Epoch: 018 Batch: 00088/00094 | Loss: 0.0711 | EWC Loss: 0.0302 | CE Loss: 0.0409\n",
      "Train Epoch: 018 Batch: 00089/00094 | Loss: 0.1683 | EWC Loss: 0.0301 | CE Loss: 0.1382\n",
      "Train Epoch: 018 Batch: 00090/00094 | Loss: 0.0867 | EWC Loss: 0.0302 | CE Loss: 0.0565\n",
      "Train Epoch: 018 Batch: 00091/00094 | Loss: 0.1644 | EWC Loss: 0.0302 | CE Loss: 0.1342\n",
      "Train Epoch: 018 Batch: 00092/00094 | Loss: 0.1420 | EWC Loss: 0.0302 | CE Loss: 0.1118\n",
      "Train Epoch: 018 Batch: 00093/00094 | Loss: 0.0933 | EWC Loss: 0.0304 | CE Loss: 0.0629\n",
      "Train Epoch: 018 Batch: 00094/00094 | Loss: 0.0756 | EWC Loss: 0.0304 | CE Loss: 0.0452\n",
      "Train Epoch: 018 |Acc 96.71667 | Loss: 0.12034 | Task Loss 0.09017 | EWC Loss: 0.03017\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0898 | acc:96.6000\n",
      "[VAL Acc] Target: 96.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:3.0695 | acc:48.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:2.0352 | acc:51.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.0613 | acc:44.4656\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.47%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.1024 | acc:54.0361\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5571 | acc:78.3734\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.37%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.3502 | acc:65.3605\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.36%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9826 | acc:61.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.56%\n",
      "[VAL Acc] Avg 62.53%\n",
      "\n",
      "\n",
      "---------- Starting epoch 19 ----------\n",
      "Train Epoch: 019 Batch: 00001/00094 | Loss: 0.0882 | EWC Loss: 0.0303 | CE Loss: 0.0579\n",
      "Train Epoch: 019 Batch: 00002/00094 | Loss: 0.1822 | EWC Loss: 0.0306 | CE Loss: 0.1515\n",
      "Train Epoch: 019 Batch: 00003/00094 | Loss: 0.0861 | EWC Loss: 0.0312 | CE Loss: 0.0549\n",
      "Train Epoch: 019 Batch: 00004/00094 | Loss: 0.1374 | EWC Loss: 0.0311 | CE Loss: 0.1063\n",
      "Train Epoch: 019 Batch: 00005/00094 | Loss: 0.1196 | EWC Loss: 0.0311 | CE Loss: 0.0885\n",
      "Train Epoch: 019 Batch: 00006/00094 | Loss: 0.0940 | EWC Loss: 0.0314 | CE Loss: 0.0626\n",
      "Train Epoch: 019 Batch: 00007/00094 | Loss: 0.1827 | EWC Loss: 0.0311 | CE Loss: 0.1516\n",
      "Train Epoch: 019 Batch: 00008/00094 | Loss: 0.1039 | EWC Loss: 0.0310 | CE Loss: 0.0729\n",
      "Train Epoch: 019 Batch: 00009/00094 | Loss: 0.1414 | EWC Loss: 0.0311 | CE Loss: 0.1103\n",
      "Train Epoch: 019 Batch: 00010/00094 | Loss: 0.1301 | EWC Loss: 0.0312 | CE Loss: 0.0989\n",
      "Train Epoch: 019 Batch: 00011/00094 | Loss: 0.1226 | EWC Loss: 0.0313 | CE Loss: 0.0913\n",
      "Train Epoch: 019 Batch: 00012/00094 | Loss: 0.0807 | EWC Loss: 0.0314 | CE Loss: 0.0493\n",
      "Train Epoch: 019 Batch: 00013/00094 | Loss: 0.1209 | EWC Loss: 0.0313 | CE Loss: 0.0896\n",
      "Train Epoch: 019 Batch: 00014/00094 | Loss: 0.0503 | EWC Loss: 0.0310 | CE Loss: 0.0193\n",
      "Train Epoch: 019 Batch: 00015/00094 | Loss: 0.1244 | EWC Loss: 0.0309 | CE Loss: 0.0936\n",
      "Train Epoch: 019 Batch: 00016/00094 | Loss: 0.0693 | EWC Loss: 0.0309 | CE Loss: 0.0385\n",
      "Train Epoch: 019 Batch: 00017/00094 | Loss: 0.1322 | EWC Loss: 0.0308 | CE Loss: 0.1014\n",
      "Train Epoch: 019 Batch: 00018/00094 | Loss: 0.0589 | EWC Loss: 0.0306 | CE Loss: 0.0283\n",
      "Train Epoch: 019 Batch: 00019/00094 | Loss: 0.0562 | EWC Loss: 0.0304 | CE Loss: 0.0258\n",
      "Train Epoch: 019 Batch: 00020/00094 | Loss: 0.0511 | EWC Loss: 0.0303 | CE Loss: 0.0208\n",
      "Train Epoch: 019 Batch: 00021/00094 | Loss: 0.1155 | EWC Loss: 0.0302 | CE Loss: 0.0852\n",
      "Train Epoch: 019 Batch: 00022/00094 | Loss: 0.1552 | EWC Loss: 0.0306 | CE Loss: 0.1246\n",
      "Train Epoch: 019 Batch: 00023/00094 | Loss: 0.1266 | EWC Loss: 0.0307 | CE Loss: 0.0959\n",
      "Train Epoch: 019 Batch: 00024/00094 | Loss: 0.1277 | EWC Loss: 0.0306 | CE Loss: 0.0971\n",
      "Train Epoch: 019 Batch: 00025/00094 | Loss: 0.0880 | EWC Loss: 0.0308 | CE Loss: 0.0572\n",
      "Train Epoch: 019 Batch: 00026/00094 | Loss: 0.0957 | EWC Loss: 0.0308 | CE Loss: 0.0650\n",
      "Train Epoch: 019 Batch: 00027/00094 | Loss: 0.1265 | EWC Loss: 0.0309 | CE Loss: 0.0956\n",
      "Train Epoch: 019 Batch: 00028/00094 | Loss: 0.1377 | EWC Loss: 0.0309 | CE Loss: 0.1068\n",
      "Train Epoch: 019 Batch: 00029/00094 | Loss: 0.0865 | EWC Loss: 0.0315 | CE Loss: 0.0550\n",
      "Train Epoch: 019 Batch: 00030/00094 | Loss: 0.1160 | EWC Loss: 0.0314 | CE Loss: 0.0846\n",
      "Train Epoch: 019 Batch: 00031/00094 | Loss: 0.0758 | EWC Loss: 0.0311 | CE Loss: 0.0446\n",
      "Train Epoch: 019 Batch: 00032/00094 | Loss: 0.0622 | EWC Loss: 0.0310 | CE Loss: 0.0312\n",
      "Train Epoch: 019 Batch: 00033/00094 | Loss: 0.0870 | EWC Loss: 0.0310 | CE Loss: 0.0560\n",
      "Train Epoch: 019 Batch: 00034/00094 | Loss: 0.1083 | EWC Loss: 0.0307 | CE Loss: 0.0775\n",
      "Train Epoch: 019 Batch: 00035/00094 | Loss: 0.0933 | EWC Loss: 0.0308 | CE Loss: 0.0626\n",
      "Train Epoch: 019 Batch: 00036/00094 | Loss: 0.0698 | EWC Loss: 0.0308 | CE Loss: 0.0390\n",
      "Train Epoch: 019 Batch: 00037/00094 | Loss: 0.1292 | EWC Loss: 0.0307 | CE Loss: 0.0985\n",
      "Train Epoch: 019 Batch: 00038/00094 | Loss: 0.1294 | EWC Loss: 0.0306 | CE Loss: 0.0988\n",
      "Train Epoch: 019 Batch: 00039/00094 | Loss: 0.1411 | EWC Loss: 0.0305 | CE Loss: 0.1106\n",
      "Train Epoch: 019 Batch: 00040/00094 | Loss: 0.2839 | EWC Loss: 0.0307 | CE Loss: 0.2532\n",
      "Train Epoch: 019 Batch: 00041/00094 | Loss: 0.0808 | EWC Loss: 0.0315 | CE Loss: 0.0493\n",
      "Train Epoch: 019 Batch: 00042/00094 | Loss: 0.1403 | EWC Loss: 0.0312 | CE Loss: 0.1091\n",
      "Train Epoch: 019 Batch: 00043/00094 | Loss: 0.1456 | EWC Loss: 0.0311 | CE Loss: 0.1146\n",
      "Train Epoch: 019 Batch: 00044/00094 | Loss: 0.1354 | EWC Loss: 0.0310 | CE Loss: 0.1044\n",
      "Train Epoch: 019 Batch: 00045/00094 | Loss: 0.1708 | EWC Loss: 0.0312 | CE Loss: 0.1396\n",
      "Train Epoch: 019 Batch: 00046/00094 | Loss: 0.1495 | EWC Loss: 0.0307 | CE Loss: 0.1188\n",
      "Train Epoch: 019 Batch: 00047/00094 | Loss: 0.1207 | EWC Loss: 0.0309 | CE Loss: 0.0899\n",
      "Train Epoch: 019 Batch: 00048/00094 | Loss: 0.2915 | EWC Loss: 0.0309 | CE Loss: 0.2606\n",
      "Train Epoch: 019 Batch: 00049/00094 | Loss: 0.0930 | EWC Loss: 0.0313 | CE Loss: 0.0617\n",
      "Train Epoch: 019 Batch: 00050/00094 | Loss: 0.0657 | EWC Loss: 0.0311 | CE Loss: 0.0346\n",
      "Train Epoch: 019 Batch: 00051/00094 | Loss: 0.0921 | EWC Loss: 0.0309 | CE Loss: 0.0612\n",
      "Train Epoch: 019 Batch: 00052/00094 | Loss: 0.1389 | EWC Loss: 0.0309 | CE Loss: 0.1080\n",
      "Train Epoch: 019 Batch: 00053/00094 | Loss: 0.0646 | EWC Loss: 0.0310 | CE Loss: 0.0336\n",
      "Train Epoch: 019 Batch: 00054/00094 | Loss: 0.0987 | EWC Loss: 0.0309 | CE Loss: 0.0678\n",
      "Train Epoch: 019 Batch: 00055/00094 | Loss: 0.2101 | EWC Loss: 0.0307 | CE Loss: 0.1794\n",
      "Train Epoch: 019 Batch: 00056/00094 | Loss: 0.2193 | EWC Loss: 0.0310 | CE Loss: 0.1883\n",
      "Train Epoch: 019 Batch: 00057/00094 | Loss: 0.2612 | EWC Loss: 0.0313 | CE Loss: 0.2299\n",
      "Train Epoch: 019 Batch: 00058/00094 | Loss: 0.1622 | EWC Loss: 0.0330 | CE Loss: 0.1291\n",
      "Train Epoch: 019 Batch: 00059/00094 | Loss: 0.0991 | EWC Loss: 0.0323 | CE Loss: 0.0668\n",
      "Train Epoch: 019 Batch: 00060/00094 | Loss: 0.1331 | EWC Loss: 0.0317 | CE Loss: 0.1014\n",
      "Train Epoch: 019 Batch: 00061/00094 | Loss: 0.0795 | EWC Loss: 0.0316 | CE Loss: 0.0480\n",
      "Train Epoch: 019 Batch: 00062/00094 | Loss: 0.1143 | EWC Loss: 0.0315 | CE Loss: 0.0827\n",
      "Train Epoch: 019 Batch: 00063/00094 | Loss: 0.0737 | EWC Loss: 0.0317 | CE Loss: 0.0420\n",
      "Train Epoch: 019 Batch: 00064/00094 | Loss: 0.1176 | EWC Loss: 0.0314 | CE Loss: 0.0862\n",
      "Train Epoch: 019 Batch: 00065/00094 | Loss: 0.0735 | EWC Loss: 0.0316 | CE Loss: 0.0420\n",
      "Train Epoch: 019 Batch: 00066/00094 | Loss: 0.1007 | EWC Loss: 0.0317 | CE Loss: 0.0690\n",
      "Train Epoch: 019 Batch: 00067/00094 | Loss: 0.0689 | EWC Loss: 0.0317 | CE Loss: 0.0372\n",
      "Train Epoch: 019 Batch: 00068/00094 | Loss: 0.2317 | EWC Loss: 0.0315 | CE Loss: 0.2002\n",
      "Train Epoch: 019 Batch: 00069/00094 | Loss: 0.0750 | EWC Loss: 0.0317 | CE Loss: 0.0434\n",
      "Train Epoch: 019 Batch: 00070/00094 | Loss: 0.0706 | EWC Loss: 0.0315 | CE Loss: 0.0391\n",
      "Train Epoch: 019 Batch: 00071/00094 | Loss: 0.1266 | EWC Loss: 0.0313 | CE Loss: 0.0953\n",
      "Train Epoch: 019 Batch: 00072/00094 | Loss: 0.0576 | EWC Loss: 0.0310 | CE Loss: 0.0266\n",
      "Train Epoch: 019 Batch: 00073/00094 | Loss: 0.1474 | EWC Loss: 0.0308 | CE Loss: 0.1165\n",
      "Train Epoch: 019 Batch: 00074/00094 | Loss: 0.1871 | EWC Loss: 0.0305 | CE Loss: 0.1567\n",
      "Train Epoch: 019 Batch: 00075/00094 | Loss: 0.1128 | EWC Loss: 0.0308 | CE Loss: 0.0820\n",
      "Train Epoch: 019 Batch: 00076/00094 | Loss: 0.0723 | EWC Loss: 0.0307 | CE Loss: 0.0416\n",
      "Train Epoch: 019 Batch: 00077/00094 | Loss: 0.0907 | EWC Loss: 0.0306 | CE Loss: 0.0602\n",
      "Train Epoch: 019 Batch: 00078/00094 | Loss: 0.1441 | EWC Loss: 0.0306 | CE Loss: 0.1135\n",
      "Train Epoch: 019 Batch: 00079/00094 | Loss: 0.0681 | EWC Loss: 0.0308 | CE Loss: 0.0373\n",
      "Train Epoch: 019 Batch: 00080/00094 | Loss: 0.1047 | EWC Loss: 0.0309 | CE Loss: 0.0738\n",
      "Train Epoch: 019 Batch: 00081/00094 | Loss: 0.0796 | EWC Loss: 0.0309 | CE Loss: 0.0487\n",
      "Train Epoch: 019 Batch: 00082/00094 | Loss: 0.1205 | EWC Loss: 0.0307 | CE Loss: 0.0898\n",
      "Train Epoch: 019 Batch: 00083/00094 | Loss: 0.1722 | EWC Loss: 0.0306 | CE Loss: 0.1415\n",
      "Train Epoch: 019 Batch: 00084/00094 | Loss: 0.1518 | EWC Loss: 0.0310 | CE Loss: 0.1208\n",
      "Train Epoch: 019 Batch: 00085/00094 | Loss: 0.0587 | EWC Loss: 0.0306 | CE Loss: 0.0281\n",
      "Train Epoch: 019 Batch: 00086/00094 | Loss: 0.2422 | EWC Loss: 0.0304 | CE Loss: 0.2118\n",
      "Train Epoch: 019 Batch: 00087/00094 | Loss: 0.4569 | EWC Loss: 0.0308 | CE Loss: 0.4261\n",
      "Train Epoch: 019 Batch: 00088/00094 | Loss: 0.1942 | EWC Loss: 0.0314 | CE Loss: 0.1628\n",
      "Train Epoch: 019 Batch: 00089/00094 | Loss: 0.1624 | EWC Loss: 0.0313 | CE Loss: 0.1311\n",
      "Train Epoch: 019 Batch: 00090/00094 | Loss: 0.0844 | EWC Loss: 0.0312 | CE Loss: 0.0532\n",
      "Train Epoch: 019 Batch: 00091/00094 | Loss: 0.0622 | EWC Loss: 0.0309 | CE Loss: 0.0313\n",
      "Train Epoch: 019 Batch: 00092/00094 | Loss: 0.0882 | EWC Loss: 0.0307 | CE Loss: 0.0575\n",
      "Train Epoch: 019 Batch: 00093/00094 | Loss: 0.1596 | EWC Loss: 0.0307 | CE Loss: 0.1288\n",
      "Train Epoch: 019 Batch: 00094/00094 | Loss: 0.1640 | EWC Loss: 0.0309 | CE Loss: 0.1331\n",
      "Train Epoch: 019 |Acc 96.43333 | Loss: 0.12419 | Task Loss 0.09317 | EWC Loss: 0.03102\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1190 | acc:95.2000\n",
      "[VAL Acc] Target: 95.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:3.2112 | acc:49.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:2.2557 | acc:51.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.2136 | acc:45.9924\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.2332 | acc:53.7618\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4690 | acc:81.2384\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 81.24%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.6337 | acc:62.0298\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 62.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0363 | acc:60.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.81%\n",
      "[VAL Acc] Avg 62.46%\n",
      "\n",
      "\n",
      "---------- Starting epoch 20 ----------\n",
      "Train Epoch: 020 Batch: 00001/00094 | Loss: 0.0602 | EWC Loss: 0.0312 | CE Loss: 0.0289\n",
      "Train Epoch: 020 Batch: 00002/00094 | Loss: 0.0931 | EWC Loss: 0.0312 | CE Loss: 0.0619\n",
      "Train Epoch: 020 Batch: 00003/00094 | Loss: 0.1434 | EWC Loss: 0.0313 | CE Loss: 0.1121\n",
      "Train Epoch: 020 Batch: 00004/00094 | Loss: 0.1665 | EWC Loss: 0.0311 | CE Loss: 0.1354\n",
      "Train Epoch: 020 Batch: 00005/00094 | Loss: 0.2352 | EWC Loss: 0.0313 | CE Loss: 0.2039\n",
      "Train Epoch: 020 Batch: 00006/00094 | Loss: 0.1713 | EWC Loss: 0.0311 | CE Loss: 0.1402\n",
      "Train Epoch: 020 Batch: 00007/00094 | Loss: 0.1650 | EWC Loss: 0.0307 | CE Loss: 0.1343\n",
      "Train Epoch: 020 Batch: 00008/00094 | Loss: 0.1300 | EWC Loss: 0.0308 | CE Loss: 0.0992\n",
      "Train Epoch: 020 Batch: 00009/00094 | Loss: 0.0774 | EWC Loss: 0.0306 | CE Loss: 0.0467\n",
      "Train Epoch: 020 Batch: 00010/00094 | Loss: 0.1603 | EWC Loss: 0.0306 | CE Loss: 0.1297\n",
      "Train Epoch: 020 Batch: 00011/00094 | Loss: 0.0979 | EWC Loss: 0.0309 | CE Loss: 0.0670\n",
      "Train Epoch: 020 Batch: 00012/00094 | Loss: 0.0868 | EWC Loss: 0.0310 | CE Loss: 0.0557\n",
      "Train Epoch: 020 Batch: 00013/00094 | Loss: 0.0983 | EWC Loss: 0.0308 | CE Loss: 0.0675\n",
      "Train Epoch: 020 Batch: 00014/00094 | Loss: 0.1577 | EWC Loss: 0.0310 | CE Loss: 0.1266\n",
      "Train Epoch: 020 Batch: 00015/00094 | Loss: 0.1233 | EWC Loss: 0.0312 | CE Loss: 0.0922\n",
      "Train Epoch: 020 Batch: 00016/00094 | Loss: 0.1295 | EWC Loss: 0.0310 | CE Loss: 0.0985\n",
      "Train Epoch: 020 Batch: 00017/00094 | Loss: 0.0860 | EWC Loss: 0.0308 | CE Loss: 0.0552\n",
      "Train Epoch: 020 Batch: 00018/00094 | Loss: 0.1931 | EWC Loss: 0.0305 | CE Loss: 0.1626\n",
      "Train Epoch: 020 Batch: 00019/00094 | Loss: 0.0858 | EWC Loss: 0.0306 | CE Loss: 0.0552\n",
      "Train Epoch: 020 Batch: 00020/00094 | Loss: 0.1317 | EWC Loss: 0.0306 | CE Loss: 0.1012\n",
      "Train Epoch: 020 Batch: 00021/00094 | Loss: 0.1603 | EWC Loss: 0.0303 | CE Loss: 0.1300\n",
      "Train Epoch: 020 Batch: 00022/00094 | Loss: 0.0904 | EWC Loss: 0.0306 | CE Loss: 0.0598\n",
      "Train Epoch: 020 Batch: 00023/00094 | Loss: 0.1038 | EWC Loss: 0.0304 | CE Loss: 0.0735\n",
      "Train Epoch: 020 Batch: 00024/00094 | Loss: 0.0920 | EWC Loss: 0.0303 | CE Loss: 0.0617\n",
      "Train Epoch: 020 Batch: 00025/00094 | Loss: 0.2675 | EWC Loss: 0.0303 | CE Loss: 0.2372\n",
      "Train Epoch: 020 Batch: 00026/00094 | Loss: 0.0737 | EWC Loss: 0.0304 | CE Loss: 0.0433\n",
      "Train Epoch: 020 Batch: 00027/00094 | Loss: 0.0676 | EWC Loss: 0.0306 | CE Loss: 0.0370\n",
      "Train Epoch: 020 Batch: 00028/00094 | Loss: 0.0923 | EWC Loss: 0.0305 | CE Loss: 0.0618\n",
      "Train Epoch: 020 Batch: 00029/00094 | Loss: 0.2025 | EWC Loss: 0.0303 | CE Loss: 0.1721\n",
      "Train Epoch: 020 Batch: 00030/00094 | Loss: 0.1825 | EWC Loss: 0.0304 | CE Loss: 0.1521\n",
      "Train Epoch: 020 Batch: 00031/00094 | Loss: 0.0939 | EWC Loss: 0.0303 | CE Loss: 0.0636\n",
      "Train Epoch: 020 Batch: 00032/00094 | Loss: 0.2366 | EWC Loss: 0.0303 | CE Loss: 0.2063\n",
      "Train Epoch: 020 Batch: 00033/00094 | Loss: 0.1515 | EWC Loss: 0.0304 | CE Loss: 0.1210\n",
      "Train Epoch: 020 Batch: 00034/00094 | Loss: 0.1470 | EWC Loss: 0.0307 | CE Loss: 0.1163\n",
      "Train Epoch: 020 Batch: 00035/00094 | Loss: 0.1515 | EWC Loss: 0.0305 | CE Loss: 0.1210\n",
      "Train Epoch: 020 Batch: 00036/00094 | Loss: 0.0754 | EWC Loss: 0.0307 | CE Loss: 0.0447\n",
      "Train Epoch: 020 Batch: 00037/00094 | Loss: 0.1484 | EWC Loss: 0.0306 | CE Loss: 0.1178\n",
      "Train Epoch: 020 Batch: 00038/00094 | Loss: 0.2068 | EWC Loss: 0.0303 | CE Loss: 0.1766\n",
      "Train Epoch: 020 Batch: 00039/00094 | Loss: 0.0983 | EWC Loss: 0.0304 | CE Loss: 0.0679\n",
      "Train Epoch: 020 Batch: 00040/00094 | Loss: 0.2120 | EWC Loss: 0.0306 | CE Loss: 0.1813\n",
      "Train Epoch: 020 Batch: 00041/00094 | Loss: 0.1272 | EWC Loss: 0.0310 | CE Loss: 0.0963\n",
      "Train Epoch: 020 Batch: 00042/00094 | Loss: 0.2232 | EWC Loss: 0.0310 | CE Loss: 0.1923\n",
      "Train Epoch: 020 Batch: 00043/00094 | Loss: 0.1110 | EWC Loss: 0.0314 | CE Loss: 0.0796\n",
      "Train Epoch: 020 Batch: 00044/00094 | Loss: 0.1408 | EWC Loss: 0.0316 | CE Loss: 0.1093\n",
      "Train Epoch: 020 Batch: 00045/00094 | Loss: 0.1152 | EWC Loss: 0.0316 | CE Loss: 0.0836\n",
      "Train Epoch: 020 Batch: 00046/00094 | Loss: 0.1417 | EWC Loss: 0.0314 | CE Loss: 0.1103\n",
      "Train Epoch: 020 Batch: 00047/00094 | Loss: 0.0601 | EWC Loss: 0.0313 | CE Loss: 0.0288\n",
      "Train Epoch: 020 Batch: 00048/00094 | Loss: 0.1665 | EWC Loss: 0.0312 | CE Loss: 0.1352\n",
      "Train Epoch: 020 Batch: 00049/00094 | Loss: 0.2281 | EWC Loss: 0.0313 | CE Loss: 0.1968\n",
      "Train Epoch: 020 Batch: 00050/00094 | Loss: 0.0595 | EWC Loss: 0.0311 | CE Loss: 0.0284\n",
      "Train Epoch: 020 Batch: 00051/00094 | Loss: 0.0674 | EWC Loss: 0.0310 | CE Loss: 0.0364\n",
      "Train Epoch: 020 Batch: 00052/00094 | Loss: 0.0985 | EWC Loss: 0.0308 | CE Loss: 0.0677\n",
      "Train Epoch: 020 Batch: 00053/00094 | Loss: 0.0509 | EWC Loss: 0.0306 | CE Loss: 0.0203\n",
      "Train Epoch: 020 Batch: 00054/00094 | Loss: 0.0944 | EWC Loss: 0.0305 | CE Loss: 0.0640\n",
      "Train Epoch: 020 Batch: 00055/00094 | Loss: 0.1049 | EWC Loss: 0.0303 | CE Loss: 0.0746\n",
      "Train Epoch: 020 Batch: 00056/00094 | Loss: 0.1654 | EWC Loss: 0.0306 | CE Loss: 0.1348\n",
      "Train Epoch: 020 Batch: 00057/00094 | Loss: 0.1942 | EWC Loss: 0.0313 | CE Loss: 0.1629\n",
      "Train Epoch: 020 Batch: 00058/00094 | Loss: 0.1860 | EWC Loss: 0.0307 | CE Loss: 0.1552\n",
      "Train Epoch: 020 Batch: 00059/00094 | Loss: 0.1707 | EWC Loss: 0.0305 | CE Loss: 0.1402\n",
      "Train Epoch: 020 Batch: 00060/00094 | Loss: 0.0976 | EWC Loss: 0.0305 | CE Loss: 0.0671\n",
      "Train Epoch: 020 Batch: 00061/00094 | Loss: 0.0897 | EWC Loss: 0.0305 | CE Loss: 0.0592\n",
      "Train Epoch: 020 Batch: 00062/00094 | Loss: 0.0674 | EWC Loss: 0.0303 | CE Loss: 0.0371\n",
      "Train Epoch: 020 Batch: 00063/00094 | Loss: 0.0648 | EWC Loss: 0.0302 | CE Loss: 0.0346\n",
      "Train Epoch: 020 Batch: 00064/00094 | Loss: 0.2164 | EWC Loss: 0.0302 | CE Loss: 0.1863\n",
      "Train Epoch: 020 Batch: 00065/00094 | Loss: 0.0476 | EWC Loss: 0.0308 | CE Loss: 0.0168\n",
      "Train Epoch: 020 Batch: 00066/00094 | Loss: 0.1039 | EWC Loss: 0.0307 | CE Loss: 0.0732\n",
      "Train Epoch: 020 Batch: 00067/00094 | Loss: 0.1454 | EWC Loss: 0.0305 | CE Loss: 0.1150\n",
      "Train Epoch: 020 Batch: 00068/00094 | Loss: 0.0942 | EWC Loss: 0.0307 | CE Loss: 0.0635\n",
      "Train Epoch: 020 Batch: 00069/00094 | Loss: 0.0591 | EWC Loss: 0.0309 | CE Loss: 0.0281\n",
      "Train Epoch: 020 Batch: 00070/00094 | Loss: 0.1151 | EWC Loss: 0.0306 | CE Loss: 0.0844\n",
      "Train Epoch: 020 Batch: 00071/00094 | Loss: 0.0962 | EWC Loss: 0.0303 | CE Loss: 0.0659\n",
      "Train Epoch: 020 Batch: 00072/00094 | Loss: 0.1006 | EWC Loss: 0.0303 | CE Loss: 0.0702\n",
      "Train Epoch: 020 Batch: 00073/00094 | Loss: 0.1810 | EWC Loss: 0.0303 | CE Loss: 0.1507\n",
      "Train Epoch: 020 Batch: 00074/00094 | Loss: 0.0841 | EWC Loss: 0.0304 | CE Loss: 0.0537\n",
      "Train Epoch: 020 Batch: 00075/00094 | Loss: 0.0650 | EWC Loss: 0.0303 | CE Loss: 0.0347\n",
      "Train Epoch: 020 Batch: 00076/00094 | Loss: 0.1499 | EWC Loss: 0.0302 | CE Loss: 0.1197\n",
      "Train Epoch: 020 Batch: 00077/00094 | Loss: 0.0772 | EWC Loss: 0.0301 | CE Loss: 0.0471\n",
      "Train Epoch: 020 Batch: 00078/00094 | Loss: 0.2582 | EWC Loss: 0.0300 | CE Loss: 0.2282\n",
      "Train Epoch: 020 Batch: 00079/00094 | Loss: 0.0955 | EWC Loss: 0.0301 | CE Loss: 0.0654\n",
      "Train Epoch: 020 Batch: 00080/00094 | Loss: 0.1866 | EWC Loss: 0.0301 | CE Loss: 0.1565\n",
      "Train Epoch: 020 Batch: 00081/00094 | Loss: 0.1048 | EWC Loss: 0.0301 | CE Loss: 0.0748\n",
      "Train Epoch: 020 Batch: 00082/00094 | Loss: 0.1901 | EWC Loss: 0.0302 | CE Loss: 0.1599\n",
      "Train Epoch: 020 Batch: 00083/00094 | Loss: 0.0832 | EWC Loss: 0.0321 | CE Loss: 0.0511\n",
      "Train Epoch: 020 Batch: 00084/00094 | Loss: 0.0991 | EWC Loss: 0.0319 | CE Loss: 0.0672\n",
      "Train Epoch: 020 Batch: 00085/00094 | Loss: 0.1129 | EWC Loss: 0.0315 | CE Loss: 0.0813\n",
      "Train Epoch: 020 Batch: 00086/00094 | Loss: 0.0908 | EWC Loss: 0.0313 | CE Loss: 0.0595\n",
      "Train Epoch: 020 Batch: 00087/00094 | Loss: 0.1244 | EWC Loss: 0.0315 | CE Loss: 0.0929\n",
      "Train Epoch: 020 Batch: 00088/00094 | Loss: 0.0908 | EWC Loss: 0.0316 | CE Loss: 0.0592\n",
      "Train Epoch: 020 Batch: 00089/00094 | Loss: 0.1562 | EWC Loss: 0.0316 | CE Loss: 0.1246\n",
      "Train Epoch: 020 Batch: 00090/00094 | Loss: 0.0998 | EWC Loss: 0.0318 | CE Loss: 0.0679\n",
      "Train Epoch: 020 Batch: 00091/00094 | Loss: 0.0675 | EWC Loss: 0.0316 | CE Loss: 0.0359\n",
      "Train Epoch: 020 Batch: 00092/00094 | Loss: 0.1392 | EWC Loss: 0.0315 | CE Loss: 0.1077\n",
      "Train Epoch: 020 Batch: 00093/00094 | Loss: 0.0867 | EWC Loss: 0.0312 | CE Loss: 0.0555\n",
      "Train Epoch: 020 Batch: 00094/00094 | Loss: 0.1119 | EWC Loss: 0.0310 | CE Loss: 0.0809\n",
      "Train Epoch: 020 |Acc 96.41667 | Loss: 0.12664 | Task Loss 0.09587 | EWC Loss: 0.03078\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1143 | acc:95.6500\n",
      "[VAL Acc] Target: 95.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7920 | acc:48.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8256 | acc:51.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8726 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.2970 | acc:52.1944\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 52.19%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5246 | acc:79.1128\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 79.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.4755 | acc:61.2461\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 61.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0616 | acc:59.1875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.19%\n",
      "[VAL Acc] Avg 61.74%\n",
      "\n",
      "\n",
      "---------- Starting epoch 21 ----------\n",
      "Train Epoch: 021 Batch: 00001/00094 | Loss: 0.0769 | EWC Loss: 0.0312 | CE Loss: 0.0457\n",
      "Train Epoch: 021 Batch: 00002/00094 | Loss: 0.0717 | EWC Loss: 0.0311 | CE Loss: 0.0407\n",
      "Train Epoch: 021 Batch: 00003/00094 | Loss: 0.1799 | EWC Loss: 0.0310 | CE Loss: 0.1489\n",
      "Train Epoch: 021 Batch: 00004/00094 | Loss: 0.1316 | EWC Loss: 0.0309 | CE Loss: 0.1007\n",
      "Train Epoch: 021 Batch: 00005/00094 | Loss: 0.0720 | EWC Loss: 0.0308 | CE Loss: 0.0412\n",
      "Train Epoch: 021 Batch: 00006/00094 | Loss: 0.0596 | EWC Loss: 0.0307 | CE Loss: 0.0288\n",
      "Train Epoch: 021 Batch: 00007/00094 | Loss: 0.1179 | EWC Loss: 0.0306 | CE Loss: 0.0873\n",
      "Train Epoch: 021 Batch: 00008/00094 | Loss: 0.1396 | EWC Loss: 0.0306 | CE Loss: 0.1089\n",
      "Train Epoch: 021 Batch: 00009/00094 | Loss: 0.0593 | EWC Loss: 0.0310 | CE Loss: 0.0284\n",
      "Train Epoch: 021 Batch: 00010/00094 | Loss: 0.1089 | EWC Loss: 0.0311 | CE Loss: 0.0778\n",
      "Train Epoch: 021 Batch: 00011/00094 | Loss: 0.1729 | EWC Loss: 0.0310 | CE Loss: 0.1420\n",
      "Train Epoch: 021 Batch: 00012/00094 | Loss: 0.1210 | EWC Loss: 0.0307 | CE Loss: 0.0903\n",
      "Train Epoch: 021 Batch: 00013/00094 | Loss: 0.1037 | EWC Loss: 0.0308 | CE Loss: 0.0729\n",
      "Train Epoch: 021 Batch: 00014/00094 | Loss: 0.1181 | EWC Loss: 0.0306 | CE Loss: 0.0875\n",
      "Train Epoch: 021 Batch: 00015/00094 | Loss: 0.0699 | EWC Loss: 0.0308 | CE Loss: 0.0392\n",
      "Train Epoch: 021 Batch: 00016/00094 | Loss: 0.1634 | EWC Loss: 0.0308 | CE Loss: 0.1326\n",
      "Train Epoch: 021 Batch: 00017/00094 | Loss: 0.0910 | EWC Loss: 0.0308 | CE Loss: 0.0603\n",
      "Train Epoch: 021 Batch: 00018/00094 | Loss: 0.1233 | EWC Loss: 0.0307 | CE Loss: 0.0926\n",
      "Train Epoch: 021 Batch: 00019/00094 | Loss: 0.0918 | EWC Loss: 0.0313 | CE Loss: 0.0604\n",
      "Train Epoch: 021 Batch: 00020/00094 | Loss: 0.0532 | EWC Loss: 0.0309 | CE Loss: 0.0223\n",
      "Train Epoch: 021 Batch: 00021/00094 | Loss: 0.1333 | EWC Loss: 0.0307 | CE Loss: 0.1026\n",
      "Train Epoch: 021 Batch: 00022/00094 | Loss: 0.1122 | EWC Loss: 0.0307 | CE Loss: 0.0815\n",
      "Train Epoch: 021 Batch: 00023/00094 | Loss: 0.1213 | EWC Loss: 0.0303 | CE Loss: 0.0910\n",
      "Train Epoch: 021 Batch: 00024/00094 | Loss: 0.0961 | EWC Loss: 0.0304 | CE Loss: 0.0657\n",
      "Train Epoch: 021 Batch: 00025/00094 | Loss: 0.0992 | EWC Loss: 0.0304 | CE Loss: 0.0688\n",
      "Train Epoch: 021 Batch: 00026/00094 | Loss: 0.0769 | EWC Loss: 0.0306 | CE Loss: 0.0463\n",
      "Train Epoch: 021 Batch: 00027/00094 | Loss: 0.1714 | EWC Loss: 0.0304 | CE Loss: 0.1410\n",
      "Train Epoch: 021 Batch: 00028/00094 | Loss: 0.2349 | EWC Loss: 0.0306 | CE Loss: 0.2043\n",
      "Train Epoch: 021 Batch: 00029/00094 | Loss: 0.1488 | EWC Loss: 0.0308 | CE Loss: 0.1180\n",
      "Train Epoch: 021 Batch: 00030/00094 | Loss: 0.1344 | EWC Loss: 0.0310 | CE Loss: 0.1034\n",
      "Train Epoch: 021 Batch: 00031/00094 | Loss: 0.1456 | EWC Loss: 0.0308 | CE Loss: 0.1147\n",
      "Train Epoch: 021 Batch: 00032/00094 | Loss: 0.1155 | EWC Loss: 0.0309 | CE Loss: 0.0846\n",
      "Train Epoch: 021 Batch: 00033/00094 | Loss: 0.3334 | EWC Loss: 0.0309 | CE Loss: 0.3025\n",
      "Train Epoch: 021 Batch: 00034/00094 | Loss: 0.1372 | EWC Loss: 0.0311 | CE Loss: 0.1062\n",
      "Train Epoch: 021 Batch: 00035/00094 | Loss: 0.0594 | EWC Loss: 0.0312 | CE Loss: 0.0282\n",
      "Train Epoch: 021 Batch: 00036/00094 | Loss: 0.0852 | EWC Loss: 0.0309 | CE Loss: 0.0543\n",
      "Train Epoch: 021 Batch: 00037/00094 | Loss: 0.1002 | EWC Loss: 0.0306 | CE Loss: 0.0696\n",
      "Train Epoch: 021 Batch: 00038/00094 | Loss: 0.1739 | EWC Loss: 0.0306 | CE Loss: 0.1433\n",
      "Train Epoch: 021 Batch: 00039/00094 | Loss: 0.0657 | EWC Loss: 0.0309 | CE Loss: 0.0348\n",
      "Train Epoch: 021 Batch: 00040/00094 | Loss: 0.1440 | EWC Loss: 0.0309 | CE Loss: 0.1131\n",
      "Train Epoch: 021 Batch: 00041/00094 | Loss: 0.1732 | EWC Loss: 0.0309 | CE Loss: 0.1423\n",
      "Train Epoch: 021 Batch: 00042/00094 | Loss: 0.0850 | EWC Loss: 0.0309 | CE Loss: 0.0541\n",
      "Train Epoch: 021 Batch: 00043/00094 | Loss: 0.0889 | EWC Loss: 0.0311 | CE Loss: 0.0578\n",
      "Train Epoch: 021 Batch: 00044/00094 | Loss: 0.1004 | EWC Loss: 0.0313 | CE Loss: 0.0691\n",
      "Train Epoch: 021 Batch: 00045/00094 | Loss: 0.0753 | EWC Loss: 0.0310 | CE Loss: 0.0443\n",
      "Train Epoch: 021 Batch: 00046/00094 | Loss: 0.0738 | EWC Loss: 0.0308 | CE Loss: 0.0430\n",
      "Train Epoch: 021 Batch: 00047/00094 | Loss: 0.0714 | EWC Loss: 0.0306 | CE Loss: 0.0408\n",
      "Train Epoch: 021 Batch: 00048/00094 | Loss: 0.1101 | EWC Loss: 0.0305 | CE Loss: 0.0796\n",
      "Train Epoch: 021 Batch: 00049/00094 | Loss: 0.2255 | EWC Loss: 0.0304 | CE Loss: 0.1951\n",
      "Train Epoch: 021 Batch: 00050/00094 | Loss: 0.0662 | EWC Loss: 0.0306 | CE Loss: 0.0356\n",
      "Train Epoch: 021 Batch: 00051/00094 | Loss: 0.1413 | EWC Loss: 0.0306 | CE Loss: 0.1107\n",
      "Train Epoch: 021 Batch: 00052/00094 | Loss: 0.0524 | EWC Loss: 0.0304 | CE Loss: 0.0220\n",
      "Train Epoch: 021 Batch: 00053/00094 | Loss: 0.0921 | EWC Loss: 0.0303 | CE Loss: 0.0618\n",
      "Train Epoch: 021 Batch: 00054/00094 | Loss: 0.0838 | EWC Loss: 0.0302 | CE Loss: 0.0537\n",
      "Train Epoch: 021 Batch: 00055/00094 | Loss: 0.1042 | EWC Loss: 0.0301 | CE Loss: 0.0741\n",
      "Train Epoch: 021 Batch: 00056/00094 | Loss: 0.0659 | EWC Loss: 0.0302 | CE Loss: 0.0358\n",
      "Train Epoch: 021 Batch: 00057/00094 | Loss: 0.0937 | EWC Loss: 0.0302 | CE Loss: 0.0635\n",
      "Train Epoch: 021 Batch: 00058/00094 | Loss: 0.1150 | EWC Loss: 0.0302 | CE Loss: 0.0847\n",
      "Train Epoch: 021 Batch: 00059/00094 | Loss: 0.0783 | EWC Loss: 0.0305 | CE Loss: 0.0478\n",
      "Train Epoch: 021 Batch: 00060/00094 | Loss: 0.1005 | EWC Loss: 0.0305 | CE Loss: 0.0700\n",
      "Train Epoch: 021 Batch: 00061/00094 | Loss: 0.1346 | EWC Loss: 0.0304 | CE Loss: 0.1042\n",
      "Train Epoch: 021 Batch: 00062/00094 | Loss: 0.1716 | EWC Loss: 0.0308 | CE Loss: 0.1408\n",
      "Train Epoch: 021 Batch: 00063/00094 | Loss: 0.0625 | EWC Loss: 0.0311 | CE Loss: 0.0314\n",
      "Train Epoch: 021 Batch: 00064/00094 | Loss: 0.0962 | EWC Loss: 0.0310 | CE Loss: 0.0652\n",
      "Train Epoch: 021 Batch: 00065/00094 | Loss: 0.1328 | EWC Loss: 0.0311 | CE Loss: 0.1017\n",
      "Train Epoch: 021 Batch: 00066/00094 | Loss: 0.1303 | EWC Loss: 0.0309 | CE Loss: 0.0994\n",
      "Train Epoch: 021 Batch: 00067/00094 | Loss: 0.0571 | EWC Loss: 0.0310 | CE Loss: 0.0260\n",
      "Train Epoch: 021 Batch: 00068/00094 | Loss: 0.0721 | EWC Loss: 0.0309 | CE Loss: 0.0413\n",
      "Train Epoch: 021 Batch: 00069/00094 | Loss: 0.1014 | EWC Loss: 0.0309 | CE Loss: 0.0705\n",
      "Train Epoch: 021 Batch: 00070/00094 | Loss: 0.0771 | EWC Loss: 0.0310 | CE Loss: 0.0461\n",
      "Train Epoch: 021 Batch: 00071/00094 | Loss: 0.0976 | EWC Loss: 0.0310 | CE Loss: 0.0666\n",
      "Train Epoch: 021 Batch: 00072/00094 | Loss: 0.1169 | EWC Loss: 0.0309 | CE Loss: 0.0860\n",
      "Train Epoch: 021 Batch: 00073/00094 | Loss: 0.0603 | EWC Loss: 0.0308 | CE Loss: 0.0296\n",
      "Train Epoch: 021 Batch: 00074/00094 | Loss: 0.1518 | EWC Loss: 0.0306 | CE Loss: 0.1212\n",
      "Train Epoch: 021 Batch: 00075/00094 | Loss: 0.1147 | EWC Loss: 0.0306 | CE Loss: 0.0840\n",
      "Train Epoch: 021 Batch: 00076/00094 | Loss: 0.1195 | EWC Loss: 0.0308 | CE Loss: 0.0887\n",
      "Train Epoch: 021 Batch: 00077/00094 | Loss: 0.0672 | EWC Loss: 0.0310 | CE Loss: 0.0361\n",
      "Train Epoch: 021 Batch: 00078/00094 | Loss: 0.1704 | EWC Loss: 0.0310 | CE Loss: 0.1394\n",
      "Train Epoch: 021 Batch: 00079/00094 | Loss: 0.1720 | EWC Loss: 0.0309 | CE Loss: 0.1411\n",
      "Train Epoch: 021 Batch: 00080/00094 | Loss: 0.1027 | EWC Loss: 0.0312 | CE Loss: 0.0715\n",
      "Train Epoch: 021 Batch: 00081/00094 | Loss: 0.0758 | EWC Loss: 0.0309 | CE Loss: 0.0449\n",
      "Train Epoch: 021 Batch: 00082/00094 | Loss: 0.1184 | EWC Loss: 0.0307 | CE Loss: 0.0877\n",
      "Train Epoch: 021 Batch: 00083/00094 | Loss: 0.0577 | EWC Loss: 0.0307 | CE Loss: 0.0270\n",
      "Train Epoch: 021 Batch: 00084/00094 | Loss: 0.0791 | EWC Loss: 0.0305 | CE Loss: 0.0485\n",
      "Train Epoch: 021 Batch: 00085/00094 | Loss: 0.0878 | EWC Loss: 0.0304 | CE Loss: 0.0573\n",
      "Train Epoch: 021 Batch: 00086/00094 | Loss: 0.1236 | EWC Loss: 0.0306 | CE Loss: 0.0930\n",
      "Train Epoch: 021 Batch: 00087/00094 | Loss: 0.1220 | EWC Loss: 0.0305 | CE Loss: 0.0914\n",
      "Train Epoch: 021 Batch: 00088/00094 | Loss: 0.1584 | EWC Loss: 0.0307 | CE Loss: 0.1277\n",
      "Train Epoch: 021 Batch: 00089/00094 | Loss: 0.1011 | EWC Loss: 0.0305 | CE Loss: 0.0707\n",
      "Train Epoch: 021 Batch: 00090/00094 | Loss: 0.0852 | EWC Loss: 0.0306 | CE Loss: 0.0546\n",
      "Train Epoch: 021 Batch: 00091/00094 | Loss: 0.0537 | EWC Loss: 0.0306 | CE Loss: 0.0231\n",
      "Train Epoch: 021 Batch: 00092/00094 | Loss: 0.1051 | EWC Loss: 0.0305 | CE Loss: 0.0746\n",
      "Train Epoch: 021 Batch: 00093/00094 | Loss: 0.1603 | EWC Loss: 0.0305 | CE Loss: 0.1299\n",
      "Train Epoch: 021 Batch: 00094/00094 | Loss: 0.1196 | EWC Loss: 0.0307 | CE Loss: 0.0889\n",
      "Train Epoch: 021 |Acc 96.71667 | Loss: 0.11136 | Task Loss 0.08062 | EWC Loss: 0.03074\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1718 | acc:93.6000\n",
      "[VAL Acc] Target: 93.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:3.1609 | acc:47.9000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9360 | acc:50.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.1018 | acc:47.9008\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.1238 | acc:55.7210\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.72%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9729 | acc:65.3420\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 65.34%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.4023 | acc:64.3809\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1108 | acc:61.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.81%\n",
      "[VAL Acc] Avg 60.91%\n",
      "\n",
      "\n",
      "---------- Starting epoch 22 ----------\n",
      "Train Epoch: 022 Batch: 00001/00094 | Loss: 0.1034 | EWC Loss: 0.0307 | CE Loss: 0.0727\n",
      "Train Epoch: 022 Batch: 00002/00094 | Loss: 0.1390 | EWC Loss: 0.0304 | CE Loss: 0.1086\n",
      "Train Epoch: 022 Batch: 00003/00094 | Loss: 0.1512 | EWC Loss: 0.0305 | CE Loss: 0.1207\n",
      "Train Epoch: 022 Batch: 00004/00094 | Loss: 0.0815 | EWC Loss: 0.0307 | CE Loss: 0.0508\n",
      "Train Epoch: 022 Batch: 00005/00094 | Loss: 0.0556 | EWC Loss: 0.0305 | CE Loss: 0.0251\n",
      "Train Epoch: 022 Batch: 00006/00094 | Loss: 0.1134 | EWC Loss: 0.0304 | CE Loss: 0.0830\n",
      "Train Epoch: 022 Batch: 00007/00094 | Loss: 0.1301 | EWC Loss: 0.0305 | CE Loss: 0.0996\n",
      "Train Epoch: 022 Batch: 00008/00094 | Loss: 0.0906 | EWC Loss: 0.0308 | CE Loss: 0.0598\n",
      "Train Epoch: 022 Batch: 00009/00094 | Loss: 0.0731 | EWC Loss: 0.0306 | CE Loss: 0.0424\n",
      "Train Epoch: 022 Batch: 00010/00094 | Loss: 0.1385 | EWC Loss: 0.0304 | CE Loss: 0.1081\n",
      "Train Epoch: 022 Batch: 00011/00094 | Loss: 0.1838 | EWC Loss: 0.0305 | CE Loss: 0.1533\n",
      "Train Epoch: 022 Batch: 00012/00094 | Loss: 0.0928 | EWC Loss: 0.0307 | CE Loss: 0.0620\n",
      "Train Epoch: 022 Batch: 00013/00094 | Loss: 0.1619 | EWC Loss: 0.0308 | CE Loss: 0.1311\n",
      "Train Epoch: 022 Batch: 00014/00094 | Loss: 0.0682 | EWC Loss: 0.0315 | CE Loss: 0.0367\n",
      "Train Epoch: 022 Batch: 00015/00094 | Loss: 0.1892 | EWC Loss: 0.0314 | CE Loss: 0.1578\n",
      "Train Epoch: 022 Batch: 00016/00094 | Loss: 0.1221 | EWC Loss: 0.0316 | CE Loss: 0.0905\n",
      "Train Epoch: 022 Batch: 00017/00094 | Loss: 0.0965 | EWC Loss: 0.0317 | CE Loss: 0.0648\n",
      "Train Epoch: 022 Batch: 00018/00094 | Loss: 0.1037 | EWC Loss: 0.0312 | CE Loss: 0.0725\n",
      "Train Epoch: 022 Batch: 00019/00094 | Loss: 0.1422 | EWC Loss: 0.0309 | CE Loss: 0.1112\n",
      "Train Epoch: 022 Batch: 00020/00094 | Loss: 0.1732 | EWC Loss: 0.0311 | CE Loss: 0.1421\n",
      "Train Epoch: 022 Batch: 00021/00094 | Loss: 0.0776 | EWC Loss: 0.0314 | CE Loss: 0.0462\n",
      "Train Epoch: 022 Batch: 00022/00094 | Loss: 0.1620 | EWC Loss: 0.0312 | CE Loss: 0.1308\n",
      "Train Epoch: 022 Batch: 00023/00094 | Loss: 0.0599 | EWC Loss: 0.0309 | CE Loss: 0.0289\n",
      "Train Epoch: 022 Batch: 00024/00094 | Loss: 0.1602 | EWC Loss: 0.0308 | CE Loss: 0.1294\n",
      "Train Epoch: 022 Batch: 00025/00094 | Loss: 0.0756 | EWC Loss: 0.0311 | CE Loss: 0.0445\n",
      "Train Epoch: 022 Batch: 00026/00094 | Loss: 0.1698 | EWC Loss: 0.0311 | CE Loss: 0.1386\n",
      "Train Epoch: 022 Batch: 00027/00094 | Loss: 0.1224 | EWC Loss: 0.0310 | CE Loss: 0.0914\n",
      "Train Epoch: 022 Batch: 00028/00094 | Loss: 0.0965 | EWC Loss: 0.0309 | CE Loss: 0.0656\n",
      "Train Epoch: 022 Batch: 00029/00094 | Loss: 0.0614 | EWC Loss: 0.0308 | CE Loss: 0.0307\n",
      "Train Epoch: 022 Batch: 00030/00094 | Loss: 0.0706 | EWC Loss: 0.0307 | CE Loss: 0.0400\n",
      "Train Epoch: 022 Batch: 00031/00094 | Loss: 0.0981 | EWC Loss: 0.0306 | CE Loss: 0.0675\n",
      "Train Epoch: 022 Batch: 00032/00094 | Loss: 0.1328 | EWC Loss: 0.0304 | CE Loss: 0.1024\n",
      "Train Epoch: 022 Batch: 00033/00094 | Loss: 0.0531 | EWC Loss: 0.0304 | CE Loss: 0.0227\n",
      "Train Epoch: 022 Batch: 00034/00094 | Loss: 0.0928 | EWC Loss: 0.0303 | CE Loss: 0.0625\n",
      "Train Epoch: 022 Batch: 00035/00094 | Loss: 0.1123 | EWC Loss: 0.0303 | CE Loss: 0.0819\n",
      "Train Epoch: 022 Batch: 00036/00094 | Loss: 0.1686 | EWC Loss: 0.0305 | CE Loss: 0.1381\n",
      "Train Epoch: 022 Batch: 00037/00094 | Loss: 0.1409 | EWC Loss: 0.0306 | CE Loss: 0.1103\n",
      "Train Epoch: 022 Batch: 00038/00094 | Loss: 0.1460 | EWC Loss: 0.0305 | CE Loss: 0.1155\n",
      "Train Epoch: 022 Batch: 00039/00094 | Loss: 0.0467 | EWC Loss: 0.0306 | CE Loss: 0.0162\n",
      "Train Epoch: 022 Batch: 00040/00094 | Loss: 0.0671 | EWC Loss: 0.0304 | CE Loss: 0.0368\n",
      "Train Epoch: 022 Batch: 00041/00094 | Loss: 0.0439 | EWC Loss: 0.0304 | CE Loss: 0.0135\n",
      "Train Epoch: 022 Batch: 00042/00094 | Loss: 0.1397 | EWC Loss: 0.0303 | CE Loss: 0.1094\n",
      "Train Epoch: 022 Batch: 00043/00094 | Loss: 0.0772 | EWC Loss: 0.0301 | CE Loss: 0.0471\n",
      "Train Epoch: 022 Batch: 00044/00094 | Loss: 0.0847 | EWC Loss: 0.0299 | CE Loss: 0.0547\n",
      "Train Epoch: 022 Batch: 00045/00094 | Loss: 0.1297 | EWC Loss: 0.0299 | CE Loss: 0.0999\n",
      "Train Epoch: 022 Batch: 00046/00094 | Loss: 0.0730 | EWC Loss: 0.0302 | CE Loss: 0.0428\n",
      "Train Epoch: 022 Batch: 00047/00094 | Loss: 0.1498 | EWC Loss: 0.0301 | CE Loss: 0.1196\n",
      "Train Epoch: 022 Batch: 00048/00094 | Loss: 0.0736 | EWC Loss: 0.0302 | CE Loss: 0.0434\n",
      "Train Epoch: 022 Batch: 00049/00094 | Loss: 0.1136 | EWC Loss: 0.0302 | CE Loss: 0.0835\n",
      "Train Epoch: 022 Batch: 00050/00094 | Loss: 0.0729 | EWC Loss: 0.0301 | CE Loss: 0.0428\n",
      "Train Epoch: 022 Batch: 00051/00094 | Loss: 0.0675 | EWC Loss: 0.0302 | CE Loss: 0.0373\n",
      "Train Epoch: 022 Batch: 00052/00094 | Loss: 0.1788 | EWC Loss: 0.0301 | CE Loss: 0.1487\n",
      "Train Epoch: 022 Batch: 00053/00094 | Loss: 0.0702 | EWC Loss: 0.0303 | CE Loss: 0.0400\n",
      "Train Epoch: 022 Batch: 00054/00094 | Loss: 0.0856 | EWC Loss: 0.0303 | CE Loss: 0.0553\n",
      "Train Epoch: 022 Batch: 00055/00094 | Loss: 0.0692 | EWC Loss: 0.0303 | CE Loss: 0.0388\n",
      "Train Epoch: 022 Batch: 00056/00094 | Loss: 0.0980 | EWC Loss: 0.0302 | CE Loss: 0.0677\n",
      "Train Epoch: 022 Batch: 00057/00094 | Loss: 0.0647 | EWC Loss: 0.0305 | CE Loss: 0.0342\n",
      "Train Epoch: 022 Batch: 00058/00094 | Loss: 0.0756 | EWC Loss: 0.0305 | CE Loss: 0.0451\n",
      "Train Epoch: 022 Batch: 00059/00094 | Loss: 0.1115 | EWC Loss: 0.0304 | CE Loss: 0.0811\n",
      "Train Epoch: 022 Batch: 00060/00094 | Loss: 0.1487 | EWC Loss: 0.0305 | CE Loss: 0.1182\n",
      "Train Epoch: 022 Batch: 00061/00094 | Loss: 0.1219 | EWC Loss: 0.0306 | CE Loss: 0.0914\n",
      "Train Epoch: 022 Batch: 00062/00094 | Loss: 0.0614 | EWC Loss: 0.0305 | CE Loss: 0.0309\n",
      "Train Epoch: 022 Batch: 00063/00094 | Loss: 0.0514 | EWC Loss: 0.0306 | CE Loss: 0.0208\n",
      "Train Epoch: 022 Batch: 00064/00094 | Loss: 0.0677 | EWC Loss: 0.0304 | CE Loss: 0.0373\n",
      "Train Epoch: 022 Batch: 00065/00094 | Loss: 0.0693 | EWC Loss: 0.0304 | CE Loss: 0.0389\n",
      "Train Epoch: 022 Batch: 00066/00094 | Loss: 0.1126 | EWC Loss: 0.0303 | CE Loss: 0.0823\n",
      "Train Epoch: 022 Batch: 00067/00094 | Loss: 0.1044 | EWC Loss: 0.0302 | CE Loss: 0.0742\n",
      "Train Epoch: 022 Batch: 00068/00094 | Loss: 0.1399 | EWC Loss: 0.0303 | CE Loss: 0.1097\n",
      "Train Epoch: 022 Batch: 00069/00094 | Loss: 0.0523 | EWC Loss: 0.0304 | CE Loss: 0.0219\n",
      "Train Epoch: 022 Batch: 00070/00094 | Loss: 0.1327 | EWC Loss: 0.0303 | CE Loss: 0.1024\n",
      "Train Epoch: 022 Batch: 00071/00094 | Loss: 0.0969 | EWC Loss: 0.0304 | CE Loss: 0.0665\n",
      "Train Epoch: 022 Batch: 00072/00094 | Loss: 0.0969 | EWC Loss: 0.0302 | CE Loss: 0.0667\n",
      "Train Epoch: 022 Batch: 00073/00094 | Loss: 0.0828 | EWC Loss: 0.0303 | CE Loss: 0.0524\n",
      "Train Epoch: 022 Batch: 00074/00094 | Loss: 0.0794 | EWC Loss: 0.0302 | CE Loss: 0.0492\n",
      "Train Epoch: 022 Batch: 00075/00094 | Loss: 0.1946 | EWC Loss: 0.0303 | CE Loss: 0.1643\n",
      "Train Epoch: 022 Batch: 00076/00094 | Loss: 0.1930 | EWC Loss: 0.0308 | CE Loss: 0.1622\n",
      "Train Epoch: 022 Batch: 00077/00094 | Loss: 0.0588 | EWC Loss: 0.0308 | CE Loss: 0.0280\n",
      "Train Epoch: 022 Batch: 00078/00094 | Loss: 0.0990 | EWC Loss: 0.0306 | CE Loss: 0.0684\n",
      "Train Epoch: 022 Batch: 00079/00094 | Loss: 0.0723 | EWC Loss: 0.0309 | CE Loss: 0.0415\n",
      "Train Epoch: 022 Batch: 00080/00094 | Loss: 0.1328 | EWC Loss: 0.0307 | CE Loss: 0.1020\n",
      "Train Epoch: 022 Batch: 00081/00094 | Loss: 0.1156 | EWC Loss: 0.0310 | CE Loss: 0.0847\n",
      "Train Epoch: 022 Batch: 00082/00094 | Loss: 0.0780 | EWC Loss: 0.0310 | CE Loss: 0.0470\n",
      "Train Epoch: 022 Batch: 00083/00094 | Loss: 0.0752 | EWC Loss: 0.0308 | CE Loss: 0.0444\n",
      "Train Epoch: 022 Batch: 00084/00094 | Loss: 0.1042 | EWC Loss: 0.0306 | CE Loss: 0.0735\n",
      "Train Epoch: 022 Batch: 00085/00094 | Loss: 0.1256 | EWC Loss: 0.0304 | CE Loss: 0.0952\n",
      "Train Epoch: 022 Batch: 00086/00094 | Loss: 0.0866 | EWC Loss: 0.0306 | CE Loss: 0.0560\n",
      "Train Epoch: 022 Batch: 00087/00094 | Loss: 0.0866 | EWC Loss: 0.0306 | CE Loss: 0.0560\n",
      "Train Epoch: 022 Batch: 00088/00094 | Loss: 0.1451 | EWC Loss: 0.0303 | CE Loss: 0.1148\n",
      "Train Epoch: 022 Batch: 00089/00094 | Loss: 0.0708 | EWC Loss: 0.0305 | CE Loss: 0.0403\n",
      "Train Epoch: 022 Batch: 00090/00094 | Loss: 0.1801 | EWC Loss: 0.0303 | CE Loss: 0.1498\n",
      "Train Epoch: 022 Batch: 00091/00094 | Loss: 0.1740 | EWC Loss: 0.0305 | CE Loss: 0.1435\n",
      "Train Epoch: 022 Batch: 00092/00094 | Loss: 0.0793 | EWC Loss: 0.0306 | CE Loss: 0.0487\n",
      "Train Epoch: 022 Batch: 00093/00094 | Loss: 0.0755 | EWC Loss: 0.0305 | CE Loss: 0.0449\n",
      "Train Epoch: 022 Batch: 00094/00094 | Loss: 0.1490 | EWC Loss: 0.0305 | CE Loss: 0.1185\n",
      "Train Epoch: 022 |Acc 96.95000 | Loss: 0.10658 | Task Loss 0.07601 | EWC Loss: 0.03057\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0973 | acc:96.7500\n",
      "[VAL Acc] Target: 96.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.9300 | acc:49.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9864 | acc:52.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.0275 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.4880 | acc:61.7947\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 61.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6773 | acc:76.5250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 76.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1454 | acc:69.1223\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.3027 | acc:56.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 56.00%\n",
      "[VAL Acc] Avg 63.50%\n",
      "\n",
      "\n",
      "---------- Starting epoch 23 ----------\n",
      "Train Epoch: 023 Batch: 00001/00094 | Loss: 0.1001 | EWC Loss: 0.0305 | CE Loss: 0.0696\n",
      "Train Epoch: 023 Batch: 00002/00094 | Loss: 0.0746 | EWC Loss: 0.0307 | CE Loss: 0.0440\n",
      "Train Epoch: 023 Batch: 00003/00094 | Loss: 0.0758 | EWC Loss: 0.0304 | CE Loss: 0.0454\n",
      "Train Epoch: 023 Batch: 00004/00094 | Loss: 0.1280 | EWC Loss: 0.0304 | CE Loss: 0.0976\n",
      "Train Epoch: 023 Batch: 00005/00094 | Loss: 0.0963 | EWC Loss: 0.0301 | CE Loss: 0.0662\n",
      "Train Epoch: 023 Batch: 00006/00094 | Loss: 0.1684 | EWC Loss: 0.0300 | CE Loss: 0.1384\n",
      "Train Epoch: 023 Batch: 00007/00094 | Loss: 0.0763 | EWC Loss: 0.0300 | CE Loss: 0.0463\n",
      "Train Epoch: 023 Batch: 00008/00094 | Loss: 0.1304 | EWC Loss: 0.0300 | CE Loss: 0.1004\n",
      "Train Epoch: 023 Batch: 00009/00094 | Loss: 0.1100 | EWC Loss: 0.0299 | CE Loss: 0.0801\n",
      "Train Epoch: 023 Batch: 00010/00094 | Loss: 0.0711 | EWC Loss: 0.0300 | CE Loss: 0.0411\n",
      "Train Epoch: 023 Batch: 00011/00094 | Loss: 0.1792 | EWC Loss: 0.0300 | CE Loss: 0.1492\n",
      "Train Epoch: 023 Batch: 00012/00094 | Loss: 0.1359 | EWC Loss: 0.0300 | CE Loss: 0.1059\n",
      "Train Epoch: 023 Batch: 00013/00094 | Loss: 0.1174 | EWC Loss: 0.0301 | CE Loss: 0.0873\n",
      "Train Epoch: 023 Batch: 00014/00094 | Loss: 0.2260 | EWC Loss: 0.0300 | CE Loss: 0.1959\n",
      "Train Epoch: 023 Batch: 00015/00094 | Loss: 0.1429 | EWC Loss: 0.0302 | CE Loss: 0.1127\n",
      "Train Epoch: 023 Batch: 00016/00094 | Loss: 0.0838 | EWC Loss: 0.0302 | CE Loss: 0.0536\n",
      "Train Epoch: 023 Batch: 00017/00094 | Loss: 0.1279 | EWC Loss: 0.0300 | CE Loss: 0.0978\n",
      "Train Epoch: 023 Batch: 00018/00094 | Loss: 0.0555 | EWC Loss: 0.0300 | CE Loss: 0.0254\n",
      "Train Epoch: 023 Batch: 00019/00094 | Loss: 0.1530 | EWC Loss: 0.0300 | CE Loss: 0.1230\n",
      "Train Epoch: 023 Batch: 00020/00094 | Loss: 0.1147 | EWC Loss: 0.0302 | CE Loss: 0.0845\n",
      "Train Epoch: 023 Batch: 00021/00094 | Loss: 0.0907 | EWC Loss: 0.0301 | CE Loss: 0.0605\n",
      "Train Epoch: 023 Batch: 00022/00094 | Loss: 0.1311 | EWC Loss: 0.0300 | CE Loss: 0.1011\n",
      "Train Epoch: 023 Batch: 00023/00094 | Loss: 0.0888 | EWC Loss: 0.0301 | CE Loss: 0.0587\n",
      "Train Epoch: 023 Batch: 00024/00094 | Loss: 0.2685 | EWC Loss: 0.0300 | CE Loss: 0.2386\n",
      "Train Epoch: 023 Batch: 00025/00094 | Loss: 0.0569 | EWC Loss: 0.0299 | CE Loss: 0.0269\n",
      "Train Epoch: 023 Batch: 00026/00094 | Loss: 0.1333 | EWC Loss: 0.0299 | CE Loss: 0.1034\n",
      "Train Epoch: 023 Batch: 00027/00094 | Loss: 0.0649 | EWC Loss: 0.0299 | CE Loss: 0.0349\n",
      "Train Epoch: 023 Batch: 00028/00094 | Loss: 0.0684 | EWC Loss: 0.0300 | CE Loss: 0.0384\n",
      "Train Epoch: 023 Batch: 00029/00094 | Loss: 0.0624 | EWC Loss: 0.0300 | CE Loss: 0.0324\n",
      "Train Epoch: 023 Batch: 00030/00094 | Loss: 0.0868 | EWC Loss: 0.0301 | CE Loss: 0.0567\n",
      "Train Epoch: 023 Batch: 00031/00094 | Loss: 0.0865 | EWC Loss: 0.0302 | CE Loss: 0.0563\n",
      "Train Epoch: 023 Batch: 00032/00094 | Loss: 0.0526 | EWC Loss: 0.0301 | CE Loss: 0.0225\n",
      "Train Epoch: 023 Batch: 00033/00094 | Loss: 0.1407 | EWC Loss: 0.0301 | CE Loss: 0.1106\n",
      "Train Epoch: 023 Batch: 00034/00094 | Loss: 0.1301 | EWC Loss: 0.0302 | CE Loss: 0.0999\n",
      "Train Epoch: 023 Batch: 00035/00094 | Loss: 0.1082 | EWC Loss: 0.0301 | CE Loss: 0.0781\n",
      "Train Epoch: 023 Batch: 00036/00094 | Loss: 0.1014 | EWC Loss: 0.0301 | CE Loss: 0.0713\n",
      "Train Epoch: 023 Batch: 00037/00094 | Loss: 0.1289 | EWC Loss: 0.0303 | CE Loss: 0.0985\n",
      "Train Epoch: 023 Batch: 00038/00094 | Loss: 0.1230 | EWC Loss: 0.0304 | CE Loss: 0.0926\n",
      "Train Epoch: 023 Batch: 00039/00094 | Loss: 0.0822 | EWC Loss: 0.0301 | CE Loss: 0.0521\n",
      "Train Epoch: 023 Batch: 00040/00094 | Loss: 0.1230 | EWC Loss: 0.0300 | CE Loss: 0.0930\n",
      "Train Epoch: 023 Batch: 00041/00094 | Loss: 0.1446 | EWC Loss: 0.0299 | CE Loss: 0.1147\n",
      "Train Epoch: 023 Batch: 00042/00094 | Loss: 0.1553 | EWC Loss: 0.0299 | CE Loss: 0.1255\n",
      "Train Epoch: 023 Batch: 00043/00094 | Loss: 0.0753 | EWC Loss: 0.0301 | CE Loss: 0.0452\n",
      "Train Epoch: 023 Batch: 00044/00094 | Loss: 0.1022 | EWC Loss: 0.0300 | CE Loss: 0.0722\n",
      "Train Epoch: 023 Batch: 00045/00094 | Loss: 0.1471 | EWC Loss: 0.0302 | CE Loss: 0.1169\n",
      "Train Epoch: 023 Batch: 00046/00094 | Loss: 0.0878 | EWC Loss: 0.0303 | CE Loss: 0.0574\n",
      "Train Epoch: 023 Batch: 00047/00094 | Loss: 0.0578 | EWC Loss: 0.0304 | CE Loss: 0.0274\n",
      "Train Epoch: 023 Batch: 00048/00094 | Loss: 0.1168 | EWC Loss: 0.0303 | CE Loss: 0.0864\n",
      "Train Epoch: 023 Batch: 00049/00094 | Loss: 0.0903 | EWC Loss: 0.0305 | CE Loss: 0.0598\n",
      "Train Epoch: 023 Batch: 00050/00094 | Loss: 0.0610 | EWC Loss: 0.0303 | CE Loss: 0.0307\n",
      "Train Epoch: 023 Batch: 00051/00094 | Loss: 0.0942 | EWC Loss: 0.0301 | CE Loss: 0.0641\n",
      "Train Epoch: 023 Batch: 00052/00094 | Loss: 0.1872 | EWC Loss: 0.0298 | CE Loss: 0.1574\n",
      "Train Epoch: 023 Batch: 00053/00094 | Loss: 0.0690 | EWC Loss: 0.0298 | CE Loss: 0.0392\n",
      "Train Epoch: 023 Batch: 00054/00094 | Loss: 0.1113 | EWC Loss: 0.0298 | CE Loss: 0.0815\n",
      "Train Epoch: 023 Batch: 00055/00094 | Loss: 0.1231 | EWC Loss: 0.0297 | CE Loss: 0.0934\n",
      "Train Epoch: 023 Batch: 00056/00094 | Loss: 0.1061 | EWC Loss: 0.0297 | CE Loss: 0.0765\n",
      "Train Epoch: 023 Batch: 00057/00094 | Loss: 0.1096 | EWC Loss: 0.0298 | CE Loss: 0.0799\n",
      "Train Epoch: 023 Batch: 00058/00094 | Loss: 0.1298 | EWC Loss: 0.0298 | CE Loss: 0.1000\n",
      "Train Epoch: 023 Batch: 00059/00094 | Loss: 0.2004 | EWC Loss: 0.0298 | CE Loss: 0.1705\n",
      "Train Epoch: 023 Batch: 00060/00094 | Loss: 0.1061 | EWC Loss: 0.0301 | CE Loss: 0.0761\n",
      "Train Epoch: 023 Batch: 00061/00094 | Loss: 0.0886 | EWC Loss: 0.0303 | CE Loss: 0.0583\n",
      "Train Epoch: 023 Batch: 00062/00094 | Loss: 0.1038 | EWC Loss: 0.0302 | CE Loss: 0.0735\n",
      "Train Epoch: 023 Batch: 00063/00094 | Loss: 0.0744 | EWC Loss: 0.0303 | CE Loss: 0.0441\n",
      "Train Epoch: 023 Batch: 00064/00094 | Loss: 0.1165 | EWC Loss: 0.0303 | CE Loss: 0.0862\n",
      "Train Epoch: 023 Batch: 00065/00094 | Loss: 0.1016 | EWC Loss: 0.0302 | CE Loss: 0.0714\n",
      "Train Epoch: 023 Batch: 00066/00094 | Loss: 0.0734 | EWC Loss: 0.0302 | CE Loss: 0.0432\n",
      "Train Epoch: 023 Batch: 00067/00094 | Loss: 0.2562 | EWC Loss: 0.0302 | CE Loss: 0.2260\n",
      "Train Epoch: 023 Batch: 00068/00094 | Loss: 0.2309 | EWC Loss: 0.0307 | CE Loss: 0.2002\n",
      "Train Epoch: 023 Batch: 00069/00094 | Loss: 0.0651 | EWC Loss: 0.0309 | CE Loss: 0.0342\n",
      "Train Epoch: 023 Batch: 00070/00094 | Loss: 0.0773 | EWC Loss: 0.0307 | CE Loss: 0.0466\n",
      "Train Epoch: 023 Batch: 00071/00094 | Loss: 0.1731 | EWC Loss: 0.0307 | CE Loss: 0.1424\n",
      "Train Epoch: 023 Batch: 00072/00094 | Loss: 0.0935 | EWC Loss: 0.0310 | CE Loss: 0.0625\n",
      "Train Epoch: 023 Batch: 00073/00094 | Loss: 0.0536 | EWC Loss: 0.0307 | CE Loss: 0.0229\n",
      "Train Epoch: 023 Batch: 00074/00094 | Loss: 0.0489 | EWC Loss: 0.0305 | CE Loss: 0.0184\n",
      "Train Epoch: 023 Batch: 00075/00094 | Loss: 0.1854 | EWC Loss: 0.0303 | CE Loss: 0.1551\n",
      "Train Epoch: 023 Batch: 00076/00094 | Loss: 0.1104 | EWC Loss: 0.0306 | CE Loss: 0.0798\n",
      "Train Epoch: 023 Batch: 00077/00094 | Loss: 0.1574 | EWC Loss: 0.0306 | CE Loss: 0.1268\n",
      "Train Epoch: 023 Batch: 00078/00094 | Loss: 0.1026 | EWC Loss: 0.0306 | CE Loss: 0.0721\n",
      "Train Epoch: 023 Batch: 00079/00094 | Loss: 0.1269 | EWC Loss: 0.0306 | CE Loss: 0.0963\n",
      "Train Epoch: 023 Batch: 00080/00094 | Loss: 0.1089 | EWC Loss: 0.0306 | CE Loss: 0.0783\n",
      "Train Epoch: 023 Batch: 00081/00094 | Loss: 0.1131 | EWC Loss: 0.0308 | CE Loss: 0.0823\n",
      "Train Epoch: 023 Batch: 00082/00094 | Loss: 0.0909 | EWC Loss: 0.0307 | CE Loss: 0.0601\n",
      "Train Epoch: 023 Batch: 00083/00094 | Loss: 0.2012 | EWC Loss: 0.0310 | CE Loss: 0.1702\n",
      "Train Epoch: 023 Batch: 00084/00094 | Loss: 0.0588 | EWC Loss: 0.0311 | CE Loss: 0.0277\n",
      "Train Epoch: 023 Batch: 00085/00094 | Loss: 0.1571 | EWC Loss: 0.0310 | CE Loss: 0.1260\n",
      "Train Epoch: 023 Batch: 00086/00094 | Loss: 0.1681 | EWC Loss: 0.0310 | CE Loss: 0.1372\n",
      "Train Epoch: 023 Batch: 00087/00094 | Loss: 0.0537 | EWC Loss: 0.0311 | CE Loss: 0.0226\n",
      "Train Epoch: 023 Batch: 00088/00094 | Loss: 0.1636 | EWC Loss: 0.0309 | CE Loss: 0.1326\n",
      "Train Epoch: 023 Batch: 00089/00094 | Loss: 0.0707 | EWC Loss: 0.0310 | CE Loss: 0.0397\n",
      "Train Epoch: 023 Batch: 00090/00094 | Loss: 0.1833 | EWC Loss: 0.0308 | CE Loss: 0.1525\n",
      "Train Epoch: 023 Batch: 00091/00094 | Loss: 0.1515 | EWC Loss: 0.0308 | CE Loss: 0.1206\n",
      "Train Epoch: 023 Batch: 00092/00094 | Loss: 0.1019 | EWC Loss: 0.0306 | CE Loss: 0.0713\n",
      "Train Epoch: 023 Batch: 00093/00094 | Loss: 0.1359 | EWC Loss: 0.0305 | CE Loss: 0.1054\n",
      "Train Epoch: 023 Batch: 00094/00094 | Loss: 0.0536 | EWC Loss: 0.0305 | CE Loss: 0.0231\n",
      "Train Epoch: 023 |Acc 96.81667 | Loss: 0.11513 | Task Loss 0.08484 | EWC Loss: 0.03029\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1107 | acc:95.5500\n",
      "[VAL Acc] Target: 95.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:3.3607 | acc:48.9500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:2.1305 | acc:50.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.2631 | acc:46.9466\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.1604 | acc:56.0737\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.07%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5529 | acc:79.2976\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 79.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.5171 | acc:64.1066\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 64.11%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:0.9827 | acc:62.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 62.25%\n",
      "[VAL Acc] Avg 62.97%\n",
      "\n",
      "\n",
      "---------- Starting epoch 24 ----------\n",
      "Train Epoch: 024 Batch: 00001/00094 | Loss: 0.2215 | EWC Loss: 0.0303 | CE Loss: 0.1912\n",
      "Train Epoch: 024 Batch: 00002/00094 | Loss: 0.0590 | EWC Loss: 0.0303 | CE Loss: 0.0287\n",
      "Train Epoch: 024 Batch: 00003/00094 | Loss: 0.0709 | EWC Loss: 0.0302 | CE Loss: 0.0407\n",
      "Train Epoch: 024 Batch: 00004/00094 | Loss: 0.1192 | EWC Loss: 0.0301 | CE Loss: 0.0891\n",
      "Train Epoch: 024 Batch: 00005/00094 | Loss: 0.0751 | EWC Loss: 0.0302 | CE Loss: 0.0449\n",
      "Train Epoch: 024 Batch: 00006/00094 | Loss: 0.1100 | EWC Loss: 0.0302 | CE Loss: 0.0798\n",
      "Train Epoch: 024 Batch: 00007/00094 | Loss: 0.1365 | EWC Loss: 0.0303 | CE Loss: 0.1063\n",
      "Train Epoch: 024 Batch: 00008/00094 | Loss: 0.0845 | EWC Loss: 0.0303 | CE Loss: 0.0542\n",
      "Train Epoch: 024 Batch: 00009/00094 | Loss: 0.2207 | EWC Loss: 0.0303 | CE Loss: 0.1904\n",
      "Train Epoch: 024 Batch: 00010/00094 | Loss: 0.1079 | EWC Loss: 0.0304 | CE Loss: 0.0774\n",
      "Train Epoch: 024 Batch: 00011/00094 | Loss: 0.0826 | EWC Loss: 0.0305 | CE Loss: 0.0521\n",
      "Train Epoch: 024 Batch: 00012/00094 | Loss: 0.0478 | EWC Loss: 0.0304 | CE Loss: 0.0173\n",
      "Train Epoch: 024 Batch: 00013/00094 | Loss: 0.1320 | EWC Loss: 0.0304 | CE Loss: 0.1016\n",
      "Train Epoch: 024 Batch: 00014/00094 | Loss: 0.0924 | EWC Loss: 0.0303 | CE Loss: 0.0621\n",
      "Train Epoch: 024 Batch: 00015/00094 | Loss: 0.1857 | EWC Loss: 0.0302 | CE Loss: 0.1555\n",
      "Train Epoch: 024 Batch: 00016/00094 | Loss: 0.0752 | EWC Loss: 0.0303 | CE Loss: 0.0449\n",
      "Train Epoch: 024 Batch: 00017/00094 | Loss: 0.1201 | EWC Loss: 0.0305 | CE Loss: 0.0896\n",
      "Train Epoch: 024 Batch: 00018/00094 | Loss: 0.0770 | EWC Loss: 0.0304 | CE Loss: 0.0467\n",
      "Train Epoch: 024 Batch: 00019/00094 | Loss: 0.0727 | EWC Loss: 0.0303 | CE Loss: 0.0424\n",
      "Train Epoch: 024 Batch: 00020/00094 | Loss: 0.0859 | EWC Loss: 0.0302 | CE Loss: 0.0557\n",
      "Train Epoch: 024 Batch: 00021/00094 | Loss: 0.1643 | EWC Loss: 0.0302 | CE Loss: 0.1341\n",
      "Train Epoch: 024 Batch: 00022/00094 | Loss: 0.1128 | EWC Loss: 0.0304 | CE Loss: 0.0824\n",
      "Train Epoch: 024 Batch: 00023/00094 | Loss: 0.1541 | EWC Loss: 0.0304 | CE Loss: 0.1237\n",
      "Train Epoch: 024 Batch: 00024/00094 | Loss: 0.1249 | EWC Loss: 0.0302 | CE Loss: 0.0947\n",
      "Train Epoch: 024 Batch: 00025/00094 | Loss: 0.1799 | EWC Loss: 0.0301 | CE Loss: 0.1497\n",
      "Train Epoch: 024 Batch: 00026/00094 | Loss: 0.1102 | EWC Loss: 0.0303 | CE Loss: 0.0799\n",
      "Train Epoch: 024 Batch: 00027/00094 | Loss: 0.1250 | EWC Loss: 0.0302 | CE Loss: 0.0949\n",
      "Train Epoch: 024 Batch: 00028/00094 | Loss: 0.1968 | EWC Loss: 0.0301 | CE Loss: 0.1666\n",
      "Train Epoch: 024 Batch: 00029/00094 | Loss: 0.0952 | EWC Loss: 0.0301 | CE Loss: 0.0651\n",
      "Train Epoch: 024 Batch: 00030/00094 | Loss: 0.1078 | EWC Loss: 0.0299 | CE Loss: 0.0779\n",
      "Train Epoch: 024 Batch: 00031/00094 | Loss: 0.0541 | EWC Loss: 0.0298 | CE Loss: 0.0243\n",
      "Train Epoch: 024 Batch: 00032/00094 | Loss: 0.0493 | EWC Loss: 0.0297 | CE Loss: 0.0196\n",
      "Train Epoch: 024 Batch: 00033/00094 | Loss: 0.0689 | EWC Loss: 0.0297 | CE Loss: 0.0393\n",
      "Train Epoch: 024 Batch: 00034/00094 | Loss: 0.0628 | EWC Loss: 0.0296 | CE Loss: 0.0332\n",
      "Train Epoch: 024 Batch: 00035/00094 | Loss: 0.0943 | EWC Loss: 0.0296 | CE Loss: 0.0647\n",
      "Train Epoch: 024 Batch: 00036/00094 | Loss: 0.0496 | EWC Loss: 0.0296 | CE Loss: 0.0201\n",
      "Train Epoch: 024 Batch: 00037/00094 | Loss: 0.0926 | EWC Loss: 0.0295 | CE Loss: 0.0631\n",
      "Train Epoch: 024 Batch: 00038/00094 | Loss: 0.0957 | EWC Loss: 0.0294 | CE Loss: 0.0663\n",
      "Train Epoch: 024 Batch: 00039/00094 | Loss: 0.0949 | EWC Loss: 0.0296 | CE Loss: 0.0654\n",
      "Train Epoch: 024 Batch: 00040/00094 | Loss: 0.0677 | EWC Loss: 0.0295 | CE Loss: 0.0381\n",
      "Train Epoch: 024 Batch: 00041/00094 | Loss: 0.0482 | EWC Loss: 0.0295 | CE Loss: 0.0187\n",
      "Train Epoch: 024 Batch: 00042/00094 | Loss: 0.0794 | EWC Loss: 0.0295 | CE Loss: 0.0499\n",
      "Train Epoch: 024 Batch: 00043/00094 | Loss: 0.0440 | EWC Loss: 0.0295 | CE Loss: 0.0145\n",
      "Train Epoch: 024 Batch: 00044/00094 | Loss: 0.0503 | EWC Loss: 0.0295 | CE Loss: 0.0208\n",
      "Train Epoch: 024 Batch: 00045/00094 | Loss: 0.0752 | EWC Loss: 0.0294 | CE Loss: 0.0458\n",
      "Train Epoch: 024 Batch: 00046/00094 | Loss: 0.0703 | EWC Loss: 0.0293 | CE Loss: 0.0410\n",
      "Train Epoch: 024 Batch: 00047/00094 | Loss: 0.1208 | EWC Loss: 0.0293 | CE Loss: 0.0914\n",
      "Train Epoch: 024 Batch: 00048/00094 | Loss: 0.1685 | EWC Loss: 0.0293 | CE Loss: 0.1392\n",
      "Train Epoch: 024 Batch: 00049/00094 | Loss: 0.0993 | EWC Loss: 0.0294 | CE Loss: 0.0699\n",
      "Train Epoch: 024 Batch: 00050/00094 | Loss: 0.1998 | EWC Loss: 0.0294 | CE Loss: 0.1705\n",
      "Train Epoch: 024 Batch: 00051/00094 | Loss: 0.0680 | EWC Loss: 0.0297 | CE Loss: 0.0383\n",
      "Train Epoch: 024 Batch: 00052/00094 | Loss: 0.0633 | EWC Loss: 0.0296 | CE Loss: 0.0337\n",
      "Train Epoch: 024 Batch: 00053/00094 | Loss: 0.0774 | EWC Loss: 0.0296 | CE Loss: 0.0479\n",
      "Train Epoch: 024 Batch: 00054/00094 | Loss: 0.1206 | EWC Loss: 0.0295 | CE Loss: 0.0911\n",
      "Train Epoch: 024 Batch: 00055/00094 | Loss: 0.1074 | EWC Loss: 0.0296 | CE Loss: 0.0778\n",
      "Train Epoch: 024 Batch: 00056/00094 | Loss: 0.0538 | EWC Loss: 0.0296 | CE Loss: 0.0242\n",
      "Train Epoch: 024 Batch: 00057/00094 | Loss: 0.0580 | EWC Loss: 0.0295 | CE Loss: 0.0285\n",
      "Train Epoch: 024 Batch: 00058/00094 | Loss: 0.1110 | EWC Loss: 0.0294 | CE Loss: 0.0816\n",
      "Train Epoch: 024 Batch: 00059/00094 | Loss: 0.0506 | EWC Loss: 0.0294 | CE Loss: 0.0213\n",
      "Train Epoch: 024 Batch: 00060/00094 | Loss: 0.0475 | EWC Loss: 0.0294 | CE Loss: 0.0181\n",
      "Train Epoch: 024 Batch: 00061/00094 | Loss: 0.1319 | EWC Loss: 0.0293 | CE Loss: 0.1026\n",
      "Train Epoch: 024 Batch: 00062/00094 | Loss: 0.0562 | EWC Loss: 0.0293 | CE Loss: 0.0269\n",
      "Train Epoch: 024 Batch: 00063/00094 | Loss: 0.0952 | EWC Loss: 0.0293 | CE Loss: 0.0660\n",
      "Train Epoch: 024 Batch: 00064/00094 | Loss: 0.1220 | EWC Loss: 0.0294 | CE Loss: 0.0926\n",
      "Train Epoch: 024 Batch: 00065/00094 | Loss: 0.0832 | EWC Loss: 0.0294 | CE Loss: 0.0537\n",
      "Train Epoch: 024 Batch: 00066/00094 | Loss: 0.0661 | EWC Loss: 0.0295 | CE Loss: 0.0366\n",
      "Train Epoch: 024 Batch: 00067/00094 | Loss: 0.1021 | EWC Loss: 0.0295 | CE Loss: 0.0727\n",
      "Train Epoch: 024 Batch: 00068/00094 | Loss: 0.0510 | EWC Loss: 0.0294 | CE Loss: 0.0216\n",
      "Train Epoch: 024 Batch: 00069/00094 | Loss: 0.0821 | EWC Loss: 0.0294 | CE Loss: 0.0528\n",
      "Train Epoch: 024 Batch: 00070/00094 | Loss: 0.1461 | EWC Loss: 0.0294 | CE Loss: 0.1168\n",
      "Train Epoch: 024 Batch: 00071/00094 | Loss: 0.0685 | EWC Loss: 0.0296 | CE Loss: 0.0389\n",
      "Train Epoch: 024 Batch: 00072/00094 | Loss: 0.1556 | EWC Loss: 0.0296 | CE Loss: 0.1260\n",
      "Train Epoch: 024 Batch: 00073/00094 | Loss: 0.0807 | EWC Loss: 0.0294 | CE Loss: 0.0513\n",
      "Train Epoch: 024 Batch: 00074/00094 | Loss: 0.0595 | EWC Loss: 0.0294 | CE Loss: 0.0301\n",
      "Train Epoch: 024 Batch: 00075/00094 | Loss: 0.1023 | EWC Loss: 0.0294 | CE Loss: 0.0729\n",
      "Train Epoch: 024 Batch: 00076/00094 | Loss: 0.0632 | EWC Loss: 0.0294 | CE Loss: 0.0338\n",
      "Train Epoch: 024 Batch: 00077/00094 | Loss: 0.0817 | EWC Loss: 0.0293 | CE Loss: 0.0525\n",
      "Train Epoch: 024 Batch: 00078/00094 | Loss: 0.0937 | EWC Loss: 0.0292 | CE Loss: 0.0645\n",
      "Train Epoch: 024 Batch: 00079/00094 | Loss: 0.0811 | EWC Loss: 0.0292 | CE Loss: 0.0519\n",
      "Train Epoch: 024 Batch: 00080/00094 | Loss: 0.0505 | EWC Loss: 0.0292 | CE Loss: 0.0213\n",
      "Train Epoch: 024 Batch: 00081/00094 | Loss: 0.0399 | EWC Loss: 0.0292 | CE Loss: 0.0107\n",
      "Train Epoch: 024 Batch: 00082/00094 | Loss: 0.1090 | EWC Loss: 0.0291 | CE Loss: 0.0799\n",
      "Train Epoch: 024 Batch: 00083/00094 | Loss: 0.0985 | EWC Loss: 0.0291 | CE Loss: 0.0694\n",
      "Train Epoch: 024 Batch: 00084/00094 | Loss: 0.0865 | EWC Loss: 0.0292 | CE Loss: 0.0573\n",
      "Train Epoch: 024 Batch: 00085/00094 | Loss: 0.1317 | EWC Loss: 0.0292 | CE Loss: 0.1025\n",
      "Train Epoch: 024 Batch: 00086/00094 | Loss: 0.0662 | EWC Loss: 0.0292 | CE Loss: 0.0369\n",
      "Train Epoch: 024 Batch: 00087/00094 | Loss: 0.0476 | EWC Loss: 0.0292 | CE Loss: 0.0184\n",
      "Train Epoch: 024 Batch: 00088/00094 | Loss: 0.0826 | EWC Loss: 0.0291 | CE Loss: 0.0535\n",
      "Train Epoch: 024 Batch: 00089/00094 | Loss: 0.0731 | EWC Loss: 0.0291 | CE Loss: 0.0440\n",
      "Train Epoch: 024 Batch: 00090/00094 | Loss: 0.1394 | EWC Loss: 0.0291 | CE Loss: 0.1102\n",
      "Train Epoch: 024 Batch: 00091/00094 | Loss: 0.0748 | EWC Loss: 0.0294 | CE Loss: 0.0454\n",
      "Train Epoch: 024 Batch: 00092/00094 | Loss: 0.1000 | EWC Loss: 0.0293 | CE Loss: 0.0708\n",
      "Train Epoch: 024 Batch: 00093/00094 | Loss: 0.1270 | EWC Loss: 0.0293 | CE Loss: 0.0977\n",
      "Train Epoch: 024 Batch: 00094/00094 | Loss: 0.1962 | EWC Loss: 0.0292 | CE Loss: 0.1670\n",
      "Train Epoch: 024 |Acc 97.36667 | Loss: 0.09719 | Task Loss 0.06752 | EWC Loss: 0.02968\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0992 | acc:96.4500\n",
      "[VAL Acc] Target: 96.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.9173 | acc:49.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9088 | acc:51.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9776 | acc:45.6107\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.8014 | acc:56.5439\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.9795 | acc:67.0055\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 67.01%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.2210 | acc:66.8103\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.81%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1580 | acc:60.0625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.06%\n",
      "[VAL Acc] Avg 61.59%\n",
      "\n",
      "\n",
      "---------- Starting epoch 25 ----------\n",
      "Train Epoch: 025 Batch: 00001/00094 | Loss: 0.1218 | EWC Loss: 0.0292 | CE Loss: 0.0925\n",
      "Train Epoch: 025 Batch: 00002/00094 | Loss: 0.0465 | EWC Loss: 0.0293 | CE Loss: 0.0172\n",
      "Train Epoch: 025 Batch: 00003/00094 | Loss: 0.1647 | EWC Loss: 0.0292 | CE Loss: 0.1355\n",
      "Train Epoch: 025 Batch: 00004/00094 | Loss: 0.1101 | EWC Loss: 0.0291 | CE Loss: 0.0809\n",
      "Train Epoch: 025 Batch: 00005/00094 | Loss: 0.0487 | EWC Loss: 0.0292 | CE Loss: 0.0195\n",
      "Train Epoch: 025 Batch: 00006/00094 | Loss: 0.1179 | EWC Loss: 0.0291 | CE Loss: 0.0888\n",
      "Train Epoch: 025 Batch: 00007/00094 | Loss: 0.0626 | EWC Loss: 0.0291 | CE Loss: 0.0335\n",
      "Train Epoch: 025 Batch: 00008/00094 | Loss: 0.0600 | EWC Loss: 0.0291 | CE Loss: 0.0309\n",
      "Train Epoch: 025 Batch: 00009/00094 | Loss: 0.0867 | EWC Loss: 0.0291 | CE Loss: 0.0576\n",
      "Train Epoch: 025 Batch: 00010/00094 | Loss: 0.1110 | EWC Loss: 0.0291 | CE Loss: 0.0819\n",
      "Train Epoch: 025 Batch: 00011/00094 | Loss: 0.1005 | EWC Loss: 0.0291 | CE Loss: 0.0714\n",
      "Train Epoch: 025 Batch: 00012/00094 | Loss: 0.0992 | EWC Loss: 0.0291 | CE Loss: 0.0701\n",
      "Train Epoch: 025 Batch: 00013/00094 | Loss: 0.1778 | EWC Loss: 0.0291 | CE Loss: 0.1488\n",
      "Train Epoch: 025 Batch: 00014/00094 | Loss: 0.0971 | EWC Loss: 0.0290 | CE Loss: 0.0682\n",
      "Train Epoch: 025 Batch: 00015/00094 | Loss: 0.0636 | EWC Loss: 0.0291 | CE Loss: 0.0345\n",
      "Train Epoch: 025 Batch: 00016/00094 | Loss: 0.0528 | EWC Loss: 0.0291 | CE Loss: 0.0237\n",
      "Train Epoch: 025 Batch: 00017/00094 | Loss: 0.1292 | EWC Loss: 0.0290 | CE Loss: 0.1002\n",
      "Train Epoch: 025 Batch: 00018/00094 | Loss: 0.1160 | EWC Loss: 0.0291 | CE Loss: 0.0870\n",
      "Train Epoch: 025 Batch: 00019/00094 | Loss: 0.1145 | EWC Loss: 0.0291 | CE Loss: 0.0854\n",
      "Train Epoch: 025 Batch: 00020/00094 | Loss: 0.0951 | EWC Loss: 0.0291 | CE Loss: 0.0660\n",
      "Train Epoch: 025 Batch: 00021/00094 | Loss: 0.1365 | EWC Loss: 0.0291 | CE Loss: 0.1074\n",
      "Train Epoch: 025 Batch: 00022/00094 | Loss: 0.0591 | EWC Loss: 0.0291 | CE Loss: 0.0301\n",
      "Train Epoch: 025 Batch: 00023/00094 | Loss: 0.2005 | EWC Loss: 0.0290 | CE Loss: 0.1716\n",
      "Train Epoch: 025 Batch: 00024/00094 | Loss: 0.1415 | EWC Loss: 0.0291 | CE Loss: 0.1124\n",
      "Train Epoch: 025 Batch: 00025/00094 | Loss: 0.2001 | EWC Loss: 0.0291 | CE Loss: 0.1710\n",
      "Train Epoch: 025 Batch: 00026/00094 | Loss: 0.0818 | EWC Loss: 0.0290 | CE Loss: 0.0528\n",
      "Train Epoch: 025 Batch: 00027/00094 | Loss: 0.0909 | EWC Loss: 0.0289 | CE Loss: 0.0620\n",
      "Train Epoch: 025 Batch: 00028/00094 | Loss: 0.1422 | EWC Loss: 0.0289 | CE Loss: 0.1133\n",
      "Train Epoch: 025 Batch: 00029/00094 | Loss: 0.1189 | EWC Loss: 0.0290 | CE Loss: 0.0899\n",
      "Train Epoch: 025 Batch: 00030/00094 | Loss: 0.0833 | EWC Loss: 0.0291 | CE Loss: 0.0543\n",
      "Train Epoch: 025 Batch: 00031/00094 | Loss: 0.0746 | EWC Loss: 0.0290 | CE Loss: 0.0456\n",
      "Train Epoch: 025 Batch: 00032/00094 | Loss: 0.0837 | EWC Loss: 0.0289 | CE Loss: 0.0548\n",
      "Train Epoch: 025 Batch: 00033/00094 | Loss: 0.0829 | EWC Loss: 0.0288 | CE Loss: 0.0541\n",
      "Train Epoch: 025 Batch: 00034/00094 | Loss: 0.0604 | EWC Loss: 0.0288 | CE Loss: 0.0316\n",
      "Train Epoch: 025 Batch: 00035/00094 | Loss: 0.0889 | EWC Loss: 0.0288 | CE Loss: 0.0601\n",
      "Train Epoch: 025 Batch: 00036/00094 | Loss: 0.0676 | EWC Loss: 0.0288 | CE Loss: 0.0389\n",
      "Train Epoch: 025 Batch: 00037/00094 | Loss: 0.0778 | EWC Loss: 0.0287 | CE Loss: 0.0491\n",
      "Train Epoch: 025 Batch: 00038/00094 | Loss: 0.0836 | EWC Loss: 0.0288 | CE Loss: 0.0548\n",
      "Train Epoch: 025 Batch: 00039/00094 | Loss: 0.0711 | EWC Loss: 0.0289 | CE Loss: 0.0422\n",
      "Train Epoch: 025 Batch: 00040/00094 | Loss: 0.0984 | EWC Loss: 0.0288 | CE Loss: 0.0696\n",
      "Train Epoch: 025 Batch: 00041/00094 | Loss: 0.0765 | EWC Loss: 0.0288 | CE Loss: 0.0477\n",
      "Train Epoch: 025 Batch: 00042/00094 | Loss: 0.0628 | EWC Loss: 0.0288 | CE Loss: 0.0340\n",
      "Train Epoch: 025 Batch: 00043/00094 | Loss: 0.1503 | EWC Loss: 0.0288 | CE Loss: 0.1215\n",
      "Train Epoch: 025 Batch: 00044/00094 | Loss: 0.0810 | EWC Loss: 0.0288 | CE Loss: 0.0522\n",
      "Train Epoch: 025 Batch: 00045/00094 | Loss: 0.1084 | EWC Loss: 0.0288 | CE Loss: 0.0796\n",
      "Train Epoch: 025 Batch: 00046/00094 | Loss: 0.1740 | EWC Loss: 0.0288 | CE Loss: 0.1452\n",
      "Train Epoch: 025 Batch: 00047/00094 | Loss: 0.1958 | EWC Loss: 0.0289 | CE Loss: 0.1669\n",
      "Train Epoch: 025 Batch: 00048/00094 | Loss: 0.1042 | EWC Loss: 0.0290 | CE Loss: 0.0753\n",
      "Train Epoch: 025 Batch: 00049/00094 | Loss: 0.0952 | EWC Loss: 0.0289 | CE Loss: 0.0663\n",
      "Train Epoch: 025 Batch: 00050/00094 | Loss: 0.1673 | EWC Loss: 0.0289 | CE Loss: 0.1384\n",
      "Train Epoch: 025 Batch: 00051/00094 | Loss: 0.0697 | EWC Loss: 0.0290 | CE Loss: 0.0407\n",
      "Train Epoch: 025 Batch: 00052/00094 | Loss: 0.0568 | EWC Loss: 0.0290 | CE Loss: 0.0279\n",
      "Train Epoch: 025 Batch: 00053/00094 | Loss: 0.1106 | EWC Loss: 0.0289 | CE Loss: 0.0817\n",
      "Train Epoch: 025 Batch: 00054/00094 | Loss: 0.0657 | EWC Loss: 0.0288 | CE Loss: 0.0369\n",
      "Train Epoch: 025 Batch: 00055/00094 | Loss: 0.0742 | EWC Loss: 0.0287 | CE Loss: 0.0454\n",
      "Train Epoch: 025 Batch: 00056/00094 | Loss: 0.1276 | EWC Loss: 0.0287 | CE Loss: 0.0989\n",
      "Train Epoch: 025 Batch: 00057/00094 | Loss: 0.1116 | EWC Loss: 0.0290 | CE Loss: 0.0826\n",
      "Train Epoch: 025 Batch: 00058/00094 | Loss: 0.1293 | EWC Loss: 0.0291 | CE Loss: 0.1002\n",
      "Train Epoch: 025 Batch: 00059/00094 | Loss: 0.0890 | EWC Loss: 0.0288 | CE Loss: 0.0602\n",
      "Train Epoch: 025 Batch: 00060/00094 | Loss: 0.1484 | EWC Loss: 0.0287 | CE Loss: 0.1197\n",
      "Train Epoch: 025 Batch: 00061/00094 | Loss: 0.0871 | EWC Loss: 0.0288 | CE Loss: 0.0583\n",
      "Train Epoch: 025 Batch: 00062/00094 | Loss: 0.0697 | EWC Loss: 0.0288 | CE Loss: 0.0409\n",
      "Train Epoch: 025 Batch: 00063/00094 | Loss: 0.0477 | EWC Loss: 0.0288 | CE Loss: 0.0189\n",
      "Train Epoch: 025 Batch: 00064/00094 | Loss: 0.0553 | EWC Loss: 0.0288 | CE Loss: 0.0266\n",
      "Train Epoch: 025 Batch: 00065/00094 | Loss: 0.0529 | EWC Loss: 0.0287 | CE Loss: 0.0242\n",
      "Train Epoch: 025 Batch: 00066/00094 | Loss: 0.0744 | EWC Loss: 0.0287 | CE Loss: 0.0457\n",
      "Train Epoch: 025 Batch: 00067/00094 | Loss: 0.0762 | EWC Loss: 0.0286 | CE Loss: 0.0476\n",
      "Train Epoch: 025 Batch: 00068/00094 | Loss: 0.0694 | EWC Loss: 0.0285 | CE Loss: 0.0409\n",
      "Train Epoch: 025 Batch: 00069/00094 | Loss: 0.0520 | EWC Loss: 0.0285 | CE Loss: 0.0235\n",
      "Train Epoch: 025 Batch: 00070/00094 | Loss: 0.0563 | EWC Loss: 0.0285 | CE Loss: 0.0278\n",
      "Train Epoch: 025 Batch: 00071/00094 | Loss: 0.1676 | EWC Loss: 0.0285 | CE Loss: 0.1391\n",
      "Train Epoch: 025 Batch: 00072/00094 | Loss: 0.0692 | EWC Loss: 0.0286 | CE Loss: 0.0405\n",
      "Train Epoch: 025 Batch: 00073/00094 | Loss: 0.1175 | EWC Loss: 0.0286 | CE Loss: 0.0889\n",
      "Train Epoch: 025 Batch: 00074/00094 | Loss: 0.0737 | EWC Loss: 0.0287 | CE Loss: 0.0450\n",
      "Train Epoch: 025 Batch: 00075/00094 | Loss: 0.1198 | EWC Loss: 0.0287 | CE Loss: 0.0911\n",
      "Train Epoch: 025 Batch: 00076/00094 | Loss: 0.0674 | EWC Loss: 0.0286 | CE Loss: 0.0389\n",
      "Train Epoch: 025 Batch: 00077/00094 | Loss: 0.0464 | EWC Loss: 0.0285 | CE Loss: 0.0178\n",
      "Train Epoch: 025 Batch: 00078/00094 | Loss: 0.0825 | EWC Loss: 0.0285 | CE Loss: 0.0540\n",
      "Train Epoch: 025 Batch: 00079/00094 | Loss: 0.1366 | EWC Loss: 0.0285 | CE Loss: 0.1081\n",
      "Train Epoch: 025 Batch: 00080/00094 | Loss: 0.1041 | EWC Loss: 0.0286 | CE Loss: 0.0755\n",
      "Train Epoch: 025 Batch: 00081/00094 | Loss: 0.1301 | EWC Loss: 0.0286 | CE Loss: 0.1015\n",
      "Train Epoch: 025 Batch: 00082/00094 | Loss: 0.0499 | EWC Loss: 0.0288 | CE Loss: 0.0211\n",
      "Train Epoch: 025 Batch: 00083/00094 | Loss: 0.1021 | EWC Loss: 0.0288 | CE Loss: 0.0732\n",
      "Train Epoch: 025 Batch: 00084/00094 | Loss: 0.1318 | EWC Loss: 0.0289 | CE Loss: 0.1029\n",
      "Train Epoch: 025 Batch: 00085/00094 | Loss: 0.0785 | EWC Loss: 0.0288 | CE Loss: 0.0497\n",
      "Train Epoch: 025 Batch: 00086/00094 | Loss: 0.0860 | EWC Loss: 0.0288 | CE Loss: 0.0572\n",
      "Train Epoch: 025 Batch: 00087/00094 | Loss: 0.0698 | EWC Loss: 0.0288 | CE Loss: 0.0411\n",
      "Train Epoch: 025 Batch: 00088/00094 | Loss: 0.0754 | EWC Loss: 0.0287 | CE Loss: 0.0467\n",
      "Train Epoch: 025 Batch: 00089/00094 | Loss: 0.0725 | EWC Loss: 0.0287 | CE Loss: 0.0438\n",
      "Train Epoch: 025 Batch: 00090/00094 | Loss: 0.1479 | EWC Loss: 0.0287 | CE Loss: 0.1191\n",
      "Train Epoch: 025 Batch: 00091/00094 | Loss: 0.0581 | EWC Loss: 0.0288 | CE Loss: 0.0293\n",
      "Train Epoch: 025 Batch: 00092/00094 | Loss: 0.0686 | EWC Loss: 0.0288 | CE Loss: 0.0398\n",
      "Train Epoch: 025 Batch: 00093/00094 | Loss: 0.0463 | EWC Loss: 0.0287 | CE Loss: 0.0176\n",
      "Train Epoch: 025 Batch: 00094/00094 | Loss: 0.1377 | EWC Loss: 0.0287 | CE Loss: 0.1091\n",
      "Train Epoch: 025 |Acc 97.46667 | Loss: 0.09679 | Task Loss 0.06793 | EWC Loss: 0.02886\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0715 | acc:97.4000\n",
      "[VAL Acc] Target: 97.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8456 | acc:48.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8465 | acc:51.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9563 | acc:43.8931\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6411 | acc:57.9154\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.8581 | acc:70.3327\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 70.33%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0767 | acc:68.6129\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1528 | acc:60.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.56%\n",
      "[VAL Acc] Avg 62.32%\n",
      "\n",
      "\n",
      "---------- Starting epoch 26 ----------\n",
      "Train Epoch: 026 Batch: 00001/00094 | Loss: 0.0985 | EWC Loss: 0.0288 | CE Loss: 0.0697\n",
      "Train Epoch: 026 Batch: 00002/00094 | Loss: 0.1269 | EWC Loss: 0.0288 | CE Loss: 0.0981\n",
      "Train Epoch: 026 Batch: 00003/00094 | Loss: 0.0483 | EWC Loss: 0.0288 | CE Loss: 0.0195\n",
      "Train Epoch: 026 Batch: 00004/00094 | Loss: 0.1600 | EWC Loss: 0.0287 | CE Loss: 0.1313\n",
      "Train Epoch: 026 Batch: 00005/00094 | Loss: 0.1927 | EWC Loss: 0.0288 | CE Loss: 0.1639\n",
      "Train Epoch: 026 Batch: 00006/00094 | Loss: 0.0923 | EWC Loss: 0.0288 | CE Loss: 0.0635\n",
      "Train Epoch: 026 Batch: 00007/00094 | Loss: 0.0996 | EWC Loss: 0.0288 | CE Loss: 0.0708\n",
      "Train Epoch: 026 Batch: 00008/00094 | Loss: 0.0710 | EWC Loss: 0.0287 | CE Loss: 0.0423\n",
      "Train Epoch: 026 Batch: 00009/00094 | Loss: 0.0635 | EWC Loss: 0.0286 | CE Loss: 0.0349\n",
      "Train Epoch: 026 Batch: 00010/00094 | Loss: 0.1555 | EWC Loss: 0.0286 | CE Loss: 0.1269\n",
      "Train Epoch: 026 Batch: 00011/00094 | Loss: 0.0545 | EWC Loss: 0.0286 | CE Loss: 0.0259\n",
      "Train Epoch: 026 Batch: 00012/00094 | Loss: 0.0608 | EWC Loss: 0.0286 | CE Loss: 0.0323\n",
      "Train Epoch: 026 Batch: 00013/00094 | Loss: 0.0828 | EWC Loss: 0.0286 | CE Loss: 0.0542\n",
      "Train Epoch: 026 Batch: 00014/00094 | Loss: 0.0631 | EWC Loss: 0.0285 | CE Loss: 0.0346\n",
      "Train Epoch: 026 Batch: 00015/00094 | Loss: 0.0604 | EWC Loss: 0.0285 | CE Loss: 0.0318\n",
      "Train Epoch: 026 Batch: 00016/00094 | Loss: 0.0694 | EWC Loss: 0.0285 | CE Loss: 0.0409\n",
      "Train Epoch: 026 Batch: 00017/00094 | Loss: 0.0707 | EWC Loss: 0.0285 | CE Loss: 0.0422\n",
      "Train Epoch: 026 Batch: 00018/00094 | Loss: 0.1054 | EWC Loss: 0.0285 | CE Loss: 0.0769\n",
      "Train Epoch: 026 Batch: 00019/00094 | Loss: 0.1132 | EWC Loss: 0.0286 | CE Loss: 0.0846\n",
      "Train Epoch: 026 Batch: 00020/00094 | Loss: 0.1699 | EWC Loss: 0.0286 | CE Loss: 0.1413\n",
      "Train Epoch: 026 Batch: 00021/00094 | Loss: 0.1106 | EWC Loss: 0.0286 | CE Loss: 0.0819\n",
      "Train Epoch: 026 Batch: 00022/00094 | Loss: 0.2524 | EWC Loss: 0.0287 | CE Loss: 0.2237\n",
      "Train Epoch: 026 Batch: 00023/00094 | Loss: 0.0459 | EWC Loss: 0.0287 | CE Loss: 0.0172\n",
      "Train Epoch: 026 Batch: 00024/00094 | Loss: 0.0688 | EWC Loss: 0.0287 | CE Loss: 0.0401\n",
      "Train Epoch: 026 Batch: 00025/00094 | Loss: 0.0475 | EWC Loss: 0.0286 | CE Loss: 0.0188\n",
      "Train Epoch: 026 Batch: 00026/00094 | Loss: 0.0711 | EWC Loss: 0.0286 | CE Loss: 0.0425\n",
      "Train Epoch: 026 Batch: 00027/00094 | Loss: 0.0430 | EWC Loss: 0.0286 | CE Loss: 0.0144\n",
      "Train Epoch: 026 Batch: 00028/00094 | Loss: 0.0569 | EWC Loss: 0.0286 | CE Loss: 0.0282\n",
      "Train Epoch: 026 Batch: 00029/00094 | Loss: 0.1191 | EWC Loss: 0.0286 | CE Loss: 0.0904\n",
      "Train Epoch: 026 Batch: 00030/00094 | Loss: 0.0524 | EWC Loss: 0.0286 | CE Loss: 0.0238\n",
      "Train Epoch: 026 Batch: 00031/00094 | Loss: 0.0974 | EWC Loss: 0.0286 | CE Loss: 0.0688\n",
      "Train Epoch: 026 Batch: 00032/00094 | Loss: 0.1328 | EWC Loss: 0.0286 | CE Loss: 0.1042\n",
      "Train Epoch: 026 Batch: 00033/00094 | Loss: 0.2029 | EWC Loss: 0.0286 | CE Loss: 0.1743\n",
      "Train Epoch: 026 Batch: 00034/00094 | Loss: 0.1363 | EWC Loss: 0.0286 | CE Loss: 0.1078\n",
      "Train Epoch: 026 Batch: 00035/00094 | Loss: 0.0645 | EWC Loss: 0.0286 | CE Loss: 0.0359\n",
      "Train Epoch: 026 Batch: 00036/00094 | Loss: 0.1146 | EWC Loss: 0.0285 | CE Loss: 0.0860\n",
      "Train Epoch: 026 Batch: 00037/00094 | Loss: 0.0783 | EWC Loss: 0.0285 | CE Loss: 0.0498\n",
      "Train Epoch: 026 Batch: 00038/00094 | Loss: 0.1316 | EWC Loss: 0.0285 | CE Loss: 0.1031\n",
      "Train Epoch: 026 Batch: 00039/00094 | Loss: 0.1131 | EWC Loss: 0.0286 | CE Loss: 0.0845\n",
      "Train Epoch: 026 Batch: 00040/00094 | Loss: 0.0966 | EWC Loss: 0.0287 | CE Loss: 0.0679\n",
      "Train Epoch: 026 Batch: 00041/00094 | Loss: 0.0525 | EWC Loss: 0.0287 | CE Loss: 0.0238\n",
      "Train Epoch: 026 Batch: 00042/00094 | Loss: 0.0825 | EWC Loss: 0.0287 | CE Loss: 0.0538\n",
      "Train Epoch: 026 Batch: 00043/00094 | Loss: 0.0866 | EWC Loss: 0.0287 | CE Loss: 0.0579\n",
      "Train Epoch: 026 Batch: 00044/00094 | Loss: 0.0578 | EWC Loss: 0.0287 | CE Loss: 0.0291\n",
      "Train Epoch: 026 Batch: 00045/00094 | Loss: 0.0469 | EWC Loss: 0.0287 | CE Loss: 0.0182\n",
      "Train Epoch: 026 Batch: 00046/00094 | Loss: 0.1055 | EWC Loss: 0.0287 | CE Loss: 0.0768\n",
      "Train Epoch: 026 Batch: 00047/00094 | Loss: 0.0735 | EWC Loss: 0.0287 | CE Loss: 0.0448\n",
      "Train Epoch: 026 Batch: 00048/00094 | Loss: 0.0825 | EWC Loss: 0.0287 | CE Loss: 0.0538\n",
      "Train Epoch: 026 Batch: 00049/00094 | Loss: 0.0726 | EWC Loss: 0.0286 | CE Loss: 0.0440\n",
      "Train Epoch: 026 Batch: 00050/00094 | Loss: 0.0583 | EWC Loss: 0.0286 | CE Loss: 0.0297\n",
      "Train Epoch: 026 Batch: 00051/00094 | Loss: 0.0815 | EWC Loss: 0.0286 | CE Loss: 0.0529\n",
      "Train Epoch: 026 Batch: 00052/00094 | Loss: 0.0796 | EWC Loss: 0.0286 | CE Loss: 0.0510\n",
      "Train Epoch: 026 Batch: 00053/00094 | Loss: 0.0449 | EWC Loss: 0.0286 | CE Loss: 0.0163\n",
      "Train Epoch: 026 Batch: 00054/00094 | Loss: 0.1069 | EWC Loss: 0.0286 | CE Loss: 0.0783\n",
      "Train Epoch: 026 Batch: 00055/00094 | Loss: 0.0877 | EWC Loss: 0.0286 | CE Loss: 0.0591\n",
      "Train Epoch: 026 Batch: 00056/00094 | Loss: 0.1121 | EWC Loss: 0.0286 | CE Loss: 0.0835\n",
      "Train Epoch: 026 Batch: 00057/00094 | Loss: 0.1578 | EWC Loss: 0.0286 | CE Loss: 0.1292\n",
      "Train Epoch: 026 Batch: 00058/00094 | Loss: 0.1350 | EWC Loss: 0.0285 | CE Loss: 0.1064\n",
      "Train Epoch: 026 Batch: 00059/00094 | Loss: 0.0686 | EWC Loss: 0.0285 | CE Loss: 0.0401\n",
      "Train Epoch: 026 Batch: 00060/00094 | Loss: 0.0644 | EWC Loss: 0.0285 | CE Loss: 0.0359\n",
      "Train Epoch: 026 Batch: 00061/00094 | Loss: 0.1304 | EWC Loss: 0.0285 | CE Loss: 0.1020\n",
      "Train Epoch: 026 Batch: 00062/00094 | Loss: 0.0474 | EWC Loss: 0.0284 | CE Loss: 0.0190\n",
      "Train Epoch: 026 Batch: 00063/00094 | Loss: 0.1713 | EWC Loss: 0.0284 | CE Loss: 0.1429\n",
      "Train Epoch: 026 Batch: 00064/00094 | Loss: 0.0670 | EWC Loss: 0.0284 | CE Loss: 0.0386\n",
      "Train Epoch: 026 Batch: 00065/00094 | Loss: 0.1804 | EWC Loss: 0.0284 | CE Loss: 0.1521\n",
      "Train Epoch: 026 Batch: 00066/00094 | Loss: 0.0460 | EWC Loss: 0.0283 | CE Loss: 0.0177\n",
      "Train Epoch: 026 Batch: 00067/00094 | Loss: 0.0767 | EWC Loss: 0.0283 | CE Loss: 0.0483\n",
      "Train Epoch: 026 Batch: 00068/00094 | Loss: 0.0906 | EWC Loss: 0.0283 | CE Loss: 0.0623\n",
      "Train Epoch: 026 Batch: 00069/00094 | Loss: 0.0989 | EWC Loss: 0.0283 | CE Loss: 0.0706\n",
      "Train Epoch: 026 Batch: 00070/00094 | Loss: 0.1284 | EWC Loss: 0.0283 | CE Loss: 0.1001\n",
      "Train Epoch: 026 Batch: 00071/00094 | Loss: 0.0466 | EWC Loss: 0.0283 | CE Loss: 0.0183\n",
      "Train Epoch: 026 Batch: 00072/00094 | Loss: 0.1470 | EWC Loss: 0.0283 | CE Loss: 0.1187\n",
      "Train Epoch: 026 Batch: 00073/00094 | Loss: 0.0550 | EWC Loss: 0.0283 | CE Loss: 0.0267\n",
      "Train Epoch: 026 Batch: 00074/00094 | Loss: 0.0997 | EWC Loss: 0.0283 | CE Loss: 0.0714\n",
      "Train Epoch: 026 Batch: 00075/00094 | Loss: 0.0522 | EWC Loss: 0.0284 | CE Loss: 0.0238\n",
      "Train Epoch: 026 Batch: 00076/00094 | Loss: 0.1088 | EWC Loss: 0.0284 | CE Loss: 0.0805\n",
      "Train Epoch: 026 Batch: 00077/00094 | Loss: 0.0636 | EWC Loss: 0.0284 | CE Loss: 0.0352\n",
      "Train Epoch: 026 Batch: 00078/00094 | Loss: 0.0840 | EWC Loss: 0.0284 | CE Loss: 0.0557\n",
      "Train Epoch: 026 Batch: 00079/00094 | Loss: 0.0650 | EWC Loss: 0.0284 | CE Loss: 0.0366\n",
      "Train Epoch: 026 Batch: 00080/00094 | Loss: 0.0556 | EWC Loss: 0.0283 | CE Loss: 0.0273\n",
      "Train Epoch: 026 Batch: 00081/00094 | Loss: 0.1131 | EWC Loss: 0.0283 | CE Loss: 0.0848\n",
      "Train Epoch: 026 Batch: 00082/00094 | Loss: 0.0491 | EWC Loss: 0.0283 | CE Loss: 0.0208\n",
      "Train Epoch: 026 Batch: 00083/00094 | Loss: 0.1794 | EWC Loss: 0.0283 | CE Loss: 0.1512\n",
      "Train Epoch: 026 Batch: 00084/00094 | Loss: 0.0698 | EWC Loss: 0.0285 | CE Loss: 0.0413\n",
      "Train Epoch: 026 Batch: 00085/00094 | Loss: 0.0980 | EWC Loss: 0.0285 | CE Loss: 0.0696\n",
      "Train Epoch: 026 Batch: 00086/00094 | Loss: 0.0439 | EWC Loss: 0.0285 | CE Loss: 0.0155\n",
      "Train Epoch: 026 Batch: 00087/00094 | Loss: 0.1127 | EWC Loss: 0.0284 | CE Loss: 0.0843\n",
      "Train Epoch: 026 Batch: 00088/00094 | Loss: 0.0923 | EWC Loss: 0.0284 | CE Loss: 0.0639\n",
      "Train Epoch: 026 Batch: 00089/00094 | Loss: 0.2456 | EWC Loss: 0.0284 | CE Loss: 0.2173\n",
      "Train Epoch: 026 Batch: 00090/00094 | Loss: 0.1816 | EWC Loss: 0.0285 | CE Loss: 0.1531\n",
      "Train Epoch: 026 Batch: 00091/00094 | Loss: 0.0869 | EWC Loss: 0.0285 | CE Loss: 0.0584\n",
      "Train Epoch: 026 Batch: 00092/00094 | Loss: 0.1359 | EWC Loss: 0.0285 | CE Loss: 0.1074\n",
      "Train Epoch: 026 Batch: 00093/00094 | Loss: 0.0842 | EWC Loss: 0.0285 | CE Loss: 0.0557\n",
      "Train Epoch: 026 Batch: 00094/00094 | Loss: 0.1309 | EWC Loss: 0.0284 | CE Loss: 0.1025\n",
      "Train Epoch: 026 |Acc 97.60000 | Loss: 0.09670 | Task Loss 0.06816 | EWC Loss: 0.02854\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0648 | acc:97.4000\n",
      "[VAL Acc] Target: 97.40%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7326 | acc:48.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8740 | acc:52.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8846 | acc:45.6107\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6793 | acc:57.2884\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 57.29%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5029 | acc:79.6673\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 79.67%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0799 | acc:67.9467\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.95%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0910 | acc:59.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.00%\n",
      "[VAL Acc] Avg 63.46%\n",
      "\n",
      "\n",
      "---------- Starting epoch 27 ----------\n",
      "Train Epoch: 027 Batch: 00001/00094 | Loss: 0.0617 | EWC Loss: 0.0284 | CE Loss: 0.0333\n",
      "Train Epoch: 027 Batch: 00002/00094 | Loss: 0.0551 | EWC Loss: 0.0284 | CE Loss: 0.0267\n",
      "Train Epoch: 027 Batch: 00003/00094 | Loss: 0.0811 | EWC Loss: 0.0284 | CE Loss: 0.0527\n",
      "Train Epoch: 027 Batch: 00004/00094 | Loss: 0.0691 | EWC Loss: 0.0284 | CE Loss: 0.0407\n",
      "Train Epoch: 027 Batch: 00005/00094 | Loss: 0.0614 | EWC Loss: 0.0284 | CE Loss: 0.0330\n",
      "Train Epoch: 027 Batch: 00006/00094 | Loss: 0.1405 | EWC Loss: 0.0284 | CE Loss: 0.1121\n",
      "Train Epoch: 027 Batch: 00007/00094 | Loss: 0.1099 | EWC Loss: 0.0283 | CE Loss: 0.0815\n",
      "Train Epoch: 027 Batch: 00008/00094 | Loss: 0.0964 | EWC Loss: 0.0284 | CE Loss: 0.0680\n",
      "Train Epoch: 027 Batch: 00009/00094 | Loss: 0.0867 | EWC Loss: 0.0284 | CE Loss: 0.0583\n",
      "Train Epoch: 027 Batch: 00010/00094 | Loss: 0.0643 | EWC Loss: 0.0284 | CE Loss: 0.0359\n",
      "Train Epoch: 027 Batch: 00011/00094 | Loss: 0.0849 | EWC Loss: 0.0284 | CE Loss: 0.0565\n",
      "Train Epoch: 027 Batch: 00012/00094 | Loss: 0.1143 | EWC Loss: 0.0284 | CE Loss: 0.0859\n",
      "Train Epoch: 027 Batch: 00013/00094 | Loss: 0.1099 | EWC Loss: 0.0283 | CE Loss: 0.0815\n",
      "Train Epoch: 027 Batch: 00014/00094 | Loss: 0.0467 | EWC Loss: 0.0283 | CE Loss: 0.0184\n",
      "Train Epoch: 027 Batch: 00015/00094 | Loss: 0.0849 | EWC Loss: 0.0283 | CE Loss: 0.0566\n",
      "Train Epoch: 027 Batch: 00016/00094 | Loss: 0.1153 | EWC Loss: 0.0283 | CE Loss: 0.0869\n",
      "Train Epoch: 027 Batch: 00017/00094 | Loss: 0.1366 | EWC Loss: 0.0283 | CE Loss: 0.1083\n",
      "Train Epoch: 027 Batch: 00018/00094 | Loss: 0.0730 | EWC Loss: 0.0283 | CE Loss: 0.0447\n",
      "Train Epoch: 027 Batch: 00019/00094 | Loss: 0.0756 | EWC Loss: 0.0283 | CE Loss: 0.0473\n",
      "Train Epoch: 027 Batch: 00020/00094 | Loss: 0.0602 | EWC Loss: 0.0283 | CE Loss: 0.0319\n",
      "Train Epoch: 027 Batch: 00021/00094 | Loss: 0.0723 | EWC Loss: 0.0283 | CE Loss: 0.0440\n",
      "Train Epoch: 027 Batch: 00022/00094 | Loss: 0.0834 | EWC Loss: 0.0283 | CE Loss: 0.0551\n",
      "Train Epoch: 027 Batch: 00023/00094 | Loss: 0.0559 | EWC Loss: 0.0283 | CE Loss: 0.0276\n",
      "Train Epoch: 027 Batch: 00024/00094 | Loss: 0.0517 | EWC Loss: 0.0283 | CE Loss: 0.0234\n",
      "Train Epoch: 027 Batch: 00025/00094 | Loss: 0.1210 | EWC Loss: 0.0283 | CE Loss: 0.0927\n",
      "Train Epoch: 027 Batch: 00026/00094 | Loss: 0.1211 | EWC Loss: 0.0283 | CE Loss: 0.0929\n",
      "Train Epoch: 027 Batch: 00027/00094 | Loss: 0.1877 | EWC Loss: 0.0283 | CE Loss: 0.1594\n",
      "Train Epoch: 027 Batch: 00028/00094 | Loss: 0.0818 | EWC Loss: 0.0283 | CE Loss: 0.0536\n",
      "Train Epoch: 027 Batch: 00029/00094 | Loss: 0.0623 | EWC Loss: 0.0283 | CE Loss: 0.0341\n",
      "Train Epoch: 027 Batch: 00030/00094 | Loss: 0.0891 | EWC Loss: 0.0282 | CE Loss: 0.0609\n",
      "Train Epoch: 027 Batch: 00031/00094 | Loss: 0.0639 | EWC Loss: 0.0282 | CE Loss: 0.0356\n",
      "Train Epoch: 027 Batch: 00032/00094 | Loss: 0.0564 | EWC Loss: 0.0282 | CE Loss: 0.0282\n",
      "Train Epoch: 027 Batch: 00033/00094 | Loss: 0.1187 | EWC Loss: 0.0282 | CE Loss: 0.0905\n",
      "Train Epoch: 027 Batch: 00034/00094 | Loss: 0.1560 | EWC Loss: 0.0282 | CE Loss: 0.1278\n",
      "Train Epoch: 027 Batch: 00035/00094 | Loss: 0.0864 | EWC Loss: 0.0282 | CE Loss: 0.0583\n",
      "Train Epoch: 027 Batch: 00036/00094 | Loss: 0.1562 | EWC Loss: 0.0282 | CE Loss: 0.1281\n",
      "Train Epoch: 027 Batch: 00037/00094 | Loss: 0.0556 | EWC Loss: 0.0282 | CE Loss: 0.0275\n",
      "Train Epoch: 027 Batch: 00038/00094 | Loss: 0.1906 | EWC Loss: 0.0281 | CE Loss: 0.1625\n",
      "Train Epoch: 027 Batch: 00039/00094 | Loss: 0.0785 | EWC Loss: 0.0282 | CE Loss: 0.0503\n",
      "Train Epoch: 027 Batch: 00040/00094 | Loss: 0.0615 | EWC Loss: 0.0282 | CE Loss: 0.0333\n",
      "Train Epoch: 027 Batch: 00041/00094 | Loss: 0.1917 | EWC Loss: 0.0282 | CE Loss: 0.1635\n",
      "Train Epoch: 027 Batch: 00042/00094 | Loss: 0.1706 | EWC Loss: 0.0281 | CE Loss: 0.1424\n",
      "Train Epoch: 027 Batch: 00043/00094 | Loss: 0.0588 | EWC Loss: 0.0281 | CE Loss: 0.0307\n",
      "Train Epoch: 027 Batch: 00044/00094 | Loss: 0.1291 | EWC Loss: 0.0281 | CE Loss: 0.1010\n",
      "Train Epoch: 027 Batch: 00045/00094 | Loss: 0.0645 | EWC Loss: 0.0282 | CE Loss: 0.0363\n",
      "Train Epoch: 027 Batch: 00046/00094 | Loss: 0.0867 | EWC Loss: 0.0282 | CE Loss: 0.0586\n",
      "Train Epoch: 027 Batch: 00047/00094 | Loss: 0.1503 | EWC Loss: 0.0282 | CE Loss: 0.1221\n",
      "Train Epoch: 027 Batch: 00048/00094 | Loss: 0.0466 | EWC Loss: 0.0282 | CE Loss: 0.0184\n",
      "Train Epoch: 027 Batch: 00049/00094 | Loss: 0.0612 | EWC Loss: 0.0282 | CE Loss: 0.0329\n",
      "Train Epoch: 027 Batch: 00050/00094 | Loss: 0.1172 | EWC Loss: 0.0282 | CE Loss: 0.0890\n",
      "Train Epoch: 027 Batch: 00051/00094 | Loss: 0.0638 | EWC Loss: 0.0282 | CE Loss: 0.0356\n",
      "Train Epoch: 027 Batch: 00052/00094 | Loss: 0.0634 | EWC Loss: 0.0282 | CE Loss: 0.0352\n",
      "Train Epoch: 027 Batch: 00053/00094 | Loss: 0.0988 | EWC Loss: 0.0282 | CE Loss: 0.0706\n",
      "Train Epoch: 027 Batch: 00054/00094 | Loss: 0.0619 | EWC Loss: 0.0282 | CE Loss: 0.0338\n",
      "Train Epoch: 027 Batch: 00055/00094 | Loss: 0.0571 | EWC Loss: 0.0282 | CE Loss: 0.0289\n",
      "Train Epoch: 027 Batch: 00056/00094 | Loss: 0.1012 | EWC Loss: 0.0282 | CE Loss: 0.0730\n",
      "Train Epoch: 027 Batch: 00057/00094 | Loss: 0.0964 | EWC Loss: 0.0282 | CE Loss: 0.0682\n",
      "Train Epoch: 027 Batch: 00058/00094 | Loss: 0.0993 | EWC Loss: 0.0281 | CE Loss: 0.0712\n",
      "Train Epoch: 027 Batch: 00059/00094 | Loss: 0.0500 | EWC Loss: 0.0281 | CE Loss: 0.0219\n",
      "Train Epoch: 027 Batch: 00060/00094 | Loss: 0.1416 | EWC Loss: 0.0281 | CE Loss: 0.1135\n",
      "Train Epoch: 027 Batch: 00061/00094 | Loss: 0.1271 | EWC Loss: 0.0281 | CE Loss: 0.0989\n",
      "Train Epoch: 027 Batch: 00062/00094 | Loss: 0.0412 | EWC Loss: 0.0281 | CE Loss: 0.0131\n",
      "Train Epoch: 027 Batch: 00063/00094 | Loss: 0.0646 | EWC Loss: 0.0281 | CE Loss: 0.0365\n",
      "Train Epoch: 027 Batch: 00064/00094 | Loss: 0.0738 | EWC Loss: 0.0281 | CE Loss: 0.0457\n",
      "Train Epoch: 027 Batch: 00065/00094 | Loss: 0.0814 | EWC Loss: 0.0281 | CE Loss: 0.0533\n",
      "Train Epoch: 027 Batch: 00066/00094 | Loss: 0.0993 | EWC Loss: 0.0281 | CE Loss: 0.0712\n",
      "Train Epoch: 027 Batch: 00067/00094 | Loss: 0.0680 | EWC Loss: 0.0281 | CE Loss: 0.0399\n",
      "Train Epoch: 027 Batch: 00068/00094 | Loss: 0.0840 | EWC Loss: 0.0281 | CE Loss: 0.0559\n",
      "Train Epoch: 027 Batch: 00069/00094 | Loss: 0.0525 | EWC Loss: 0.0281 | CE Loss: 0.0244\n",
      "Train Epoch: 027 Batch: 00070/00094 | Loss: 0.0852 | EWC Loss: 0.0281 | CE Loss: 0.0572\n",
      "Train Epoch: 027 Batch: 00071/00094 | Loss: 0.0643 | EWC Loss: 0.0281 | CE Loss: 0.0363\n",
      "Train Epoch: 027 Batch: 00072/00094 | Loss: 0.0804 | EWC Loss: 0.0280 | CE Loss: 0.0524\n",
      "Train Epoch: 027 Batch: 00073/00094 | Loss: 0.1518 | EWC Loss: 0.0281 | CE Loss: 0.1237\n",
      "Train Epoch: 027 Batch: 00074/00094 | Loss: 0.0546 | EWC Loss: 0.0281 | CE Loss: 0.0266\n",
      "Train Epoch: 027 Batch: 00075/00094 | Loss: 0.0550 | EWC Loss: 0.0280 | CE Loss: 0.0270\n",
      "Train Epoch: 027 Batch: 00076/00094 | Loss: 0.0497 | EWC Loss: 0.0280 | CE Loss: 0.0217\n",
      "Train Epoch: 027 Batch: 00077/00094 | Loss: 0.0867 | EWC Loss: 0.0280 | CE Loss: 0.0587\n",
      "Train Epoch: 027 Batch: 00078/00094 | Loss: 0.1032 | EWC Loss: 0.0280 | CE Loss: 0.0752\n",
      "Train Epoch: 027 Batch: 00079/00094 | Loss: 0.1705 | EWC Loss: 0.0281 | CE Loss: 0.1425\n",
      "Train Epoch: 027 Batch: 00080/00094 | Loss: 0.1323 | EWC Loss: 0.0280 | CE Loss: 0.1042\n",
      "Train Epoch: 027 Batch: 00081/00094 | Loss: 0.1568 | EWC Loss: 0.0281 | CE Loss: 0.1287\n",
      "Train Epoch: 027 Batch: 00082/00094 | Loss: 0.1365 | EWC Loss: 0.0282 | CE Loss: 0.1083\n",
      "Train Epoch: 027 Batch: 00083/00094 | Loss: 0.0618 | EWC Loss: 0.0282 | CE Loss: 0.0336\n",
      "Train Epoch: 027 Batch: 00084/00094 | Loss: 0.0469 | EWC Loss: 0.0282 | CE Loss: 0.0188\n",
      "Train Epoch: 027 Batch: 00085/00094 | Loss: 0.0858 | EWC Loss: 0.0282 | CE Loss: 0.0577\n",
      "Train Epoch: 027 Batch: 00086/00094 | Loss: 0.0475 | EWC Loss: 0.0282 | CE Loss: 0.0194\n",
      "Train Epoch: 027 Batch: 00087/00094 | Loss: 0.0625 | EWC Loss: 0.0281 | CE Loss: 0.0343\n",
      "Train Epoch: 027 Batch: 00088/00094 | Loss: 0.0539 | EWC Loss: 0.0281 | CE Loss: 0.0257\n",
      "Train Epoch: 027 Batch: 00089/00094 | Loss: 0.1237 | EWC Loss: 0.0281 | CE Loss: 0.0956\n",
      "Train Epoch: 027 Batch: 00090/00094 | Loss: 0.1131 | EWC Loss: 0.0282 | CE Loss: 0.0849\n",
      "Train Epoch: 027 Batch: 00091/00094 | Loss: 0.0589 | EWC Loss: 0.0282 | CE Loss: 0.0307\n",
      "Train Epoch: 027 Batch: 00092/00094 | Loss: 0.0783 | EWC Loss: 0.0281 | CE Loss: 0.0502\n",
      "Train Epoch: 027 Batch: 00093/00094 | Loss: 0.0905 | EWC Loss: 0.0282 | CE Loss: 0.0623\n",
      "Train Epoch: 027 Batch: 00094/00094 | Loss: 0.0960 | EWC Loss: 0.0282 | CE Loss: 0.0678\n",
      "Train Epoch: 027 |Acc 97.78333 | Loss: 0.09070 | Task Loss 0.06250 | EWC Loss: 0.02820\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0722 | acc:97.0500\n",
      "[VAL Acc] Target: 97.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.7756 | acc:48.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.50%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8854 | acc:54.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 54.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8940 | acc:45.6107\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6191 | acc:59.1693\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.17%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.6737 | acc:75.5083\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.51%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0157 | acc:70.0235\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 70.02%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1120 | acc:60.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.50%\n",
      "[VAL Acc] Avg 63.87%\n",
      "\n",
      "\n",
      "---------- Starting epoch 28 ----------\n",
      "Train Epoch: 028 Batch: 00001/00094 | Loss: 0.4109 | EWC Loss: 0.0282 | CE Loss: 0.3827\n",
      "Train Epoch: 028 Batch: 00002/00094 | Loss: 0.1099 | EWC Loss: 0.0281 | CE Loss: 0.0818\n",
      "Train Epoch: 028 Batch: 00003/00094 | Loss: 0.0476 | EWC Loss: 0.0281 | CE Loss: 0.0195\n",
      "Train Epoch: 028 Batch: 00004/00094 | Loss: 0.0964 | EWC Loss: 0.0281 | CE Loss: 0.0683\n",
      "Train Epoch: 028 Batch: 00005/00094 | Loss: 0.1037 | EWC Loss: 0.0281 | CE Loss: 0.0756\n",
      "Train Epoch: 028 Batch: 00006/00094 | Loss: 0.0640 | EWC Loss: 0.0281 | CE Loss: 0.0359\n",
      "Train Epoch: 028 Batch: 00007/00094 | Loss: 0.0654 | EWC Loss: 0.0281 | CE Loss: 0.0373\n",
      "Train Epoch: 028 Batch: 00008/00094 | Loss: 0.0443 | EWC Loss: 0.0281 | CE Loss: 0.0162\n",
      "Train Epoch: 028 Batch: 00009/00094 | Loss: 0.1873 | EWC Loss: 0.0281 | CE Loss: 0.1592\n",
      "Train Epoch: 028 Batch: 00010/00094 | Loss: 0.0519 | EWC Loss: 0.0280 | CE Loss: 0.0239\n",
      "Train Epoch: 028 Batch: 00011/00094 | Loss: 0.1537 | EWC Loss: 0.0280 | CE Loss: 0.1256\n",
      "Train Epoch: 028 Batch: 00012/00094 | Loss: 0.0873 | EWC Loss: 0.0280 | CE Loss: 0.0593\n",
      "Train Epoch: 028 Batch: 00013/00094 | Loss: 0.0787 | EWC Loss: 0.0281 | CE Loss: 0.0507\n",
      "Train Epoch: 028 Batch: 00014/00094 | Loss: 0.0900 | EWC Loss: 0.0281 | CE Loss: 0.0620\n",
      "Train Epoch: 028 Batch: 00015/00094 | Loss: 0.0899 | EWC Loss: 0.0281 | CE Loss: 0.0618\n",
      "Train Epoch: 028 Batch: 00016/00094 | Loss: 0.0572 | EWC Loss: 0.0281 | CE Loss: 0.0291\n",
      "Train Epoch: 028 Batch: 00017/00094 | Loss: 0.0951 | EWC Loss: 0.0280 | CE Loss: 0.0670\n",
      "Train Epoch: 028 Batch: 00018/00094 | Loss: 0.0874 | EWC Loss: 0.0280 | CE Loss: 0.0594\n",
      "Train Epoch: 028 Batch: 00019/00094 | Loss: 0.0690 | EWC Loss: 0.0280 | CE Loss: 0.0409\n",
      "Train Epoch: 028 Batch: 00020/00094 | Loss: 0.0778 | EWC Loss: 0.0280 | CE Loss: 0.0497\n",
      "Train Epoch: 028 Batch: 00021/00094 | Loss: 0.0636 | EWC Loss: 0.0280 | CE Loss: 0.0356\n",
      "Train Epoch: 028 Batch: 00022/00094 | Loss: 0.1009 | EWC Loss: 0.0280 | CE Loss: 0.0728\n",
      "Train Epoch: 028 Batch: 00023/00094 | Loss: 0.1243 | EWC Loss: 0.0281 | CE Loss: 0.0962\n",
      "Train Epoch: 028 Batch: 00024/00094 | Loss: 0.1207 | EWC Loss: 0.0281 | CE Loss: 0.0926\n",
      "Train Epoch: 028 Batch: 00025/00094 | Loss: 0.0939 | EWC Loss: 0.0281 | CE Loss: 0.0659\n",
      "Train Epoch: 028 Batch: 00026/00094 | Loss: 0.0432 | EWC Loss: 0.0281 | CE Loss: 0.0152\n",
      "Train Epoch: 028 Batch: 00027/00094 | Loss: 0.0664 | EWC Loss: 0.0280 | CE Loss: 0.0383\n",
      "Train Epoch: 028 Batch: 00028/00094 | Loss: 0.0887 | EWC Loss: 0.0280 | CE Loss: 0.0607\n",
      "Train Epoch: 028 Batch: 00029/00094 | Loss: 0.1008 | EWC Loss: 0.0281 | CE Loss: 0.0727\n",
      "Train Epoch: 028 Batch: 00030/00094 | Loss: 0.0553 | EWC Loss: 0.0280 | CE Loss: 0.0273\n",
      "Train Epoch: 028 Batch: 00031/00094 | Loss: 0.0651 | EWC Loss: 0.0281 | CE Loss: 0.0370\n",
      "Train Epoch: 028 Batch: 00032/00094 | Loss: 0.0578 | EWC Loss: 0.0281 | CE Loss: 0.0298\n",
      "Train Epoch: 028 Batch: 00033/00094 | Loss: 0.1151 | EWC Loss: 0.0281 | CE Loss: 0.0870\n",
      "Train Epoch: 028 Batch: 00034/00094 | Loss: 0.0517 | EWC Loss: 0.0281 | CE Loss: 0.0237\n",
      "Train Epoch: 028 Batch: 00035/00094 | Loss: 0.1090 | EWC Loss: 0.0281 | CE Loss: 0.0809\n",
      "Train Epoch: 028 Batch: 00036/00094 | Loss: 0.1097 | EWC Loss: 0.0281 | CE Loss: 0.0817\n",
      "Train Epoch: 028 Batch: 00037/00094 | Loss: 0.0676 | EWC Loss: 0.0281 | CE Loss: 0.0395\n",
      "Train Epoch: 028 Batch: 00038/00094 | Loss: 0.0849 | EWC Loss: 0.0280 | CE Loss: 0.0569\n",
      "Train Epoch: 028 Batch: 00039/00094 | Loss: 0.0463 | EWC Loss: 0.0280 | CE Loss: 0.0183\n",
      "Train Epoch: 028 Batch: 00040/00094 | Loss: 0.0794 | EWC Loss: 0.0280 | CE Loss: 0.0513\n",
      "Train Epoch: 028 Batch: 00041/00094 | Loss: 0.1024 | EWC Loss: 0.0280 | CE Loss: 0.0743\n",
      "Train Epoch: 028 Batch: 00042/00094 | Loss: 0.0834 | EWC Loss: 0.0280 | CE Loss: 0.0554\n",
      "Train Epoch: 028 Batch: 00043/00094 | Loss: 0.1369 | EWC Loss: 0.0280 | CE Loss: 0.1088\n",
      "Train Epoch: 028 Batch: 00044/00094 | Loss: 0.0643 | EWC Loss: 0.0280 | CE Loss: 0.0362\n",
      "Train Epoch: 028 Batch: 00045/00094 | Loss: 0.1022 | EWC Loss: 0.0280 | CE Loss: 0.0742\n",
      "Train Epoch: 028 Batch: 00046/00094 | Loss: 0.0771 | EWC Loss: 0.0280 | CE Loss: 0.0490\n",
      "Train Epoch: 028 Batch: 00047/00094 | Loss: 0.0717 | EWC Loss: 0.0280 | CE Loss: 0.0437\n",
      "Train Epoch: 028 Batch: 00048/00094 | Loss: 0.0681 | EWC Loss: 0.0280 | CE Loss: 0.0401\n",
      "Train Epoch: 028 Batch: 00049/00094 | Loss: 0.1088 | EWC Loss: 0.0280 | CE Loss: 0.0808\n",
      "Train Epoch: 028 Batch: 00050/00094 | Loss: 0.0582 | EWC Loss: 0.0280 | CE Loss: 0.0302\n",
      "Train Epoch: 028 Batch: 00051/00094 | Loss: 0.0823 | EWC Loss: 0.0280 | CE Loss: 0.0543\n",
      "Train Epoch: 028 Batch: 00052/00094 | Loss: 0.3434 | EWC Loss: 0.0280 | CE Loss: 0.3154\n",
      "Train Epoch: 028 Batch: 00053/00094 | Loss: 0.0599 | EWC Loss: 0.0280 | CE Loss: 0.0319\n",
      "Train Epoch: 028 Batch: 00054/00094 | Loss: 0.0744 | EWC Loss: 0.0280 | CE Loss: 0.0464\n",
      "Train Epoch: 028 Batch: 00055/00094 | Loss: 0.0574 | EWC Loss: 0.0280 | CE Loss: 0.0294\n",
      "Train Epoch: 028 Batch: 00056/00094 | Loss: 0.0526 | EWC Loss: 0.0280 | CE Loss: 0.0246\n",
      "Train Epoch: 028 Batch: 00057/00094 | Loss: 0.0741 | EWC Loss: 0.0280 | CE Loss: 0.0461\n",
      "Train Epoch: 028 Batch: 00058/00094 | Loss: 0.0689 | EWC Loss: 0.0280 | CE Loss: 0.0409\n",
      "Train Epoch: 028 Batch: 00059/00094 | Loss: 0.1079 | EWC Loss: 0.0280 | CE Loss: 0.0799\n",
      "Train Epoch: 028 Batch: 00060/00094 | Loss: 0.0670 | EWC Loss: 0.0280 | CE Loss: 0.0389\n",
      "Train Epoch: 028 Batch: 00061/00094 | Loss: 0.0513 | EWC Loss: 0.0280 | CE Loss: 0.0232\n",
      "Train Epoch: 028 Batch: 00062/00094 | Loss: 0.0685 | EWC Loss: 0.0280 | CE Loss: 0.0405\n",
      "Train Epoch: 028 Batch: 00063/00094 | Loss: 0.0990 | EWC Loss: 0.0280 | CE Loss: 0.0710\n",
      "Train Epoch: 028 Batch: 00064/00094 | Loss: 0.0480 | EWC Loss: 0.0280 | CE Loss: 0.0200\n",
      "Train Epoch: 028 Batch: 00065/00094 | Loss: 0.1222 | EWC Loss: 0.0280 | CE Loss: 0.0942\n",
      "Train Epoch: 028 Batch: 00066/00094 | Loss: 0.0596 | EWC Loss: 0.0280 | CE Loss: 0.0316\n",
      "Train Epoch: 028 Batch: 00067/00094 | Loss: 0.1187 | EWC Loss: 0.0280 | CE Loss: 0.0907\n",
      "Train Epoch: 028 Batch: 00068/00094 | Loss: 0.0946 | EWC Loss: 0.0280 | CE Loss: 0.0666\n",
      "Train Epoch: 028 Batch: 00069/00094 | Loss: 0.0653 | EWC Loss: 0.0280 | CE Loss: 0.0373\n",
      "Train Epoch: 028 Batch: 00070/00094 | Loss: 0.0708 | EWC Loss: 0.0280 | CE Loss: 0.0428\n",
      "Train Epoch: 028 Batch: 00071/00094 | Loss: 0.0679 | EWC Loss: 0.0280 | CE Loss: 0.0399\n",
      "Train Epoch: 028 Batch: 00072/00094 | Loss: 0.1466 | EWC Loss: 0.0280 | CE Loss: 0.1186\n",
      "Train Epoch: 028 Batch: 00073/00094 | Loss: 0.0566 | EWC Loss: 0.0280 | CE Loss: 0.0286\n",
      "Train Epoch: 028 Batch: 00074/00094 | Loss: 0.0919 | EWC Loss: 0.0280 | CE Loss: 0.0639\n",
      "Train Epoch: 028 Batch: 00075/00094 | Loss: 0.1584 | EWC Loss: 0.0280 | CE Loss: 0.1304\n",
      "Train Epoch: 028 Batch: 00076/00094 | Loss: 0.0634 | EWC Loss: 0.0280 | CE Loss: 0.0354\n",
      "Train Epoch: 028 Batch: 00077/00094 | Loss: 0.0535 | EWC Loss: 0.0280 | CE Loss: 0.0254\n",
      "Train Epoch: 028 Batch: 00078/00094 | Loss: 0.0929 | EWC Loss: 0.0280 | CE Loss: 0.0649\n",
      "Train Epoch: 028 Batch: 00079/00094 | Loss: 0.0511 | EWC Loss: 0.0280 | CE Loss: 0.0231\n",
      "Train Epoch: 028 Batch: 00080/00094 | Loss: 0.1037 | EWC Loss: 0.0280 | CE Loss: 0.0757\n",
      "Train Epoch: 028 Batch: 00081/00094 | Loss: 0.1134 | EWC Loss: 0.0280 | CE Loss: 0.0854\n",
      "Train Epoch: 028 Batch: 00082/00094 | Loss: 0.0991 | EWC Loss: 0.0280 | CE Loss: 0.0711\n",
      "Train Epoch: 028 Batch: 00083/00094 | Loss: 0.0891 | EWC Loss: 0.0280 | CE Loss: 0.0610\n",
      "Train Epoch: 028 Batch: 00084/00094 | Loss: 0.0842 | EWC Loss: 0.0280 | CE Loss: 0.0561\n",
      "Train Epoch: 028 Batch: 00085/00094 | Loss: 0.0820 | EWC Loss: 0.0280 | CE Loss: 0.0539\n",
      "Train Epoch: 028 Batch: 00086/00094 | Loss: 0.0679 | EWC Loss: 0.0280 | CE Loss: 0.0399\n",
      "Train Epoch: 028 Batch: 00087/00094 | Loss: 0.0920 | EWC Loss: 0.0280 | CE Loss: 0.0640\n",
      "Train Epoch: 028 Batch: 00088/00094 | Loss: 0.1054 | EWC Loss: 0.0280 | CE Loss: 0.0774\n",
      "Train Epoch: 028 Batch: 00089/00094 | Loss: 0.1222 | EWC Loss: 0.0280 | CE Loss: 0.0942\n",
      "Train Epoch: 028 Batch: 00090/00094 | Loss: 0.0803 | EWC Loss: 0.0280 | CE Loss: 0.0523\n",
      "Train Epoch: 028 Batch: 00091/00094 | Loss: 0.1276 | EWC Loss: 0.0280 | CE Loss: 0.0996\n",
      "Train Epoch: 028 Batch: 00092/00094 | Loss: 0.0496 | EWC Loss: 0.0280 | CE Loss: 0.0216\n",
      "Train Epoch: 028 Batch: 00093/00094 | Loss: 0.1610 | EWC Loss: 0.0280 | CE Loss: 0.1329\n",
      "Train Epoch: 028 Batch: 00094/00094 | Loss: 0.1603 | EWC Loss: 0.0280 | CE Loss: 0.1323\n",
      "Train Epoch: 028 |Acc 97.66667 | Loss: 0.09245 | Task Loss 0.06442 | EWC Loss: 0.02804\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0601 | acc:97.9000\n",
      "[VAL Acc] Target: 97.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.9203 | acc:48.6500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9210 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.0412 | acc:45.8015\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.80%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.9053 | acc:56.0345\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.03%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5355 | acc:78.5582\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.56%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1959 | acc:66.6928\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.69%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0377 | acc:59.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.50%\n",
      "[VAL Acc] Avg 63.17%\n",
      "\n",
      "\n",
      "---------- Starting epoch 29 ----------\n",
      "Train Epoch: 029 Batch: 00001/00094 | Loss: 0.0648 | EWC Loss: 0.0280 | CE Loss: 0.0368\n",
      "Train Epoch: 029 Batch: 00002/00094 | Loss: 0.0718 | EWC Loss: 0.0280 | CE Loss: 0.0438\n",
      "Train Epoch: 029 Batch: 00003/00094 | Loss: 0.0486 | EWC Loss: 0.0280 | CE Loss: 0.0206\n",
      "Train Epoch: 029 Batch: 00004/00094 | Loss: 0.0856 | EWC Loss: 0.0280 | CE Loss: 0.0576\n",
      "Train Epoch: 029 Batch: 00005/00094 | Loss: 0.0915 | EWC Loss: 0.0280 | CE Loss: 0.0635\n",
      "Train Epoch: 029 Batch: 00006/00094 | Loss: 0.0568 | EWC Loss: 0.0280 | CE Loss: 0.0288\n",
      "Train Epoch: 029 Batch: 00007/00094 | Loss: 0.0819 | EWC Loss: 0.0280 | CE Loss: 0.0539\n",
      "Train Epoch: 029 Batch: 00008/00094 | Loss: 0.0612 | EWC Loss: 0.0280 | CE Loss: 0.0332\n",
      "Train Epoch: 029 Batch: 00009/00094 | Loss: 0.0591 | EWC Loss: 0.0280 | CE Loss: 0.0311\n",
      "Train Epoch: 029 Batch: 00010/00094 | Loss: 0.0637 | EWC Loss: 0.0280 | CE Loss: 0.0357\n",
      "Train Epoch: 029 Batch: 00011/00094 | Loss: 0.0974 | EWC Loss: 0.0280 | CE Loss: 0.0694\n",
      "Train Epoch: 029 Batch: 00012/00094 | Loss: 0.0648 | EWC Loss: 0.0280 | CE Loss: 0.0368\n",
      "Train Epoch: 029 Batch: 00013/00094 | Loss: 0.1163 | EWC Loss: 0.0280 | CE Loss: 0.0884\n",
      "Train Epoch: 029 Batch: 00014/00094 | Loss: 0.0869 | EWC Loss: 0.0280 | CE Loss: 0.0589\n",
      "Train Epoch: 029 Batch: 00015/00094 | Loss: 0.1100 | EWC Loss: 0.0280 | CE Loss: 0.0820\n",
      "Train Epoch: 029 Batch: 00016/00094 | Loss: 0.0725 | EWC Loss: 0.0280 | CE Loss: 0.0445\n",
      "Train Epoch: 029 Batch: 00017/00094 | Loss: 0.0802 | EWC Loss: 0.0280 | CE Loss: 0.0522\n",
      "Train Epoch: 029 Batch: 00018/00094 | Loss: 0.0516 | EWC Loss: 0.0280 | CE Loss: 0.0236\n",
      "Train Epoch: 029 Batch: 00019/00094 | Loss: 0.0877 | EWC Loss: 0.0280 | CE Loss: 0.0597\n",
      "Train Epoch: 029 Batch: 00020/00094 | Loss: 0.0855 | EWC Loss: 0.0280 | CE Loss: 0.0575\n",
      "Train Epoch: 029 Batch: 00021/00094 | Loss: 0.0582 | EWC Loss: 0.0280 | CE Loss: 0.0302\n",
      "Train Epoch: 029 Batch: 00022/00094 | Loss: 0.0988 | EWC Loss: 0.0280 | CE Loss: 0.0708\n",
      "Train Epoch: 029 Batch: 00023/00094 | Loss: 0.1941 | EWC Loss: 0.0280 | CE Loss: 0.1661\n",
      "Train Epoch: 029 Batch: 00024/00094 | Loss: 0.1230 | EWC Loss: 0.0280 | CE Loss: 0.0950\n",
      "Train Epoch: 029 Batch: 00025/00094 | Loss: 0.0452 | EWC Loss: 0.0280 | CE Loss: 0.0172\n",
      "Train Epoch: 029 Batch: 00026/00094 | Loss: 0.0560 | EWC Loss: 0.0280 | CE Loss: 0.0280\n",
      "Train Epoch: 029 Batch: 00027/00094 | Loss: 0.2410 | EWC Loss: 0.0280 | CE Loss: 0.2130\n",
      "Train Epoch: 029 Batch: 00028/00094 | Loss: 0.0799 | EWC Loss: 0.0280 | CE Loss: 0.0519\n",
      "Train Epoch: 029 Batch: 00029/00094 | Loss: 0.0954 | EWC Loss: 0.0280 | CE Loss: 0.0674\n",
      "Train Epoch: 029 Batch: 00030/00094 | Loss: 0.0509 | EWC Loss: 0.0280 | CE Loss: 0.0229\n",
      "Train Epoch: 029 Batch: 00031/00094 | Loss: 0.0659 | EWC Loss: 0.0280 | CE Loss: 0.0379\n",
      "Train Epoch: 029 Batch: 00032/00094 | Loss: 0.0639 | EWC Loss: 0.0280 | CE Loss: 0.0359\n",
      "Train Epoch: 029 Batch: 00033/00094 | Loss: 0.0677 | EWC Loss: 0.0280 | CE Loss: 0.0397\n",
      "Train Epoch: 029 Batch: 00034/00094 | Loss: 0.1236 | EWC Loss: 0.0280 | CE Loss: 0.0956\n",
      "Train Epoch: 029 Batch: 00035/00094 | Loss: 0.0698 | EWC Loss: 0.0280 | CE Loss: 0.0419\n",
      "Train Epoch: 029 Batch: 00036/00094 | Loss: 0.0365 | EWC Loss: 0.0280 | CE Loss: 0.0085\n",
      "Train Epoch: 029 Batch: 00037/00094 | Loss: 0.1220 | EWC Loss: 0.0280 | CE Loss: 0.0940\n",
      "Train Epoch: 029 Batch: 00038/00094 | Loss: 0.0874 | EWC Loss: 0.0280 | CE Loss: 0.0594\n",
      "Train Epoch: 029 Batch: 00039/00094 | Loss: 0.1970 | EWC Loss: 0.0280 | CE Loss: 0.1690\n",
      "Train Epoch: 029 Batch: 00040/00094 | Loss: 0.1291 | EWC Loss: 0.0280 | CE Loss: 0.1011\n",
      "Train Epoch: 029 Batch: 00041/00094 | Loss: 0.0691 | EWC Loss: 0.0280 | CE Loss: 0.0411\n",
      "Train Epoch: 029 Batch: 00042/00094 | Loss: 0.0950 | EWC Loss: 0.0280 | CE Loss: 0.0671\n",
      "Train Epoch: 029 Batch: 00043/00094 | Loss: 0.1559 | EWC Loss: 0.0280 | CE Loss: 0.1279\n",
      "Train Epoch: 029 Batch: 00044/00094 | Loss: 0.0797 | EWC Loss: 0.0280 | CE Loss: 0.0518\n",
      "Train Epoch: 029 Batch: 00045/00094 | Loss: 0.0469 | EWC Loss: 0.0280 | CE Loss: 0.0190\n",
      "Train Epoch: 029 Batch: 00046/00094 | Loss: 0.0389 | EWC Loss: 0.0280 | CE Loss: 0.0109\n",
      "Train Epoch: 029 Batch: 00047/00094 | Loss: 0.0578 | EWC Loss: 0.0280 | CE Loss: 0.0299\n",
      "Train Epoch: 029 Batch: 00048/00094 | Loss: 0.0567 | EWC Loss: 0.0280 | CE Loss: 0.0287\n",
      "Train Epoch: 029 Batch: 00049/00094 | Loss: 0.0592 | EWC Loss: 0.0280 | CE Loss: 0.0312\n",
      "Train Epoch: 029 Batch: 00050/00094 | Loss: 0.0945 | EWC Loss: 0.0280 | CE Loss: 0.0666\n",
      "Train Epoch: 029 Batch: 00051/00094 | Loss: 0.0535 | EWC Loss: 0.0280 | CE Loss: 0.0256\n",
      "Train Epoch: 029 Batch: 00052/00094 | Loss: 0.0813 | EWC Loss: 0.0280 | CE Loss: 0.0534\n",
      "Train Epoch: 029 Batch: 00053/00094 | Loss: 0.1000 | EWC Loss: 0.0280 | CE Loss: 0.0720\n",
      "Train Epoch: 029 Batch: 00054/00094 | Loss: 0.1008 | EWC Loss: 0.0280 | CE Loss: 0.0729\n",
      "Train Epoch: 029 Batch: 00055/00094 | Loss: 0.0654 | EWC Loss: 0.0280 | CE Loss: 0.0374\n",
      "Train Epoch: 029 Batch: 00056/00094 | Loss: 0.0878 | EWC Loss: 0.0280 | CE Loss: 0.0598\n",
      "Train Epoch: 029 Batch: 00057/00094 | Loss: 0.1342 | EWC Loss: 0.0280 | CE Loss: 0.1062\n",
      "Train Epoch: 029 Batch: 00058/00094 | Loss: 0.0735 | EWC Loss: 0.0280 | CE Loss: 0.0455\n",
      "Train Epoch: 029 Batch: 00059/00094 | Loss: 0.0677 | EWC Loss: 0.0280 | CE Loss: 0.0397\n",
      "Train Epoch: 029 Batch: 00060/00094 | Loss: 0.1400 | EWC Loss: 0.0280 | CE Loss: 0.1121\n",
      "Train Epoch: 029 Batch: 00061/00094 | Loss: 0.1120 | EWC Loss: 0.0280 | CE Loss: 0.0840\n",
      "Train Epoch: 029 Batch: 00062/00094 | Loss: 0.1244 | EWC Loss: 0.0280 | CE Loss: 0.0964\n",
      "Train Epoch: 029 Batch: 00063/00094 | Loss: 0.0540 | EWC Loss: 0.0280 | CE Loss: 0.0260\n",
      "Train Epoch: 029 Batch: 00064/00094 | Loss: 0.0905 | EWC Loss: 0.0279 | CE Loss: 0.0626\n",
      "Train Epoch: 029 Batch: 00065/00094 | Loss: 0.1045 | EWC Loss: 0.0279 | CE Loss: 0.0766\n",
      "Train Epoch: 029 Batch: 00066/00094 | Loss: 0.0596 | EWC Loss: 0.0279 | CE Loss: 0.0317\n",
      "Train Epoch: 029 Batch: 00067/00094 | Loss: 0.1410 | EWC Loss: 0.0279 | CE Loss: 0.1130\n",
      "Train Epoch: 029 Batch: 00068/00094 | Loss: 0.1291 | EWC Loss: 0.0279 | CE Loss: 0.1012\n",
      "Train Epoch: 029 Batch: 00069/00094 | Loss: 0.0563 | EWC Loss: 0.0279 | CE Loss: 0.0283\n",
      "Train Epoch: 029 Batch: 00070/00094 | Loss: 0.0928 | EWC Loss: 0.0279 | CE Loss: 0.0649\n",
      "Train Epoch: 029 Batch: 00071/00094 | Loss: 0.0578 | EWC Loss: 0.0279 | CE Loss: 0.0299\n",
      "Train Epoch: 029 Batch: 00072/00094 | Loss: 0.1041 | EWC Loss: 0.0279 | CE Loss: 0.0762\n",
      "Train Epoch: 029 Batch: 00073/00094 | Loss: 0.0729 | EWC Loss: 0.0279 | CE Loss: 0.0450\n",
      "Train Epoch: 029 Batch: 00074/00094 | Loss: 0.0856 | EWC Loss: 0.0279 | CE Loss: 0.0577\n",
      "Train Epoch: 029 Batch: 00075/00094 | Loss: 0.0891 | EWC Loss: 0.0279 | CE Loss: 0.0612\n",
      "Train Epoch: 029 Batch: 00076/00094 | Loss: 0.1720 | EWC Loss: 0.0279 | CE Loss: 0.1440\n",
      "Train Epoch: 029 Batch: 00077/00094 | Loss: 0.1257 | EWC Loss: 0.0279 | CE Loss: 0.0978\n",
      "Train Epoch: 029 Batch: 00078/00094 | Loss: 0.0871 | EWC Loss: 0.0279 | CE Loss: 0.0592\n",
      "Train Epoch: 029 Batch: 00079/00094 | Loss: 0.0840 | EWC Loss: 0.0279 | CE Loss: 0.0561\n",
      "Train Epoch: 029 Batch: 00080/00094 | Loss: 0.0602 | EWC Loss: 0.0279 | CE Loss: 0.0323\n",
      "Train Epoch: 029 Batch: 00081/00094 | Loss: 0.1446 | EWC Loss: 0.0279 | CE Loss: 0.1167\n",
      "Train Epoch: 029 Batch: 00082/00094 | Loss: 0.0659 | EWC Loss: 0.0279 | CE Loss: 0.0379\n",
      "Train Epoch: 029 Batch: 00083/00094 | Loss: 0.0914 | EWC Loss: 0.0279 | CE Loss: 0.0635\n",
      "Train Epoch: 029 Batch: 00084/00094 | Loss: 0.1095 | EWC Loss: 0.0279 | CE Loss: 0.0815\n",
      "Train Epoch: 029 Batch: 00085/00094 | Loss: 0.1348 | EWC Loss: 0.0279 | CE Loss: 0.1069\n",
      "Train Epoch: 029 Batch: 00086/00094 | Loss: 0.0812 | EWC Loss: 0.0279 | CE Loss: 0.0532\n",
      "Train Epoch: 029 Batch: 00087/00094 | Loss: 0.1053 | EWC Loss: 0.0279 | CE Loss: 0.0773\n",
      "Train Epoch: 029 Batch: 00088/00094 | Loss: 0.0455 | EWC Loss: 0.0279 | CE Loss: 0.0175\n",
      "Train Epoch: 029 Batch: 00089/00094 | Loss: 0.0526 | EWC Loss: 0.0279 | CE Loss: 0.0246\n",
      "Train Epoch: 029 Batch: 00090/00094 | Loss: 0.0938 | EWC Loss: 0.0279 | CE Loss: 0.0658\n",
      "Train Epoch: 029 Batch: 00091/00094 | Loss: 0.0620 | EWC Loss: 0.0279 | CE Loss: 0.0341\n",
      "Train Epoch: 029 Batch: 00092/00094 | Loss: 0.0505 | EWC Loss: 0.0279 | CE Loss: 0.0225\n",
      "Train Epoch: 029 Batch: 00093/00094 | Loss: 0.1129 | EWC Loss: 0.0279 | CE Loss: 0.0850\n",
      "Train Epoch: 029 Batch: 00094/00094 | Loss: 0.0768 | EWC Loss: 0.0279 | CE Loss: 0.0489\n",
      "Train Epoch: 029 |Acc 97.91667 | Loss: 0.08863 | Task Loss 0.06066 | EWC Loss: 0.02797\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0633 | acc:97.9000\n",
      "[VAL Acc] Target: 97.90%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8510 | acc:48.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8470 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9598 | acc:45.0382\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.8228 | acc:56.5439\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5609 | acc:78.0961\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 78.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1601 | acc:67.9075\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.91%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0542 | acc:60.4375\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.44%\n",
      "[VAL Acc] Avg 63.33%\n",
      "\n",
      "\n",
      "---------- Starting epoch 30 ----------\n",
      "Train Epoch: 030 Batch: 00001/00094 | Loss: 0.1179 | EWC Loss: 0.0279 | CE Loss: 0.0899\n",
      "Train Epoch: 030 Batch: 00002/00094 | Loss: 0.1119 | EWC Loss: 0.0279 | CE Loss: 0.0840\n",
      "Train Epoch: 030 Batch: 00003/00094 | Loss: 0.0799 | EWC Loss: 0.0279 | CE Loss: 0.0520\n",
      "Train Epoch: 030 Batch: 00004/00094 | Loss: 0.0708 | EWC Loss: 0.0279 | CE Loss: 0.0428\n",
      "Train Epoch: 030 Batch: 00005/00094 | Loss: 0.1123 | EWC Loss: 0.0279 | CE Loss: 0.0843\n",
      "Train Epoch: 030 Batch: 00006/00094 | Loss: 0.1444 | EWC Loss: 0.0279 | CE Loss: 0.1165\n",
      "Train Epoch: 030 Batch: 00007/00094 | Loss: 0.0751 | EWC Loss: 0.0279 | CE Loss: 0.0472\n",
      "Train Epoch: 030 Batch: 00008/00094 | Loss: 0.1903 | EWC Loss: 0.0279 | CE Loss: 0.1623\n",
      "Train Epoch: 030 Batch: 00009/00094 | Loss: 0.0461 | EWC Loss: 0.0279 | CE Loss: 0.0182\n",
      "Train Epoch: 030 Batch: 00010/00094 | Loss: 0.1068 | EWC Loss: 0.0279 | CE Loss: 0.0788\n",
      "Train Epoch: 030 Batch: 00011/00094 | Loss: 0.0957 | EWC Loss: 0.0279 | CE Loss: 0.0677\n",
      "Train Epoch: 030 Batch: 00012/00094 | Loss: 0.0945 | EWC Loss: 0.0279 | CE Loss: 0.0666\n",
      "Train Epoch: 030 Batch: 00013/00094 | Loss: 0.0528 | EWC Loss: 0.0279 | CE Loss: 0.0249\n",
      "Train Epoch: 030 Batch: 00014/00094 | Loss: 0.0917 | EWC Loss: 0.0279 | CE Loss: 0.0637\n",
      "Train Epoch: 030 Batch: 00015/00094 | Loss: 0.0806 | EWC Loss: 0.0279 | CE Loss: 0.0526\n",
      "Train Epoch: 030 Batch: 00016/00094 | Loss: 0.0461 | EWC Loss: 0.0279 | CE Loss: 0.0182\n",
      "Train Epoch: 030 Batch: 00017/00094 | Loss: 0.1005 | EWC Loss: 0.0279 | CE Loss: 0.0725\n",
      "Train Epoch: 030 Batch: 00018/00094 | Loss: 0.1203 | EWC Loss: 0.0279 | CE Loss: 0.0924\n",
      "Train Epoch: 030 Batch: 00019/00094 | Loss: 0.0979 | EWC Loss: 0.0279 | CE Loss: 0.0700\n",
      "Train Epoch: 030 Batch: 00020/00094 | Loss: 0.0718 | EWC Loss: 0.0279 | CE Loss: 0.0439\n",
      "Train Epoch: 030 Batch: 00021/00094 | Loss: 0.0895 | EWC Loss: 0.0279 | CE Loss: 0.0616\n",
      "Train Epoch: 030 Batch: 00022/00094 | Loss: 0.0719 | EWC Loss: 0.0279 | CE Loss: 0.0440\n",
      "Train Epoch: 030 Batch: 00023/00094 | Loss: 0.0551 | EWC Loss: 0.0279 | CE Loss: 0.0272\n",
      "Train Epoch: 030 Batch: 00024/00094 | Loss: 0.1246 | EWC Loss: 0.0279 | CE Loss: 0.0966\n",
      "Train Epoch: 030 Batch: 00025/00094 | Loss: 0.0959 | EWC Loss: 0.0279 | CE Loss: 0.0680\n",
      "Train Epoch: 030 Batch: 00026/00094 | Loss: 0.0465 | EWC Loss: 0.0279 | CE Loss: 0.0185\n",
      "Train Epoch: 030 Batch: 00027/00094 | Loss: 0.0724 | EWC Loss: 0.0279 | CE Loss: 0.0445\n",
      "Train Epoch: 030 Batch: 00028/00094 | Loss: 0.0705 | EWC Loss: 0.0279 | CE Loss: 0.0425\n",
      "Train Epoch: 030 Batch: 00029/00094 | Loss: 0.1105 | EWC Loss: 0.0279 | CE Loss: 0.0826\n",
      "Train Epoch: 030 Batch: 00030/00094 | Loss: 0.0508 | EWC Loss: 0.0279 | CE Loss: 0.0228\n",
      "Train Epoch: 030 Batch: 00031/00094 | Loss: 0.0855 | EWC Loss: 0.0279 | CE Loss: 0.0576\n",
      "Train Epoch: 030 Batch: 00032/00094 | Loss: 0.0527 | EWC Loss: 0.0279 | CE Loss: 0.0247\n",
      "Train Epoch: 030 Batch: 00033/00094 | Loss: 0.0485 | EWC Loss: 0.0279 | CE Loss: 0.0206\n",
      "Train Epoch: 030 Batch: 00034/00094 | Loss: 0.1570 | EWC Loss: 0.0279 | CE Loss: 0.1290\n",
      "Train Epoch: 030 Batch: 00035/00094 | Loss: 0.0525 | EWC Loss: 0.0279 | CE Loss: 0.0246\n",
      "Train Epoch: 030 Batch: 00036/00094 | Loss: 0.1599 | EWC Loss: 0.0279 | CE Loss: 0.1320\n",
      "Train Epoch: 030 Batch: 00037/00094 | Loss: 0.1488 | EWC Loss: 0.0279 | CE Loss: 0.1209\n",
      "Train Epoch: 030 Batch: 00038/00094 | Loss: 0.0548 | EWC Loss: 0.0279 | CE Loss: 0.0269\n",
      "Train Epoch: 030 Batch: 00039/00094 | Loss: 0.1051 | EWC Loss: 0.0279 | CE Loss: 0.0772\n",
      "Train Epoch: 030 Batch: 00040/00094 | Loss: 0.0587 | EWC Loss: 0.0279 | CE Loss: 0.0308\n",
      "Train Epoch: 030 Batch: 00041/00094 | Loss: 0.0955 | EWC Loss: 0.0279 | CE Loss: 0.0676\n",
      "Train Epoch: 030 Batch: 00042/00094 | Loss: 0.0473 | EWC Loss: 0.0279 | CE Loss: 0.0194\n",
      "Train Epoch: 030 Batch: 00043/00094 | Loss: 0.0665 | EWC Loss: 0.0279 | CE Loss: 0.0385\n",
      "Train Epoch: 030 Batch: 00044/00094 | Loss: 0.1322 | EWC Loss: 0.0279 | CE Loss: 0.1043\n",
      "Train Epoch: 030 Batch: 00045/00094 | Loss: 0.1389 | EWC Loss: 0.0279 | CE Loss: 0.1109\n",
      "Train Epoch: 030 Batch: 00046/00094 | Loss: 0.0801 | EWC Loss: 0.0279 | CE Loss: 0.0522\n",
      "Train Epoch: 030 Batch: 00047/00094 | Loss: 0.3440 | EWC Loss: 0.0279 | CE Loss: 0.3160\n",
      "Train Epoch: 030 Batch: 00048/00094 | Loss: 0.0771 | EWC Loss: 0.0279 | CE Loss: 0.0492\n",
      "Train Epoch: 030 Batch: 00049/00094 | Loss: 0.0664 | EWC Loss: 0.0279 | CE Loss: 0.0385\n",
      "Train Epoch: 030 Batch: 00050/00094 | Loss: 0.1505 | EWC Loss: 0.0279 | CE Loss: 0.1226\n",
      "Train Epoch: 030 Batch: 00051/00094 | Loss: 0.0624 | EWC Loss: 0.0279 | CE Loss: 0.0345\n",
      "Train Epoch: 030 Batch: 00052/00094 | Loss: 0.0547 | EWC Loss: 0.0279 | CE Loss: 0.0267\n",
      "Train Epoch: 030 Batch: 00053/00094 | Loss: 0.0612 | EWC Loss: 0.0279 | CE Loss: 0.0333\n",
      "Train Epoch: 030 Batch: 00054/00094 | Loss: 0.0588 | EWC Loss: 0.0279 | CE Loss: 0.0309\n",
      "Train Epoch: 030 Batch: 00055/00094 | Loss: 0.0850 | EWC Loss: 0.0279 | CE Loss: 0.0570\n",
      "Train Epoch: 030 Batch: 00056/00094 | Loss: 0.1751 | EWC Loss: 0.0279 | CE Loss: 0.1472\n",
      "Train Epoch: 030 Batch: 00057/00094 | Loss: 0.1359 | EWC Loss: 0.0279 | CE Loss: 0.1080\n",
      "Train Epoch: 030 Batch: 00058/00094 | Loss: 0.0576 | EWC Loss: 0.0279 | CE Loss: 0.0296\n",
      "Train Epoch: 030 Batch: 00059/00094 | Loss: 0.0931 | EWC Loss: 0.0279 | CE Loss: 0.0652\n",
      "Train Epoch: 030 Batch: 00060/00094 | Loss: 0.0966 | EWC Loss: 0.0279 | CE Loss: 0.0687\n",
      "Train Epoch: 030 Batch: 00061/00094 | Loss: 0.0919 | EWC Loss: 0.0279 | CE Loss: 0.0640\n",
      "Train Epoch: 030 Batch: 00062/00094 | Loss: 0.0595 | EWC Loss: 0.0279 | CE Loss: 0.0316\n",
      "Train Epoch: 030 Batch: 00063/00094 | Loss: 0.1053 | EWC Loss: 0.0279 | CE Loss: 0.0774\n",
      "Train Epoch: 030 Batch: 00064/00094 | Loss: 0.0799 | EWC Loss: 0.0279 | CE Loss: 0.0520\n",
      "Train Epoch: 030 Batch: 00065/00094 | Loss: 0.0654 | EWC Loss: 0.0279 | CE Loss: 0.0375\n",
      "Train Epoch: 030 Batch: 00066/00094 | Loss: 0.1083 | EWC Loss: 0.0279 | CE Loss: 0.0803\n",
      "Train Epoch: 030 Batch: 00067/00094 | Loss: 0.0750 | EWC Loss: 0.0279 | CE Loss: 0.0471\n",
      "Train Epoch: 030 Batch: 00068/00094 | Loss: 0.0806 | EWC Loss: 0.0279 | CE Loss: 0.0527\n",
      "Train Epoch: 030 Batch: 00069/00094 | Loss: 0.0753 | EWC Loss: 0.0279 | CE Loss: 0.0473\n",
      "Train Epoch: 030 Batch: 00070/00094 | Loss: 0.0662 | EWC Loss: 0.0279 | CE Loss: 0.0383\n",
      "Train Epoch: 030 Batch: 00071/00094 | Loss: 0.1168 | EWC Loss: 0.0279 | CE Loss: 0.0888\n",
      "Train Epoch: 030 Batch: 00072/00094 | Loss: 0.0438 | EWC Loss: 0.0279 | CE Loss: 0.0159\n",
      "Train Epoch: 030 Batch: 00073/00094 | Loss: 0.0384 | EWC Loss: 0.0279 | CE Loss: 0.0105\n",
      "Train Epoch: 030 Batch: 00074/00094 | Loss: 0.0693 | EWC Loss: 0.0279 | CE Loss: 0.0414\n",
      "Train Epoch: 030 Batch: 00075/00094 | Loss: 0.0664 | EWC Loss: 0.0279 | CE Loss: 0.0385\n",
      "Train Epoch: 030 Batch: 00076/00094 | Loss: 0.0782 | EWC Loss: 0.0279 | CE Loss: 0.0502\n",
      "Train Epoch: 030 Batch: 00077/00094 | Loss: 0.0803 | EWC Loss: 0.0279 | CE Loss: 0.0524\n",
      "Train Epoch: 030 Batch: 00078/00094 | Loss: 0.0697 | EWC Loss: 0.0279 | CE Loss: 0.0418\n",
      "Train Epoch: 030 Batch: 00079/00094 | Loss: 0.0461 | EWC Loss: 0.0279 | CE Loss: 0.0182\n",
      "Train Epoch: 030 Batch: 00080/00094 | Loss: 0.0862 | EWC Loss: 0.0279 | CE Loss: 0.0583\n",
      "Train Epoch: 030 Batch: 00081/00094 | Loss: 0.2386 | EWC Loss: 0.0279 | CE Loss: 0.2106\n",
      "Train Epoch: 030 Batch: 00082/00094 | Loss: 0.1479 | EWC Loss: 0.0279 | CE Loss: 0.1200\n",
      "Train Epoch: 030 Batch: 00083/00094 | Loss: 0.1298 | EWC Loss: 0.0279 | CE Loss: 0.1019\n",
      "Train Epoch: 030 Batch: 00084/00094 | Loss: 0.0576 | EWC Loss: 0.0279 | CE Loss: 0.0297\n",
      "Train Epoch: 030 Batch: 00085/00094 | Loss: 0.1192 | EWC Loss: 0.0279 | CE Loss: 0.0912\n",
      "Train Epoch: 030 Batch: 00086/00094 | Loss: 0.0821 | EWC Loss: 0.0279 | CE Loss: 0.0541\n",
      "Train Epoch: 030 Batch: 00087/00094 | Loss: 0.1054 | EWC Loss: 0.0279 | CE Loss: 0.0775\n",
      "Train Epoch: 030 Batch: 00088/00094 | Loss: 0.1260 | EWC Loss: 0.0279 | CE Loss: 0.0981\n",
      "Train Epoch: 030 Batch: 00089/00094 | Loss: 0.0762 | EWC Loss: 0.0279 | CE Loss: 0.0483\n",
      "Train Epoch: 030 Batch: 00090/00094 | Loss: 0.0609 | EWC Loss: 0.0279 | CE Loss: 0.0330\n",
      "Train Epoch: 030 Batch: 00091/00094 | Loss: 0.1359 | EWC Loss: 0.0279 | CE Loss: 0.1079\n",
      "Train Epoch: 030 Batch: 00092/00094 | Loss: 0.0591 | EWC Loss: 0.0279 | CE Loss: 0.0312\n",
      "Train Epoch: 030 Batch: 00093/00094 | Loss: 0.1209 | EWC Loss: 0.0279 | CE Loss: 0.0930\n",
      "Train Epoch: 030 Batch: 00094/00094 | Loss: 0.0878 | EWC Loss: 0.0279 | CE Loss: 0.0599\n",
      "Train Epoch: 030 |Acc 97.66667 | Loss: 0.09306 | Task Loss 0.06512 | EWC Loss: 0.02794\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0495 | acc:98.2500\n",
      "[VAL Acc] Target: 98.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8760 | acc:48.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9141 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.0746 | acc:45.6107\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.61%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.8140 | acc:56.2304\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.23%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5263 | acc:79.5749\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 79.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1838 | acc:66.5361\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.54%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0497 | acc:60.5625\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.56%\n",
      "[VAL Acc] Avg 63.38%\n",
      "\n",
      "\n",
      "---------- Starting epoch 31 ----------\n",
      "Train Epoch: 031 Batch: 00001/00094 | Loss: 0.0530 | EWC Loss: 0.0279 | CE Loss: 0.0251\n",
      "Train Epoch: 031 Batch: 00002/00094 | Loss: 0.0798 | EWC Loss: 0.0279 | CE Loss: 0.0519\n",
      "Train Epoch: 031 Batch: 00003/00094 | Loss: 0.1144 | EWC Loss: 0.0279 | CE Loss: 0.0865\n",
      "Train Epoch: 031 Batch: 00004/00094 | Loss: 0.0593 | EWC Loss: 0.0279 | CE Loss: 0.0314\n",
      "Train Epoch: 031 Batch: 00005/00094 | Loss: 0.1052 | EWC Loss: 0.0279 | CE Loss: 0.0773\n",
      "Train Epoch: 031 Batch: 00006/00094 | Loss: 0.0930 | EWC Loss: 0.0279 | CE Loss: 0.0651\n",
      "Train Epoch: 031 Batch: 00007/00094 | Loss: 0.0642 | EWC Loss: 0.0279 | CE Loss: 0.0363\n",
      "Train Epoch: 031 Batch: 00008/00094 | Loss: 0.1678 | EWC Loss: 0.0279 | CE Loss: 0.1398\n",
      "Train Epoch: 031 Batch: 00009/00094 | Loss: 0.0926 | EWC Loss: 0.0279 | CE Loss: 0.0646\n",
      "Train Epoch: 031 Batch: 00010/00094 | Loss: 0.1062 | EWC Loss: 0.0279 | CE Loss: 0.0782\n",
      "Train Epoch: 031 Batch: 00011/00094 | Loss: 0.0516 | EWC Loss: 0.0279 | CE Loss: 0.0236\n",
      "Train Epoch: 031 Batch: 00012/00094 | Loss: 0.0508 | EWC Loss: 0.0279 | CE Loss: 0.0229\n",
      "Train Epoch: 031 Batch: 00013/00094 | Loss: 0.0842 | EWC Loss: 0.0279 | CE Loss: 0.0562\n",
      "Train Epoch: 031 Batch: 00014/00094 | Loss: 0.0526 | EWC Loss: 0.0279 | CE Loss: 0.0246\n",
      "Train Epoch: 031 Batch: 00015/00094 | Loss: 0.2044 | EWC Loss: 0.0279 | CE Loss: 0.1764\n",
      "Train Epoch: 031 Batch: 00016/00094 | Loss: 0.0639 | EWC Loss: 0.0279 | CE Loss: 0.0360\n",
      "Train Epoch: 031 Batch: 00017/00094 | Loss: 0.0464 | EWC Loss: 0.0279 | CE Loss: 0.0185\n",
      "Train Epoch: 031 Batch: 00018/00094 | Loss: 0.1055 | EWC Loss: 0.0279 | CE Loss: 0.0776\n",
      "Train Epoch: 031 Batch: 00019/00094 | Loss: 0.0569 | EWC Loss: 0.0279 | CE Loss: 0.0289\n",
      "Train Epoch: 031 Batch: 00020/00094 | Loss: 0.0401 | EWC Loss: 0.0279 | CE Loss: 0.0122\n",
      "Train Epoch: 031 Batch: 00021/00094 | Loss: 0.0660 | EWC Loss: 0.0279 | CE Loss: 0.0381\n",
      "Train Epoch: 031 Batch: 00022/00094 | Loss: 0.0622 | EWC Loss: 0.0279 | CE Loss: 0.0343\n",
      "Train Epoch: 031 Batch: 00023/00094 | Loss: 0.0931 | EWC Loss: 0.0279 | CE Loss: 0.0652\n",
      "Train Epoch: 031 Batch: 00024/00094 | Loss: 0.1212 | EWC Loss: 0.0279 | CE Loss: 0.0933\n",
      "Train Epoch: 031 Batch: 00025/00094 | Loss: 0.1051 | EWC Loss: 0.0279 | CE Loss: 0.0772\n",
      "Train Epoch: 031 Batch: 00026/00094 | Loss: 0.0569 | EWC Loss: 0.0279 | CE Loss: 0.0290\n",
      "Train Epoch: 031 Batch: 00027/00094 | Loss: 0.0968 | EWC Loss: 0.0279 | CE Loss: 0.0689\n",
      "Train Epoch: 031 Batch: 00028/00094 | Loss: 0.1500 | EWC Loss: 0.0279 | CE Loss: 0.1221\n",
      "Train Epoch: 031 Batch: 00029/00094 | Loss: 0.0615 | EWC Loss: 0.0279 | CE Loss: 0.0336\n",
      "Train Epoch: 031 Batch: 00030/00094 | Loss: 0.0526 | EWC Loss: 0.0279 | CE Loss: 0.0246\n",
      "Train Epoch: 031 Batch: 00031/00094 | Loss: 0.1352 | EWC Loss: 0.0279 | CE Loss: 0.1073\n",
      "Train Epoch: 031 Batch: 00032/00094 | Loss: 0.0686 | EWC Loss: 0.0279 | CE Loss: 0.0407\n",
      "Train Epoch: 031 Batch: 00033/00094 | Loss: 0.0556 | EWC Loss: 0.0279 | CE Loss: 0.0277\n",
      "Train Epoch: 031 Batch: 00034/00094 | Loss: 0.0959 | EWC Loss: 0.0279 | CE Loss: 0.0680\n",
      "Train Epoch: 031 Batch: 00035/00094 | Loss: 0.1010 | EWC Loss: 0.0279 | CE Loss: 0.0731\n",
      "Train Epoch: 031 Batch: 00036/00094 | Loss: 0.0363 | EWC Loss: 0.0279 | CE Loss: 0.0084\n",
      "Train Epoch: 031 Batch: 00037/00094 | Loss: 0.0387 | EWC Loss: 0.0279 | CE Loss: 0.0108\n",
      "Train Epoch: 031 Batch: 00038/00094 | Loss: 0.0823 | EWC Loss: 0.0279 | CE Loss: 0.0544\n",
      "Train Epoch: 031 Batch: 00039/00094 | Loss: 0.1202 | EWC Loss: 0.0279 | CE Loss: 0.0923\n",
      "Train Epoch: 031 Batch: 00040/00094 | Loss: 0.1654 | EWC Loss: 0.0279 | CE Loss: 0.1375\n",
      "Train Epoch: 031 Batch: 00041/00094 | Loss: 0.0786 | EWC Loss: 0.0279 | CE Loss: 0.0507\n",
      "Train Epoch: 031 Batch: 00042/00094 | Loss: 0.0552 | EWC Loss: 0.0279 | CE Loss: 0.0273\n",
      "Train Epoch: 031 Batch: 00043/00094 | Loss: 0.1393 | EWC Loss: 0.0279 | CE Loss: 0.1115\n",
      "Train Epoch: 031 Batch: 00044/00094 | Loss: 0.0815 | EWC Loss: 0.0279 | CE Loss: 0.0536\n",
      "Train Epoch: 031 Batch: 00045/00094 | Loss: 0.1052 | EWC Loss: 0.0279 | CE Loss: 0.0773\n",
      "Train Epoch: 031 Batch: 00046/00094 | Loss: 0.1370 | EWC Loss: 0.0279 | CE Loss: 0.1091\n",
      "Train Epoch: 031 Batch: 00047/00094 | Loss: 0.0720 | EWC Loss: 0.0279 | CE Loss: 0.0441\n",
      "Train Epoch: 031 Batch: 00048/00094 | Loss: 0.2273 | EWC Loss: 0.0279 | CE Loss: 0.1994\n",
      "Train Epoch: 031 Batch: 00049/00094 | Loss: 0.0787 | EWC Loss: 0.0279 | CE Loss: 0.0508\n",
      "Train Epoch: 031 Batch: 00050/00094 | Loss: 0.0622 | EWC Loss: 0.0279 | CE Loss: 0.0343\n",
      "Train Epoch: 031 Batch: 00051/00094 | Loss: 0.2161 | EWC Loss: 0.0279 | CE Loss: 0.1883\n",
      "Train Epoch: 031 Batch: 00052/00094 | Loss: 0.0816 | EWC Loss: 0.0279 | CE Loss: 0.0537\n",
      "Train Epoch: 031 Batch: 00053/00094 | Loss: 0.0545 | EWC Loss: 0.0279 | CE Loss: 0.0266\n",
      "Train Epoch: 031 Batch: 00054/00094 | Loss: 0.1698 | EWC Loss: 0.0279 | CE Loss: 0.1419\n",
      "Train Epoch: 031 Batch: 00055/00094 | Loss: 0.1745 | EWC Loss: 0.0279 | CE Loss: 0.1466\n",
      "Train Epoch: 031 Batch: 00056/00094 | Loss: 0.1287 | EWC Loss: 0.0279 | CE Loss: 0.1008\n",
      "Train Epoch: 031 Batch: 00057/00094 | Loss: 0.0472 | EWC Loss: 0.0279 | CE Loss: 0.0193\n",
      "Train Epoch: 031 Batch: 00058/00094 | Loss: 0.0369 | EWC Loss: 0.0279 | CE Loss: 0.0091\n",
      "Train Epoch: 031 Batch: 00059/00094 | Loss: 0.1326 | EWC Loss: 0.0279 | CE Loss: 0.1047\n",
      "Train Epoch: 031 Batch: 00060/00094 | Loss: 0.0834 | EWC Loss: 0.0279 | CE Loss: 0.0555\n",
      "Train Epoch: 031 Batch: 00061/00094 | Loss: 0.1149 | EWC Loss: 0.0279 | CE Loss: 0.0870\n",
      "Train Epoch: 031 Batch: 00062/00094 | Loss: 0.0897 | EWC Loss: 0.0279 | CE Loss: 0.0618\n",
      "Train Epoch: 031 Batch: 00063/00094 | Loss: 0.0512 | EWC Loss: 0.0279 | CE Loss: 0.0233\n",
      "Train Epoch: 031 Batch: 00064/00094 | Loss: 0.0973 | EWC Loss: 0.0279 | CE Loss: 0.0694\n",
      "Train Epoch: 031 Batch: 00065/00094 | Loss: 0.1164 | EWC Loss: 0.0279 | CE Loss: 0.0885\n",
      "Train Epoch: 031 Batch: 00066/00094 | Loss: 0.1493 | EWC Loss: 0.0279 | CE Loss: 0.1214\n",
      "Train Epoch: 031 Batch: 00067/00094 | Loss: 0.0723 | EWC Loss: 0.0279 | CE Loss: 0.0444\n",
      "Train Epoch: 031 Batch: 00068/00094 | Loss: 0.0530 | EWC Loss: 0.0279 | CE Loss: 0.0251\n",
      "Train Epoch: 031 Batch: 00069/00094 | Loss: 0.0700 | EWC Loss: 0.0279 | CE Loss: 0.0421\n",
      "Train Epoch: 031 Batch: 00070/00094 | Loss: 0.0511 | EWC Loss: 0.0279 | CE Loss: 0.0232\n",
      "Train Epoch: 031 Batch: 00071/00094 | Loss: 0.0495 | EWC Loss: 0.0279 | CE Loss: 0.0216\n",
      "Train Epoch: 031 Batch: 00072/00094 | Loss: 0.0935 | EWC Loss: 0.0279 | CE Loss: 0.0656\n",
      "Train Epoch: 031 Batch: 00073/00094 | Loss: 0.0810 | EWC Loss: 0.0279 | CE Loss: 0.0531\n",
      "Train Epoch: 031 Batch: 00074/00094 | Loss: 0.0993 | EWC Loss: 0.0279 | CE Loss: 0.0714\n",
      "Train Epoch: 031 Batch: 00075/00094 | Loss: 0.1306 | EWC Loss: 0.0279 | CE Loss: 0.1027\n",
      "Train Epoch: 031 Batch: 00076/00094 | Loss: 0.1148 | EWC Loss: 0.0279 | CE Loss: 0.0869\n",
      "Train Epoch: 031 Batch: 00077/00094 | Loss: 0.0642 | EWC Loss: 0.0279 | CE Loss: 0.0363\n",
      "Train Epoch: 031 Batch: 00078/00094 | Loss: 0.1497 | EWC Loss: 0.0279 | CE Loss: 0.1218\n",
      "Train Epoch: 031 Batch: 00079/00094 | Loss: 0.0704 | EWC Loss: 0.0279 | CE Loss: 0.0425\n",
      "Train Epoch: 031 Batch: 00080/00094 | Loss: 0.0855 | EWC Loss: 0.0279 | CE Loss: 0.0576\n",
      "Train Epoch: 031 Batch: 00081/00094 | Loss: 0.0663 | EWC Loss: 0.0279 | CE Loss: 0.0384\n",
      "Train Epoch: 031 Batch: 00082/00094 | Loss: 0.0560 | EWC Loss: 0.0279 | CE Loss: 0.0281\n",
      "Train Epoch: 031 Batch: 00083/00094 | Loss: 0.0563 | EWC Loss: 0.0279 | CE Loss: 0.0284\n",
      "Train Epoch: 031 Batch: 00084/00094 | Loss: 0.1534 | EWC Loss: 0.0279 | CE Loss: 0.1255\n",
      "Train Epoch: 031 Batch: 00085/00094 | Loss: 0.0701 | EWC Loss: 0.0279 | CE Loss: 0.0422\n",
      "Train Epoch: 031 Batch: 00086/00094 | Loss: 0.0787 | EWC Loss: 0.0279 | CE Loss: 0.0509\n",
      "Train Epoch: 031 Batch: 00087/00094 | Loss: 0.0824 | EWC Loss: 0.0279 | CE Loss: 0.0545\n",
      "Train Epoch: 031 Batch: 00088/00094 | Loss: 0.1924 | EWC Loss: 0.0279 | CE Loss: 0.1645\n",
      "Train Epoch: 031 Batch: 00089/00094 | Loss: 0.0491 | EWC Loss: 0.0279 | CE Loss: 0.0212\n",
      "Train Epoch: 031 Batch: 00090/00094 | Loss: 0.0931 | EWC Loss: 0.0279 | CE Loss: 0.0652\n",
      "Train Epoch: 031 Batch: 00091/00094 | Loss: 0.0675 | EWC Loss: 0.0279 | CE Loss: 0.0396\n",
      "Train Epoch: 031 Batch: 00092/00094 | Loss: 0.1076 | EWC Loss: 0.0279 | CE Loss: 0.0797\n",
      "Train Epoch: 031 Batch: 00093/00094 | Loss: 0.0587 | EWC Loss: 0.0279 | CE Loss: 0.0308\n",
      "Train Epoch: 031 Batch: 00094/00094 | Loss: 0.0846 | EWC Loss: 0.0279 | CE Loss: 0.0567\n",
      "Train Epoch: 031 |Acc 97.81667 | Loss: 0.09190 | Task Loss 0.06400 | EWC Loss: 0.02790\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0590 | acc:97.8500\n",
      "[VAL Acc] Target: 97.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8938 | acc:49.2000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8424 | acc:52.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.1126 | acc:44.2748\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 44.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.8480 | acc:56.3088\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 56.31%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5947 | acc:77.3567\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 77.36%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1160 | acc:67.7900\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.79%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0472 | acc:60.0000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.00%\n",
      "[VAL Acc] Avg 63.13%\n",
      "\n",
      "\n",
      "---------- Starting epoch 32 ----------\n",
      "Train Epoch: 032 Batch: 00001/00094 | Loss: 0.2123 | EWC Loss: 0.0279 | CE Loss: 0.1845\n",
      "Train Epoch: 032 Batch: 00002/00094 | Loss: 0.1050 | EWC Loss: 0.0279 | CE Loss: 0.0772\n",
      "Train Epoch: 032 Batch: 00003/00094 | Loss: 0.1095 | EWC Loss: 0.0279 | CE Loss: 0.0816\n",
      "Train Epoch: 032 Batch: 00004/00094 | Loss: 0.1554 | EWC Loss: 0.0279 | CE Loss: 0.1275\n",
      "Train Epoch: 032 Batch: 00005/00094 | Loss: 0.1212 | EWC Loss: 0.0279 | CE Loss: 0.0933\n",
      "Train Epoch: 032 Batch: 00006/00094 | Loss: 0.0669 | EWC Loss: 0.0279 | CE Loss: 0.0390\n",
      "Train Epoch: 032 Batch: 00007/00094 | Loss: 0.0839 | EWC Loss: 0.0279 | CE Loss: 0.0560\n",
      "Train Epoch: 032 Batch: 00008/00094 | Loss: 0.0413 | EWC Loss: 0.0279 | CE Loss: 0.0134\n",
      "Train Epoch: 032 Batch: 00009/00094 | Loss: 0.0907 | EWC Loss: 0.0279 | CE Loss: 0.0628\n",
      "Train Epoch: 032 Batch: 00010/00094 | Loss: 0.0653 | EWC Loss: 0.0279 | CE Loss: 0.0375\n",
      "Train Epoch: 032 Batch: 00011/00094 | Loss: 0.0536 | EWC Loss: 0.0279 | CE Loss: 0.0258\n",
      "Train Epoch: 032 Batch: 00012/00094 | Loss: 0.1130 | EWC Loss: 0.0279 | CE Loss: 0.0851\n",
      "Train Epoch: 032 Batch: 00013/00094 | Loss: 0.1204 | EWC Loss: 0.0279 | CE Loss: 0.0925\n",
      "Train Epoch: 032 Batch: 00014/00094 | Loss: 0.0786 | EWC Loss: 0.0279 | CE Loss: 0.0507\n",
      "Train Epoch: 032 Batch: 00015/00094 | Loss: 0.1050 | EWC Loss: 0.0279 | CE Loss: 0.0771\n",
      "Train Epoch: 032 Batch: 00016/00094 | Loss: 0.0679 | EWC Loss: 0.0279 | CE Loss: 0.0400\n",
      "Train Epoch: 032 Batch: 00017/00094 | Loss: 0.0507 | EWC Loss: 0.0279 | CE Loss: 0.0228\n",
      "Train Epoch: 032 Batch: 00018/00094 | Loss: 0.1149 | EWC Loss: 0.0279 | CE Loss: 0.0870\n",
      "Train Epoch: 032 Batch: 00019/00094 | Loss: 0.0606 | EWC Loss: 0.0278 | CE Loss: 0.0328\n",
      "Train Epoch: 032 Batch: 00020/00094 | Loss: 0.0854 | EWC Loss: 0.0278 | CE Loss: 0.0576\n",
      "Train Epoch: 032 Batch: 00021/00094 | Loss: 0.1509 | EWC Loss: 0.0278 | CE Loss: 0.1231\n",
      "Train Epoch: 032 Batch: 00022/00094 | Loss: 0.0622 | EWC Loss: 0.0278 | CE Loss: 0.0343\n",
      "Train Epoch: 032 Batch: 00023/00094 | Loss: 0.1148 | EWC Loss: 0.0278 | CE Loss: 0.0870\n",
      "Train Epoch: 032 Batch: 00024/00094 | Loss: 0.0579 | EWC Loss: 0.0278 | CE Loss: 0.0301\n",
      "Train Epoch: 032 Batch: 00025/00094 | Loss: 0.1040 | EWC Loss: 0.0278 | CE Loss: 0.0763\n",
      "Train Epoch: 032 Batch: 00026/00094 | Loss: 0.0818 | EWC Loss: 0.0278 | CE Loss: 0.0540\n",
      "Train Epoch: 032 Batch: 00027/00094 | Loss: 0.0956 | EWC Loss: 0.0278 | CE Loss: 0.0678\n",
      "Train Epoch: 032 Batch: 00028/00094 | Loss: 0.1066 | EWC Loss: 0.0278 | CE Loss: 0.0788\n",
      "Train Epoch: 032 Batch: 00029/00094 | Loss: 0.0909 | EWC Loss: 0.0278 | CE Loss: 0.0631\n",
      "Train Epoch: 032 Batch: 00030/00094 | Loss: 0.0699 | EWC Loss: 0.0278 | CE Loss: 0.0421\n",
      "Train Epoch: 032 Batch: 00031/00094 | Loss: 0.0592 | EWC Loss: 0.0278 | CE Loss: 0.0315\n",
      "Train Epoch: 032 Batch: 00032/00094 | Loss: 0.0422 | EWC Loss: 0.0278 | CE Loss: 0.0144\n",
      "Train Epoch: 032 Batch: 00033/00094 | Loss: 0.0539 | EWC Loss: 0.0278 | CE Loss: 0.0261\n",
      "Train Epoch: 032 Batch: 00034/00094 | Loss: 0.0883 | EWC Loss: 0.0278 | CE Loss: 0.0606\n",
      "Train Epoch: 032 Batch: 00035/00094 | Loss: 0.0582 | EWC Loss: 0.0278 | CE Loss: 0.0305\n",
      "Train Epoch: 032 Batch: 00036/00094 | Loss: 0.1033 | EWC Loss: 0.0278 | CE Loss: 0.0755\n",
      "Train Epoch: 032 Batch: 00037/00094 | Loss: 0.0579 | EWC Loss: 0.0278 | CE Loss: 0.0301\n",
      "Train Epoch: 032 Batch: 00038/00094 | Loss: 0.1849 | EWC Loss: 0.0278 | CE Loss: 0.1571\n",
      "Train Epoch: 032 Batch: 00039/00094 | Loss: 0.0994 | EWC Loss: 0.0278 | CE Loss: 0.0716\n",
      "Train Epoch: 032 Batch: 00040/00094 | Loss: 0.0605 | EWC Loss: 0.0278 | CE Loss: 0.0327\n",
      "Train Epoch: 032 Batch: 00041/00094 | Loss: 0.0981 | EWC Loss: 0.0278 | CE Loss: 0.0703\n",
      "Train Epoch: 032 Batch: 00042/00094 | Loss: 0.0672 | EWC Loss: 0.0278 | CE Loss: 0.0394\n",
      "Train Epoch: 032 Batch: 00043/00094 | Loss: 0.0821 | EWC Loss: 0.0278 | CE Loss: 0.0543\n",
      "Train Epoch: 032 Batch: 00044/00094 | Loss: 0.0795 | EWC Loss: 0.0278 | CE Loss: 0.0517\n",
      "Train Epoch: 032 Batch: 00045/00094 | Loss: 0.2250 | EWC Loss: 0.0278 | CE Loss: 0.1972\n",
      "Train Epoch: 032 Batch: 00046/00094 | Loss: 0.0486 | EWC Loss: 0.0278 | CE Loss: 0.0208\n",
      "Train Epoch: 032 Batch: 00047/00094 | Loss: 0.0857 | EWC Loss: 0.0278 | CE Loss: 0.0579\n",
      "Train Epoch: 032 Batch: 00048/00094 | Loss: 0.0424 | EWC Loss: 0.0278 | CE Loss: 0.0146\n",
      "Train Epoch: 032 Batch: 00049/00094 | Loss: 0.0992 | EWC Loss: 0.0278 | CE Loss: 0.0714\n",
      "Train Epoch: 032 Batch: 00050/00094 | Loss: 0.0872 | EWC Loss: 0.0278 | CE Loss: 0.0595\n",
      "Train Epoch: 032 Batch: 00051/00094 | Loss: 0.0689 | EWC Loss: 0.0278 | CE Loss: 0.0411\n",
      "Train Epoch: 032 Batch: 00052/00094 | Loss: 0.1502 | EWC Loss: 0.0278 | CE Loss: 0.1224\n",
      "Train Epoch: 032 Batch: 00053/00094 | Loss: 0.0336 | EWC Loss: 0.0278 | CE Loss: 0.0058\n",
      "Train Epoch: 032 Batch: 00054/00094 | Loss: 0.0802 | EWC Loss: 0.0277 | CE Loss: 0.0525\n",
      "Train Epoch: 032 Batch: 00055/00094 | Loss: 0.0750 | EWC Loss: 0.0277 | CE Loss: 0.0472\n",
      "Train Epoch: 032 Batch: 00056/00094 | Loss: 0.0883 | EWC Loss: 0.0278 | CE Loss: 0.0605\n",
      "Train Epoch: 032 Batch: 00057/00094 | Loss: 0.0504 | EWC Loss: 0.0278 | CE Loss: 0.0226\n",
      "Train Epoch: 032 Batch: 00058/00094 | Loss: 0.0920 | EWC Loss: 0.0278 | CE Loss: 0.0643\n",
      "Train Epoch: 032 Batch: 00059/00094 | Loss: 0.1017 | EWC Loss: 0.0278 | CE Loss: 0.0740\n",
      "Train Epoch: 032 Batch: 00060/00094 | Loss: 0.0926 | EWC Loss: 0.0277 | CE Loss: 0.0649\n",
      "Train Epoch: 032 Batch: 00061/00094 | Loss: 0.1337 | EWC Loss: 0.0277 | CE Loss: 0.1059\n",
      "Train Epoch: 032 Batch: 00062/00094 | Loss: 0.0665 | EWC Loss: 0.0277 | CE Loss: 0.0387\n",
      "Train Epoch: 032 Batch: 00063/00094 | Loss: 0.0868 | EWC Loss: 0.0277 | CE Loss: 0.0591\n",
      "Train Epoch: 032 Batch: 00064/00094 | Loss: 0.0365 | EWC Loss: 0.0277 | CE Loss: 0.0088\n",
      "Train Epoch: 032 Batch: 00065/00094 | Loss: 0.1078 | EWC Loss: 0.0277 | CE Loss: 0.0800\n",
      "Train Epoch: 032 Batch: 00066/00094 | Loss: 0.0846 | EWC Loss: 0.0277 | CE Loss: 0.0569\n",
      "Train Epoch: 032 Batch: 00067/00094 | Loss: 0.1081 | EWC Loss: 0.0277 | CE Loss: 0.0804\n",
      "Train Epoch: 032 Batch: 00068/00094 | Loss: 0.0520 | EWC Loss: 0.0277 | CE Loss: 0.0243\n",
      "Train Epoch: 032 Batch: 00069/00094 | Loss: 0.1044 | EWC Loss: 0.0277 | CE Loss: 0.0767\n",
      "Train Epoch: 032 Batch: 00070/00094 | Loss: 0.0531 | EWC Loss: 0.0277 | CE Loss: 0.0253\n",
      "Train Epoch: 032 Batch: 00071/00094 | Loss: 0.1464 | EWC Loss: 0.0277 | CE Loss: 0.1187\n",
      "Train Epoch: 032 Batch: 00072/00094 | Loss: 0.0756 | EWC Loss: 0.0278 | CE Loss: 0.0478\n",
      "Train Epoch: 032 Batch: 00073/00094 | Loss: 0.0687 | EWC Loss: 0.0278 | CE Loss: 0.0409\n",
      "Train Epoch: 032 Batch: 00074/00094 | Loss: 0.0498 | EWC Loss: 0.0278 | CE Loss: 0.0220\n",
      "Train Epoch: 032 Batch: 00075/00094 | Loss: 0.0699 | EWC Loss: 0.0278 | CE Loss: 0.0422\n",
      "Train Epoch: 032 Batch: 00076/00094 | Loss: 0.0659 | EWC Loss: 0.0278 | CE Loss: 0.0382\n",
      "Train Epoch: 032 Batch: 00077/00094 | Loss: 0.0734 | EWC Loss: 0.0278 | CE Loss: 0.0457\n",
      "Train Epoch: 032 Batch: 00078/00094 | Loss: 0.0853 | EWC Loss: 0.0277 | CE Loss: 0.0576\n",
      "Train Epoch: 032 Batch: 00079/00094 | Loss: 0.1454 | EWC Loss: 0.0277 | CE Loss: 0.1177\n",
      "Train Epoch: 032 Batch: 00080/00094 | Loss: 0.0694 | EWC Loss: 0.0277 | CE Loss: 0.0416\n",
      "Train Epoch: 032 Batch: 00081/00094 | Loss: 0.0518 | EWC Loss: 0.0277 | CE Loss: 0.0240\n",
      "Train Epoch: 032 Batch: 00082/00094 | Loss: 0.0819 | EWC Loss: 0.0277 | CE Loss: 0.0542\n",
      "Train Epoch: 032 Batch: 00083/00094 | Loss: 0.1026 | EWC Loss: 0.0277 | CE Loss: 0.0748\n",
      "Train Epoch: 032 Batch: 00084/00094 | Loss: 0.0551 | EWC Loss: 0.0277 | CE Loss: 0.0274\n",
      "Train Epoch: 032 Batch: 00085/00094 | Loss: 0.1161 | EWC Loss: 0.0277 | CE Loss: 0.0883\n",
      "Train Epoch: 032 Batch: 00086/00094 | Loss: 0.0494 | EWC Loss: 0.0277 | CE Loss: 0.0217\n",
      "Train Epoch: 032 Batch: 00087/00094 | Loss: 0.0882 | EWC Loss: 0.0277 | CE Loss: 0.0605\n",
      "Train Epoch: 032 Batch: 00088/00094 | Loss: 0.0735 | EWC Loss: 0.0277 | CE Loss: 0.0458\n",
      "Train Epoch: 032 Batch: 00089/00094 | Loss: 0.1207 | EWC Loss: 0.0277 | CE Loss: 0.0930\n",
      "Train Epoch: 032 Batch: 00090/00094 | Loss: 0.0370 | EWC Loss: 0.0277 | CE Loss: 0.0092\n",
      "Train Epoch: 032 Batch: 00091/00094 | Loss: 0.1300 | EWC Loss: 0.0277 | CE Loss: 0.1022\n",
      "Train Epoch: 032 Batch: 00092/00094 | Loss: 0.0735 | EWC Loss: 0.0278 | CE Loss: 0.0457\n",
      "Train Epoch: 032 Batch: 00093/00094 | Loss: 0.0476 | EWC Loss: 0.0278 | CE Loss: 0.0198\n",
      "Train Epoch: 032 Batch: 00094/00094 | Loss: 0.0473 | EWC Loss: 0.0277 | CE Loss: 0.0196\n",
      "Train Epoch: 032 |Acc 97.81667 | Loss: 0.08670 | Task Loss 0.05892 | EWC Loss: 0.02778\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0560 | acc:98.0500\n",
      "[VAL Acc] Target: 98.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.9175 | acc:48.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9732 | acc:51.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9867 | acc:45.0382\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.6472 | acc:58.4248\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7369 | acc:72.9205\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.92%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1132 | acc:68.2602\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 68.26%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0905 | acc:60.7500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 60.75%\n",
      "[VAL Acc] Avg 62.93%\n",
      "\n",
      "\n",
      "---------- Starting epoch 33 ----------\n",
      "Train Epoch: 033 Batch: 00001/00094 | Loss: 0.0676 | EWC Loss: 0.0277 | CE Loss: 0.0399\n",
      "Train Epoch: 033 Batch: 00002/00094 | Loss: 0.2040 | EWC Loss: 0.0277 | CE Loss: 0.1763\n",
      "Train Epoch: 033 Batch: 00003/00094 | Loss: 0.0700 | EWC Loss: 0.0277 | CE Loss: 0.0423\n",
      "Train Epoch: 033 Batch: 00004/00094 | Loss: 0.0739 | EWC Loss: 0.0277 | CE Loss: 0.0461\n",
      "Train Epoch: 033 Batch: 00005/00094 | Loss: 0.1010 | EWC Loss: 0.0277 | CE Loss: 0.0733\n",
      "Train Epoch: 033 Batch: 00006/00094 | Loss: 0.0687 | EWC Loss: 0.0278 | CE Loss: 0.0410\n",
      "Train Epoch: 033 Batch: 00007/00094 | Loss: 0.1185 | EWC Loss: 0.0278 | CE Loss: 0.0907\n",
      "Train Epoch: 033 Batch: 00008/00094 | Loss: 0.0827 | EWC Loss: 0.0278 | CE Loss: 0.0549\n",
      "Train Epoch: 033 Batch: 00009/00094 | Loss: 0.1184 | EWC Loss: 0.0278 | CE Loss: 0.0906\n",
      "Train Epoch: 033 Batch: 00010/00094 | Loss: 0.1122 | EWC Loss: 0.0278 | CE Loss: 0.0843\n",
      "Train Epoch: 033 Batch: 00011/00094 | Loss: 0.0803 | EWC Loss: 0.0278 | CE Loss: 0.0525\n",
      "Train Epoch: 033 Batch: 00012/00094 | Loss: 0.0456 | EWC Loss: 0.0278 | CE Loss: 0.0178\n",
      "Train Epoch: 033 Batch: 00013/00094 | Loss: 0.0585 | EWC Loss: 0.0278 | CE Loss: 0.0307\n",
      "Train Epoch: 033 Batch: 00014/00094 | Loss: 0.0853 | EWC Loss: 0.0278 | CE Loss: 0.0575\n",
      "Train Epoch: 033 Batch: 00015/00094 | Loss: 0.0700 | EWC Loss: 0.0277 | CE Loss: 0.0423\n",
      "Train Epoch: 033 Batch: 00016/00094 | Loss: 0.1156 | EWC Loss: 0.0277 | CE Loss: 0.0879\n",
      "Train Epoch: 033 Batch: 00017/00094 | Loss: 0.0480 | EWC Loss: 0.0278 | CE Loss: 0.0202\n",
      "Train Epoch: 033 Batch: 00018/00094 | Loss: 0.0795 | EWC Loss: 0.0278 | CE Loss: 0.0517\n",
      "Train Epoch: 033 Batch: 00019/00094 | Loss: 0.0739 | EWC Loss: 0.0278 | CE Loss: 0.0461\n",
      "Train Epoch: 033 Batch: 00020/00094 | Loss: 0.1502 | EWC Loss: 0.0278 | CE Loss: 0.1225\n",
      "Train Epoch: 033 Batch: 00021/00094 | Loss: 0.1385 | EWC Loss: 0.0277 | CE Loss: 0.1108\n",
      "Train Epoch: 033 Batch: 00022/00094 | Loss: 0.1136 | EWC Loss: 0.0277 | CE Loss: 0.0860\n",
      "Train Epoch: 033 Batch: 00023/00094 | Loss: 0.0778 | EWC Loss: 0.0277 | CE Loss: 0.0501\n",
      "Train Epoch: 033 Batch: 00024/00094 | Loss: 0.0994 | EWC Loss: 0.0277 | CE Loss: 0.0717\n",
      "Train Epoch: 033 Batch: 00025/00094 | Loss: 0.1894 | EWC Loss: 0.0277 | CE Loss: 0.1618\n",
      "Train Epoch: 033 Batch: 00026/00094 | Loss: 0.0897 | EWC Loss: 0.0277 | CE Loss: 0.0620\n",
      "Train Epoch: 033 Batch: 00027/00094 | Loss: 0.0651 | EWC Loss: 0.0277 | CE Loss: 0.0374\n",
      "Train Epoch: 033 Batch: 00028/00094 | Loss: 0.0712 | EWC Loss: 0.0277 | CE Loss: 0.0435\n",
      "Train Epoch: 033 Batch: 00029/00094 | Loss: 0.1961 | EWC Loss: 0.0277 | CE Loss: 0.1684\n",
      "Train Epoch: 033 Batch: 00030/00094 | Loss: 0.0514 | EWC Loss: 0.0276 | CE Loss: 0.0237\n",
      "Train Epoch: 033 Batch: 00031/00094 | Loss: 0.0440 | EWC Loss: 0.0276 | CE Loss: 0.0164\n",
      "Train Epoch: 033 Batch: 00032/00094 | Loss: 0.2139 | EWC Loss: 0.0276 | CE Loss: 0.1862\n",
      "Train Epoch: 033 Batch: 00033/00094 | Loss: 0.1298 | EWC Loss: 0.0276 | CE Loss: 0.1021\n",
      "Train Epoch: 033 Batch: 00034/00094 | Loss: 0.0732 | EWC Loss: 0.0276 | CE Loss: 0.0455\n",
      "Train Epoch: 033 Batch: 00035/00094 | Loss: 0.1284 | EWC Loss: 0.0276 | CE Loss: 0.1008\n",
      "Train Epoch: 033 Batch: 00036/00094 | Loss: 0.0517 | EWC Loss: 0.0276 | CE Loss: 0.0240\n",
      "Train Epoch: 033 Batch: 00037/00094 | Loss: 0.0691 | EWC Loss: 0.0276 | CE Loss: 0.0415\n",
      "Train Epoch: 033 Batch: 00038/00094 | Loss: 0.1011 | EWC Loss: 0.0276 | CE Loss: 0.0735\n",
      "Train Epoch: 033 Batch: 00039/00094 | Loss: 0.0649 | EWC Loss: 0.0277 | CE Loss: 0.0373\n",
      "Train Epoch: 033 Batch: 00040/00094 | Loss: 0.0760 | EWC Loss: 0.0277 | CE Loss: 0.0484\n",
      "Train Epoch: 033 Batch: 00041/00094 | Loss: 0.0778 | EWC Loss: 0.0277 | CE Loss: 0.0501\n",
      "Train Epoch: 033 Batch: 00042/00094 | Loss: 0.0956 | EWC Loss: 0.0277 | CE Loss: 0.0679\n",
      "Train Epoch: 033 Batch: 00043/00094 | Loss: 0.0861 | EWC Loss: 0.0277 | CE Loss: 0.0584\n",
      "Train Epoch: 033 Batch: 00044/00094 | Loss: 0.1157 | EWC Loss: 0.0276 | CE Loss: 0.0881\n",
      "Train Epoch: 033 Batch: 00045/00094 | Loss: 0.1004 | EWC Loss: 0.0277 | CE Loss: 0.0727\n",
      "Train Epoch: 033 Batch: 00046/00094 | Loss: 0.1336 | EWC Loss: 0.0277 | CE Loss: 0.1059\n",
      "Train Epoch: 033 Batch: 00047/00094 | Loss: 0.0502 | EWC Loss: 0.0277 | CE Loss: 0.0225\n",
      "Train Epoch: 033 Batch: 00048/00094 | Loss: 0.1718 | EWC Loss: 0.0277 | CE Loss: 0.1441\n",
      "Train Epoch: 033 Batch: 00049/00094 | Loss: 0.0608 | EWC Loss: 0.0277 | CE Loss: 0.0331\n",
      "Train Epoch: 033 Batch: 00050/00094 | Loss: 0.0875 | EWC Loss: 0.0277 | CE Loss: 0.0598\n",
      "Train Epoch: 033 Batch: 00051/00094 | Loss: 0.1016 | EWC Loss: 0.0277 | CE Loss: 0.0739\n",
      "Train Epoch: 033 Batch: 00052/00094 | Loss: 0.1243 | EWC Loss: 0.0278 | CE Loss: 0.0965\n",
      "Train Epoch: 033 Batch: 00053/00094 | Loss: 0.1614 | EWC Loss: 0.0278 | CE Loss: 0.1336\n",
      "Train Epoch: 033 Batch: 00054/00094 | Loss: 0.0655 | EWC Loss: 0.0278 | CE Loss: 0.0377\n",
      "Train Epoch: 033 Batch: 00055/00094 | Loss: 0.0948 | EWC Loss: 0.0278 | CE Loss: 0.0670\n",
      "Train Epoch: 033 Batch: 00056/00094 | Loss: 0.0775 | EWC Loss: 0.0278 | CE Loss: 0.0497\n",
      "Train Epoch: 033 Batch: 00057/00094 | Loss: 0.0858 | EWC Loss: 0.0278 | CE Loss: 0.0580\n",
      "Train Epoch: 033 Batch: 00058/00094 | Loss: 0.0588 | EWC Loss: 0.0278 | CE Loss: 0.0309\n",
      "Train Epoch: 033 Batch: 00059/00094 | Loss: 0.0451 | EWC Loss: 0.0278 | CE Loss: 0.0172\n",
      "Train Epoch: 033 Batch: 00060/00094 | Loss: 0.0584 | EWC Loss: 0.0278 | CE Loss: 0.0305\n",
      "Train Epoch: 033 Batch: 00061/00094 | Loss: 0.0971 | EWC Loss: 0.0278 | CE Loss: 0.0693\n",
      "Train Epoch: 033 Batch: 00062/00094 | Loss: 0.0615 | EWC Loss: 0.0278 | CE Loss: 0.0337\n",
      "Train Epoch: 033 Batch: 00063/00094 | Loss: 0.0745 | EWC Loss: 0.0277 | CE Loss: 0.0468\n",
      "Train Epoch: 033 Batch: 00064/00094 | Loss: 0.1307 | EWC Loss: 0.0277 | CE Loss: 0.1030\n",
      "Train Epoch: 033 Batch: 00065/00094 | Loss: 0.0728 | EWC Loss: 0.0277 | CE Loss: 0.0451\n",
      "Train Epoch: 033 Batch: 00066/00094 | Loss: 0.1682 | EWC Loss: 0.0277 | CE Loss: 0.1405\n",
      "Train Epoch: 033 Batch: 00067/00094 | Loss: 0.0678 | EWC Loss: 0.0277 | CE Loss: 0.0401\n",
      "Train Epoch: 033 Batch: 00068/00094 | Loss: 0.1910 | EWC Loss: 0.0277 | CE Loss: 0.1633\n",
      "Train Epoch: 033 Batch: 00069/00094 | Loss: 0.0664 | EWC Loss: 0.0278 | CE Loss: 0.0386\n",
      "Train Epoch: 033 Batch: 00070/00094 | Loss: 0.0899 | EWC Loss: 0.0278 | CE Loss: 0.0621\n",
      "Train Epoch: 033 Batch: 00071/00094 | Loss: 0.1016 | EWC Loss: 0.0278 | CE Loss: 0.0738\n",
      "Train Epoch: 033 Batch: 00072/00094 | Loss: 0.1309 | EWC Loss: 0.0278 | CE Loss: 0.1031\n",
      "Train Epoch: 033 Batch: 00073/00094 | Loss: 0.1022 | EWC Loss: 0.0277 | CE Loss: 0.0744\n",
      "Train Epoch: 033 Batch: 00074/00094 | Loss: 0.1426 | EWC Loss: 0.0277 | CE Loss: 0.1149\n",
      "Train Epoch: 033 Batch: 00075/00094 | Loss: 0.1027 | EWC Loss: 0.0277 | CE Loss: 0.0750\n",
      "Train Epoch: 033 Batch: 00076/00094 | Loss: 0.1575 | EWC Loss: 0.0277 | CE Loss: 0.1298\n",
      "Train Epoch: 033 Batch: 00077/00094 | Loss: 0.0601 | EWC Loss: 0.0277 | CE Loss: 0.0324\n",
      "Train Epoch: 033 Batch: 00078/00094 | Loss: 0.0689 | EWC Loss: 0.0277 | CE Loss: 0.0411\n",
      "Train Epoch: 033 Batch: 00079/00094 | Loss: 0.1092 | EWC Loss: 0.0277 | CE Loss: 0.0815\n",
      "Train Epoch: 033 Batch: 00080/00094 | Loss: 0.0456 | EWC Loss: 0.0277 | CE Loss: 0.0178\n",
      "Train Epoch: 033 Batch: 00081/00094 | Loss: 0.0422 | EWC Loss: 0.0277 | CE Loss: 0.0145\n",
      "Train Epoch: 033 Batch: 00082/00094 | Loss: 0.0639 | EWC Loss: 0.0277 | CE Loss: 0.0362\n",
      "Train Epoch: 033 Batch: 00083/00094 | Loss: 0.0992 | EWC Loss: 0.0277 | CE Loss: 0.0716\n",
      "Train Epoch: 033 Batch: 00084/00094 | Loss: 0.1344 | EWC Loss: 0.0277 | CE Loss: 0.1067\n",
      "Train Epoch: 033 Batch: 00085/00094 | Loss: 0.1293 | EWC Loss: 0.0277 | CE Loss: 0.1016\n",
      "Train Epoch: 033 Batch: 00086/00094 | Loss: 0.0699 | EWC Loss: 0.0277 | CE Loss: 0.0421\n",
      "Train Epoch: 033 Batch: 00087/00094 | Loss: 0.0625 | EWC Loss: 0.0278 | CE Loss: 0.0347\n",
      "Train Epoch: 033 Batch: 00088/00094 | Loss: 0.1058 | EWC Loss: 0.0278 | CE Loss: 0.0781\n",
      "Train Epoch: 033 Batch: 00089/00094 | Loss: 0.0702 | EWC Loss: 0.0278 | CE Loss: 0.0424\n",
      "Train Epoch: 033 Batch: 00090/00094 | Loss: 0.0955 | EWC Loss: 0.0278 | CE Loss: 0.0677\n",
      "Train Epoch: 033 Batch: 00091/00094 | Loss: 0.1526 | EWC Loss: 0.0278 | CE Loss: 0.1248\n",
      "Train Epoch: 033 Batch: 00092/00094 | Loss: 0.1248 | EWC Loss: 0.0277 | CE Loss: 0.0971\n",
      "Train Epoch: 033 Batch: 00093/00094 | Loss: 0.1066 | EWC Loss: 0.0277 | CE Loss: 0.0789\n",
      "Train Epoch: 033 Batch: 00094/00094 | Loss: 0.0948 | EWC Loss: 0.0278 | CE Loss: 0.0670\n",
      "Train Epoch: 033 |Acc 97.45000 | Loss: 0.09695 | Task Loss 0.06922 | EWC Loss: 0.02773\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0601 | acc:98.0000\n",
      "[VAL Acc] Target: 98.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.8384 | acc:48.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.9037 | acc:52.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.0321 | acc:43.8931\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.7756 | acc:55.4467\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 55.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.5432 | acc:79.7597\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 79.76%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.1881 | acc:67.7508\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 67.75%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0908 | acc:59.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 59.25%\n",
      "[VAL Acc] Avg 63.12%\n",
      "\n",
      "\n",
      "---------- Starting epoch 34 ----------\n",
      "Train Epoch: 034 Batch: 00001/00094 | Loss: 0.1357 | EWC Loss: 0.0277 | CE Loss: 0.1080\n",
      "Train Epoch: 034 Batch: 00002/00094 | Loss: 0.0812 | EWC Loss: 0.0277 | CE Loss: 0.0535\n",
      "Train Epoch: 034 Batch: 00003/00094 | Loss: 0.0864 | EWC Loss: 0.0277 | CE Loss: 0.0586\n",
      "Train Epoch: 034 Batch: 00004/00094 | Loss: 0.0982 | EWC Loss: 0.0277 | CE Loss: 0.0705\n",
      "Train Epoch: 034 Batch: 00005/00094 | Loss: 0.1056 | EWC Loss: 0.0277 | CE Loss: 0.0778\n",
      "Train Epoch: 034 Batch: 00006/00094 | Loss: 0.1712 | EWC Loss: 0.0277 | CE Loss: 0.1435\n",
      "Train Epoch: 034 Batch: 00007/00094 | Loss: 0.0661 | EWC Loss: 0.0277 | CE Loss: 0.0384\n",
      "Train Epoch: 034 Batch: 00008/00094 | Loss: 0.0647 | EWC Loss: 0.0277 | CE Loss: 0.0370\n",
      "Train Epoch: 034 Batch: 00009/00094 | Loss: 0.0761 | EWC Loss: 0.0277 | CE Loss: 0.0484\n",
      "Train Epoch: 034 Batch: 00010/00094 | Loss: 0.0950 | EWC Loss: 0.0277 | CE Loss: 0.0674\n",
      "Train Epoch: 034 Batch: 00011/00094 | Loss: 0.0913 | EWC Loss: 0.0277 | CE Loss: 0.0635\n",
      "Train Epoch: 034 Batch: 00012/00094 | Loss: 0.1807 | EWC Loss: 0.0278 | CE Loss: 0.1529\n",
      "Train Epoch: 034 Batch: 00013/00094 | Loss: 0.0994 | EWC Loss: 0.0278 | CE Loss: 0.0716\n",
      "Train Epoch: 034 Batch: 00014/00094 | Loss: 0.0774 | EWC Loss: 0.0278 | CE Loss: 0.0496\n",
      "Train Epoch: 034 Batch: 00015/00094 | Loss: 0.0953 | EWC Loss: 0.0278 | CE Loss: 0.0675\n",
      "Train Epoch: 034 Batch: 00016/00094 | Loss: 0.0646 | EWC Loss: 0.0278 | CE Loss: 0.0368\n",
      "Train Epoch: 034 Batch: 00017/00094 | Loss: 0.1356 | EWC Loss: 0.0278 | CE Loss: 0.1078\n",
      "Train Epoch: 034 Batch: 00018/00094 | Loss: 0.1171 | EWC Loss: 0.0277 | CE Loss: 0.0894\n",
      "Train Epoch: 034 Batch: 00019/00094 | Loss: 0.0536 | EWC Loss: 0.0277 | CE Loss: 0.0259\n",
      "Train Epoch: 034 Batch: 00020/00094 | Loss: 0.0765 | EWC Loss: 0.0277 | CE Loss: 0.0488\n",
      "Train Epoch: 034 Batch: 00021/00094 | Loss: 0.1947 | EWC Loss: 0.0277 | CE Loss: 0.1670\n",
      "Train Epoch: 034 Batch: 00022/00094 | Loss: 0.0691 | EWC Loss: 0.0277 | CE Loss: 0.0414\n",
      "Train Epoch: 034 Batch: 00023/00094 | Loss: 0.1006 | EWC Loss: 0.0277 | CE Loss: 0.0729\n",
      "Train Epoch: 034 Batch: 00024/00094 | Loss: 0.0478 | EWC Loss: 0.0277 | CE Loss: 0.0201\n",
      "Train Epoch: 034 Batch: 00025/00094 | Loss: 0.0390 | EWC Loss: 0.0276 | CE Loss: 0.0113\n",
      "Train Epoch: 034 Batch: 00026/00094 | Loss: 0.0973 | EWC Loss: 0.0276 | CE Loss: 0.0696\n",
      "Train Epoch: 034 Batch: 00027/00094 | Loss: 0.0775 | EWC Loss: 0.0276 | CE Loss: 0.0499\n",
      "Train Epoch: 034 Batch: 00028/00094 | Loss: 0.0547 | EWC Loss: 0.0276 | CE Loss: 0.0271\n",
      "Train Epoch: 034 Batch: 00029/00094 | Loss: 0.0508 | EWC Loss: 0.0276 | CE Loss: 0.0232\n",
      "Train Epoch: 034 Batch: 00030/00094 | Loss: 0.1226 | EWC Loss: 0.0276 | CE Loss: 0.0951\n",
      "Train Epoch: 034 Batch: 00031/00094 | Loss: 0.1117 | EWC Loss: 0.0276 | CE Loss: 0.0841\n",
      "Train Epoch: 034 Batch: 00032/00094 | Loss: 0.0511 | EWC Loss: 0.0277 | CE Loss: 0.0234\n",
      "Train Epoch: 034 Batch: 00033/00094 | Loss: 0.1167 | EWC Loss: 0.0277 | CE Loss: 0.0890\n",
      "Train Epoch: 034 Batch: 00034/00094 | Loss: 0.1198 | EWC Loss: 0.0278 | CE Loss: 0.0920\n",
      "Train Epoch: 034 Batch: 00035/00094 | Loss: 0.0426 | EWC Loss: 0.0278 | CE Loss: 0.0148\n",
      "Train Epoch: 034 Batch: 00036/00094 | Loss: 0.0826 | EWC Loss: 0.0278 | CE Loss: 0.0549\n",
      "Train Epoch: 034 Batch: 00037/00094 | Loss: 0.0554 | EWC Loss: 0.0277 | CE Loss: 0.0277\n",
      "Train Epoch: 034 Batch: 00038/00094 | Loss: 0.0937 | EWC Loss: 0.0277 | CE Loss: 0.0659\n",
      "Train Epoch: 034 Batch: 00039/00094 | Loss: 0.0685 | EWC Loss: 0.0277 | CE Loss: 0.0408\n",
      "Train Epoch: 034 Batch: 00040/00094 | Loss: 0.0412 | EWC Loss: 0.0276 | CE Loss: 0.0135\n",
      "Train Epoch: 034 Batch: 00041/00094 | Loss: 0.1426 | EWC Loss: 0.0276 | CE Loss: 0.1150\n",
      "Train Epoch: 034 Batch: 00042/00094 | Loss: 0.1254 | EWC Loss: 0.0276 | CE Loss: 0.0978\n",
      "Train Epoch: 034 Batch: 00043/00094 | Loss: 0.0783 | EWC Loss: 0.0277 | CE Loss: 0.0506\n",
      "Train Epoch: 034 Batch: 00044/00094 | Loss: 0.1165 | EWC Loss: 0.0276 | CE Loss: 0.0889\n",
      "Train Epoch: 034 Batch: 00045/00094 | Loss: 0.0522 | EWC Loss: 0.0275 | CE Loss: 0.0247\n",
      "Train Epoch: 034 Batch: 00046/00094 | Loss: 0.0728 | EWC Loss: 0.0275 | CE Loss: 0.0453\n",
      "Train Epoch: 034 Batch: 00047/00094 | Loss: 0.0586 | EWC Loss: 0.0275 | CE Loss: 0.0311\n",
      "Train Epoch: 034 Batch: 00048/00094 | Loss: 0.0913 | EWC Loss: 0.0274 | CE Loss: 0.0638\n",
      "Train Epoch: 034 Batch: 00049/00094 | Loss: 0.0732 | EWC Loss: 0.0275 | CE Loss: 0.0457\n",
      "Train Epoch: 034 Batch: 00050/00094 | Loss: 0.0700 | EWC Loss: 0.0275 | CE Loss: 0.0425\n",
      "Train Epoch: 034 Batch: 00051/00094 | Loss: 0.0835 | EWC Loss: 0.0275 | CE Loss: 0.0560\n",
      "Train Epoch: 034 Batch: 00052/00094 | Loss: 0.0718 | EWC Loss: 0.0275 | CE Loss: 0.0444\n",
      "Train Epoch: 034 Batch: 00053/00094 | Loss: 0.0956 | EWC Loss: 0.0275 | CE Loss: 0.0682\n",
      "Train Epoch: 034 Batch: 00054/00094 | Loss: 0.1759 | EWC Loss: 0.0275 | CE Loss: 0.1485\n",
      "Train Epoch: 034 Batch: 00055/00094 | Loss: 0.0905 | EWC Loss: 0.0275 | CE Loss: 0.0630\n",
      "Train Epoch: 034 Batch: 00056/00094 | Loss: 0.0508 | EWC Loss: 0.0275 | CE Loss: 0.0233\n",
      "Train Epoch: 034 Batch: 00057/00094 | Loss: 0.1858 | EWC Loss: 0.0275 | CE Loss: 0.1583\n",
      "Train Epoch: 034 Batch: 00058/00094 | Loss: 0.0772 | EWC Loss: 0.0275 | CE Loss: 0.0497\n",
      "Train Epoch: 034 Batch: 00059/00094 | Loss: 0.0756 | EWC Loss: 0.0275 | CE Loss: 0.0482\n",
      "Train Epoch: 034 Batch: 00060/00094 | Loss: 0.0846 | EWC Loss: 0.0275 | CE Loss: 0.0571\n",
      "Train Epoch: 034 Batch: 00061/00094 | Loss: 0.0648 | EWC Loss: 0.0275 | CE Loss: 0.0373\n",
      "Train Epoch: 034 Batch: 00062/00094 | Loss: 0.1863 | EWC Loss: 0.0274 | CE Loss: 0.1588\n",
      "Train Epoch: 034 Batch: 00063/00094 | Loss: 0.1743 | EWC Loss: 0.0274 | CE Loss: 0.1469\n",
      "Train Epoch: 034 Batch: 00064/00094 | Loss: 0.0955 | EWC Loss: 0.0275 | CE Loss: 0.0680\n",
      "Train Epoch: 034 Batch: 00065/00094 | Loss: 0.0480 | EWC Loss: 0.0276 | CE Loss: 0.0204\n",
      "Train Epoch: 034 Batch: 00066/00094 | Loss: 0.0433 | EWC Loss: 0.0276 | CE Loss: 0.0157\n",
      "Train Epoch: 034 Batch: 00067/00094 | Loss: 0.0587 | EWC Loss: 0.0276 | CE Loss: 0.0311\n",
      "Train Epoch: 034 Batch: 00068/00094 | Loss: 0.1026 | EWC Loss: 0.0275 | CE Loss: 0.0751\n",
      "Train Epoch: 034 Batch: 00069/00094 | Loss: 0.1142 | EWC Loss: 0.0275 | CE Loss: 0.0866\n",
      "Train Epoch: 034 Batch: 00070/00094 | Loss: 0.0840 | EWC Loss: 0.0275 | CE Loss: 0.0565\n",
      "Train Epoch: 034 Batch: 00071/00094 | Loss: 0.0775 | EWC Loss: 0.0276 | CE Loss: 0.0500\n",
      "Train Epoch: 034 Batch: 00072/00094 | Loss: 0.0772 | EWC Loss: 0.0276 | CE Loss: 0.0495\n",
      "Train Epoch: 034 Batch: 00073/00094 | Loss: 0.0625 | EWC Loss: 0.0276 | CE Loss: 0.0349\n",
      "Train Epoch: 034 Batch: 00074/00094 | Loss: 0.1180 | EWC Loss: 0.0276 | CE Loss: 0.0904\n",
      "Train Epoch: 034 Batch: 00075/00094 | Loss: 0.1041 | EWC Loss: 0.0276 | CE Loss: 0.0764\n",
      "Train Epoch: 034 Batch: 00076/00094 | Loss: 0.0755 | EWC Loss: 0.0276 | CE Loss: 0.0478\n",
      "Train Epoch: 034 Batch: 00077/00094 | Loss: 0.0808 | EWC Loss: 0.0276 | CE Loss: 0.0532\n",
      "Train Epoch: 034 Batch: 00078/00094 | Loss: 0.0410 | EWC Loss: 0.0277 | CE Loss: 0.0134\n",
      "Train Epoch: 034 Batch: 00079/00094 | Loss: 0.1137 | EWC Loss: 0.0277 | CE Loss: 0.0861\n",
      "Train Epoch: 034 Batch: 00080/00094 | Loss: 0.1070 | EWC Loss: 0.0277 | CE Loss: 0.0793\n",
      "Train Epoch: 034 Batch: 00081/00094 | Loss: 0.1118 | EWC Loss: 0.0277 | CE Loss: 0.0841\n",
      "Train Epoch: 034 Batch: 00082/00094 | Loss: 0.0718 | EWC Loss: 0.0277 | CE Loss: 0.0441\n",
      "Train Epoch: 034 Batch: 00083/00094 | Loss: 0.0989 | EWC Loss: 0.0278 | CE Loss: 0.0711\n",
      "Train Epoch: 034 Batch: 00084/00094 | Loss: 0.1398 | EWC Loss: 0.0278 | CE Loss: 0.1120\n",
      "Train Epoch: 034 Batch: 00085/00094 | Loss: 0.1094 | EWC Loss: 0.0278 | CE Loss: 0.0816\n",
      "Train Epoch: 034 Batch: 00086/00094 | Loss: 0.0895 | EWC Loss: 0.0278 | CE Loss: 0.0617\n",
      "Train Epoch: 034 Batch: 00087/00094 | Loss: 0.0725 | EWC Loss: 0.0277 | CE Loss: 0.0448\n",
      "Train Epoch: 034 Batch: 00088/00094 | Loss: 0.0987 | EWC Loss: 0.0277 | CE Loss: 0.0710\n",
      "Train Epoch: 034 Batch: 00089/00094 | Loss: 0.0498 | EWC Loss: 0.0277 | CE Loss: 0.0222\n",
      "Train Epoch: 034 Batch: 00090/00094 | Loss: 0.0772 | EWC Loss: 0.0276 | CE Loss: 0.0496\n",
      "Train Epoch: 034 Batch: 00091/00094 | Loss: 0.1187 | EWC Loss: 0.0276 | CE Loss: 0.0911\n",
      "Train Epoch: 034 Batch: 00092/00094 | Loss: 0.0720 | EWC Loss: 0.0276 | CE Loss: 0.0443\n",
      "Train Epoch: 034 Batch: 00093/00094 | Loss: 0.1286 | EWC Loss: 0.0276 | CE Loss: 0.1010\n",
      "Train Epoch: 034 Batch: 00094/00094 | Loss: 0.0532 | EWC Loss: 0.0277 | CE Loss: 0.0256\n",
      "Train Epoch: 034 |Acc 97.60000 | Loss: 0.09151 | Task Loss 0.06388 | EWC Loss: 0.02764\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0680 | acc:97.5500\n",
      "[VAL Acc] Target: 97.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:3.1643 | acc:48.4500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.45%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:2.0538 | acc:51.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 51.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.1885 | acc:45.0382\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.1207 | acc:53.9577\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 53.96%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4494 | acc:82.6248\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 82.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.4058 | acc:65.8699\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.87%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1130 | acc:57.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 57.38%\n",
      "[VAL Acc] Avg 62.76%\n",
      "\n",
      "\n",
      "---------- Starting epoch 35 ----------\n",
      "Train Epoch: 035 Batch: 00001/00094 | Loss: 0.0789 | EWC Loss: 0.0276 | CE Loss: 0.0513\n",
      "Train Epoch: 035 Batch: 00002/00094 | Loss: 0.0794 | EWC Loss: 0.0276 | CE Loss: 0.0517\n",
      "Train Epoch: 035 Batch: 00003/00094 | Loss: 0.1376 | EWC Loss: 0.0276 | CE Loss: 0.1100\n",
      "Train Epoch: 035 Batch: 00004/00094 | Loss: 0.0874 | EWC Loss: 0.0275 | CE Loss: 0.0599\n",
      "Train Epoch: 035 Batch: 00005/00094 | Loss: 0.2618 | EWC Loss: 0.0276 | CE Loss: 0.2342\n",
      "Train Epoch: 035 Batch: 00006/00094 | Loss: 0.1038 | EWC Loss: 0.0279 | CE Loss: 0.0760\n",
      "Train Epoch: 035 Batch: 00007/00094 | Loss: 0.1053 | EWC Loss: 0.0278 | CE Loss: 0.0775\n",
      "Train Epoch: 035 Batch: 00008/00094 | Loss: 0.0776 | EWC Loss: 0.0277 | CE Loss: 0.0499\n",
      "Train Epoch: 035 Batch: 00009/00094 | Loss: 0.0591 | EWC Loss: 0.0277 | CE Loss: 0.0315\n",
      "Train Epoch: 035 Batch: 00010/00094 | Loss: 0.0957 | EWC Loss: 0.0276 | CE Loss: 0.0681\n",
      "Train Epoch: 035 Batch: 00011/00094 | Loss: 0.1590 | EWC Loss: 0.0277 | CE Loss: 0.1313\n",
      "Train Epoch: 035 Batch: 00012/00094 | Loss: 0.0848 | EWC Loss: 0.0277 | CE Loss: 0.0570\n",
      "Train Epoch: 035 Batch: 00013/00094 | Loss: 0.0507 | EWC Loss: 0.0278 | CE Loss: 0.0230\n",
      "Train Epoch: 035 Batch: 00014/00094 | Loss: 0.0817 | EWC Loss: 0.0277 | CE Loss: 0.0540\n",
      "Train Epoch: 035 Batch: 00015/00094 | Loss: 0.0502 | EWC Loss: 0.0277 | CE Loss: 0.0226\n",
      "Train Epoch: 035 Batch: 00016/00094 | Loss: 0.0553 | EWC Loss: 0.0276 | CE Loss: 0.0276\n",
      "Train Epoch: 035 Batch: 00017/00094 | Loss: 0.1225 | EWC Loss: 0.0276 | CE Loss: 0.0949\n",
      "Train Epoch: 035 Batch: 00018/00094 | Loss: 0.0989 | EWC Loss: 0.0276 | CE Loss: 0.0714\n",
      "Train Epoch: 035 Batch: 00019/00094 | Loss: 0.0799 | EWC Loss: 0.0277 | CE Loss: 0.0522\n",
      "Train Epoch: 035 Batch: 00020/00094 | Loss: 0.0905 | EWC Loss: 0.0278 | CE Loss: 0.0627\n",
      "Train Epoch: 035 Batch: 00021/00094 | Loss: 0.0684 | EWC Loss: 0.0277 | CE Loss: 0.0407\n",
      "Train Epoch: 035 Batch: 00022/00094 | Loss: 0.1881 | EWC Loss: 0.0277 | CE Loss: 0.1605\n",
      "Train Epoch: 035 Batch: 00023/00094 | Loss: 0.2947 | EWC Loss: 0.0276 | CE Loss: 0.2671\n",
      "Train Epoch: 035 Batch: 00024/00094 | Loss: 0.0724 | EWC Loss: 0.0277 | CE Loss: 0.0446\n",
      "Train Epoch: 035 Batch: 00025/00094 | Loss: 0.0402 | EWC Loss: 0.0277 | CE Loss: 0.0125\n",
      "Train Epoch: 035 Batch: 00026/00094 | Loss: 0.0907 | EWC Loss: 0.0276 | CE Loss: 0.0630\n",
      "Train Epoch: 035 Batch: 00027/00094 | Loss: 0.0532 | EWC Loss: 0.0276 | CE Loss: 0.0256\n",
      "Train Epoch: 035 Batch: 00028/00094 | Loss: 0.1061 | EWC Loss: 0.0276 | CE Loss: 0.0785\n",
      "Train Epoch: 035 Batch: 00029/00094 | Loss: 0.1019 | EWC Loss: 0.0277 | CE Loss: 0.0742\n",
      "Train Epoch: 035 Batch: 00030/00094 | Loss: 0.0902 | EWC Loss: 0.0276 | CE Loss: 0.0626\n",
      "Train Epoch: 035 Batch: 00031/00094 | Loss: 0.1597 | EWC Loss: 0.0276 | CE Loss: 0.1322\n",
      "Train Epoch: 035 Batch: 00032/00094 | Loss: 0.0883 | EWC Loss: 0.0276 | CE Loss: 0.0606\n",
      "Train Epoch: 035 Batch: 00033/00094 | Loss: 0.1048 | EWC Loss: 0.0276 | CE Loss: 0.0772\n",
      "Train Epoch: 035 Batch: 00034/00094 | Loss: 0.0521 | EWC Loss: 0.0278 | CE Loss: 0.0243\n",
      "Train Epoch: 035 Batch: 00035/00094 | Loss: 0.0643 | EWC Loss: 0.0277 | CE Loss: 0.0366\n",
      "Train Epoch: 035 Batch: 00036/00094 | Loss: 0.0994 | EWC Loss: 0.0277 | CE Loss: 0.0716\n",
      "Train Epoch: 035 Batch: 00037/00094 | Loss: 0.0740 | EWC Loss: 0.0277 | CE Loss: 0.0463\n",
      "Train Epoch: 035 Batch: 00038/00094 | Loss: 0.1240 | EWC Loss: 0.0277 | CE Loss: 0.0962\n",
      "Train Epoch: 035 Batch: 00039/00094 | Loss: 0.1092 | EWC Loss: 0.0277 | CE Loss: 0.0815\n",
      "Train Epoch: 035 Batch: 00040/00094 | Loss: 0.0442 | EWC Loss: 0.0278 | CE Loss: 0.0164\n",
      "Train Epoch: 035 Batch: 00041/00094 | Loss: 0.1121 | EWC Loss: 0.0278 | CE Loss: 0.0844\n",
      "Train Epoch: 035 Batch: 00042/00094 | Loss: 0.0660 | EWC Loss: 0.0278 | CE Loss: 0.0382\n",
      "Train Epoch: 035 Batch: 00043/00094 | Loss: 0.0531 | EWC Loss: 0.0278 | CE Loss: 0.0253\n",
      "Train Epoch: 035 Batch: 00044/00094 | Loss: 0.0489 | EWC Loss: 0.0278 | CE Loss: 0.0211\n",
      "Train Epoch: 035 Batch: 00045/00094 | Loss: 0.1018 | EWC Loss: 0.0278 | CE Loss: 0.0740\n",
      "Train Epoch: 035 Batch: 00046/00094 | Loss: 0.0613 | EWC Loss: 0.0279 | CE Loss: 0.0335\n",
      "Train Epoch: 035 Batch: 00047/00094 | Loss: 0.0754 | EWC Loss: 0.0278 | CE Loss: 0.0475\n",
      "Train Epoch: 035 Batch: 00048/00094 | Loss: 0.0561 | EWC Loss: 0.0279 | CE Loss: 0.0282\n",
      "Train Epoch: 035 Batch: 00049/00094 | Loss: 0.0685 | EWC Loss: 0.0278 | CE Loss: 0.0407\n",
      "Train Epoch: 035 Batch: 00050/00094 | Loss: 0.0794 | EWC Loss: 0.0278 | CE Loss: 0.0516\n",
      "Train Epoch: 035 Batch: 00051/00094 | Loss: 0.0691 | EWC Loss: 0.0277 | CE Loss: 0.0413\n",
      "Train Epoch: 035 Batch: 00052/00094 | Loss: 0.1385 | EWC Loss: 0.0277 | CE Loss: 0.1107\n",
      "Train Epoch: 035 Batch: 00053/00094 | Loss: 0.1141 | EWC Loss: 0.0278 | CE Loss: 0.0862\n",
      "Train Epoch: 035 Batch: 00054/00094 | Loss: 0.1171 | EWC Loss: 0.0279 | CE Loss: 0.0892\n",
      "Train Epoch: 035 Batch: 00055/00094 | Loss: 0.1365 | EWC Loss: 0.0280 | CE Loss: 0.1085\n",
      "Train Epoch: 035 Batch: 00056/00094 | Loss: 0.1378 | EWC Loss: 0.0280 | CE Loss: 0.1097\n",
      "Train Epoch: 035 Batch: 00057/00094 | Loss: 0.0431 | EWC Loss: 0.0281 | CE Loss: 0.0150\n",
      "Train Epoch: 035 Batch: 00058/00094 | Loss: 0.1174 | EWC Loss: 0.0280 | CE Loss: 0.0894\n",
      "Train Epoch: 035 Batch: 00059/00094 | Loss: 0.0570 | EWC Loss: 0.0281 | CE Loss: 0.0289\n",
      "Train Epoch: 035 Batch: 00060/00094 | Loss: 0.3560 | EWC Loss: 0.0281 | CE Loss: 0.3279\n",
      "Train Epoch: 035 Batch: 00061/00094 | Loss: 0.1773 | EWC Loss: 0.0279 | CE Loss: 0.1494\n",
      "Train Epoch: 035 Batch: 00062/00094 | Loss: 0.0386 | EWC Loss: 0.0280 | CE Loss: 0.0106\n",
      "Train Epoch: 035 Batch: 00063/00094 | Loss: 0.0425 | EWC Loss: 0.0280 | CE Loss: 0.0145\n",
      "Train Epoch: 035 Batch: 00064/00094 | Loss: 0.0545 | EWC Loss: 0.0279 | CE Loss: 0.0266\n",
      "Train Epoch: 035 Batch: 00065/00094 | Loss: 0.0942 | EWC Loss: 0.0279 | CE Loss: 0.0663\n",
      "Train Epoch: 035 Batch: 00066/00094 | Loss: 0.0515 | EWC Loss: 0.0279 | CE Loss: 0.0236\n",
      "Train Epoch: 035 Batch: 00067/00094 | Loss: 0.1212 | EWC Loss: 0.0279 | CE Loss: 0.0933\n",
      "Train Epoch: 035 Batch: 00068/00094 | Loss: 0.0389 | EWC Loss: 0.0280 | CE Loss: 0.0109\n",
      "Train Epoch: 035 Batch: 00069/00094 | Loss: 0.0829 | EWC Loss: 0.0280 | CE Loss: 0.0549\n",
      "Train Epoch: 035 Batch: 00070/00094 | Loss: 0.0746 | EWC Loss: 0.0279 | CE Loss: 0.0467\n",
      "Train Epoch: 035 Batch: 00071/00094 | Loss: 0.0582 | EWC Loss: 0.0279 | CE Loss: 0.0303\n",
      "Train Epoch: 035 Batch: 00072/00094 | Loss: 0.0865 | EWC Loss: 0.0278 | CE Loss: 0.0587\n",
      "Train Epoch: 035 Batch: 00073/00094 | Loss: 0.0752 | EWC Loss: 0.0279 | CE Loss: 0.0473\n",
      "Train Epoch: 035 Batch: 00074/00094 | Loss: 0.1657 | EWC Loss: 0.0279 | CE Loss: 0.1378\n",
      "Train Epoch: 035 Batch: 00075/00094 | Loss: 0.0528 | EWC Loss: 0.0278 | CE Loss: 0.0250\n",
      "Train Epoch: 035 Batch: 00076/00094 | Loss: 0.0725 | EWC Loss: 0.0278 | CE Loss: 0.0447\n",
      "Train Epoch: 035 Batch: 00077/00094 | Loss: 0.0733 | EWC Loss: 0.0278 | CE Loss: 0.0455\n",
      "Train Epoch: 035 Batch: 00078/00094 | Loss: 0.0567 | EWC Loss: 0.0277 | CE Loss: 0.0290\n",
      "Train Epoch: 035 Batch: 00079/00094 | Loss: 0.0676 | EWC Loss: 0.0277 | CE Loss: 0.0398\n",
      "Train Epoch: 035 Batch: 00080/00094 | Loss: 0.0807 | EWC Loss: 0.0276 | CE Loss: 0.0530\n",
      "Train Epoch: 035 Batch: 00081/00094 | Loss: 0.0689 | EWC Loss: 0.0277 | CE Loss: 0.0412\n",
      "Train Epoch: 035 Batch: 00082/00094 | Loss: 0.0828 | EWC Loss: 0.0277 | CE Loss: 0.0551\n",
      "Train Epoch: 035 Batch: 00083/00094 | Loss: 0.0880 | EWC Loss: 0.0277 | CE Loss: 0.0602\n",
      "Train Epoch: 035 Batch: 00084/00094 | Loss: 0.1738 | EWC Loss: 0.0277 | CE Loss: 0.1462\n",
      "Train Epoch: 035 Batch: 00085/00094 | Loss: 0.1269 | EWC Loss: 0.0278 | CE Loss: 0.0991\n",
      "Train Epoch: 035 Batch: 00086/00094 | Loss: 0.1281 | EWC Loss: 0.0278 | CE Loss: 0.1003\n",
      "Train Epoch: 035 Batch: 00087/00094 | Loss: 0.0670 | EWC Loss: 0.0278 | CE Loss: 0.0391\n",
      "Train Epoch: 035 Batch: 00088/00094 | Loss: 0.0539 | EWC Loss: 0.0278 | CE Loss: 0.0260\n",
      "Train Epoch: 035 Batch: 00089/00094 | Loss: 0.1659 | EWC Loss: 0.0278 | CE Loss: 0.1381\n",
      "Train Epoch: 035 Batch: 00090/00094 | Loss: 0.0673 | EWC Loss: 0.0277 | CE Loss: 0.0396\n",
      "Train Epoch: 035 Batch: 00091/00094 | Loss: 0.0832 | EWC Loss: 0.0277 | CE Loss: 0.0555\n",
      "Train Epoch: 035 Batch: 00092/00094 | Loss: 0.0882 | EWC Loss: 0.0276 | CE Loss: 0.0606\n",
      "Train Epoch: 035 Batch: 00093/00094 | Loss: 0.1166 | EWC Loss: 0.0277 | CE Loss: 0.0889\n",
      "Train Epoch: 035 Batch: 00094/00094 | Loss: 0.0742 | EWC Loss: 0.0277 | CE Loss: 0.0466\n",
      "Train Epoch: 035 |Acc 97.63333 | Loss: 0.09516 | Task Loss 0.06739 | EWC Loss: 0.02777\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0565 | acc:98.2500\n",
      "[VAL Acc] Target: 98.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.9684 | acc:48.3500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 48.35%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.8986 | acc:52.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 52.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.9653 | acc:45.0382\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.04%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.0686 | acc:54.0752\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.4628 | acc:80.5915\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 80.59%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.2919 | acc:66.6536\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 66.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0995 | acc:58.3125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 58.31%\n",
      "[VAL Acc] Avg 62.96%\n",
      "\n",
      "\n",
      "---------- Starting epoch 36 ----------\n",
      "Train Epoch: 036 Batch: 00001/00094 | Loss: 0.0499 | EWC Loss: 0.0277 | CE Loss: 0.0222\n",
      "Train Epoch: 036 Batch: 00002/00094 | Loss: 0.0657 | EWC Loss: 0.0276 | CE Loss: 0.0380\n",
      "Train Epoch: 036 Batch: 00003/00094 | Loss: 0.1176 | EWC Loss: 0.0276 | CE Loss: 0.0899\n",
      "Train Epoch: 036 Batch: 00004/00094 | Loss: 0.0743 | EWC Loss: 0.0277 | CE Loss: 0.0466\n",
      "Train Epoch: 036 Batch: 00005/00094 | Loss: 0.0458 | EWC Loss: 0.0277 | CE Loss: 0.0181\n",
      "Train Epoch: 036 Batch: 00006/00094 | Loss: 0.0664 | EWC Loss: 0.0277 | CE Loss: 0.0387\n",
      "Train Epoch: 036 Batch: 00007/00094 | Loss: 0.0532 | EWC Loss: 0.0276 | CE Loss: 0.0256\n",
      "Train Epoch: 036 Batch: 00008/00094 | Loss: 0.0502 | EWC Loss: 0.0276 | CE Loss: 0.0226\n",
      "Train Epoch: 036 Batch: 00009/00094 | Loss: 0.0621 | EWC Loss: 0.0275 | CE Loss: 0.0346\n",
      "Train Epoch: 036 Batch: 00010/00094 | Loss: 0.1154 | EWC Loss: 0.0276 | CE Loss: 0.0878\n",
      "Train Epoch: 036 Batch: 00011/00094 | Loss: 0.0929 | EWC Loss: 0.0277 | CE Loss: 0.0652\n",
      "Train Epoch: 036 Batch: 00012/00094 | Loss: 0.0891 | EWC Loss: 0.0277 | CE Loss: 0.0615\n",
      "Train Epoch: 036 Batch: 00013/00094 | Loss: 0.0780 | EWC Loss: 0.0277 | CE Loss: 0.0503\n",
      "Train Epoch: 036 Batch: 00014/00094 | Loss: 0.1812 | EWC Loss: 0.0278 | CE Loss: 0.1534\n",
      "Train Epoch: 036 Batch: 00015/00094 | Loss: 0.1211 | EWC Loss: 0.0275 | CE Loss: 0.0936\n",
      "Train Epoch: 036 Batch: 00016/00094 | Loss: 0.0729 | EWC Loss: 0.0276 | CE Loss: 0.0453\n",
      "Train Epoch: 036 Batch: 00017/00094 | Loss: 0.0961 | EWC Loss: 0.0276 | CE Loss: 0.0685\n",
      "Train Epoch: 036 Batch: 00018/00094 | Loss: 0.0619 | EWC Loss: 0.0276 | CE Loss: 0.0343\n",
      "Train Epoch: 036 Batch: 00019/00094 | Loss: 0.0459 | EWC Loss: 0.0276 | CE Loss: 0.0183\n",
      "Train Epoch: 036 Batch: 00020/00094 | Loss: 0.0542 | EWC Loss: 0.0276 | CE Loss: 0.0265\n",
      "Train Epoch: 036 Batch: 00021/00094 | Loss: 0.1037 | EWC Loss: 0.0276 | CE Loss: 0.0761\n",
      "Train Epoch: 036 Batch: 00022/00094 | Loss: 0.1363 | EWC Loss: 0.0277 | CE Loss: 0.1086\n",
      "Train Epoch: 036 Batch: 00023/00094 | Loss: 0.0480 | EWC Loss: 0.0277 | CE Loss: 0.0203\n",
      "Train Epoch: 036 Batch: 00024/00094 | Loss: 0.0447 | EWC Loss: 0.0277 | CE Loss: 0.0171\n",
      "Train Epoch: 036 Batch: 00025/00094 | Loss: 0.1512 | EWC Loss: 0.0276 | CE Loss: 0.1236\n",
      "Train Epoch: 036 Batch: 00026/00094 | Loss: 0.1172 | EWC Loss: 0.0275 | CE Loss: 0.0897\n",
      "Train Epoch: 036 Batch: 00027/00094 | Loss: 0.0490 | EWC Loss: 0.0274 | CE Loss: 0.0217\n",
      "Train Epoch: 036 Batch: 00028/00094 | Loss: 0.0583 | EWC Loss: 0.0274 | CE Loss: 0.0310\n",
      "Train Epoch: 036 Batch: 00029/00094 | Loss: 0.0988 | EWC Loss: 0.0274 | CE Loss: 0.0714\n",
      "Train Epoch: 036 Batch: 00030/00094 | Loss: 0.1101 | EWC Loss: 0.0275 | CE Loss: 0.0826\n",
      "Train Epoch: 036 Batch: 00031/00094 | Loss: 0.0934 | EWC Loss: 0.0275 | CE Loss: 0.0660\n",
      "Train Epoch: 036 Batch: 00032/00094 | Loss: 0.1865 | EWC Loss: 0.0276 | CE Loss: 0.1590\n",
      "Train Epoch: 036 Batch: 00033/00094 | Loss: 0.0809 | EWC Loss: 0.0277 | CE Loss: 0.0532\n",
      "Train Epoch: 036 Batch: 00034/00094 | Loss: 0.0869 | EWC Loss: 0.0276 | CE Loss: 0.0593\n",
      "Train Epoch: 036 Batch: 00035/00094 | Loss: 0.0502 | EWC Loss: 0.0275 | CE Loss: 0.0226\n",
      "Train Epoch: 036 Batch: 00036/00094 | Loss: 0.1498 | EWC Loss: 0.0275 | CE Loss: 0.1224\n",
      "Train Epoch: 036 Batch: 00037/00094 | Loss: 0.0511 | EWC Loss: 0.0275 | CE Loss: 0.0236\n",
      "Train Epoch: 036 Batch: 00038/00094 | Loss: 0.0662 | EWC Loss: 0.0274 | CE Loss: 0.0388\n",
      "Train Epoch: 036 Batch: 00039/00094 | Loss: 0.0505 | EWC Loss: 0.0275 | CE Loss: 0.0230\n",
      "Train Epoch: 036 Batch: 00040/00094 | Loss: 0.0485 | EWC Loss: 0.0274 | CE Loss: 0.0211\n",
      "Train Epoch: 036 Batch: 00041/00094 | Loss: 0.1857 | EWC Loss: 0.0274 | CE Loss: 0.1583\n",
      "Train Epoch: 036 Batch: 00042/00094 | Loss: 0.2029 | EWC Loss: 0.0272 | CE Loss: 0.1757\n",
      "Train Epoch: 036 Batch: 00043/00094 | Loss: 0.0600 | EWC Loss: 0.0271 | CE Loss: 0.0329\n",
      "Train Epoch: 036 Batch: 00044/00094 | Loss: 0.1176 | EWC Loss: 0.0270 | CE Loss: 0.0906\n",
      "Train Epoch: 036 Batch: 00045/00094 | Loss: 0.0644 | EWC Loss: 0.0271 | CE Loss: 0.0373\n",
      "Train Epoch: 036 Batch: 00046/00094 | Loss: 0.0827 | EWC Loss: 0.0271 | CE Loss: 0.0556\n",
      "Train Epoch: 036 Batch: 00047/00094 | Loss: 0.0596 | EWC Loss: 0.0271 | CE Loss: 0.0325\n",
      "Train Epoch: 036 Batch: 00048/00094 | Loss: 0.0717 | EWC Loss: 0.0270 | CE Loss: 0.0447\n",
      "Train Epoch: 036 Batch: 00049/00094 | Loss: 0.0987 | EWC Loss: 0.0270 | CE Loss: 0.0716\n",
      "Train Epoch: 036 Batch: 00050/00094 | Loss: 0.1563 | EWC Loss: 0.0270 | CE Loss: 0.1293\n",
      "Train Epoch: 036 Batch: 00051/00094 | Loss: 0.0859 | EWC Loss: 0.0271 | CE Loss: 0.0588\n",
      "Train Epoch: 036 Batch: 00052/00094 | Loss: 0.0646 | EWC Loss: 0.0271 | CE Loss: 0.0375\n",
      "Train Epoch: 036 Batch: 00053/00094 | Loss: 0.0567 | EWC Loss: 0.0271 | CE Loss: 0.0296\n",
      "Train Epoch: 036 Batch: 00054/00094 | Loss: 0.0752 | EWC Loss: 0.0270 | CE Loss: 0.0481\n",
      "Train Epoch: 036 Batch: 00055/00094 | Loss: 0.1016 | EWC Loss: 0.0270 | CE Loss: 0.0745\n",
      "Train Epoch: 036 Batch: 00056/00094 | Loss: 0.1516 | EWC Loss: 0.0271 | CE Loss: 0.1245\n",
      "Train Epoch: 036 Batch: 00057/00094 | Loss: 0.1776 | EWC Loss: 0.0272 | CE Loss: 0.1504\n",
      "Train Epoch: 036 Batch: 00058/00094 | Loss: 0.0533 | EWC Loss: 0.0273 | CE Loss: 0.0261\n",
      "Train Epoch: 036 Batch: 00059/00094 | Loss: 0.0794 | EWC Loss: 0.0272 | CE Loss: 0.0522\n",
      "Train Epoch: 036 Batch: 00060/00094 | Loss: 0.1083 | EWC Loss: 0.0272 | CE Loss: 0.0811\n",
      "Train Epoch: 036 Batch: 00061/00094 | Loss: 0.0609 | EWC Loss: 0.0271 | CE Loss: 0.0337\n",
      "Train Epoch: 036 Batch: 00062/00094 | Loss: 0.0625 | EWC Loss: 0.0271 | CE Loss: 0.0354\n",
      "Train Epoch: 036 Batch: 00063/00094 | Loss: 0.0529 | EWC Loss: 0.0271 | CE Loss: 0.0258\n",
      "Train Epoch: 036 Batch: 00064/00094 | Loss: 0.1915 | EWC Loss: 0.0270 | CE Loss: 0.1645\n",
      "Train Epoch: 036 Batch: 00065/00094 | Loss: 0.0671 | EWC Loss: 0.0272 | CE Loss: 0.0399\n",
      "Train Epoch: 036 Batch: 00066/00094 | Loss: 0.0497 | EWC Loss: 0.0271 | CE Loss: 0.0226\n",
      "Train Epoch: 036 Batch: 00067/00094 | Loss: 0.1322 | EWC Loss: 0.0270 | CE Loss: 0.1052\n",
      "Train Epoch: 036 Batch: 00068/00094 | Loss: 0.0754 | EWC Loss: 0.0272 | CE Loss: 0.0483\n",
      "Train Epoch: 036 Batch: 00069/00094 | Loss: 0.1864 | EWC Loss: 0.0272 | CE Loss: 0.1592\n",
      "Train Epoch: 036 Batch: 00070/00094 | Loss: 0.0613 | EWC Loss: 0.0273 | CE Loss: 0.0340\n",
      "Train Epoch: 036 Batch: 00071/00094 | Loss: 0.1320 | EWC Loss: 0.0272 | CE Loss: 0.1048\n",
      "Train Epoch: 036 Batch: 00072/00094 | Loss: 0.0905 | EWC Loss: 0.0273 | CE Loss: 0.0632\n",
      "Train Epoch: 036 Batch: 00073/00094 | Loss: 0.1485 | EWC Loss: 0.0275 | CE Loss: 0.1211\n",
      "Train Epoch: 036 Batch: 00074/00094 | Loss: 0.2037 | EWC Loss: 0.0274 | CE Loss: 0.1764\n",
      "Train Epoch: 036 Batch: 00075/00094 | Loss: 0.2093 | EWC Loss: 0.0275 | CE Loss: 0.1818\n",
      "Train Epoch: 036 Batch: 00076/00094 | Loss: 0.0860 | EWC Loss: 0.0273 | CE Loss: 0.0587\n",
      "Train Epoch: 036 Batch: 00077/00094 | Loss: 0.0760 | EWC Loss: 0.0273 | CE Loss: 0.0488\n",
      "Train Epoch: 036 Batch: 00078/00094 | Loss: 0.1240 | EWC Loss: 0.0273 | CE Loss: 0.0967\n",
      "Train Epoch: 036 Batch: 00079/00094 | Loss: 0.0684 | EWC Loss: 0.0271 | CE Loss: 0.0412\n",
      "Train Epoch: 036 Batch: 00080/00094 | Loss: 0.1524 | EWC Loss: 0.0271 | CE Loss: 0.1254\n",
      "Train Epoch: 036 Batch: 00081/00094 | Loss: 0.1522 | EWC Loss: 0.0272 | CE Loss: 0.1249\n",
      "Train Epoch: 036 Batch: 00082/00094 | Loss: 0.1157 | EWC Loss: 0.0274 | CE Loss: 0.0883\n",
      "Train Epoch: 036 Batch: 00083/00094 | Loss: 0.2141 | EWC Loss: 0.0274 | CE Loss: 0.1867\n",
      "Train Epoch: 036 Batch: 00084/00094 | Loss: 0.0763 | EWC Loss: 0.0276 | CE Loss: 0.0487\n",
      "Train Epoch: 036 Batch: 00085/00094 | Loss: 0.0533 | EWC Loss: 0.0277 | CE Loss: 0.0256\n",
      "Train Epoch: 036 Batch: 00086/00094 | Loss: 0.0585 | EWC Loss: 0.0276 | CE Loss: 0.0309\n",
      "Train Epoch: 036 Batch: 00087/00094 | Loss: 0.0644 | EWC Loss: 0.0276 | CE Loss: 0.0367\n",
      "Train Epoch: 036 Batch: 00088/00094 | Loss: 0.1439 | EWC Loss: 0.0276 | CE Loss: 0.1163\n",
      "Train Epoch: 036 Batch: 00089/00094 | Loss: 0.0752 | EWC Loss: 0.0274 | CE Loss: 0.0477\n",
      "Train Epoch: 036 Batch: 00090/00094 | Loss: 0.1344 | EWC Loss: 0.0274 | CE Loss: 0.1071\n",
      "Train Epoch: 036 Batch: 00091/00094 | Loss: 0.1116 | EWC Loss: 0.0273 | CE Loss: 0.0842\n",
      "Train Epoch: 036 Batch: 00092/00094 | Loss: 0.0835 | EWC Loss: 0.0276 | CE Loss: 0.0559\n",
      "Train Epoch: 036 Batch: 00093/00094 | Loss: 0.0921 | EWC Loss: 0.0277 | CE Loss: 0.0644\n",
      "Train Epoch: 036 Batch: 00094/00094 | Loss: 0.0729 | EWC Loss: 0.0276 | CE Loss: 0.0453\n",
      "Train Epoch: 036 |Acc 97.58333 | Loss: 0.09689 | Task Loss 0.06950 | EWC Loss: 0.02739\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0882 | acc:96.6500\n",
      "[VAL Acc] Target: 96.65%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.5711 | acc:49.0500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.05%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.6854 | acc:56.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 56.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8701 | acc:45.4198\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 45.42%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.5453 | acc:59.1693\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 59.17%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7107 | acc:75.8780\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 75.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.0463 | acc:69.1614\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 69.16%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.3430 | acc:55.8125\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 55.81%\n",
      "[VAL Acc] Avg 63.41%\n",
      "\n",
      "\n",
      "---------- Starting epoch 37 ----------\n",
      "Train Epoch: 037 Batch: 00001/00094 | Loss: 0.0595 | EWC Loss: 0.0276 | CE Loss: 0.0319\n",
      "Train Epoch: 037 Batch: 00002/00094 | Loss: 0.0638 | EWC Loss: 0.0275 | CE Loss: 0.0363\n",
      "Train Epoch: 037 Batch: 00003/00094 | Loss: 0.0665 | EWC Loss: 0.0274 | CE Loss: 0.0391\n",
      "Train Epoch: 037 Batch: 00004/00094 | Loss: 0.0558 | EWC Loss: 0.0275 | CE Loss: 0.0283\n",
      "Train Epoch: 037 Batch: 00005/00094 | Loss: 0.0595 | EWC Loss: 0.0275 | CE Loss: 0.0320\n",
      "Train Epoch: 037 Batch: 00006/00094 | Loss: 0.2052 | EWC Loss: 0.0274 | CE Loss: 0.1778\n",
      "Train Epoch: 037 Batch: 00007/00094 | Loss: 0.0712 | EWC Loss: 0.0275 | CE Loss: 0.0437\n",
      "Train Epoch: 037 Batch: 00008/00094 | Loss: 0.0855 | EWC Loss: 0.0277 | CE Loss: 0.0578\n",
      "Train Epoch: 037 Batch: 00009/00094 | Loss: 0.1995 | EWC Loss: 0.0277 | CE Loss: 0.1718\n",
      "Train Epoch: 037 Batch: 00010/00094 | Loss: 0.1031 | EWC Loss: 0.0276 | CE Loss: 0.0756\n",
      "Train Epoch: 037 Batch: 00011/00094 | Loss: 0.0963 | EWC Loss: 0.0276 | CE Loss: 0.0687\n",
      "Train Epoch: 037 Batch: 00012/00094 | Loss: 0.1677 | EWC Loss: 0.0277 | CE Loss: 0.1400\n",
      "Train Epoch: 037 Batch: 00013/00094 | Loss: 0.0444 | EWC Loss: 0.0280 | CE Loss: 0.0164\n",
      "Train Epoch: 037 Batch: 00014/00094 | Loss: 0.0479 | EWC Loss: 0.0279 | CE Loss: 0.0200\n",
      "Train Epoch: 037 Batch: 00015/00094 | Loss: 0.0483 | EWC Loss: 0.0278 | CE Loss: 0.0205\n",
      "Train Epoch: 037 Batch: 00016/00094 | Loss: 0.1193 | EWC Loss: 0.0277 | CE Loss: 0.0916\n",
      "Train Epoch: 037 Batch: 00017/00094 | Loss: 0.0739 | EWC Loss: 0.0277 | CE Loss: 0.0462\n",
      "Train Epoch: 037 Batch: 00018/00094 | Loss: 0.0494 | EWC Loss: 0.0276 | CE Loss: 0.0217\n",
      "Train Epoch: 037 Batch: 00019/00094 | Loss: 0.0590 | EWC Loss: 0.0275 | CE Loss: 0.0315\n",
      "Train Epoch: 037 Batch: 00020/00094 | Loss: 0.0500 | EWC Loss: 0.0275 | CE Loss: 0.0225\n",
      "Train Epoch: 037 Batch: 00021/00094 | Loss: 0.1568 | EWC Loss: 0.0275 | CE Loss: 0.1293\n",
      "Train Epoch: 037 Batch: 00022/00094 | Loss: 0.1363 | EWC Loss: 0.0275 | CE Loss: 0.1088\n",
      "Train Epoch: 037 Batch: 00023/00094 | Loss: 0.1104 | EWC Loss: 0.0276 | CE Loss: 0.0828\n",
      "Train Epoch: 037 Batch: 00024/00094 | Loss: 0.1653 | EWC Loss: 0.0278 | CE Loss: 0.1375\n",
      "Train Epoch: 037 Batch: 00025/00094 | Loss: 0.0845 | EWC Loss: 0.0281 | CE Loss: 0.0564\n",
      "Train Epoch: 037 Batch: 00026/00094 | Loss: 0.0411 | EWC Loss: 0.0279 | CE Loss: 0.0132\n",
      "Train Epoch: 037 Batch: 00027/00094 | Loss: 0.0718 | EWC Loss: 0.0278 | CE Loss: 0.0440\n",
      "Train Epoch: 037 Batch: 00028/00094 | Loss: 0.1214 | EWC Loss: 0.0279 | CE Loss: 0.0935\n",
      "Train Epoch: 037 Batch: 00029/00094 | Loss: 0.1563 | EWC Loss: 0.0280 | CE Loss: 0.1282\n",
      "Train Epoch: 037 Batch: 00030/00094 | Loss: 0.1636 | EWC Loss: 0.0282 | CE Loss: 0.1354\n",
      "Train Epoch: 037 Batch: 00031/00094 | Loss: 0.0935 | EWC Loss: 0.0281 | CE Loss: 0.0654\n",
      "Train Epoch: 037 Batch: 00032/00094 | Loss: 0.0902 | EWC Loss: 0.0280 | CE Loss: 0.0622\n",
      "Train Epoch: 037 Batch: 00033/00094 | Loss: 0.1856 | EWC Loss: 0.0281 | CE Loss: 0.1576\n",
      "Train Epoch: 037 Batch: 00034/00094 | Loss: 0.2061 | EWC Loss: 0.0281 | CE Loss: 0.1780\n",
      "Train Epoch: 037 Batch: 00035/00094 | Loss: 0.0531 | EWC Loss: 0.0282 | CE Loss: 0.0249\n",
      "Train Epoch: 037 Batch: 00036/00094 | Loss: 0.0538 | EWC Loss: 0.0280 | CE Loss: 0.0258\n",
      "Train Epoch: 037 Batch: 00037/00094 | Loss: 0.1137 | EWC Loss: 0.0279 | CE Loss: 0.0857\n",
      "Train Epoch: 037 Batch: 00038/00094 | Loss: 0.0633 | EWC Loss: 0.0279 | CE Loss: 0.0353\n",
      "Train Epoch: 037 Batch: 00039/00094 | Loss: 0.0606 | EWC Loss: 0.0278 | CE Loss: 0.0328\n",
      "Train Epoch: 037 Batch: 00040/00094 | Loss: 0.0872 | EWC Loss: 0.0278 | CE Loss: 0.0594\n",
      "Train Epoch: 037 Batch: 00041/00094 | Loss: 0.0423 | EWC Loss: 0.0278 | CE Loss: 0.0145\n",
      "Train Epoch: 037 Batch: 00042/00094 | Loss: 0.0868 | EWC Loss: 0.0277 | CE Loss: 0.0591\n",
      "Train Epoch: 037 Batch: 00043/00094 | Loss: 0.1106 | EWC Loss: 0.0277 | CE Loss: 0.0829\n",
      "Train Epoch: 037 Batch: 00044/00094 | Loss: 0.1287 | EWC Loss: 0.0280 | CE Loss: 0.1006\n",
      "Train Epoch: 037 Batch: 00045/00094 | Loss: 0.0487 | EWC Loss: 0.0281 | CE Loss: 0.0206\n",
      "Train Epoch: 037 Batch: 00046/00094 | Loss: 0.0807 | EWC Loss: 0.0280 | CE Loss: 0.0526\n",
      "Train Epoch: 037 Batch: 00047/00094 | Loss: 0.0567 | EWC Loss: 0.0280 | CE Loss: 0.0287\n",
      "Train Epoch: 037 Batch: 00048/00094 | Loss: 0.0723 | EWC Loss: 0.0280 | CE Loss: 0.0444\n",
      "Train Epoch: 037 Batch: 00049/00094 | Loss: 0.1492 | EWC Loss: 0.0280 | CE Loss: 0.1212\n",
      "Train Epoch: 037 Batch: 00050/00094 | Loss: 0.1532 | EWC Loss: 0.0281 | CE Loss: 0.1251\n",
      "Train Epoch: 037 Batch: 00051/00094 | Loss: 0.1850 | EWC Loss: 0.0281 | CE Loss: 0.1569\n",
      "Train Epoch: 037 Batch: 00052/00094 | Loss: 0.1292 | EWC Loss: 0.0283 | CE Loss: 0.1009\n",
      "Train Epoch: 037 Batch: 00053/00094 | Loss: 0.0872 | EWC Loss: 0.0283 | CE Loss: 0.0589\n",
      "Train Epoch: 037 Batch: 00054/00094 | Loss: 0.0771 | EWC Loss: 0.0281 | CE Loss: 0.0490\n",
      "Train Epoch: 037 Batch: 00055/00094 | Loss: 0.0895 | EWC Loss: 0.0280 | CE Loss: 0.0614\n",
      "Train Epoch: 037 Batch: 00056/00094 | Loss: 0.0484 | EWC Loss: 0.0280 | CE Loss: 0.0204\n",
      "Train Epoch: 037 Batch: 00057/00094 | Loss: 0.0703 | EWC Loss: 0.0279 | CE Loss: 0.0424\n",
      "Train Epoch: 037 Batch: 00058/00094 | Loss: 0.0680 | EWC Loss: 0.0278 | CE Loss: 0.0401\n",
      "Train Epoch: 037 Batch: 00059/00094 | Loss: 0.0701 | EWC Loss: 0.0278 | CE Loss: 0.0423\n",
      "Train Epoch: 037 Batch: 00060/00094 | Loss: 0.0388 | EWC Loss: 0.0278 | CE Loss: 0.0109\n",
      "Train Epoch: 037 Batch: 00061/00094 | Loss: 0.1089 | EWC Loss: 0.0278 | CE Loss: 0.0811\n",
      "Train Epoch: 037 Batch: 00062/00094 | Loss: 0.1731 | EWC Loss: 0.0281 | CE Loss: 0.1450\n",
      "Train Epoch: 037 Batch: 00063/00094 | Loss: 0.0955 | EWC Loss: 0.0281 | CE Loss: 0.0675\n",
      "Train Epoch: 037 Batch: 00064/00094 | Loss: 0.1666 | EWC Loss: 0.0281 | CE Loss: 0.1385\n",
      "Train Epoch: 037 Batch: 00065/00094 | Loss: 0.0723 | EWC Loss: 0.0279 | CE Loss: 0.0444\n",
      "Train Epoch: 037 Batch: 00066/00094 | Loss: 0.1161 | EWC Loss: 0.0279 | CE Loss: 0.0883\n",
      "Train Epoch: 037 Batch: 00067/00094 | Loss: 0.0776 | EWC Loss: 0.0279 | CE Loss: 0.0497\n",
      "Train Epoch: 037 Batch: 00068/00094 | Loss: 0.0686 | EWC Loss: 0.0278 | CE Loss: 0.0408\n",
      "Train Epoch: 037 Batch: 00069/00094 | Loss: 0.0505 | EWC Loss: 0.0278 | CE Loss: 0.0227\n",
      "Train Epoch: 037 Batch: 00070/00094 | Loss: 0.1417 | EWC Loss: 0.0278 | CE Loss: 0.1140\n",
      "Train Epoch: 037 Batch: 00071/00094 | Loss: 0.0613 | EWC Loss: 0.0276 | CE Loss: 0.0337\n",
      "Train Epoch: 037 Batch: 00072/00094 | Loss: 0.1239 | EWC Loss: 0.0276 | CE Loss: 0.0963\n",
      "Train Epoch: 037 Batch: 00073/00094 | Loss: 0.0550 | EWC Loss: 0.0276 | CE Loss: 0.0275\n",
      "Train Epoch: 037 Batch: 00074/00094 | Loss: 0.0698 | EWC Loss: 0.0275 | CE Loss: 0.0423\n",
      "Train Epoch: 037 Batch: 00075/00094 | Loss: 0.0658 | EWC Loss: 0.0275 | CE Loss: 0.0383\n",
      "Train Epoch: 037 Batch: 00076/00094 | Loss: 0.0531 | EWC Loss: 0.0276 | CE Loss: 0.0255\n",
      "Train Epoch: 037 Batch: 00077/00094 | Loss: 0.0565 | EWC Loss: 0.0276 | CE Loss: 0.0289\n",
      "Train Epoch: 037 Batch: 00078/00094 | Loss: 0.1011 | EWC Loss: 0.0276 | CE Loss: 0.0735\n",
      "Train Epoch: 037 Batch: 00079/00094 | Loss: 0.0694 | EWC Loss: 0.0276 | CE Loss: 0.0417\n",
      "Train Epoch: 037 Batch: 00080/00094 | Loss: 0.1409 | EWC Loss: 0.0276 | CE Loss: 0.1133\n",
      "Train Epoch: 037 Batch: 00081/00094 | Loss: 0.1378 | EWC Loss: 0.0277 | CE Loss: 0.1101\n",
      "Train Epoch: 037 Batch: 00082/00094 | Loss: 0.1058 | EWC Loss: 0.0280 | CE Loss: 0.0779\n",
      "Train Epoch: 037 Batch: 00083/00094 | Loss: 0.0602 | EWC Loss: 0.0281 | CE Loss: 0.0321\n",
      "Train Epoch: 037 Batch: 00084/00094 | Loss: 0.0839 | EWC Loss: 0.0281 | CE Loss: 0.0558\n",
      "Train Epoch: 037 Batch: 00085/00094 | Loss: 0.0891 | EWC Loss: 0.0281 | CE Loss: 0.0611\n",
      "Train Epoch: 037 Batch: 00086/00094 | Loss: 0.1907 | EWC Loss: 0.0281 | CE Loss: 0.1626\n",
      "Train Epoch: 037 Batch: 00087/00094 | Loss: 0.1649 | EWC Loss: 0.0279 | CE Loss: 0.1370\n",
      "Train Epoch: 037 Batch: 00088/00094 | Loss: 0.1216 | EWC Loss: 0.0279 | CE Loss: 0.0937\n",
      "Train Epoch: 037 Batch: 00089/00094 | Loss: 0.2511 | EWC Loss: 0.0277 | CE Loss: 0.2234\n",
      "Train Epoch: 037 Batch: 00090/00094 | Loss: 0.0844 | EWC Loss: 0.0278 | CE Loss: 0.0566\n",
      "Train Epoch: 037 Batch: 00091/00094 | Loss: 0.1695 | EWC Loss: 0.0279 | CE Loss: 0.1417\n",
      "Train Epoch: 037 Batch: 00092/00094 | Loss: 0.1584 | EWC Loss: 0.0281 | CE Loss: 0.1303\n",
      "Train Epoch: 037 Batch: 00093/00094 | Loss: 0.1627 | EWC Loss: 0.0283 | CE Loss: 0.1344\n",
      "Train Epoch: 037 Batch: 00094/00094 | Loss: 0.0764 | EWC Loss: 0.0282 | CE Loss: 0.0481\n",
      "Train Epoch: 037 |Acc 97.33333 | Loss: 0.10061 | Task Loss 0.07277 | EWC Loss: 0.02784\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.0693 | acc:97.3000\n",
      "[VAL Acc] Target: 97.30%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:3.1470 | acc:49.5500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 49.55%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:2.2026 | acc:50.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 50.12%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:2.2666 | acc:43.8931\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 43.89%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:2.1777 | acc:54.0752\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 54.08%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:0.7821 | acc:72.2736\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 72.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.5205 | acc:63.0094\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 63.01%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.0264 | acc:61.1250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 61.12%\n",
      "[VAL Acc] Avg 61.42%\n",
      "Early stopping ...\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : tewc18_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/12966a43cddf4d31ab58b15a05dece0b\u001b[0m\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/biggan_val_acc [38]        : (50.125, 56.125)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/crn_val_acc [38]           : (61.2460815047022, 71.27742946708464)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/cyclegan_val_acc [38]      : (43.320610687022906, 52.48091603053435)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/faceforensics_val_acc [38] : (65.34195933456563, 83.08687615526802)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/gaugan_val_acc [38]        : (47.9, 50.1)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/imle_val_acc [38]          : (52.19435736677116, 62.460815047021946)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/../KD-AIGC-Detection/datasets/custom/wild_val_acc [38]          : (55.8125, 66.6875)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/target_val_acc [38]                                             : (88.64999999999999, 98.25)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/train_acc [38]                                                  : (80.60000000000001, 97.91666666666666)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     acc/val_acc [38]                                                    : (60.9102629463859, 64.86469740138702)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss [38]                                                    : (0.08670215742127534, 0.485009855263807)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_ewc [38]                                                : (0.027394052555269385, 0.033429784977689704)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     losses/loss_main [38]                                               : (0.05891722200458196, 0.45739640263800924)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     start_acc                                                           : 52.7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : tewc18_diff\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size   : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config_file  : ewc_model_config.conf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stop   : 35\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs       : 250\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flip         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     importance   : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr           : 0.005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_schedule  : cosine\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_gpu      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resolution   : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     sample_batch : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (3.41 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1 (9.28 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Error sending a notification, make sure you have opted-in for notifications. Failure: HTTPSConnectionPool(host='www.comet.com', port=443): Read timed out. (read timeout=10)\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 80 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 77 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 76 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 76 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 76 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 73 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\", \"biggan\", \"cyclegan\", \"imle\", \"faceforensics\", \"crn\", \"wild\"] \n",
    "TARGET_DATASET = \"diffusionshort\"\n",
    "NETWORK = \"ResNet18\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/EWC_r18/cddb_easy\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/EWC_r18/diff\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "COMET_NAME = \"tewc18_diff\"\n",
    "\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/ewc_train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_file='ewc_model_config.conf', network='Xception', input_model='../KD-AIGC-Detection/checkpoints/EWC_Xception/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild', output_dir='../KD-AIGC-Detection/checkpoints/EWC_Xception/diff/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild_diffusionshort', source_datasets='../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild', target_dataset='../KD-AIGC-Detection/datasets/custom/diffusionshort', use_comet=True, comet_name='tewcxc_diff', num_gpu='0', epochs='250', early_stop='35', batch_size='64', resolution='128', lr_schedule='cosine', lr='0.005', importance='10', sample_batch='4', flip='False')\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com \u001b[38;5;39mhttps://www.comet.com/francescotss/paper-review/29d30e9477c44fa689372f7bc7bca73c\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ Creating Loaders ------\n",
      "GPU num is 0\n",
      "\n",
      "===> Making Loader for Continual Learning..\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/imle\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/crn\n",
      "===> Making Loader : ../KD-AIGC-Detection/datasets/custom/wild\n",
      "DATASET PATHS\n",
      "val_source_dir  ../KD-AIGC-Detection/datasets/custom/gaugan,../KD-AIGC-Detection/datasets/custom/biggan,../KD-AIGC-Detection/datasets/custom/cyclegan,../KD-AIGC-Detection/datasets/custom/imle,../KD-AIGC-Detection/datasets/custom/faceforensics,../KD-AIGC-Detection/datasets/custom/crn,../KD-AIGC-Detection/datasets/custom/wild\n",
      "val_target_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/val\n",
      "train_dir  ../KD-AIGC-Detection/datasets/custom/diffusionshort/train\n",
      "Dataset available in target_loaders:  train / val\n",
      "Dataset available in val_loaders:  ../KD-AIGC-Detection/datasets/custom/gaugan / ../KD-AIGC-Detection/datasets/custom/biggan / ../KD-AIGC-Detection/datasets/custom/cyclegan / ../KD-AIGC-Detection/datasets/custom/imle / ../KD-AIGC-Detection/datasets/custom/faceforensics / ../KD-AIGC-Detection/datasets/custom/crn / ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "\n",
      "\n",
      " ------ Loading models ------\n",
      "Loading Xception from ../KD-AIGC-Detection/checkpoints/EWC_Xception/cddb_easy/gaugan_biggan_cyclegan_imle_faceforensics_crn_wild/model_best_accuracy.pth\n",
      "Apply Cosine learning rate schedule\n",
      "/home/fra/miniconda3/envs/paper/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:3.0697 | acc:52.5500\n",
      "Start Target Validation ACC: 52.55%\n",
      "Start training in 250 epochs\n",
      "\n",
      "\n",
      "---------- Starting epoch 0 ----------\n",
      "Train Epoch: 000 Batch: 00001/00094 | Loss: 0.7735 | EWC Loss: 0.0000 | CE Loss: 0.7735\n",
      "Train Epoch: 000 Batch: 00002/00094 | Loss: 0.6195 | EWC Loss: 0.0000 | CE Loss: 0.6195\n",
      "Train Epoch: 000 Batch: 00003/00094 | Loss: 0.6810 | EWC Loss: 0.0000 | CE Loss: 0.6810\n",
      "Train Epoch: 000 Batch: 00004/00094 | Loss: 0.6104 | EWC Loss: 0.0001 | CE Loss: 0.6103\n",
      "Train Epoch: 000 Batch: 00005/00094 | Loss: 0.6640 | EWC Loss: 0.0001 | CE Loss: 0.6639\n",
      "Train Epoch: 000 Batch: 00006/00094 | Loss: 0.6018 | EWC Loss: 0.0001 | CE Loss: 0.6017\n",
      "Train Epoch: 000 Batch: 00007/00094 | Loss: 0.6178 | EWC Loss: 0.0001 | CE Loss: 0.6176\n",
      "Train Epoch: 000 Batch: 00008/00094 | Loss: 0.5608 | EWC Loss: 0.0001 | CE Loss: 0.5607\n",
      "Train Epoch: 000 Batch: 00009/00094 | Loss: 0.6680 | EWC Loss: 0.0001 | CE Loss: 0.6679\n",
      "Train Epoch: 000 Batch: 00010/00094 | Loss: 0.5457 | EWC Loss: 0.0001 | CE Loss: 0.5456\n",
      "Train Epoch: 000 Batch: 00011/00094 | Loss: 0.5415 | EWC Loss: 0.0002 | CE Loss: 0.5413\n",
      "Train Epoch: 000 Batch: 00012/00094 | Loss: 0.5687 | EWC Loss: 0.0002 | CE Loss: 0.5685\n",
      "Train Epoch: 000 Batch: 00013/00094 | Loss: 0.5140 | EWC Loss: 0.0002 | CE Loss: 0.5138\n",
      "Train Epoch: 000 Batch: 00014/00094 | Loss: 0.5535 | EWC Loss: 0.0002 | CE Loss: 0.5533\n",
      "Train Epoch: 000 Batch: 00015/00094 | Loss: 0.5177 | EWC Loss: 0.0003 | CE Loss: 0.5175\n",
      "Train Epoch: 000 Batch: 00016/00094 | Loss: 0.5435 | EWC Loss: 0.0003 | CE Loss: 0.5432\n",
      "Train Epoch: 000 Batch: 00017/00094 | Loss: 0.5660 | EWC Loss: 0.0003 | CE Loss: 0.5657\n",
      "Train Epoch: 000 Batch: 00018/00094 | Loss: 0.4949 | EWC Loss: 0.0003 | CE Loss: 0.4946\n",
      "Train Epoch: 000 Batch: 00019/00094 | Loss: 0.4177 | EWC Loss: 0.0004 | CE Loss: 0.4173\n",
      "Train Epoch: 000 Batch: 00020/00094 | Loss: 0.5291 | EWC Loss: 0.0004 | CE Loss: 0.5287\n",
      "Train Epoch: 000 Batch: 00021/00094 | Loss: 0.3989 | EWC Loss: 0.0004 | CE Loss: 0.3984\n",
      "Train Epoch: 000 Batch: 00022/00094 | Loss: 0.5257 | EWC Loss: 0.0004 | CE Loss: 0.5253\n",
      "Train Epoch: 000 Batch: 00023/00094 | Loss: 0.3690 | EWC Loss: 0.0004 | CE Loss: 0.3686\n",
      "Train Epoch: 000 Batch: 00024/00094 | Loss: 0.3722 | EWC Loss: 0.0005 | CE Loss: 0.3717\n",
      "Train Epoch: 000 Batch: 00025/00094 | Loss: 0.4399 | EWC Loss: 0.0005 | CE Loss: 0.4394\n",
      "Train Epoch: 000 Batch: 00026/00094 | Loss: 0.3741 | EWC Loss: 0.0005 | CE Loss: 0.3735\n",
      "Train Epoch: 000 Batch: 00027/00094 | Loss: 0.3483 | EWC Loss: 0.0006 | CE Loss: 0.3478\n",
      "Train Epoch: 000 Batch: 00028/00094 | Loss: 0.4252 | EWC Loss: 0.0006 | CE Loss: 0.4247\n",
      "Train Epoch: 000 Batch: 00029/00094 | Loss: 0.4490 | EWC Loss: 0.0006 | CE Loss: 0.4484\n",
      "Train Epoch: 000 Batch: 00030/00094 | Loss: 0.4984 | EWC Loss: 0.0007 | CE Loss: 0.4977\n",
      "Train Epoch: 000 Batch: 00031/00094 | Loss: 0.3952 | EWC Loss: 0.0007 | CE Loss: 0.3945\n",
      "Train Epoch: 000 Batch: 00032/00094 | Loss: 0.3890 | EWC Loss: 0.0007 | CE Loss: 0.3883\n",
      "Train Epoch: 000 Batch: 00033/00094 | Loss: 0.4814 | EWC Loss: 0.0008 | CE Loss: 0.4806\n",
      "Train Epoch: 000 Batch: 00034/00094 | Loss: 0.3190 | EWC Loss: 0.0008 | CE Loss: 0.3182\n",
      "Train Epoch: 000 Batch: 00035/00094 | Loss: 0.3593 | EWC Loss: 0.0008 | CE Loss: 0.3584\n",
      "Train Epoch: 000 Batch: 00036/00094 | Loss: 0.4722 | EWC Loss: 0.0008 | CE Loss: 0.4713\n",
      "Train Epoch: 000 Batch: 00037/00094 | Loss: 0.3865 | EWC Loss: 0.0008 | CE Loss: 0.3856\n",
      "Train Epoch: 000 Batch: 00038/00094 | Loss: 0.3711 | EWC Loss: 0.0009 | CE Loss: 0.3703\n",
      "Train Epoch: 000 Batch: 00039/00094 | Loss: 0.3485 | EWC Loss: 0.0009 | CE Loss: 0.3476\n",
      "Train Epoch: 000 Batch: 00040/00094 | Loss: 0.4604 | EWC Loss: 0.0010 | CE Loss: 0.4594\n",
      "Train Epoch: 000 Batch: 00041/00094 | Loss: 0.4883 | EWC Loss: 0.0010 | CE Loss: 0.4874\n",
      "Train Epoch: 000 Batch: 00042/00094 | Loss: 0.3150 | EWC Loss: 0.0010 | CE Loss: 0.3140\n",
      "Train Epoch: 000 Batch: 00043/00094 | Loss: 0.4643 | EWC Loss: 0.0010 | CE Loss: 0.4633\n",
      "Train Epoch: 000 Batch: 00044/00094 | Loss: 0.3387 | EWC Loss: 0.0010 | CE Loss: 0.3377\n",
      "Train Epoch: 000 Batch: 00045/00094 | Loss: 0.4885 | EWC Loss: 0.0012 | CE Loss: 0.4873\n",
      "Train Epoch: 000 Batch: 00046/00094 | Loss: 0.3368 | EWC Loss: 0.0012 | CE Loss: 0.3356\n",
      "Train Epoch: 000 Batch: 00047/00094 | Loss: 0.3460 | EWC Loss: 0.0013 | CE Loss: 0.3447\n",
      "Train Epoch: 000 Batch: 00048/00094 | Loss: 0.3266 | EWC Loss: 0.0013 | CE Loss: 0.3253\n",
      "Train Epoch: 000 Batch: 00049/00094 | Loss: 0.3417 | EWC Loss: 0.0014 | CE Loss: 0.3403\n",
      "Train Epoch: 000 Batch: 00050/00094 | Loss: 0.2801 | EWC Loss: 0.0014 | CE Loss: 0.2788\n",
      "Train Epoch: 000 Batch: 00051/00094 | Loss: 0.2754 | EWC Loss: 0.0014 | CE Loss: 0.2740\n",
      "Train Epoch: 000 Batch: 00052/00094 | Loss: 0.2386 | EWC Loss: 0.0014 | CE Loss: 0.2372\n",
      "Train Epoch: 000 Batch: 00053/00094 | Loss: 0.3683 | EWC Loss: 0.0015 | CE Loss: 0.3668\n",
      "Train Epoch: 000 Batch: 00054/00094 | Loss: 0.4668 | EWC Loss: 0.0015 | CE Loss: 0.4653\n",
      "Train Epoch: 000 Batch: 00055/00094 | Loss: 0.3129 | EWC Loss: 0.0016 | CE Loss: 0.3113\n",
      "Train Epoch: 000 Batch: 00056/00094 | Loss: 0.2851 | EWC Loss: 0.0016 | CE Loss: 0.2834\n",
      "Train Epoch: 000 Batch: 00057/00094 | Loss: 0.2927 | EWC Loss: 0.0017 | CE Loss: 0.2910\n",
      "Train Epoch: 000 Batch: 00058/00094 | Loss: 0.3064 | EWC Loss: 0.0017 | CE Loss: 0.3047\n",
      "Train Epoch: 000 Batch: 00059/00094 | Loss: 0.3952 | EWC Loss: 0.0018 | CE Loss: 0.3934\n",
      "Train Epoch: 000 Batch: 00060/00094 | Loss: 0.3303 | EWC Loss: 0.0018 | CE Loss: 0.3285\n",
      "Train Epoch: 000 Batch: 00061/00094 | Loss: 0.3081 | EWC Loss: 0.0019 | CE Loss: 0.3062\n",
      "Train Epoch: 000 Batch: 00062/00094 | Loss: 0.4420 | EWC Loss: 0.0019 | CE Loss: 0.4401\n",
      "Train Epoch: 000 Batch: 00063/00094 | Loss: 0.3186 | EWC Loss: 0.0019 | CE Loss: 0.3167\n",
      "Train Epoch: 000 Batch: 00064/00094 | Loss: 0.2986 | EWC Loss: 0.0020 | CE Loss: 0.2967\n",
      "Train Epoch: 000 Batch: 00065/00094 | Loss: 0.3589 | EWC Loss: 0.0020 | CE Loss: 0.3569\n",
      "Train Epoch: 000 Batch: 00066/00094 | Loss: 0.2967 | EWC Loss: 0.0020 | CE Loss: 0.2947\n",
      "Train Epoch: 000 Batch: 00067/00094 | Loss: 0.2918 | EWC Loss: 0.0021 | CE Loss: 0.2897\n",
      "Train Epoch: 000 Batch: 00068/00094 | Loss: 0.2758 | EWC Loss: 0.0021 | CE Loss: 0.2737\n",
      "Train Epoch: 000 Batch: 00069/00094 | Loss: 0.3996 | EWC Loss: 0.0022 | CE Loss: 0.3974\n",
      "Train Epoch: 000 Batch: 00070/00094 | Loss: 0.3058 | EWC Loss: 0.0023 | CE Loss: 0.3035\n",
      "Train Epoch: 000 Batch: 00071/00094 | Loss: 0.3289 | EWC Loss: 0.0023 | CE Loss: 0.3266\n",
      "Train Epoch: 000 Batch: 00072/00094 | Loss: 0.2782 | EWC Loss: 0.0023 | CE Loss: 0.2759\n",
      "Train Epoch: 000 Batch: 00073/00094 | Loss: 0.1921 | EWC Loss: 0.0024 | CE Loss: 0.1897\n",
      "Train Epoch: 000 Batch: 00074/00094 | Loss: 0.3220 | EWC Loss: 0.0024 | CE Loss: 0.3196\n",
      "Train Epoch: 000 Batch: 00075/00094 | Loss: 0.2621 | EWC Loss: 0.0025 | CE Loss: 0.2596\n",
      "Train Epoch: 000 Batch: 00076/00094 | Loss: 0.2872 | EWC Loss: 0.0025 | CE Loss: 0.2847\n",
      "Train Epoch: 000 Batch: 00077/00094 | Loss: 0.3858 | EWC Loss: 0.0025 | CE Loss: 0.3833\n",
      "Train Epoch: 000 Batch: 00078/00094 | Loss: 0.2319 | EWC Loss: 0.0025 | CE Loss: 0.2294\n",
      "Train Epoch: 000 Batch: 00079/00094 | Loss: 0.3431 | EWC Loss: 0.0026 | CE Loss: 0.3405\n",
      "Train Epoch: 000 Batch: 00080/00094 | Loss: 0.4521 | EWC Loss: 0.0027 | CE Loss: 0.4494\n",
      "Train Epoch: 000 Batch: 00081/00094 | Loss: 0.3344 | EWC Loss: 0.0028 | CE Loss: 0.3316\n",
      "Train Epoch: 000 Batch: 00082/00094 | Loss: 0.2844 | EWC Loss: 0.0027 | CE Loss: 0.2817\n",
      "Train Epoch: 000 Batch: 00083/00094 | Loss: 0.2861 | EWC Loss: 0.0027 | CE Loss: 0.2834\n",
      "Train Epoch: 000 Batch: 00084/00094 | Loss: 0.2379 | EWC Loss: 0.0028 | CE Loss: 0.2352\n",
      "Train Epoch: 000 Batch: 00085/00094 | Loss: 0.2248 | EWC Loss: 0.0028 | CE Loss: 0.2220\n",
      "Train Epoch: 000 Batch: 00086/00094 | Loss: 0.2736 | EWC Loss: 0.0028 | CE Loss: 0.2708\n",
      "Train Epoch: 000 Batch: 00087/00094 | Loss: 0.2582 | EWC Loss: 0.0029 | CE Loss: 0.2554\n",
      "Train Epoch: 000 Batch: 00088/00094 | Loss: 0.3358 | EWC Loss: 0.0029 | CE Loss: 0.3329\n",
      "Train Epoch: 000 Batch: 00089/00094 | Loss: 0.1946 | EWC Loss: 0.0029 | CE Loss: 0.1917\n",
      "Train Epoch: 000 Batch: 00090/00094 | Loss: 0.2001 | EWC Loss: 0.0029 | CE Loss: 0.1972\n",
      "Train Epoch: 000 Batch: 00091/00094 | Loss: 0.3634 | EWC Loss: 0.0029 | CE Loss: 0.3605\n",
      "Train Epoch: 000 Batch: 00092/00094 | Loss: 0.3028 | EWC Loss: 0.0030 | CE Loss: 0.2999\n",
      "Train Epoch: 000 Batch: 00093/00094 | Loss: 0.2012 | EWC Loss: 0.0030 | CE Loss: 0.1982\n",
      "Train Epoch: 000 Batch: 00094/00094 | Loss: 0.2250 | EWC Loss: 0.0030 | CE Loss: 0.2220\n",
      "Train Epoch: 000 |Acc 82.55000 | Loss: 0.39226 | Task Loss 0.39088 | EWC Loss: 0.00138\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2800 | acc:89.6000\n",
      "[VAL Acc] Target: 89.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.7507 | acc:45.8500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 45.85%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.7606 | acc:67.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 67.38%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.2501 | acc:50.5725\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 50.57%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.6759 | acc:64.9295\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 64.93%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0571 | acc:43.9926\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 43.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.6261 | acc:71.7085\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 71.71%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.1388 | acc:50.6875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 50.69%\n",
      "[VAL Acc] Avg 60.59%\n",
      "VAL Acc improve from 0.00% to 60.59%\n",
      "Save best model\n",
      "\n",
      "\n",
      "---------- Starting epoch 1 ----------\n",
      "Train Epoch: 001 Batch: 00001/00094 | Loss: 0.1991 | EWC Loss: 0.0030 | CE Loss: 0.1960\n",
      "Train Epoch: 001 Batch: 00002/00094 | Loss: 0.1778 | EWC Loss: 0.0030 | CE Loss: 0.1748\n",
      "Train Epoch: 001 Batch: 00003/00094 | Loss: 0.3397 | EWC Loss: 0.0031 | CE Loss: 0.3366\n",
      "Train Epoch: 001 Batch: 00004/00094 | Loss: 0.2716 | EWC Loss: 0.0031 | CE Loss: 0.2684\n",
      "Train Epoch: 001 Batch: 00005/00094 | Loss: 0.2321 | EWC Loss: 0.0032 | CE Loss: 0.2289\n",
      "Train Epoch: 001 Batch: 00006/00094 | Loss: 0.1887 | EWC Loss: 0.0033 | CE Loss: 0.1854\n",
      "Train Epoch: 001 Batch: 00007/00094 | Loss: 0.2176 | EWC Loss: 0.0033 | CE Loss: 0.2142\n",
      "Train Epoch: 001 Batch: 00008/00094 | Loss: 0.2201 | EWC Loss: 0.0034 | CE Loss: 0.2167\n",
      "Train Epoch: 001 Batch: 00009/00094 | Loss: 0.3734 | EWC Loss: 0.0034 | CE Loss: 0.3700\n",
      "Train Epoch: 001 Batch: 00010/00094 | Loss: 0.2563 | EWC Loss: 0.0035 | CE Loss: 0.2528\n",
      "Train Epoch: 001 Batch: 00011/00094 | Loss: 0.1946 | EWC Loss: 0.0036 | CE Loss: 0.1910\n",
      "Train Epoch: 001 Batch: 00012/00094 | Loss: 0.1383 | EWC Loss: 0.0036 | CE Loss: 0.1347\n",
      "Train Epoch: 001 Batch: 00013/00094 | Loss: 0.2015 | EWC Loss: 0.0036 | CE Loss: 0.1979\n",
      "Train Epoch: 001 Batch: 00014/00094 | Loss: 0.3134 | EWC Loss: 0.0036 | CE Loss: 0.3099\n",
      "Train Epoch: 001 Batch: 00015/00094 | Loss: 0.1819 | EWC Loss: 0.0037 | CE Loss: 0.1781\n",
      "Train Epoch: 001 Batch: 00016/00094 | Loss: 0.2447 | EWC Loss: 0.0038 | CE Loss: 0.2409\n",
      "Train Epoch: 001 Batch: 00017/00094 | Loss: 0.1862 | EWC Loss: 0.0038 | CE Loss: 0.1824\n",
      "Train Epoch: 001 Batch: 00018/00094 | Loss: 0.2729 | EWC Loss: 0.0038 | CE Loss: 0.2691\n",
      "Train Epoch: 001 Batch: 00019/00094 | Loss: 0.2815 | EWC Loss: 0.0040 | CE Loss: 0.2774\n",
      "Train Epoch: 001 Batch: 00020/00094 | Loss: 0.2051 | EWC Loss: 0.0042 | CE Loss: 0.2009\n",
      "Train Epoch: 001 Batch: 00021/00094 | Loss: 0.2763 | EWC Loss: 0.0043 | CE Loss: 0.2721\n",
      "Train Epoch: 001 Batch: 00022/00094 | Loss: 0.1906 | EWC Loss: 0.0042 | CE Loss: 0.1864\n",
      "Train Epoch: 001 Batch: 00023/00094 | Loss: 0.2032 | EWC Loss: 0.0042 | CE Loss: 0.1990\n",
      "Train Epoch: 001 Batch: 00024/00094 | Loss: 0.2546 | EWC Loss: 0.0042 | CE Loss: 0.2504\n",
      "Train Epoch: 001 Batch: 00025/00094 | Loss: 0.1767 | EWC Loss: 0.0042 | CE Loss: 0.1725\n",
      "Train Epoch: 001 Batch: 00026/00094 | Loss: 0.2396 | EWC Loss: 0.0043 | CE Loss: 0.2354\n",
      "Train Epoch: 001 Batch: 00027/00094 | Loss: 0.1693 | EWC Loss: 0.0043 | CE Loss: 0.1650\n",
      "Train Epoch: 001 Batch: 00028/00094 | Loss: 0.2223 | EWC Loss: 0.0042 | CE Loss: 0.2181\n",
      "Train Epoch: 001 Batch: 00029/00094 | Loss: 0.1521 | EWC Loss: 0.0043 | CE Loss: 0.1478\n",
      "Train Epoch: 001 Batch: 00030/00094 | Loss: 0.2943 | EWC Loss: 0.0043 | CE Loss: 0.2900\n",
      "Train Epoch: 001 Batch: 00031/00094 | Loss: 0.2280 | EWC Loss: 0.0043 | CE Loss: 0.2237\n",
      "Train Epoch: 001 Batch: 00032/00094 | Loss: 0.2457 | EWC Loss: 0.0043 | CE Loss: 0.2414\n",
      "Train Epoch: 001 Batch: 00033/00094 | Loss: 0.3280 | EWC Loss: 0.0043 | CE Loss: 0.3237\n",
      "Train Epoch: 001 Batch: 00034/00094 | Loss: 0.2061 | EWC Loss: 0.0042 | CE Loss: 0.2020\n",
      "Train Epoch: 001 Batch: 00035/00094 | Loss: 0.1846 | EWC Loss: 0.0041 | CE Loss: 0.1805\n",
      "Train Epoch: 001 Batch: 00036/00094 | Loss: 0.1831 | EWC Loss: 0.0041 | CE Loss: 0.1790\n",
      "Train Epoch: 001 Batch: 00037/00094 | Loss: 0.2940 | EWC Loss: 0.0041 | CE Loss: 0.2899\n",
      "Train Epoch: 001 Batch: 00038/00094 | Loss: 0.1110 | EWC Loss: 0.0041 | CE Loss: 0.1068\n",
      "Train Epoch: 001 Batch: 00039/00094 | Loss: 0.3146 | EWC Loss: 0.0042 | CE Loss: 0.3104\n",
      "Train Epoch: 001 Batch: 00040/00094 | Loss: 0.2286 | EWC Loss: 0.0043 | CE Loss: 0.2244\n",
      "Train Epoch: 001 Batch: 00041/00094 | Loss: 0.2643 | EWC Loss: 0.0044 | CE Loss: 0.2600\n",
      "Train Epoch: 001 Batch: 00042/00094 | Loss: 0.1568 | EWC Loss: 0.0045 | CE Loss: 0.1524\n",
      "Train Epoch: 001 Batch: 00043/00094 | Loss: 0.2111 | EWC Loss: 0.0045 | CE Loss: 0.2066\n",
      "Train Epoch: 001 Batch: 00044/00094 | Loss: 0.3165 | EWC Loss: 0.0045 | CE Loss: 0.3120\n",
      "Train Epoch: 001 Batch: 00045/00094 | Loss: 0.0889 | EWC Loss: 0.0045 | CE Loss: 0.0844\n",
      "Train Epoch: 001 Batch: 00046/00094 | Loss: 0.2201 | EWC Loss: 0.0045 | CE Loss: 0.2156\n",
      "Train Epoch: 001 Batch: 00047/00094 | Loss: 0.2458 | EWC Loss: 0.0044 | CE Loss: 0.2414\n",
      "Train Epoch: 001 Batch: 00048/00094 | Loss: 0.2208 | EWC Loss: 0.0046 | CE Loss: 0.2162\n",
      "Train Epoch: 001 Batch: 00049/00094 | Loss: 0.2057 | EWC Loss: 0.0045 | CE Loss: 0.2012\n",
      "Train Epoch: 001 Batch: 00050/00094 | Loss: 0.3796 | EWC Loss: 0.0045 | CE Loss: 0.3752\n",
      "Train Epoch: 001 Batch: 00051/00094 | Loss: 0.2111 | EWC Loss: 0.0044 | CE Loss: 0.2067\n",
      "Train Epoch: 001 Batch: 00052/00094 | Loss: 0.3068 | EWC Loss: 0.0045 | CE Loss: 0.3023\n",
      "Train Epoch: 001 Batch: 00053/00094 | Loss: 0.1889 | EWC Loss: 0.0045 | CE Loss: 0.1844\n",
      "Train Epoch: 001 Batch: 00054/00094 | Loss: 0.2214 | EWC Loss: 0.0047 | CE Loss: 0.2167\n",
      "Train Epoch: 001 Batch: 00055/00094 | Loss: 0.2398 | EWC Loss: 0.0046 | CE Loss: 0.2352\n",
      "Train Epoch: 001 Batch: 00056/00094 | Loss: 0.2129 | EWC Loss: 0.0047 | CE Loss: 0.2081\n",
      "Train Epoch: 001 Batch: 00057/00094 | Loss: 0.1642 | EWC Loss: 0.0048 | CE Loss: 0.1593\n",
      "Train Epoch: 001 Batch: 00058/00094 | Loss: 0.1559 | EWC Loss: 0.0049 | CE Loss: 0.1511\n",
      "Train Epoch: 001 Batch: 00059/00094 | Loss: 0.1670 | EWC Loss: 0.0050 | CE Loss: 0.1620\n",
      "Train Epoch: 001 Batch: 00060/00094 | Loss: 0.1205 | EWC Loss: 0.0050 | CE Loss: 0.1154\n",
      "Train Epoch: 001 Batch: 00061/00094 | Loss: 0.1051 | EWC Loss: 0.0051 | CE Loss: 0.1000\n",
      "Train Epoch: 001 Batch: 00062/00094 | Loss: 0.2664 | EWC Loss: 0.0052 | CE Loss: 0.2612\n",
      "Train Epoch: 001 Batch: 00063/00094 | Loss: 0.3091 | EWC Loss: 0.0051 | CE Loss: 0.3040\n",
      "Train Epoch: 001 Batch: 00064/00094 | Loss: 0.2713 | EWC Loss: 0.0052 | CE Loss: 0.2661\n",
      "Train Epoch: 001 Batch: 00065/00094 | Loss: 0.1192 | EWC Loss: 0.0052 | CE Loss: 0.1140\n",
      "Train Epoch: 001 Batch: 00066/00094 | Loss: 0.1549 | EWC Loss: 0.0051 | CE Loss: 0.1498\n",
      "Train Epoch: 001 Batch: 00067/00094 | Loss: 0.1644 | EWC Loss: 0.0051 | CE Loss: 0.1592\n",
      "Train Epoch: 001 Batch: 00068/00094 | Loss: 0.1309 | EWC Loss: 0.0051 | CE Loss: 0.1258\n",
      "Train Epoch: 001 Batch: 00069/00094 | Loss: 0.2525 | EWC Loss: 0.0050 | CE Loss: 0.2475\n",
      "Train Epoch: 001 Batch: 00070/00094 | Loss: 0.1789 | EWC Loss: 0.0050 | CE Loss: 0.1739\n",
      "Train Epoch: 001 Batch: 00071/00094 | Loss: 0.1583 | EWC Loss: 0.0051 | CE Loss: 0.1532\n",
      "Train Epoch: 001 Batch: 00072/00094 | Loss: 0.0898 | EWC Loss: 0.0052 | CE Loss: 0.0846\n",
      "Train Epoch: 001 Batch: 00073/00094 | Loss: 0.1016 | EWC Loss: 0.0052 | CE Loss: 0.0964\n",
      "Train Epoch: 001 Batch: 00074/00094 | Loss: 0.1839 | EWC Loss: 0.0053 | CE Loss: 0.1787\n",
      "Train Epoch: 001 Batch: 00075/00094 | Loss: 0.2260 | EWC Loss: 0.0052 | CE Loss: 0.2208\n",
      "Train Epoch: 001 Batch: 00076/00094 | Loss: 0.1954 | EWC Loss: 0.0053 | CE Loss: 0.1901\n",
      "Train Epoch: 001 Batch: 00077/00094 | Loss: 0.2987 | EWC Loss: 0.0053 | CE Loss: 0.2934\n",
      "Train Epoch: 001 Batch: 00078/00094 | Loss: 0.2834 | EWC Loss: 0.0052 | CE Loss: 0.2781\n",
      "Train Epoch: 001 Batch: 00079/00094 | Loss: 0.2112 | EWC Loss: 0.0054 | CE Loss: 0.2059\n",
      "Train Epoch: 001 Batch: 00080/00094 | Loss: 0.0803 | EWC Loss: 0.0053 | CE Loss: 0.0750\n",
      "Train Epoch: 001 Batch: 00081/00094 | Loss: 0.1801 | EWC Loss: 0.0053 | CE Loss: 0.1748\n",
      "Train Epoch: 001 Batch: 00082/00094 | Loss: 0.1388 | EWC Loss: 0.0054 | CE Loss: 0.1334\n",
      "Train Epoch: 001 Batch: 00083/00094 | Loss: 0.2028 | EWC Loss: 0.0054 | CE Loss: 0.1974\n",
      "Train Epoch: 001 Batch: 00084/00094 | Loss: 0.1968 | EWC Loss: 0.0055 | CE Loss: 0.1913\n",
      "Train Epoch: 001 Batch: 00085/00094 | Loss: 0.2402 | EWC Loss: 0.0056 | CE Loss: 0.2346\n",
      "Train Epoch: 001 Batch: 00086/00094 | Loss: 0.1984 | EWC Loss: 0.0056 | CE Loss: 0.1928\n",
      "Train Epoch: 001 Batch: 00087/00094 | Loss: 0.1706 | EWC Loss: 0.0056 | CE Loss: 0.1650\n",
      "Train Epoch: 001 Batch: 00088/00094 | Loss: 0.1683 | EWC Loss: 0.0056 | CE Loss: 0.1627\n",
      "Train Epoch: 001 Batch: 00089/00094 | Loss: 0.3026 | EWC Loss: 0.0056 | CE Loss: 0.2970\n",
      "Train Epoch: 001 Batch: 00090/00094 | Loss: 0.1175 | EWC Loss: 0.0057 | CE Loss: 0.1118\n",
      "Train Epoch: 001 Batch: 00091/00094 | Loss: 0.1028 | EWC Loss: 0.0057 | CE Loss: 0.0971\n",
      "Train Epoch: 001 Batch: 00092/00094 | Loss: 0.2657 | EWC Loss: 0.0056 | CE Loss: 0.2600\n",
      "Train Epoch: 001 Batch: 00093/00094 | Loss: 0.1285 | EWC Loss: 0.0059 | CE Loss: 0.1226\n",
      "Train Epoch: 001 Batch: 00094/00094 | Loss: 0.3357 | EWC Loss: 0.0059 | CE Loss: 0.3299\n",
      "Train Epoch: 001 |Acc 91.60000 | Loss: 0.21306 | Task Loss 0.20850 | EWC Loss: 0.00456\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.2021 | acc:94.0000\n",
      "[VAL Acc] Target: 94.00%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:1.9255 | acc:46.6000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 46.60%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:0.8018 | acc:68.6250\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 68.62%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.6274 | acc:47.5191\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 47.52%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:0.8865 | acc:63.8323\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 63.83%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.1750 | acc:46.6728\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 46.67%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:0.9496 | acc:65.9875\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 65.99%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.5303 | acc:48.5000\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 48.50%\n",
      "[VAL Acc] Avg 60.22%\n",
      "\n",
      "\n",
      "---------- Starting epoch 2 ----------\n",
      "Train Epoch: 002 Batch: 00001/00094 | Loss: 0.1806 | EWC Loss: 0.0057 | CE Loss: 0.1749\n",
      "Train Epoch: 002 Batch: 00002/00094 | Loss: 0.2818 | EWC Loss: 0.0057 | CE Loss: 0.2761\n",
      "Train Epoch: 002 Batch: 00003/00094 | Loss: 0.0905 | EWC Loss: 0.0057 | CE Loss: 0.0847\n",
      "Train Epoch: 002 Batch: 00004/00094 | Loss: 0.2142 | EWC Loss: 0.0057 | CE Loss: 0.2085\n",
      "Train Epoch: 002 Batch: 00005/00094 | Loss: 0.1502 | EWC Loss: 0.0057 | CE Loss: 0.1445\n",
      "Train Epoch: 002 Batch: 00006/00094 | Loss: 0.1568 | EWC Loss: 0.0057 | CE Loss: 0.1511\n",
      "Train Epoch: 002 Batch: 00007/00094 | Loss: 0.0967 | EWC Loss: 0.0057 | CE Loss: 0.0910\n",
      "Train Epoch: 002 Batch: 00008/00094 | Loss: 0.2615 | EWC Loss: 0.0057 | CE Loss: 0.2558\n",
      "Train Epoch: 002 Batch: 00009/00094 | Loss: 0.2472 | EWC Loss: 0.0058 | CE Loss: 0.2415\n",
      "Train Epoch: 002 Batch: 00010/00094 | Loss: 0.2269 | EWC Loss: 0.0058 | CE Loss: 0.2211\n",
      "Train Epoch: 002 Batch: 00011/00094 | Loss: 0.1192 | EWC Loss: 0.0058 | CE Loss: 0.1134\n",
      "Train Epoch: 002 Batch: 00012/00094 | Loss: 0.2599 | EWC Loss: 0.0059 | CE Loss: 0.2541\n",
      "Train Epoch: 002 Batch: 00013/00094 | Loss: 0.1787 | EWC Loss: 0.0059 | CE Loss: 0.1728\n",
      "Train Epoch: 002 Batch: 00014/00094 | Loss: 0.0947 | EWC Loss: 0.0059 | CE Loss: 0.0887\n",
      "Train Epoch: 002 Batch: 00015/00094 | Loss: 0.1667 | EWC Loss: 0.0059 | CE Loss: 0.1607\n",
      "Train Epoch: 002 Batch: 00016/00094 | Loss: 0.2170 | EWC Loss: 0.0058 | CE Loss: 0.2112\n",
      "Train Epoch: 002 Batch: 00017/00094 | Loss: 0.2371 | EWC Loss: 0.0058 | CE Loss: 0.2313\n",
      "Train Epoch: 002 Batch: 00018/00094 | Loss: 0.2060 | EWC Loss: 0.0057 | CE Loss: 0.2002\n",
      "Train Epoch: 002 Batch: 00019/00094 | Loss: 0.1975 | EWC Loss: 0.0057 | CE Loss: 0.1917\n",
      "Train Epoch: 002 Batch: 00020/00094 | Loss: 0.0979 | EWC Loss: 0.0057 | CE Loss: 0.0922\n",
      "Train Epoch: 002 Batch: 00021/00094 | Loss: 0.1639 | EWC Loss: 0.0057 | CE Loss: 0.1582\n",
      "Train Epoch: 002 Batch: 00022/00094 | Loss: 0.1564 | EWC Loss: 0.0058 | CE Loss: 0.1506\n",
      "Train Epoch: 002 Batch: 00023/00094 | Loss: 0.1243 | EWC Loss: 0.0060 | CE Loss: 0.1184\n",
      "Train Epoch: 002 Batch: 00024/00094 | Loss: 0.1447 | EWC Loss: 0.0059 | CE Loss: 0.1388\n",
      "Train Epoch: 002 Batch: 00025/00094 | Loss: 0.2002 | EWC Loss: 0.0059 | CE Loss: 0.1942\n",
      "Train Epoch: 002 Batch: 00026/00094 | Loss: 0.1066 | EWC Loss: 0.0060 | CE Loss: 0.1006\n",
      "Train Epoch: 002 Batch: 00027/00094 | Loss: 0.1455 | EWC Loss: 0.0060 | CE Loss: 0.1395\n",
      "Train Epoch: 002 Batch: 00028/00094 | Loss: 0.2999 | EWC Loss: 0.0060 | CE Loss: 0.2939\n",
      "Train Epoch: 002 Batch: 00029/00094 | Loss: 0.0994 | EWC Loss: 0.0060 | CE Loss: 0.0934\n",
      "Train Epoch: 002 Batch: 00030/00094 | Loss: 0.1641 | EWC Loss: 0.0059 | CE Loss: 0.1582\n",
      "Train Epoch: 002 Batch: 00031/00094 | Loss: 0.2705 | EWC Loss: 0.0059 | CE Loss: 0.2646\n",
      "Train Epoch: 002 Batch: 00032/00094 | Loss: 0.1844 | EWC Loss: 0.0060 | CE Loss: 0.1784\n",
      "Train Epoch: 002 Batch: 00033/00094 | Loss: 0.1384 | EWC Loss: 0.0061 | CE Loss: 0.1323\n",
      "Train Epoch: 002 Batch: 00034/00094 | Loss: 0.1617 | EWC Loss: 0.0062 | CE Loss: 0.1555\n",
      "Train Epoch: 002 Batch: 00035/00094 | Loss: 0.1507 | EWC Loss: 0.0062 | CE Loss: 0.1445\n",
      "Train Epoch: 002 Batch: 00036/00094 | Loss: 0.0833 | EWC Loss: 0.0062 | CE Loss: 0.0771\n",
      "Train Epoch: 002 Batch: 00037/00094 | Loss: 0.1429 | EWC Loss: 0.0062 | CE Loss: 0.1367\n",
      "Train Epoch: 002 Batch: 00038/00094 | Loss: 0.1002 | EWC Loss: 0.0062 | CE Loss: 0.0939\n",
      "Train Epoch: 002 Batch: 00039/00094 | Loss: 0.2160 | EWC Loss: 0.0063 | CE Loss: 0.2098\n",
      "Train Epoch: 002 Batch: 00040/00094 | Loss: 0.1364 | EWC Loss: 0.0063 | CE Loss: 0.1301\n",
      "Train Epoch: 002 Batch: 00041/00094 | Loss: 0.1707 | EWC Loss: 0.0063 | CE Loss: 0.1644\n",
      "Train Epoch: 002 Batch: 00042/00094 | Loss: 0.3867 | EWC Loss: 0.0062 | CE Loss: 0.3805\n",
      "Train Epoch: 002 Batch: 00043/00094 | Loss: 0.1239 | EWC Loss: 0.0065 | CE Loss: 0.1174\n",
      "Train Epoch: 002 Batch: 00044/00094 | Loss: 0.1566 | EWC Loss: 0.0065 | CE Loss: 0.1501\n",
      "Train Epoch: 002 Batch: 00045/00094 | Loss: 0.2202 | EWC Loss: 0.0065 | CE Loss: 0.2137\n",
      "Train Epoch: 002 Batch: 00046/00094 | Loss: 0.2166 | EWC Loss: 0.0065 | CE Loss: 0.2101\n",
      "Train Epoch: 002 Batch: 00047/00094 | Loss: 0.4454 | EWC Loss: 0.0066 | CE Loss: 0.4388\n",
      "Train Epoch: 002 Batch: 00048/00094 | Loss: 0.1853 | EWC Loss: 0.0065 | CE Loss: 0.1788\n",
      "Train Epoch: 002 Batch: 00049/00094 | Loss: 0.1379 | EWC Loss: 0.0064 | CE Loss: 0.1315\n",
      "Train Epoch: 002 Batch: 00050/00094 | Loss: 0.1539 | EWC Loss: 0.0064 | CE Loss: 0.1475\n",
      "Train Epoch: 002 Batch: 00051/00094 | Loss: 0.2140 | EWC Loss: 0.0064 | CE Loss: 0.2076\n",
      "Train Epoch: 002 Batch: 00052/00094 | Loss: 0.1415 | EWC Loss: 0.0065 | CE Loss: 0.1349\n",
      "Train Epoch: 002 Batch: 00053/00094 | Loss: 0.0977 | EWC Loss: 0.0066 | CE Loss: 0.0912\n",
      "Train Epoch: 002 Batch: 00054/00094 | Loss: 0.1266 | EWC Loss: 0.0065 | CE Loss: 0.1200\n",
      "Train Epoch: 002 Batch: 00055/00094 | Loss: 0.1391 | EWC Loss: 0.0065 | CE Loss: 0.1326\n",
      "Train Epoch: 002 Batch: 00056/00094 | Loss: 0.1667 | EWC Loss: 0.0066 | CE Loss: 0.1601\n",
      "Train Epoch: 002 Batch: 00057/00094 | Loss: 0.0933 | EWC Loss: 0.0067 | CE Loss: 0.0866\n",
      "Train Epoch: 002 Batch: 00058/00094 | Loss: 0.1928 | EWC Loss: 0.0067 | CE Loss: 0.1861\n",
      "Train Epoch: 002 Batch: 00059/00094 | Loss: 0.1196 | EWC Loss: 0.0066 | CE Loss: 0.1130\n",
      "Train Epoch: 002 Batch: 00060/00094 | Loss: 0.2195 | EWC Loss: 0.0065 | CE Loss: 0.2130\n",
      "Train Epoch: 002 Batch: 00061/00094 | Loss: 0.2398 | EWC Loss: 0.0066 | CE Loss: 0.2332\n",
      "Train Epoch: 002 Batch: 00062/00094 | Loss: 0.1186 | EWC Loss: 0.0066 | CE Loss: 0.1121\n",
      "Train Epoch: 002 Batch: 00063/00094 | Loss: 0.1475 | EWC Loss: 0.0067 | CE Loss: 0.1408\n",
      "Train Epoch: 002 Batch: 00064/00094 | Loss: 0.2645 | EWC Loss: 0.0066 | CE Loss: 0.2579\n",
      "Train Epoch: 002 Batch: 00065/00094 | Loss: 0.0834 | EWC Loss: 0.0067 | CE Loss: 0.0768\n",
      "Train Epoch: 002 Batch: 00066/00094 | Loss: 0.1278 | EWC Loss: 0.0066 | CE Loss: 0.1212\n",
      "Train Epoch: 002 Batch: 00067/00094 | Loss: 0.1308 | EWC Loss: 0.0066 | CE Loss: 0.1242\n",
      "Train Epoch: 002 Batch: 00068/00094 | Loss: 0.1104 | EWC Loss: 0.0066 | CE Loss: 0.1038\n",
      "Train Epoch: 002 Batch: 00069/00094 | Loss: 0.1710 | EWC Loss: 0.0066 | CE Loss: 0.1644\n",
      "Train Epoch: 002 Batch: 00070/00094 | Loss: 0.0783 | EWC Loss: 0.0068 | CE Loss: 0.0715\n",
      "Train Epoch: 002 Batch: 00071/00094 | Loss: 0.0952 | EWC Loss: 0.0068 | CE Loss: 0.0884\n",
      "Train Epoch: 002 Batch: 00072/00094 | Loss: 0.0909 | EWC Loss: 0.0068 | CE Loss: 0.0840\n",
      "Train Epoch: 002 Batch: 00073/00094 | Loss: 0.1560 | EWC Loss: 0.0068 | CE Loss: 0.1491\n",
      "Train Epoch: 002 Batch: 00074/00094 | Loss: 0.1642 | EWC Loss: 0.0069 | CE Loss: 0.1573\n",
      "Train Epoch: 002 Batch: 00075/00094 | Loss: 0.0995 | EWC Loss: 0.0069 | CE Loss: 0.0926\n",
      "Train Epoch: 002 Batch: 00076/00094 | Loss: 0.0631 | EWC Loss: 0.0069 | CE Loss: 0.0562\n",
      "Train Epoch: 002 Batch: 00077/00094 | Loss: 0.0885 | EWC Loss: 0.0070 | CE Loss: 0.0815\n",
      "Train Epoch: 002 Batch: 00078/00094 | Loss: 0.1702 | EWC Loss: 0.0069 | CE Loss: 0.1633\n",
      "Train Epoch: 002 Batch: 00079/00094 | Loss: 0.1127 | EWC Loss: 0.0069 | CE Loss: 0.1058\n",
      "Train Epoch: 002 Batch: 00080/00094 | Loss: 0.1584 | EWC Loss: 0.0069 | CE Loss: 0.1515\n",
      "Train Epoch: 002 Batch: 00081/00094 | Loss: 0.1512 | EWC Loss: 0.0069 | CE Loss: 0.1442\n",
      "Train Epoch: 002 Batch: 00082/00094 | Loss: 0.1599 | EWC Loss: 0.0069 | CE Loss: 0.1530\n",
      "Train Epoch: 002 Batch: 00083/00094 | Loss: 0.0765 | EWC Loss: 0.0069 | CE Loss: 0.0696\n",
      "Train Epoch: 002 Batch: 00084/00094 | Loss: 0.2318 | EWC Loss: 0.0069 | CE Loss: 0.2249\n",
      "Train Epoch: 002 Batch: 00085/00094 | Loss: 0.1890 | EWC Loss: 0.0070 | CE Loss: 0.1820\n",
      "Train Epoch: 002 Batch: 00086/00094 | Loss: 0.1312 | EWC Loss: 0.0070 | CE Loss: 0.1242\n",
      "Train Epoch: 002 Batch: 00087/00094 | Loss: 0.0818 | EWC Loss: 0.0070 | CE Loss: 0.0748\n",
      "Train Epoch: 002 Batch: 00088/00094 | Loss: 0.1348 | EWC Loss: 0.0071 | CE Loss: 0.1277\n",
      "Train Epoch: 002 Batch: 00089/00094 | Loss: 0.1124 | EWC Loss: 0.0070 | CE Loss: 0.1053\n",
      "Train Epoch: 002 Batch: 00090/00094 | Loss: 0.1729 | EWC Loss: 0.0071 | CE Loss: 0.1658\n",
      "Train Epoch: 002 Batch: 00091/00094 | Loss: 0.1600 | EWC Loss: 0.0073 | CE Loss: 0.1527\n",
      "Train Epoch: 002 Batch: 00092/00094 | Loss: 0.1799 | EWC Loss: 0.0073 | CE Loss: 0.1726\n",
      "Train Epoch: 002 Batch: 00093/00094 | Loss: 0.2824 | EWC Loss: 0.0073 | CE Loss: 0.2752\n",
      "Train Epoch: 002 Batch: 00094/00094 | Loss: 0.1369 | EWC Loss: 0.0073 | CE Loss: 0.1295\n",
      "Train Epoch: 002 |Acc 93.81667 | Loss: 0.16436 | Task Loss 0.15797 | EWC Loss: 0.00639\n",
      "===> Starting TEST\n",
      "\n",
      "Test results | Loss:0.1620 | acc:94.2000\n",
      "[VAL Acc] Target: 94.20%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/gaugan\n",
      "\n",
      "Test results | Loss:2.2729 | acc:47.2500\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/gaugan: 47.25%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/biggan\n",
      "\n",
      "Test results | Loss:1.0273 | acc:62.8750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/biggan: 62.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/cyclegan\n",
      "\n",
      "Test results | Loss:1.8824 | acc:46.1832\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/cyclegan: 46.18%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/imle\n",
      "\n",
      "Test results | Loss:1.2141 | acc:58.2680\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/imle: 58.27%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/faceforensics\n",
      "\n",
      "Test results | Loss:1.0494 | acc:56.0998\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/faceforensics: 56.10%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/crn\n",
      "\n",
      "Test results | Loss:1.4863 | acc:57.8762\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/crn: 57.88%\n",
      "===> Starting the dataset ../KD-AIGC-Detection/datasets/custom/wild\n",
      "\n",
      "Test results | Loss:1.2224 | acc:51.3750\n",
      "[VAL Acc] Source ../KD-AIGC-Detection/datasets/custom/wild: 51.38%\n",
      "[VAL Acc] Avg 59.27%\n",
      "\n",
      "\n",
      "---------- Starting epoch 3 ----------\n",
      "Train Epoch: 003 Batch: 00001/00094 | Loss: 0.1446 | EWC Loss: 0.0074 | CE Loss: 0.1373\n",
      "Train Epoch: 003 Batch: 00002/00094 | Loss: 0.1984 | EWC Loss: 0.0074 | CE Loss: 0.1910\n",
      "Train Epoch: 003 Batch: 00003/00094 | Loss: 0.1026 | EWC Loss: 0.0073 | CE Loss: 0.0953\n",
      "Train Epoch: 003 Batch: 00004/00094 | Loss: 0.1931 | EWC Loss: 0.0073 | CE Loss: 0.1859\n",
      "Train Epoch: 003 Batch: 00005/00094 | Loss: 0.2019 | EWC Loss: 0.0073 | CE Loss: 0.1946\n",
      "Train Epoch: 003 Batch: 00006/00094 | Loss: 0.0710 | EWC Loss: 0.0074 | CE Loss: 0.0636\n",
      "Train Epoch: 003 Batch: 00007/00094 | Loss: 0.1421 | EWC Loss: 0.0074 | CE Loss: 0.1347\n",
      "Train Epoch: 003 Batch: 00008/00094 | Loss: 0.1156 | EWC Loss: 0.0073 | CE Loss: 0.1084\n",
      "Train Epoch: 003 Batch: 00009/00094 | Loss: 0.0975 | EWC Loss: 0.0072 | CE Loss: 0.0903\n",
      "Train Epoch: 003 Batch: 00010/00094 | Loss: 0.1882 | EWC Loss: 0.0072 | CE Loss: 0.1811\n",
      "Train Epoch: 003 Batch: 00011/00094 | Loss: 0.0864 | EWC Loss: 0.0072 | CE Loss: 0.0792\n",
      "Train Epoch: 003 Batch: 00012/00094 | Loss: 0.1466 | EWC Loss: 0.0072 | CE Loss: 0.1394\n",
      "Train Epoch: 003 Batch: 00013/00094 | Loss: 0.0889 | EWC Loss: 0.0073 | CE Loss: 0.0816\n",
      "Train Epoch: 003 Batch: 00014/00094 | Loss: 0.0943 | EWC Loss: 0.0073 | CE Loss: 0.0870\n",
      "Train Epoch: 003 Batch: 00015/00094 | Loss: 0.1004 | EWC Loss: 0.0074 | CE Loss: 0.0931\n",
      "Train Epoch: 003 Batch: 00016/00094 | Loss: 0.0767 | EWC Loss: 0.0074 | CE Loss: 0.0693\n",
      "Train Epoch: 003 Batch: 00017/00094 | Loss: 0.0933 | EWC Loss: 0.0074 | CE Loss: 0.0859\n",
      "Train Epoch: 003 Batch: 00018/00094 | Loss: 0.1353 | EWC Loss: 0.0074 | CE Loss: 0.1278\n",
      "Train Epoch: 003 Batch: 00019/00094 | Loss: 0.1668 | EWC Loss: 0.0074 | CE Loss: 0.1594\n",
      "Train Epoch: 003 Batch: 00020/00094 | Loss: 0.0886 | EWC Loss: 0.0074 | CE Loss: 0.0811\n",
      "Train Epoch: 003 Batch: 00021/00094 | Loss: 0.1954 | EWC Loss: 0.0075 | CE Loss: 0.1879\n",
      "Train Epoch: 003 Batch: 00022/00094 | Loss: 0.1767 | EWC Loss: 0.0075 | CE Loss: 0.1692\n",
      "Train Epoch: 003 Batch: 00023/00094 | Loss: 0.0701 | EWC Loss: 0.0075 | CE Loss: 0.0626\n",
      "Train Epoch: 003 Batch: 00024/00094 | Loss: 0.1218 | EWC Loss: 0.0075 | CE Loss: 0.1142\n",
      "Train Epoch: 003 Batch: 00025/00094 | Loss: 0.1325 | EWC Loss: 0.0075 | CE Loss: 0.1250\n",
      "Train Epoch: 003 Batch: 00026/00094 | Loss: 0.1241 | EWC Loss: 0.0074 | CE Loss: 0.1167\n",
      "Train Epoch: 003 Batch: 00027/00094 | Loss: 0.1513 | EWC Loss: 0.0075 | CE Loss: 0.1438\n",
      "Train Epoch: 003 Batch: 00028/00094 | Loss: 0.0982 | EWC Loss: 0.0075 | CE Loss: 0.0907\n",
      "Train Epoch: 003 Batch: 00029/00094 | Loss: 0.1296 | EWC Loss: 0.0075 | CE Loss: 0.1221\n",
      "Train Epoch: 003 Batch: 00030/00094 | Loss: 0.1904 | EWC Loss: 0.0075 | CE Loss: 0.1829\n",
      "Train Epoch: 003 Batch: 00031/00094 | Loss: 0.1476 | EWC Loss: 0.0074 | CE Loss: 0.1402\n",
      "Train Epoch: 003 Batch: 00032/00094 | Loss: 0.1799 | EWC Loss: 0.0076 | CE Loss: 0.1723\n",
      "Train Epoch: 003 Batch: 00033/00094 | Loss: 0.1565 | EWC Loss: 0.0076 | CE Loss: 0.1489\n",
      "Train Epoch: 003 Batch: 00034/00094 | Loss: 0.1497 | EWC Loss: 0.0075 | CE Loss: 0.1422\n",
      "Train Epoch: 003 Batch: 00035/00094 | Loss: 0.1216 | EWC Loss: 0.0075 | CE Loss: 0.1141\n",
      "Train Epoch: 003 Batch: 00036/00094 | Loss: 0.0984 | EWC Loss: 0.0075 | CE Loss: 0.0908\n",
      "Train Epoch: 003 Batch: 00037/00094 | Loss: 0.0911 | EWC Loss: 0.0076 | CE Loss: 0.0836\n",
      "Train Epoch: 003 Batch: 00038/00094 | Loss: 0.1257 | EWC Loss: 0.0075 | CE Loss: 0.1181\n",
      "Train Epoch: 003 Batch: 00039/00094 | Loss: 0.1116 | EWC Loss: 0.0075 | CE Loss: 0.1040\n",
      "Train Epoch: 003 Batch: 00040/00094 | Loss: 0.1544 | EWC Loss: 0.0076 | CE Loss: 0.1468\n",
      "Train Epoch: 003 Batch: 00041/00094 | Loss: 0.0626 | EWC Loss: 0.0077 | CE Loss: 0.0549\n",
      "Train Epoch: 003 Batch: 00042/00094 | Loss: 0.1431 | EWC Loss: 0.0077 | CE Loss: 0.1354\n",
      "Train Epoch: 003 Batch: 00043/00094 | Loss: 0.1685 | EWC Loss: 0.0077 | CE Loss: 0.1608\n",
      "Train Epoch: 003 Batch: 00044/00094 | Loss: 0.1295 | EWC Loss: 0.0078 | CE Loss: 0.1217\n",
      "Train Epoch: 003 Batch: 00045/00094 | Loss: 0.1487 | EWC Loss: 0.0077 | CE Loss: 0.1410\n",
      "Train Epoch: 003 Batch: 00046/00094 | Loss: 0.1096 | EWC Loss: 0.0076 | CE Loss: 0.1020\n",
      "Train Epoch: 003 Batch: 00047/00094 | Loss: 0.1708 | EWC Loss: 0.0076 | CE Loss: 0.1632\n",
      "Train Epoch: 003 Batch: 00048/00094 | Loss: 0.1103 | EWC Loss: 0.0077 | CE Loss: 0.1026\n",
      "Train Epoch: 003 Batch: 00049/00094 | Loss: 0.0932 | EWC Loss: 0.0076 | CE Loss: 0.0856\n",
      "Train Epoch: 003 Batch: 00050/00094 | Loss: 0.2104 | EWC Loss: 0.0076 | CE Loss: 0.2028\n",
      "Train Epoch: 003 Batch: 00051/00094 | Loss: 0.0647 | EWC Loss: 0.0076 | CE Loss: 0.0571\n",
      "Train Epoch: 003 Batch: 00052/00094 | Loss: 0.1067 | EWC Loss: 0.0076 | CE Loss: 0.0990\n",
      "Train Epoch: 003 Batch: 00053/00094 | Loss: 0.2639 | EWC Loss: 0.0076 | CE Loss: 0.2563\n",
      "Train Epoch: 003 Batch: 00054/00094 | Loss: 0.1205 | EWC Loss: 0.0078 | CE Loss: 0.1127\n",
      "Train Epoch: 003 Batch: 00055/00094 | Loss: 0.1084 | EWC Loss: 0.0077 | CE Loss: 0.1007\n",
      "Train Epoch: 003 Batch: 00056/00094 | Loss: 0.0741 | EWC Loss: 0.0077 | CE Loss: 0.0664\n",
      "Train Epoch: 003 Batch: 00057/00094 | Loss: 0.0962 | EWC Loss: 0.0077 | CE Loss: 0.0885\n",
      "Train Epoch: 003 Batch: 00058/00094 | Loss: 0.1426 | EWC Loss: 0.0078 | CE Loss: 0.1349\n",
      "Train Epoch: 003 Batch: 00059/00094 | Loss: 0.1970 | EWC Loss: 0.0079 | CE Loss: 0.1891\n",
      "Train Epoch: 003 Batch: 00060/00094 | Loss: 0.0453 | EWC Loss: 0.0079 | CE Loss: 0.0374\n",
      "Train Epoch: 003 Batch: 00061/00094 | Loss: 0.1142 | EWC Loss: 0.0079 | CE Loss: 0.1063\n",
      "Train Epoch: 003 Batch: 00062/00094 | Loss: 0.1390 | EWC Loss: 0.0079 | CE Loss: 0.1311\n",
      "Train Epoch: 003 Batch: 00063/00094 | Loss: 0.0617 | EWC Loss: 0.0080 | CE Loss: 0.0537\n",
      "Train Epoch: 003 Batch: 00064/00094 | Loss: 0.1888 | EWC Loss: 0.0079 | CE Loss: 0.1809\n",
      "Train Epoch: 003 Batch: 00065/00094 | Loss: 0.2311 | EWC Loss: 0.0078 | CE Loss: 0.2232\n",
      "Train Epoch: 003 Batch: 00066/00094 | Loss: 0.0734 | EWC Loss: 0.0079 | CE Loss: 0.0656\n",
      "Train Epoch: 003 Batch: 00067/00094 | Loss: 0.0924 | EWC Loss: 0.0079 | CE Loss: 0.0845\n",
      "Train Epoch: 003 Batch: 00068/00094 | Loss: 0.1421 | EWC Loss: 0.0079 | CE Loss: 0.1342\n",
      "Train Epoch: 003 Batch: 00069/00094 | Loss: 0.0893 | EWC Loss: 0.0079 | CE Loss: 0.0814\n",
      "Train Epoch: 003 Batch: 00070/00094 | Loss: 0.1169 | EWC Loss: 0.0079 | CE Loss: 0.1090\n",
      "Train Epoch: 003 Batch: 00071/00094 | Loss: 0.1443 | EWC Loss: 0.0080 | CE Loss: 0.1364\n",
      "Train Epoch: 003 Batch: 00072/00094 | Loss: 0.0672 | EWC Loss: 0.0080 | CE Loss: 0.0592\n",
      "Train Epoch: 003 Batch: 00073/00094 | Loss: 0.1228 | EWC Loss: 0.0079 | CE Loss: 0.1149\n",
      "Train Epoch: 003 Batch: 00074/00094 | Loss: 0.1802 | EWC Loss: 0.0079 | CE Loss: 0.1723\n",
      "Train Epoch: 003 Batch: 00075/00094 | Loss: 0.1213 | EWC Loss: 0.0079 | CE Loss: 0.1133\n",
      "Train Epoch: 003 Batch: 00076/00094 | Loss: 0.1369 | EWC Loss: 0.0079 | CE Loss: 0.1289\n",
      "Train Epoch: 003 Batch: 00077/00094 | Loss: 0.0755 | EWC Loss: 0.0079 | CE Loss: 0.0676\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "SOURCE_DATASETS = [\"gaugan\", \"biggan\", \"cyclegan\", \"imle\", \"faceforensics\", \"crn\", \"wild\"] \n",
    "TARGET_DATASET = \"diffusionshort\"\n",
    "NETWORK = \"Xception\"\n",
    "CHECKPOINT_DIR_SOURCE = \"../KD-AIGC-Detection/checkpoints/EWC_Xception/cddb_easy\"\n",
    "CHECKPOINT_DIR_TARGET = \"../KD-AIGC-Detection/checkpoints/EWC_Xception/diff\"\n",
    "DATASET_DIR = \"../KD-AIGC-Detection/datasets/custom\"\n",
    "COMET_NAME = \"tewcxc_diff\"\n",
    "\n",
    "\n",
    "source_string = \"_\".join(SOURCE_DATASETS)\n",
    "complete_string = f\"{source_string}_{TARGET_DATASET}\"\n",
    "\n",
    "input_model = f\"{CHECKPOINT_DIR_SOURCE}/{source_string}\"\n",
    "output_dir = f\"{CHECKPOINT_DIR_TARGET}/{complete_string}\"\n",
    "source_datasets_string = \",\".join([f\"{DATASET_DIR}/{ds}\" for ds in SOURCE_DATASETS])\n",
    "target_dataset_string = f\"{DATASET_DIR}/{TARGET_DATASET}\"\n",
    "\n",
    "!python ./src/ewc_train.py --network $NETWORK \\\n",
    "    --input_model $input_model \\\n",
    "    --output_dir $output_dir \\\n",
    "    --source_datasets $source_datasets_string \\\n",
    "    --target_dataset $target_dataset_string \\\n",
    "    --use_comet \\\n",
    "    --comet_name $COMET_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
