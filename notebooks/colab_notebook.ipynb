{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI1CsdAZN7PS"
      },
      "source": [
        "### Startup\n",
        "\n",
        "This code is meant to be executed on Google Colab.\n",
        "To use it locally change *COLAB_MODE* to False.\n",
        "\n",
        "**Note**: remember to change *workdir* accordingly, the notebook must be runned inside the root project folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzSZjcNvoPgs",
        "outputId": "13bebaa2-3950-48c0-d7b6-73a9bb887b86"
      },
      "outputs": [],
      "source": [
        "COLAB_MODE = True\n",
        "\n",
        "if COLAB_MODE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  workdir = \"/content/drive/<workspace>\"\n",
        "  %cd $workdir\n",
        "\n",
        "else: workdir = \"./\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0F-IgeM6ckf"
      },
      "source": [
        "We use TensorBoardX + Comet.com for logging. (Update the comet config with your data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV9bRtdIOB-J",
        "outputId": "46635d7e-6b1f-4835-e758-01360b99ffe7"
      },
      "outputs": [],
      "source": [
        "# Comet + Tensorboard\n",
        "!pip install comet_ml --quiet\n",
        "!pip install tensorboardX --quiet\n",
        "import comet_ml\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "comet_config = {\n",
        "   \"api_key\": \"<key>\", #Insert your Comet API key\n",
        "   \"workspace\":\"<workspace>\", # Comet workspace\n",
        "   \"project_name\":\"<project>\",\n",
        "   \"disabled\": False # Disable Comet logging\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4IaJsJClVcS"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Dataset configuration and builders. Each dataset must be stored in\n",
        "`datasets/<type>/<dataset_name>.zip`. They will be extracted at runtime in `$destination_path/<dataset_name>` (working directly on Google Drive is not recommended)\n",
        "\n",
        "Expected dataset structure\n",
        "\n",
        "\n",
        "```\n",
        "train/\n",
        "---0_real/\n",
        "------img1.png\n",
        "------img2.png\n",
        "---1_fake/\n",
        "------img1.png\n",
        "------img2.png\n",
        "val/\n",
        "---0_real/\n",
        "------img1.png\n",
        "------img2.png\n",
        "---1_fake/\n",
        "------img1.png\n",
        "------img2.png\n",
        "test/\n",
        "---0_real/\n",
        "------img1.png\n",
        "------img2.png\n",
        "---1_fake/\n",
        "------img1.png\n",
        "------img2.png\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AmihFPLdyG-B"
      },
      "outputs": [],
      "source": [
        "def get_dataset_cfg(name, args):\n",
        "  if args.ds_cfg[\"type\"] == \"guarnera\":\n",
        "    return get_guarnera_cfg(name)\n",
        "  elif args.ds_cfg[\"type\"] == \"cddb\":\n",
        "    return get_cddb_cfg(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOLoqya2xXHu"
      },
      "source": [
        "## Guarnera\n",
        "\n",
        "Available here: https://iplab.dmi.unict.it/mfs/FightingDeepfake/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3mt3Fx6U4FWW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os, zipfile, shutil\n",
        "\n",
        "def get_guarnera_cfg(name):\n",
        "\n",
        "  ds_infos = {\n",
        "      \"celeba\": {\n",
        "          \"real\":       True,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/CELEBA.zip\",\n",
        "          \"n_total\":    35976,\n",
        "          \"n_train\":    2000,\n",
        "          \"n_val\":       1000,\n",
        "          \"n_test\":      3000,\n",
        "          \"shuffle\":     False,\n",
        "          \"batch\":       True,\n",
        "          },\n",
        "      \"ffhq\": {\n",
        "          \"real\":       True,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/FFHQ.zip\",\n",
        "          \"n_total\":    4000,\n",
        "          \"n_train\":    2000,\n",
        "          \"n_val\":       600,\n",
        "          \"n_test\":      1400,\n",
        "          },\n",
        "      \"attgan\": {\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/AttGAN.zip\",\n",
        "          \"n_total\":    6005,\n",
        "          \"n_train\":    2000,\n",
        "          \"n_val\":       1000,\n",
        "          \"n_test\":      3000,\n",
        "          \"batch_id\":    0,\n",
        "          },\n",
        "      \"cyclegan\": {\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/CYCLEGAN.zip\",\n",
        "          \"n_total\":    2190,\n",
        "          \"n_train\":    1000,\n",
        "          \"n_val\":       500,\n",
        "          \"n_test\":      690,\n",
        "          \"batch_id\":    5,\n",
        "          },\n",
        "      \"faceapp\":{\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/FACEAPP.zip\",\n",
        "          \"n_total\":    471,\n",
        "          \"n_train\":    250,\n",
        "          \"n_val\":      50,\n",
        "          \"n_test\":      171,\n",
        "          \"batch_id\":    -1,\n",
        "          },\n",
        "      \"gdwct\": {\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/GDWCT.zip\",\n",
        "          \"n_total\":    3367,\n",
        "          \"n_train\":    1700,\n",
        "          \"n_val\":      500,\n",
        "          \"n_test\":      1167,\n",
        "          \"batch_id\":    4,\n",
        "          },\n",
        "      \"imle\": {\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/IMLE.zip\",\n",
        "          \"n_total\":    2006,\n",
        "          \"n_train\":    1000,\n",
        "          \"n_val\":      500,\n",
        "          \"n_test\":      506,\n",
        "          \"batch_id\":    5,\n",
        "          },\n",
        "      \"progan\": {\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/ProGAN.zip\",\n",
        "          \"n_total\":    4000,\n",
        "          \"n_train\":    2000,\n",
        "          \"n_val\":      1000,\n",
        "          \"n_test\":     1000,\n",
        "          \"batch_id\":   -1,\n",
        "          },\n",
        "      \"stargan\": {\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/STARGAN.zip\",\n",
        "          \"n_total\":    5648,\n",
        "          \"n_train\":    2000,\n",
        "          \"n_val\":      1000,\n",
        "          \"n_test\":     2648,\n",
        "          \"batch_id\":   1,\n",
        "          },\n",
        "      \"stylegan\": {\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/STYLEGAN.zip\",\n",
        "          \"n_total\":    9999,\n",
        "          \"n_train\":    2000,\n",
        "          \"n_val\":      1000,\n",
        "          \"n_test\":      7000,\n",
        "          \"batch_id\":    2,\n",
        "          },\n",
        "      \"stylegan2\": {\n",
        "          \"real\":       False,\n",
        "          \"zip_path\":   workdir + \"/datasets/guarnera/STYLEGAN2.zip\",\n",
        "          \"n_total\":    6000,\n",
        "          \"n_train\":    2000,\n",
        "          \"n_val\":      1000,\n",
        "          \"n_test\":      3000,\n",
        "          \"batch_id\":    3,\n",
        "          },\n",
        "      }\n",
        "\n",
        "  return ds_infos.get(name)\n",
        "\n",
        "\n",
        "\n",
        "def build_guarnera_dataset(real_ds, fake_ds, destination_path=\"/dataset\", erase=False, continual_mode=False):\n",
        "\n",
        "  def unzip_dataset(ds_info, real_fake , des_path, batch_id=-1, ds_name=''):\n",
        "\n",
        "    tmp_dir = destination_path+\"/tmp\"\n",
        "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
        "    os.makedirs(tmp_dir, exist_ok=True)\n",
        "\n",
        "    zip_path = ds_info[\"zip_path\"]\n",
        "    n_tr = ds_info[\"n_train\"]\n",
        "    n_v = ds_info[\"n_val\"]\n",
        "    n_te = ds_info[\"n_test\"]\n",
        "\n",
        "    os.makedirs(des_path+\"/test/0_real\", exist_ok=True)\n",
        "    os.makedirs(des_path+\"/test/1_fake\", exist_ok=True)\n",
        "\n",
        "    os.makedirs(des_path+\"/train/0_real\", exist_ok=True)\n",
        "    os.makedirs(des_path+\"/train/1_fake\", exist_ok=True)\n",
        "\n",
        "    os.makedirs(des_path+\"/val/0_real\", exist_ok=True)\n",
        "    os.makedirs(des_path+\"/val/1_fake\", exist_ok=True)\n",
        "\n",
        "    print(f\"extracting in {des_path}\")\n",
        "    with zipfile.ZipFile(zip_path) as zip:\n",
        "      zip_list = zip.infolist()\n",
        "\n",
        "      start = 0\n",
        "      limit = n_tr + n_v + n_te + 10\n",
        "      limit = limit if limit < len(zip_list) else len(zip_list)\n",
        "\n",
        "      if \"CELEBA\" in zip_path:\n",
        "        if ds_info.get(\"shuffle\") is True:\n",
        "          print(\"Random sampling\")\n",
        "          random.shuffle(zip_list)\n",
        "        elif batch_id != -1:\n",
        "          print(f\"Using batch {batch_id}\")\n",
        "          start = batch_id * 6000\n",
        "\n",
        "      print(f\"Extracting images {start} - {start+limit}\")\n",
        "      for i in range(start, start+limit):\n",
        "        zip.extract(zip_list[i],path=tmp_dir)\n",
        "\n",
        "      count = 0\n",
        "      for (path, dirlist, filelist) in os.walk(tmp_dir):\n",
        "\n",
        "        for filename in filelist:\n",
        "          if count < n_tr:\n",
        "            shutil.move(path+\"/\"+filename, f\"{des_path}/train/{real_fake}/{count}_{filename}\")\n",
        "          elif count < n_tr+n_v:\n",
        "            shutil.move(path+\"/\"+filename, f\"{des_path}/val/{real_fake}/{count}_{filename}\")\n",
        "          elif count < n_tr+n_v+n_te:\n",
        "            shutil.move(path+\"/\"+filename, f\"{des_path}/test/{real_fake}/{count}_{filename}\")\n",
        "\n",
        "          count += 1\n",
        "\n",
        "        if count != 0: print(f\"{count-1} images extracted\")\n",
        "\n",
        "\n",
        "  if erase: shutil.rmtree(destination_path, ignore_errors=True)\n",
        "\n",
        "  for ds in fake_ds:\n",
        "    fake_cfg = get_dataset_cfg(ds, real=False)\n",
        "    real_cfg = get_dataset_cfg(real_ds)\n",
        "    batch_id = fake_cfg[\"batch_id\"]\n",
        "    if continual_mode:\n",
        "      des_path = f\"{destination_path}/{ds}\"\n",
        "    else:\n",
        "      des_path = destination_path\n",
        "\n",
        "    print(f\"Extracting fake dataset {ds}\")\n",
        "    unzip_dataset(fake_cfg, \"1_fake\", des_path, ds_name=ds)\n",
        "\n",
        "    print(f\"Extracting real dataset {real_ds}\")\n",
        "    unzip_dataset(real_cfg, \"0_real\", des_path, batch_id=batch_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HHNc0LJxd2Y"
      },
      "source": [
        "## CDDB + Diffusion\n",
        "\n",
        "* CDDB is available here: https://coral79.github.io/CDDB_web/\n",
        "* The diffusion dataset is custom made"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hs0JAFMfxgEl"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import MultiOutputMixin\n",
        "import random\n",
        "import os, zipfile, shutil\n",
        "\n",
        "def get_cddb_cfg(name):\n",
        "\n",
        "  ds_infos = {\n",
        "      \"biggan\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/biggan.zip\",\n",
        "          },\n",
        "      \"crn\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/crn.zip\",\n",
        "          },\n",
        "      \"cyclegan\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/cyclegan.zip\",\n",
        "          },\n",
        "      \"faceforensics\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/faceforensics.zip\",\n",
        "          },\n",
        "      \"gaugan\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/gaugan.zip\",\n",
        "          },\n",
        "      \"glow\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/glow.zip\",\n",
        "          },\n",
        "      \"imle\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/imle.zip\",\n",
        "          },\n",
        "      \"san\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/san.zip\",\n",
        "          },\n",
        "      \"stargan\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/stargan.zip\",\n",
        "          },\n",
        "      \"stylegan\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/stylegan.zip\",\n",
        "          },\n",
        "      \"whichfaceisreal\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/whichfaceisreal.zip\",\n",
        "          },\n",
        "      \"wild\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/wild.zip\",\n",
        "          },\n",
        "      \"diffusionshort\": {\n",
        "          \"zip_path\":   workdir + \"/datasets/custom/diffusionshort.zip\",\n",
        "          },\n",
        "\n",
        "      }\n",
        "\n",
        "  return ds_infos.get(name)\n",
        "\n",
        "\n",
        "# Erase: Empty the destination_path folder\n",
        "# Continual_mode = extract on destination_path/dataset_name instead of destination_path/\n",
        "# Limit = Limit the number of extracted images (test, validation, test)\n",
        "def build_cddb_dataset(ds_list, destination_path=\"/dataset\", erase=False, continual_mode=True, shuffle=False, limit=(100000,100000,100000)):\n",
        "\n",
        "  def unzip_dataset(ds_info , des_path):\n",
        "\n",
        "    zip_path = ds_info[\"zip_path\"]\n",
        "    os.makedirs(des_path, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path) as zip:\n",
        "      zip_list = zip.infolist()\n",
        "\n",
        "      if shuffle:\n",
        "          print(\"Random sampling\")\n",
        "          random.shuffle(zip_list)\n",
        "\n",
        "      test_real, train_real, val_real = 0,0,0\n",
        "      test_fake, train_fake, val_fake = 0,0,0\n",
        "      for zfile in zip_list:\n",
        "        if not zfile.is_dir():\n",
        "          if \"train\" in zfile.filename:\n",
        "            if \"real\" in zfile.filename and train_real < limit[0]:\n",
        "              zip.extract(zfile,path=des_path)\n",
        "              train_real += 1\n",
        "            elif \"fake\" in zfile.filename and train_fake < limit[0]:\n",
        "              zip.extract(zfile,path=des_path)\n",
        "              train_fake += 1\n",
        "\n",
        "          elif \"val\" in zfile.filename:\n",
        "            if \"real\" in zfile.filename and val_real < limit[1]:\n",
        "              zip.extract(zfile,path=des_path)\n",
        "              val_real += 1\n",
        "            elif \"fake\" in zfile.filename and val_fake < limit[1]:\n",
        "              zip.extract(zfile,path=des_path)\n",
        "              val_fake += 1\n",
        "\n",
        "          elif \"test\" in zfile.filename:\n",
        "            if \"real\" in zfile.filename and test_real < limit[2]:\n",
        "              zip.extract(zfile,path=des_path)\n",
        "              test_real += 1\n",
        "            elif \"fake\" in zfile.filename and test_fake < limit[2]:\n",
        "              zip.extract(zfile,path=des_path)\n",
        "              test_fake += 1\n",
        "\n",
        "      print(f\"TOT (for each class): {(train_real+val_real+test_real)}, train {train_real}, val {val_real}, test {test_real}\\n\")\n",
        "\n",
        "  if erase: shutil.rmtree(destination_path, ignore_errors=True)\n",
        "\n",
        "  for ds_cluster in ds_list:\n",
        "    if continual_mode:\n",
        "      des_path = f\"{destination_path}/{ds_cluster}\"\n",
        "    else:\n",
        "      des_path = destination_path\n",
        "\n",
        "    cluster_list = ds_cluster.split(\".\")\n",
        "    for ds in cluster_list:\n",
        "      cfg = get_cddb_cfg(ds)\n",
        "      print(f\"Extracting dataset {ds} in {des_path}\")\n",
        "      unzip_dataset(cfg, des_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56aBetYsOINm"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YaGSNyhYjc6u"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "from common_functions import *\n",
        "from cored_functions import *\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from comet_ml.integration.pytorch import log_model\n",
        "\n",
        "\n",
        "def tl_train(args, log=None):\n",
        "    # Init\n",
        "    lr = args.lr\n",
        "    savepath = f\"{args.checkpoints_dir}/{args.name_target}/\"\n",
        "    savepath = savepath.replace('//','/')\n",
        "    if not os.path.isdir(savepath):\n",
        "        os.makedirs(savepath)\n",
        "    print(f'save path : {savepath}')\n",
        "\n",
        "    # Logger\n",
        "    writer = SummaryWriter(comet_config=comet_config)\n",
        "    writer.add_hparams(hparam_dict=vars(args), metric_dict={})\n",
        "    experiment = comet_ml.get_global_experiment()\n",
        "    if experiment: experiment.set_name(args.name)\n",
        "\n",
        "\n",
        "    # Load datasets and models\n",
        "    dicLoader, _, _ = initialization(args)\n",
        "    _, model = load_models(args.weight, nameNet=args.network, num_gpu=args.num_gpu, TrainMode=True)\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.1)\n",
        "    lr_scheduler = CosineAnnealingLR(optimizer=optimizer,\n",
        "                                             T_max=10,\n",
        "                                             eta_min=1e-5,\n",
        "                                             verbose=True)\n",
        "    scaler = GradScaler()\n",
        "    summary(model, (3, 128, 128), 64)\n",
        "\n",
        "\n",
        "\n",
        "    best_acc, epochs = 0, args.epochs\n",
        "    print('epochs={}'.format(epochs))\n",
        "    is_best_acc = False\n",
        "    step = 0\n",
        "    cur_patience = 0\n",
        "\n",
        "    # ------- START TRAINING ------- #\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = []\n",
        "        correct,total = 0,0\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dicLoader['train_target']):\n",
        "            step = (batch_idx+1) * (epoch+1)\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Forward\n",
        "            with autocast(enabled=True):\n",
        "                outputs = model(inputs)\n",
        "                loss_main = criterion(outputs, targets)\n",
        "                loss = loss_main\n",
        "\n",
        "            # Learn\n",
        "            optimizer.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += len(targets)\n",
        "\n",
        "            # Log and print\n",
        "            running_loss.append(loss_main.cpu().detach().numpy())\n",
        "            writer.add_scalar('losses/loss', loss.item(), step, display_name=\"Loss\")\n",
        "            writer.add_scalar('acc/train_acc', correct/total, step, display_name=\"Train Accuracy\")\n",
        "            print(\"Train Epoch: {e:03d} Batch: {batch:05d}/{size:05d} | Loss: {loss:.4f}\"\n",
        "                            .format(e=epoch+1, batch=batch_idx, size=len(dicLoader['train_target']), loss=loss.item()))\n",
        "\n",
        "        writer.add_scalar('losses/CE_loss', np.mean(running_loss), step, display_name=\"CE Loss\")\n",
        "        print(\"\\nEpoch: {}/{} - CE_Loss: {:.4f} | ACC: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss), correct / total))\n",
        "        lr_scheduler.step()\n",
        "\n",
        "\n",
        "        # ----- Validation ------ #\n",
        "        _, _, test_acc = Test(dicLoader['val_target'], model, criterion)\n",
        "        writer.add_scalar('acc/val_acc', test_acc, step, display_name=\"Validation Accuracy\")\n",
        "        print(f\"Test accuracy: {test_acc}\")\n",
        "        total_acc = test_acc\n",
        "\n",
        "        is_best_acc = total_acc > best_acc\n",
        "        if is_best_acc:\n",
        "            cur_patience = 0\n",
        "        else:\n",
        "            cur_patience += 1\n",
        "\n",
        "        if is_best_acc:\n",
        "            print(\"Save model ...\")\n",
        "            best_acc = max(total_acc, best_acc)\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict()\n",
        "            },\n",
        "            checkpoint = savepath,\n",
        "            filename = 'epoch_{}'.format(epoch+1 if (epoch+1)%10==0 else ''),\n",
        "            ACC_BEST=is_best_acc )\n",
        "            if experiment: log_model(experiment, model, model_name=args.name)\n",
        "\n",
        "        if args.early_stop and (cur_patience == args.patience):\n",
        "              print(\"Early stopping ...\")\n",
        "              writer.close()\n",
        "              return\n",
        "\n",
        "    writer.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MlHM4qAGEMR"
      },
      "source": [
        "# Knowledge Distillation\n",
        "\n",
        "The code incorporates elements derived from the code originally published in the research paper, which can be found here: https://github.com/alsgkals2/CoReD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEuLI5ANwixJ",
        "outputId": "b27e1eb0-c412-403d-cfd4-f89a869c9159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "from common_functions import *\n",
        "from cored_functions import *\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "from train_utils import loss_clampping, ReduceWeightOnPlateau\n",
        "\n",
        "from torchsummary import summary\n",
        "from tensorboardX import SummaryWriter\n",
        "from comet_ml.integration.pytorch import log_model\n",
        "\n",
        "def kd_train(args, log = None):\n",
        "\n",
        "    # Init\n",
        "    torch.cuda.empty_cache()\n",
        "    device = 'cuda' if args.num_gpu else 'cpu'\n",
        "    lr = args.lr\n",
        "    KD_alpha = args.KD_alpha\n",
        "    num_class = args.num_class\n",
        "    num_store_per = args.num_store\n",
        "    savepath = f\"{args.checkpoints_dir}/{args.name_sources}_{args.name_target}/\"\n",
        "    savepath = savepath.replace('//','/')\n",
        "    if not os.path.isdir(savepath):\n",
        "        os.makedirs(savepath)\n",
        "    print(f'save path: {savepath}')\n",
        "\n",
        "    # Logger\n",
        "    writer = SummaryWriter(comet_config=comet_config)\n",
        "    writer.add_hparams(hparam_dict=vars(args), metric_dict={})\n",
        "    experiment = comet_ml.get_global_experiment()\n",
        "    if experiment: experiment.set_name(args.name)\n",
        "\n",
        "    # Load datasets and models\n",
        "    dicLoader, dicCoReD, dicSourceName = initialization(args)\n",
        "    print(\"Dataset available in dicLoader: \", \" / \".join([n for n in dicLoader]))\n",
        "    print(\"Dataset available in dicCoReD: \", \" / \".join([n for n in dicCoReD]))\n",
        "    teacher_model, student_model = load_models(args.weight, args.network, num_gpu = args.num_gpu)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=0.1)\n",
        "\n",
        "    print(\"Teacher summary\")\n",
        "    summary(teacher_model, (3, args.resolution,args.resolution), 32)\n",
        "    print(\"Student summary\")\n",
        "    summary(student_model, (3, args.resolution,args.resolution), 32)\n",
        "\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    if args.lr_schedule == \"cosine\":\n",
        "        print(\"Apply Cosine learning rate schedule\")\n",
        "        lr_scheduler = CosineAnnealingLR(optimizer=optimizer,\n",
        "                                        T_max=10,\n",
        "                                        eta_min=1e-5,\n",
        "                                        verbose=True)\n",
        "    elif args.lr_schedule == \"onecycle\":\n",
        "        print(\"Apply OneCycle learning rate schedule\")\n",
        "        lr_scheduler = OneCycleLR(optimizer=optimizer,\n",
        "                                    max_lr=lr,\n",
        "                                    epochs=args.epochs,\n",
        "                                    steps_per_epoch=len(dicLoader['train_target']),\n",
        "                                    pct_start=0.05,\n",
        "                                    total_steps=None,\n",
        "                                    verbose=False)\n",
        "    else:\n",
        "\n",
        "        print(f\"Input: {args.lr_schedule}, No learning rate schedule applied ... \")\n",
        "        return\n",
        "    watching_step = len(dicLoader['train_target']) // 10\n",
        "\n",
        "\n",
        "    # Pre-evaluation\n",
        "    print(\"Loading train target for correcting ...  \")\n",
        "    _list_correct, _ = func_correct(teacher_model.to(device), dicCoReD['train_target_forCorrect'])\n",
        "    _correct_loaders, already_correct_ratio = GetSplitLoaders_BinaryClasses(_list_correct, dicCoReD['train_target_dataset'], get_augs(args)[0], num_store_per)\n",
        "    print(\"Ratio of already correctly predicted in training set: {:.3f}\".format(already_correct_ratio))\n",
        "    writer.add_scalar('start_acc', already_correct_ratio, 0, display_name=\"Target accuracy before traning\")\n",
        "    list_features = GetListTeacherFeatureFakeReal(teacher_model.module if ',' in args.num_gpu else teacher_model ,_correct_loaders, mode=args.network)\n",
        "    list_features = np.array(list_features)\n",
        "    print(\"List feature size: \", list_features.shape)\n",
        "\n",
        "    # Initial validation\n",
        "    _, _, test_acc = Test(dicLoader['val_target'], student_model, criterion, log = log, source_name = args.name_target)\n",
        "    total_acc = test_acc\n",
        "    print(\"[VAL Acc] Target: {:.2f}%\".format( test_acc))\n",
        "    cnt = 1\n",
        "    for name in dicLoader:\n",
        "        if 'val_dataset' in name or 'val_source' in name:\n",
        "            if 'val_dataset' in name:\n",
        "                source_name = dicSourceName[f'source{cnt}']\n",
        "            else:\n",
        "                source_name = dicSourceName['source']\n",
        "\n",
        "            _, _, source_acc = Test(dicLoader[name], student_model, criterion, log = log, source_name = source_name)\n",
        "            total_acc += source_acc\n",
        "            print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "            cnt += 1\n",
        "\n",
        "    print(\"[VAL Acc] Avg {:.2f}%\\n Save initial model weight\".format(total_acc / cnt))\n",
        "    best_acc = total_acc\n",
        "    save_checkpoint({\n",
        "            'epoch': 0,\n",
        "            'state_dict': student_model.state_dict(),\n",
        "            'best_acc': best_acc,\n",
        "            'optimizer': optimizer.state_dict()},\n",
        "                checkpoint = savepath,\n",
        "                filename = '',\n",
        "                ACC_BEST=True\n",
        "                )\n",
        "\n",
        "    is_best_acc = False\n",
        "    cur_patience = 0 # Early stop and saving\n",
        "    l_weight = 1.0 # reduce the conservation when performance does not gain much\n",
        "    print(f\"Start training in {args.epochs} epochs\")\n",
        "\n",
        "\n",
        "    # ------- START TRAINING ------- #\n",
        "    for epoch in range(args.epochs):\n",
        "        correct,total = 0,0\n",
        "        teacher_model.eval()\n",
        "        student_model.train()\n",
        "        disp = {}\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dicLoader['train_target']):\n",
        "            # Load data\n",
        "            step = (batch_idx+1) * (epoch+1)\n",
        "            inputs = inputs.to(device).to(torch.float32)\n",
        "            targets = targets.to(device).to(torch.long)\n",
        "            if torch.isnan(inputs).any() or torch.isnan(targets).any():\n",
        "                raise ValueError(\"There is Nan values in input or target\")\n",
        "\n",
        "            # Forward\n",
        "            teacher_outputs = teacher_model(inputs)\n",
        "            penul_ft, outputs = student_model(inputs, True)\n",
        "\n",
        "            # Losses\n",
        "            loss_main = criterion(outputs, targets)\n",
        "            loss_kd = loss_fn_kd(outputs, targets, teacher_outputs)\n",
        "            loss_kd = loss_clampping(loss_kd, 0, 1800)\n",
        "\n",
        "            #REP loss\n",
        "            list_features_std = [list(), list()]\n",
        "            rep_ft_partitions = correct_binary_simple(inputs=inputs, penul_ft=penul_ft, outputs=outputs, targets=targets) # rep_ft_partitions : 5 x 2\n",
        "            for j in range(num_store_per):\n",
        "                for i in range(num_class):\n",
        "                    if(np.count_nonzero(list_features[i][j])==0 or len(rep_ft_partitions[j][i])==0):\n",
        "                      continue\n",
        "                    feat = torch.stack(rep_ft_partitions[j][i], dim=0).mean(dim=0)\n",
        "                    assert feat.size(-1) == 2048 or feat.size(-1) == 512 or feat.size(-1) == 1280\n",
        "                    rep_loss = (feat.to(torch.float32)  - torch.tensor(list_features[i][j]).to(device).to(torch.float32)).pow(2).mean()\n",
        "                    list_features_std[i].append(rep_loss)\n",
        "            sne_loss = 0.0\n",
        "            for fs in list_features_std:\n",
        "                for ss in fs:\n",
        "                    if ss.requires_grad:\n",
        "                        sne_loss += ss\n",
        "            sne_loss = loss_clampping(sne_loss, 0, 1) # REP Loss is clampped in this project\n",
        "\n",
        "            # Total loss\n",
        "            loss = loss_main  + l_weight*(loss_kd + sne_loss)\n",
        "            sne_item = sne_loss if type(sne_loss) == float else sne_loss.item()\n",
        "\n",
        "            # Log and display\n",
        "            writer.add_scalar('losses/loss', loss.item(), step, display_name=\"Total Loss\")\n",
        "            writer.add_scalar('losses/loss_main', loss_main.item(), step, display_name=\"Main Loss\")\n",
        "            writer.add_scalar('losses/loss_kd', loss_kd.item(), step, display_name=\"KD Loss\")\n",
        "            writer.add_scalar('losses/loss_sne', sne_item, step, display_name=\"SNE Loss\")\n",
        "            disp[\"CE\"] = loss_main.item()\n",
        "            disp[\"KD\"] = loss_kd.item() if loss_kd > 0 else 0.0\n",
        "            disp[\"REP\"] = sne_item if sne_loss > 0 else 0.0\n",
        "            call = ' | '.join([\"{}: {:.4f}\".format(k, v) for k, v in disp.items()])\n",
        "            print(\"Train Epoch: {e:03d} Batch: {batch:05d}/{size:05d} | Loss: {loss:.4f} | {call}\"\n",
        "                            .format(e=epoch+1, batch=batch_idx+1, size=len(dicLoader['train_target']), loss=loss.item(), call=call))\n",
        "\n",
        "            # Learn!\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if args.lr_schedule == \"onecycle\":\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            # Predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += len(targets)\n",
        "\n",
        "        if args.lr_schedule == \"cosine\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "\n",
        "        # ----- Validation ------ #\n",
        "\n",
        "        # Current task\n",
        "        _, _, test_acc = Test(dicLoader['val_target'], student_model, criterion, log = None, source_name = args.name_target)\n",
        "        total_acc = test_acc\n",
        "        print(\"[VAL Acc] Target: {:.2f}%\".format( test_acc))\n",
        "        writer.add_scalar(f'acc/{args.name_target}_val_acc', test_acc, step, display_name=f\"Target {args.name_target} Validation Accuracy\")\n",
        "\n",
        "        # Past tasks\n",
        "        cnt = 1\n",
        "        for name in dicLoader:\n",
        "            if 'val_dataset' in name or 'val_source' in name:\n",
        "                if 'val_dataset' in name:\n",
        "                    source_name = dicSourceName[f'source{cnt}']\n",
        "                else:\n",
        "                    source_name = dicSourceName['source']\n",
        "\n",
        "                _, _, source_acc = Test(dicLoader[name], student_model, criterion, log = None, source_name = source_name)\n",
        "                total_acc += source_acc\n",
        "                print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "                writer.add_scalar(f'acc/{source_name}_val_acc', source_acc, step, display_name=f\"{source_name} Validation Accuracy\")\n",
        "                cnt += 1\n",
        "        print(\"[VAL Acc] Avg {:.2f}%\".format(total_acc / cnt))\n",
        "        writer.add_scalar('acc/val_acc', total_acc/cnt, step, display_name=\"Average Validation Accuracy\")\n",
        "\n",
        "        # Early stop\n",
        "        is_best_acc = total_acc > best_acc\n",
        "        if is_best_acc:\n",
        "                print(\"VAL Acc improve from {:.2f}% to {:.2f}%\".format(best_acc/cnt, total_acc/cnt))\n",
        "                cur_patience = 0\n",
        "        else:\n",
        "            cur_patience += 1\n",
        "        if args.loss_schedule and (cur_patience > 0 and cur_patience % 4 == 0):\n",
        "                l_weight = ReduceWeightOnPlateau(l_weight, args.decay_factor)\n",
        "\n",
        "        # Save\n",
        "        best_acc = max(total_acc,best_acc)\n",
        "        if  is_best_acc:\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': student_model.state_dict(),\n",
        "                'best_acc': best_acc,\n",
        "                'optimizer': optimizer.state_dict()},\n",
        "            checkpoint = savepath,\n",
        "            filename = 'epoch_{}'.format( epoch+1 if (epoch+1)%10==0 else ''),\n",
        "            ACC_BEST=is_best_acc\n",
        "            )\n",
        "            if experiment: log_model(experiment, student_model, model_name=args.name)\n",
        "            print('Save best model' if is_best_acc else f'Save checkpoint model @ {epoch+1}')\n",
        "        if args.early_stop and (cur_patience == args.patience):\n",
        "            print(\"Early stopping ...\")\n",
        "            writer.close()\n",
        "            return\n",
        "\n",
        "    writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tS-8fPJP8xL"
      },
      "source": [
        "# Elastic Weight Consolidation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hCJmQXKoN9Lo"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.utils.data\n",
        "\n",
        "class EWC(object):\n",
        "    def __init__(self, model: nn.Module, datasets, optimizer , criterion, device, n_sample_batches):\n",
        "\n",
        "        self.model = model\n",
        "        self.datasets = datasets\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.criterion = criterion\n",
        "        self.n_sample_batches = n_sample_batches\n",
        "\n",
        "        self.params = {n: p for n, p in self.model.named_parameters() if p.requires_grad}\n",
        "        self._means = {}\n",
        "        self._precision_matrices = self._diag_fisher()\n",
        "\n",
        "        for n, p in deepcopy(self.params).items():\n",
        "            self._means[n] = p.data.to(self.device)\n",
        "\n",
        "    # Compute the diagonal Fisher information matrix\n",
        "    def _diag_fisher(self):\n",
        "        self.model.eval()\n",
        "        precision_matrices = {}\n",
        "        for n, p in deepcopy(self.params).items():\n",
        "            p.data.zero_()\n",
        "            precision_matrices[n] = p.data.to(self.device)\n",
        "\n",
        "        for dataset in self.datasets:\n",
        "          for batch_idx, (input, target) in enumerate(dataset):\n",
        "            if batch_idx < self.n_sample_batches:\n",
        "              input = input.to(self.device)\n",
        "              target = target.to(self.device)\n",
        "              self.optimizer.zero_grad()\n",
        "              output = self.model(input)\n",
        "              loss = self.criterion(output, target)\n",
        "              loss.backward()\n",
        "\n",
        "              for n, p in self.model.named_parameters():\n",
        "                if p.grad is not None:\n",
        "                  precision_matrices[n].data += p.grad.data.clone().pow(2) / len(dataset)\n",
        "\n",
        "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
        "        self.model.train()\n",
        "        return precision_matrices\n",
        "\n",
        "    # Compute the EWC penalty\n",
        "    def penalty(self, model: nn.Module):\n",
        "        loss = 0.0\n",
        "        for n, p in model.named_parameters():\n",
        "          if n not in self._precision_matrices:\n",
        "            continue\n",
        "          _loss = self._precision_matrices[n] * ((p - self._means[n]) ** 2)\n",
        "          loss += _loss.sum()\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trHGxdGTW4v4",
        "outputId": "5cb89442-9399-4f6e-a83a-66a2304c37c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "from common_functions import *\n",
        "from cored_functions import *\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from torchsummary import summary\n",
        "from tensorboardX import SummaryWriter\n",
        "from comet_ml.integration.pytorch import log_model\n",
        "\n",
        "def ewc_train(args, log = None):\n",
        "\n",
        "    # Init\n",
        "    torch.cuda.empty_cache()\n",
        "    device = 'cuda' if args.num_gpu else 'cpu'\n",
        "    lr = args.lr\n",
        "    num_class = args.num_class\n",
        "    savepath = f\"{args.checkpoints_dir}/{args.name_sources}_{args.name_target}/\"\n",
        "    savepath = savepath.replace('//','/')\n",
        "    if not os.path.isdir(savepath):\n",
        "        os.makedirs(savepath)\n",
        "\n",
        "    # Logger\n",
        "    writer = SummaryWriter(comet_config=comet_config)\n",
        "    writer.add_hparams(hparam_dict=vars(args), metric_dict={})\n",
        "    experiment = comet_ml.get_global_experiment()\n",
        "    if experiment: experiment.set_name(args.name)\n",
        "\n",
        "    # Load datasets and models\n",
        "    dicLoader, _, dicSourceName = initialization(args)\n",
        "    print(\"Dataset available in dicLoader: \", \" / \".join([n for n in dicLoader]))\n",
        "    _, model = load_models(args.weight, args.network, num_gpu = args.num_gpu)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.1)\n",
        "    print(\"Model summary\")\n",
        "    summary(model, (3, args.resolution,args.resolution), 32)\n",
        "\n",
        "\n",
        "    # Initial validation\n",
        "    _, _, test_acc = Test(dicLoader['val_target'], model, criterion, log = log, source_name = args.name_target)\n",
        "    writer.add_scalar('start_acc', test_acc, 0, display_name=\"Target accuracy before traning\")\n",
        "    print(\"Start Target Validation ACC: {:.2f}%\".format(test_acc))\n",
        "\n",
        "\n",
        "    # Get past tasks data loaders\n",
        "    train_loader = dicLoader['train_target']\n",
        "    old_tasks_loaders = []\n",
        "    for name in dicLoader:\n",
        "      if 'val_dataset' in name or 'val_source' in name:\n",
        "        old_tasks_loaders.append(dicLoader[name])\n",
        "\n",
        "    # Compute weight importance\n",
        "    ewc = EWC(model, old_tasks_loaders, optimizer, criterion, device, args.sample_batch)\n",
        "\n",
        "\n",
        "     # ------- START TRAINING ------- #\n",
        "    is_best_acc = False\n",
        "    best_acc = 0\n",
        "    cur_patience = 0\n",
        "    print(f\"Start training in {args.epochs} epochs\")\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\n\\n---------- Starting epoch {epoch} ----------\")\n",
        "        correct,total = 0,0\n",
        "        model.train()\n",
        "\n",
        "        correct, total = 0, 0\n",
        "        tot_ewc_loss, tot_task_loss = 0.0, 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            # Load data\n",
        "            inputs = inputs.to(device).to(torch.float32)\n",
        "            targets = targets.to(device).to(torch.long)\n",
        "\n",
        "             # Forward\n",
        "            outputs = model(inputs)\n",
        "\n",
        "             # Losses\n",
        "            task_loss = criterion(outputs, targets)\n",
        "            ewc_loss = args.importance * ewc.penalty(model)\n",
        "            loss = task_loss + ewc_loss\n",
        "\n",
        "            # Learn\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Predict\n",
        "            tot_task_loss += task_loss.item()\n",
        "            tot_ewc_loss += ewc_loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += len(targets)\n",
        "\n",
        "\n",
        "        # Metrics\n",
        "        epoch_task_loss = tot_task_loss / len(train_loader)\n",
        "        epoch_ewc_loss = tot_ewc_loss / len(train_loader)\n",
        "        tot_loss = epoch_task_loss + epoch_ewc_loss\n",
        "        acc = (correct / total) * 100\n",
        "\n",
        "        # Logging\n",
        "        writer.add_scalar('losses/loss', tot_loss, epoch, display_name=\"Total Loss\")\n",
        "        writer.add_scalar('losses/task_loss', epoch_task_loss, epoch, display_name=\"Task Loss\")\n",
        "        writer.add_scalar('losses/ewc_loss', epoch_ewc_loss, epoch, display_name=\"EWC Loss\")\n",
        "        writer.add_scalar('acc/train_acc', acc, epoch, display_name=\"Train ACC\")\n",
        "        print(f\"Train Epoch: {epoch:03d} |Acc {acc:0.5f} | Loss: {tot_loss:.5f} | Task Loss {epoch_task_loss:.5f} | EWC Loss: {epoch_ewc_loss:.5f}\")\n",
        "\n",
        "\n",
        "        # ---- START VALIDATION ---- #\n",
        "\n",
        "        # Target\n",
        "        _, _, test_acc = Test(dicLoader['val_target'], model, criterion, log = None, source_name = args.name_target)\n",
        "        total_acc = test_acc\n",
        "        writer.add_scalar(f'acc/{args.name_target}_val_acc', test_acc, epoch, display_name=f\"Target {args.name_target} Validation Accuracy\")\n",
        "        print(\"[VAL Acc] Target: {:.2f}%\".format(test_acc))\n",
        "\n",
        "        # Old tasks\n",
        "        cnt = 1\n",
        "        for name in dicLoader:\n",
        "          if 'val_dataset' in name or 'val_source' in name:\n",
        "            if 'val_dataset' in name:\n",
        "              source_name = dicSourceName[f'source{cnt}']\n",
        "            else:\n",
        "              source_name = dicSourceName['source']\n",
        "\n",
        "            _, _, source_acc = Test(dicLoader[name], model, criterion, log = None, source_name = source_name)\n",
        "            total_acc += source_acc\n",
        "            print(\"[VAL Acc] Source {}-th: {:.2f}%\".format(cnt, source_acc))\n",
        "            writer.add_scalar(f'acc/{source_name}_val_acc', source_acc, epoch, display_name=f\"{source_name} Validation Accuracy\")\n",
        "            cnt += 1\n",
        "        print(\"[VAL Acc] Avg {:.2f}%\".format(total_acc / cnt))\n",
        "        writer.add_scalar('acc/val_acc', total_acc/cnt, epoch, display_name=\"Average Validation Accuracy\")\n",
        "\n",
        "\n",
        "        # Evaluate performances\n",
        "        is_best_acc = total_acc > best_acc\n",
        "        best_acc = max(total_acc,best_acc)\n",
        "        if is_best_acc:\n",
        "          print(\"VAL Acc improve from {:.2f}% to {:.2f}%\".format(best_acc/cnt, total_acc/cnt))\n",
        "          cur_patience = 0\n",
        "        else:\n",
        "          cur_patience += 1\n",
        "\n",
        "        # Log validation\n",
        "        if  is_best_acc:\n",
        "          save_checkpoint({\n",
        "              'epoch': epoch + 1,\n",
        "              'state_dict': model.state_dict(),\n",
        "              'best_acc': best_acc,\n",
        "              'optimizer': optimizer.state_dict()},\n",
        "          checkpoint = savepath,\n",
        "          filename = 'epoch_{}'.format( epoch+1 if (epoch+1)%10==0 else ''),\n",
        "          ACC_BEST=is_best_acc\n",
        "          )\n",
        "          if experiment: log_model(experiment, model, model_name=args.name) #Comet\n",
        "          print('Save best model' if is_best_acc else f'Save checkpoint model @ {epoch+1}')\n",
        "\n",
        "        # Early stop\n",
        "        if args.early_stop and (cur_patience == args.patience):\n",
        "                print(\"Early stopping ...\")\n",
        "                writer.close()\n",
        "                return\n",
        "\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q7csYl99zAH"
      },
      "source": [
        "# Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avxhBq4HFhOO",
        "outputId": "60506fd7-31dc-443e-8a10-30b5a64132fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from common_functions import initialization, load_models, AverageMeter\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, average_precision_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def evaluate(args, global_writer=None):\n",
        "\n",
        "    # Config\n",
        "    setattr(args,\"name_sources\", \"\")\n",
        "    setattr(args,\"name_target\", \"\")\n",
        "\n",
        "\n",
        "    # Logger\n",
        "    if global_writer is None:\n",
        "      writer = SummaryWriter(comet_config=comet_config)\n",
        "    else:\n",
        "      writer = global_writer\n",
        "    writer.add_hparams(hparam_dict=vars(args), metric_dict={})\n",
        "    experiment = comet_ml.get_global_experiment()\n",
        "    if experiment: experiment.set_name(args.name)\n",
        "\n",
        "\n",
        "    if \"real_ds\" in args.ds_cfg:\n",
        "      writer.add_hparams(hparam_dict=get_dataset_cfg(args.ds_cfg[\"real_ds\"]), metric_dict={})\n",
        "\n",
        "\n",
        "    # Load model\n",
        "    _, model = load_models(args.weight, args.network, args.num_gpu, not args.test)\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Load datasets\n",
        "    tot_avg_acc, real_avg_acc, fake_avg_acc = 0.0, 0.0 ,0.0\n",
        "    for ds_name in args.ds_cfg[\"fake_ds\"]:\n",
        "      writer.add_hparams(hparam_dict=get_dataset_cfg(ds_name, args), metric_dict={})\n",
        "      data_folder = f\"{args.dataroot}/{ds_name}/test\"\n",
        "      setattr(args,\"data\", data_folder)\n",
        "      dicLoader,_, dicSourceName = initialization(args)\n",
        "\n",
        "\n",
        "      for key, name in zip(dicLoader, dicSourceName):\n",
        "        # Init\n",
        "        global best_acc\n",
        "        correct, total =0,0\n",
        "        losses = AverageMeter()\n",
        "        arc = AverageMeter()\n",
        "        acc_real = AverageMeter()\n",
        "        acc_fake = AverageMeter()\n",
        "        sum_of_AUROC=[]\n",
        "        target=[]\n",
        "        output = []\n",
        "        y_true=np.zeros((0,2),dtype=np.int8)\n",
        "        y_pred=np.zeros((0,2),dtype=np.int8)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          model.eval()\n",
        "          model.cuda()\n",
        "\n",
        "          for (inputs, targets) in tqdm(dicLoader[key], ncols=50):\n",
        "              # Predict\n",
        "              inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, targets)\n",
        "              _, predicted = torch.max(outputs, 1)\n",
        "              correct = (predicted == targets).squeeze()\n",
        "              total += len(targets)\n",
        "              losses.update(loss.data.tolist(), inputs.size(0))\n",
        "              _y_pred = outputs.cpu().detach()\n",
        "              _y_gt = targets.cpu().detach().numpy()\n",
        "              acc = [0, 0]\n",
        "              class_total = [0, 0]\n",
        "              for i in range(len(targets)):\n",
        "                  label = targets[i]\n",
        "                  acc[label] += 1 if correct[i].item() == True else 0\n",
        "                  class_total[label] += 1\n",
        "\n",
        "              losses.update(loss.data.tolist(), inputs.size(0))\n",
        "              if (class_total[0] != 0):\n",
        "                  acc_real.update(acc[0] / class_total[0])\n",
        "              if (class_total[1] != 0):\n",
        "                  acc_fake.update(acc[1] / class_total[1])\n",
        "\n",
        "              target.append(_y_gt)\n",
        "              output.append(_y_pred.numpy()[:,1])\n",
        "              auroc=None\n",
        "              try:\n",
        "                  auroc = roc_auc_score(_y_gt, outputs[:,1].cpu().detach().numpy())\n",
        "              except ValueError:\n",
        "                  pass\n",
        "              sum_of_AUROC.append(auroc)\n",
        "              _y_true = np.array(torch.zeros(targets.shape[0],2), dtype=np.int8)\n",
        "              _y_gt = _y_gt.astype(int)\n",
        "              for _ in range(len(targets)):\n",
        "                  _y_true[_][_y_gt[_]] = 1\n",
        "              y_true = np.concatenate((y_true,_y_true))\n",
        "              a = _y_pred.argmax(1)\n",
        "              _y_pred = np.array(torch.zeros(_y_pred.shape).scatter(1, a.unsqueeze(1), 1),dtype=np.int8)\n",
        "              y_pred = np.concatenate((y_pred,_y_pred))\n",
        "\n",
        "          n_real_samples = np.count_nonzero(y_true, axis=0)[0]\n",
        "          n_fake_samples = np.count_nonzero(y_true, axis=0)[1]\n",
        "          acc = accuracy_score(y_true, y_pred)\n",
        "          ap = average_precision_score(y_true, y_pred)\n",
        "\n",
        "          result = classification_report(y_true, y_pred,\n",
        "                                              labels=None,\n",
        "                                              target_names=None,\n",
        "                                              sample_weight=None,\n",
        "                                              digits=4,\n",
        "                                              output_dict=False,\n",
        "                                              zero_division='warn')\n",
        "\n",
        "\n",
        "          print(f\"\\nLoss:{losses.avg:.4f} | Acc:{acc:.4f} | Acc Real:{acc_real.avg:.4f} | Acc Fake:{acc_fake.avg:.4f} | Ap:{ap:.4f}\")\n",
        "          print(f'Num reals: {n_real_samples}, Num fakes: {n_fake_samples}')\n",
        "          print(\"\\n\\n\",result)\n",
        "\n",
        "          tot_avg_acc += acc\n",
        "          real_avg_acc += acc_real.avg\n",
        "          fake_avg_acc += acc_fake.avg\n",
        "\n",
        "          if experiment is not None:\n",
        "            experiment.log_metrics(\n",
        "                  {\n",
        "                      \"real_acc\": acc_real.avg*100.,\n",
        "                      \"fake_acc\": acc_fake.avg*100.,\n",
        "                      \"tot_acc\": acc*100.,\n",
        "                      \"ap\": ap*100.,\n",
        "                  },\n",
        "                  prefix=str(ds_name)\n",
        "              )\n",
        "            experiment.log_metrics(\n",
        "                  {\n",
        "                      \"num_reals\": n_real_samples,\n",
        "                      \"num_fakes\": n_fake_samples,\n",
        "                  },\n",
        "                  prefix=str(ds_name)\n",
        "              )\n",
        "\n",
        "    total_ds = len(args.ds_cfg[\"fake_ds\"])\n",
        "    if experiment is not None:\n",
        "              experiment.log_metrics(\n",
        "                    {\n",
        "                        \"real_acc\": (real_avg_acc/total_ds)*100.,\n",
        "                        \"fake_acc\": (fake_avg_acc/total_ds)*100.,\n",
        "                        \"tot_acc\": (tot_avg_acc/total_ds)*100.,\n",
        "                    },\n",
        "                    prefix=\"avg_acc\"\n",
        "                )\n",
        "    print(f\"Avg: | Acc:{tot_avg_acc/total_ds:.4f} | Acc Real:{real_avg_acc/total_ds:.4f} | Acc Fake:{fake_avg_acc/total_ds:.4f}\")\n",
        "    if global_writer is None: writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RE9O_Ego1iy"
      },
      "source": [
        "# Workspace\n",
        "\n",
        "In this section it is possible to run trainings and evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO7-OzjtSrus"
      },
      "source": [
        "## Traning\n",
        "\n",
        "\n",
        "1.   Configurate the training\n",
        "2.   Build the dataset\n",
        "3.   Train!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP4dDZ4dqyBE"
      },
      "source": [
        "### Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5dS5XTeMq2Ee"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild,diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"gaugan\"],             # for task 1 use only one dataset\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    \"name\":         \"ttl_gau\",                # Used for tagging the experiment on logs\n",
        "    \"data\":         \"/dataset\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"\",                     # OPTIONAL: load weights from file .pth, if empty preweights will be downloaded\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"name_sources\": \"gaugan\",               # Training dataset\n",
        "    \"name_target\":   \"gaugan\",              # Training dataset, must be the same as name_sources\n",
        "    \"checkpoints_dir\": \"./checkpoints/TL/demo\", # Save folder\n",
        "    \"lr_schedule\":  \"cosine\",               # cosine\n",
        "    \"test\":         False,                  # False\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"loss_schedule\": True,                  # True, False\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"lr\":           0.005,                  # Learning rate\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"epochs\":       200,                    # Traning epochs\n",
        "    \"early_stop\":   True,                   # True, False\n",
        "    \"patience\":     25,                     # Early stop patience\n",
        "    \"ds_cfg\":        ds_cfg,\n",
        "}\n",
        "\n",
        "cfg = SimpleNamespace(**train_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNiW7QEdr7sc",
        "outputId": "c6dcf62a-a734-4971-dd63-43f9eeb3c23e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset gaugan in /dataset/gaugan\n",
            "TOT (for each class): 5000, train 3000, val 1000, test 1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "build_cddb_dataset(cfg.ds_cfg[\"fake_ds\"], erase=True, continual_mode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNB79clUr-E_",
        "outputId": "054557b8-4f3c-43e9-ec1c-e89f420bd0f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "save path : ./checkpoints/TL/demo/gaugan/\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from /dataset\n",
            "Source: gaugan\n",
            "Target: gaugan\n",
            "\n",
            "---DATASET PATHS---\n",
            "Train dir: /dataset/gaugan/train\n",
            "Validation Source dir /dataset/gaugan/val\n",
            "Validation Target dir /dataset/gaugan/val\n",
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 186MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 5.0000e-03.\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [64, 64, 64, 64]           9,408\n",
            "       BatchNorm2d-2           [64, 64, 64, 64]             128\n",
            "              ReLU-3           [64, 64, 64, 64]               0\n",
            "         MaxPool2d-4           [64, 64, 32, 32]               0\n",
            "            Conv2d-5           [64, 64, 32, 32]           4,096\n",
            "       BatchNorm2d-6           [64, 64, 32, 32]             128\n",
            "              ReLU-7           [64, 64, 32, 32]               0\n",
            "            Conv2d-8           [64, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-9           [64, 64, 32, 32]             128\n",
            "             ReLU-10           [64, 64, 32, 32]               0\n",
            "           Conv2d-11          [64, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-12          [64, 256, 32, 32]             512\n",
            "           Conv2d-13          [64, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-14          [64, 256, 32, 32]             512\n",
            "             ReLU-15          [64, 256, 32, 32]               0\n",
            "       Bottleneck-16          [64, 256, 32, 32]               0\n",
            "           Conv2d-17           [64, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-18           [64, 64, 32, 32]             128\n",
            "             ReLU-19           [64, 64, 32, 32]               0\n",
            "           Conv2d-20           [64, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-21           [64, 64, 32, 32]             128\n",
            "             ReLU-22           [64, 64, 32, 32]               0\n",
            "           Conv2d-23          [64, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-24          [64, 256, 32, 32]             512\n",
            "             ReLU-25          [64, 256, 32, 32]               0\n",
            "       Bottleneck-26          [64, 256, 32, 32]               0\n",
            "           Conv2d-27           [64, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-28           [64, 64, 32, 32]             128\n",
            "             ReLU-29           [64, 64, 32, 32]               0\n",
            "           Conv2d-30           [64, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-31           [64, 64, 32, 32]             128\n",
            "             ReLU-32           [64, 64, 32, 32]               0\n",
            "           Conv2d-33          [64, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-34          [64, 256, 32, 32]             512\n",
            "             ReLU-35          [64, 256, 32, 32]               0\n",
            "       Bottleneck-36          [64, 256, 32, 32]               0\n",
            "           Conv2d-37          [64, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-38          [64, 128, 32, 32]             256\n",
            "             ReLU-39          [64, 128, 32, 32]               0\n",
            "           Conv2d-40          [64, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-41          [64, 128, 16, 16]             256\n",
            "             ReLU-42          [64, 128, 16, 16]               0\n",
            "           Conv2d-43          [64, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-44          [64, 512, 16, 16]           1,024\n",
            "           Conv2d-45          [64, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-46          [64, 512, 16, 16]           1,024\n",
            "             ReLU-47          [64, 512, 16, 16]               0\n",
            "       Bottleneck-48          [64, 512, 16, 16]               0\n",
            "           Conv2d-49          [64, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-50          [64, 128, 16, 16]             256\n",
            "             ReLU-51          [64, 128, 16, 16]               0\n",
            "           Conv2d-52          [64, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-53          [64, 128, 16, 16]             256\n",
            "             ReLU-54          [64, 128, 16, 16]               0\n",
            "           Conv2d-55          [64, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-56          [64, 512, 16, 16]           1,024\n",
            "             ReLU-57          [64, 512, 16, 16]               0\n",
            "       Bottleneck-58          [64, 512, 16, 16]               0\n",
            "           Conv2d-59          [64, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-60          [64, 128, 16, 16]             256\n",
            "             ReLU-61          [64, 128, 16, 16]               0\n",
            "           Conv2d-62          [64, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-63          [64, 128, 16, 16]             256\n",
            "             ReLU-64          [64, 128, 16, 16]               0\n",
            "           Conv2d-65          [64, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-66          [64, 512, 16, 16]           1,024\n",
            "             ReLU-67          [64, 512, 16, 16]               0\n",
            "       Bottleneck-68          [64, 512, 16, 16]               0\n",
            "           Conv2d-69          [64, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-70          [64, 128, 16, 16]             256\n",
            "             ReLU-71          [64, 128, 16, 16]               0\n",
            "           Conv2d-72          [64, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-73          [64, 128, 16, 16]             256\n",
            "             ReLU-74          [64, 128, 16, 16]               0\n",
            "           Conv2d-75          [64, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-76          [64, 512, 16, 16]           1,024\n",
            "             ReLU-77          [64, 512, 16, 16]               0\n",
            "       Bottleneck-78          [64, 512, 16, 16]               0\n",
            "           Conv2d-79          [64, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-80          [64, 256, 16, 16]             512\n",
            "             ReLU-81          [64, 256, 16, 16]               0\n",
            "           Conv2d-82            [64, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-83            [64, 256, 8, 8]             512\n",
            "             ReLU-84            [64, 256, 8, 8]               0\n",
            "           Conv2d-85           [64, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-86           [64, 1024, 8, 8]           2,048\n",
            "           Conv2d-87           [64, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-88           [64, 1024, 8, 8]           2,048\n",
            "             ReLU-89           [64, 1024, 8, 8]               0\n",
            "       Bottleneck-90           [64, 1024, 8, 8]               0\n",
            "           Conv2d-91            [64, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-92            [64, 256, 8, 8]             512\n",
            "             ReLU-93            [64, 256, 8, 8]               0\n",
            "           Conv2d-94            [64, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-95            [64, 256, 8, 8]             512\n",
            "             ReLU-96            [64, 256, 8, 8]               0\n",
            "           Conv2d-97           [64, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-98           [64, 1024, 8, 8]           2,048\n",
            "             ReLU-99           [64, 1024, 8, 8]               0\n",
            "      Bottleneck-100           [64, 1024, 8, 8]               0\n",
            "          Conv2d-101            [64, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-102            [64, 256, 8, 8]             512\n",
            "            ReLU-103            [64, 256, 8, 8]               0\n",
            "          Conv2d-104            [64, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-105            [64, 256, 8, 8]             512\n",
            "            ReLU-106            [64, 256, 8, 8]               0\n",
            "          Conv2d-107           [64, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-108           [64, 1024, 8, 8]           2,048\n",
            "            ReLU-109           [64, 1024, 8, 8]               0\n",
            "      Bottleneck-110           [64, 1024, 8, 8]               0\n",
            "          Conv2d-111            [64, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-112            [64, 256, 8, 8]             512\n",
            "            ReLU-113            [64, 256, 8, 8]               0\n",
            "          Conv2d-114            [64, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-115            [64, 256, 8, 8]             512\n",
            "            ReLU-116            [64, 256, 8, 8]               0\n",
            "          Conv2d-117           [64, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-118           [64, 1024, 8, 8]           2,048\n",
            "            ReLU-119           [64, 1024, 8, 8]               0\n",
            "      Bottleneck-120           [64, 1024, 8, 8]               0\n",
            "          Conv2d-121            [64, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-122            [64, 256, 8, 8]             512\n",
            "            ReLU-123            [64, 256, 8, 8]               0\n",
            "          Conv2d-124            [64, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-125            [64, 256, 8, 8]             512\n",
            "            ReLU-126            [64, 256, 8, 8]               0\n",
            "          Conv2d-127           [64, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-128           [64, 1024, 8, 8]           2,048\n",
            "            ReLU-129           [64, 1024, 8, 8]               0\n",
            "      Bottleneck-130           [64, 1024, 8, 8]               0\n",
            "          Conv2d-131            [64, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-132            [64, 256, 8, 8]             512\n",
            "            ReLU-133            [64, 256, 8, 8]               0\n",
            "          Conv2d-134            [64, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-135            [64, 256, 8, 8]             512\n",
            "            ReLU-136            [64, 256, 8, 8]               0\n",
            "          Conv2d-137           [64, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-138           [64, 1024, 8, 8]           2,048\n",
            "            ReLU-139           [64, 1024, 8, 8]               0\n",
            "      Bottleneck-140           [64, 1024, 8, 8]               0\n",
            "          Conv2d-141            [64, 512, 8, 8]         524,288\n",
            "     BatchNorm2d-142            [64, 512, 8, 8]           1,024\n",
            "            ReLU-143            [64, 512, 8, 8]               0\n",
            "          Conv2d-144            [64, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-145            [64, 512, 4, 4]           1,024\n",
            "            ReLU-146            [64, 512, 4, 4]               0\n",
            "          Conv2d-147           [64, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-148           [64, 2048, 4, 4]           4,096\n",
            "          Conv2d-149           [64, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-150           [64, 2048, 4, 4]           4,096\n",
            "            ReLU-151           [64, 2048, 4, 4]               0\n",
            "      Bottleneck-152           [64, 2048, 4, 4]               0\n",
            "          Conv2d-153            [64, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-154            [64, 512, 4, 4]           1,024\n",
            "            ReLU-155            [64, 512, 4, 4]               0\n",
            "          Conv2d-156            [64, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-157            [64, 512, 4, 4]           1,024\n",
            "            ReLU-158            [64, 512, 4, 4]               0\n",
            "          Conv2d-159           [64, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-160           [64, 2048, 4, 4]           4,096\n",
            "            ReLU-161           [64, 2048, 4, 4]               0\n",
            "      Bottleneck-162           [64, 2048, 4, 4]               0\n",
            "          Conv2d-163            [64, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-164            [64, 512, 4, 4]           1,024\n",
            "            ReLU-165            [64, 512, 4, 4]               0\n",
            "          Conv2d-166            [64, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-167            [64, 512, 4, 4]           1,024\n",
            "            ReLU-168            [64, 512, 4, 4]               0\n",
            "          Conv2d-169           [64, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-170           [64, 2048, 4, 4]           4,096\n",
            "            ReLU-171           [64, 2048, 4, 4]               0\n",
            "      Bottleneck-172           [64, 2048, 4, 4]               0\n",
            "AdaptiveAvgPool2d-173           [64, 2048, 1, 1]               0\n",
            "          Linear-174                    [64, 1]           2,049\n",
            "          Linear-175                    [64, 2]               2\n",
            "================================================================\n",
            "Total params: 23,510,083\n",
            "Trainable params: 23,510,081\n",
            "Non-trainable params: 2\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 12.00\n",
            "Forward/backward pass size (MB): 5989.00\n",
            "Params size (MB): 89.68\n",
            "Estimated Total Size (MB): 6090.69\n",
            "----------------------------------------------------------------\n",
            "epochs=2\n",
            "Train Epoch: 001 Batch: 00000/00094 | Loss: 0.6975\n",
            "Train Epoch: 001 Batch: 00001/00094 | Loss: 0.7097\n",
            "Train Epoch: 001 Batch: 00002/00094 | Loss: 0.6987\n",
            "Train Epoch: 001 Batch: 00003/00094 | Loss: 0.7618\n",
            "Train Epoch: 001 Batch: 00004/00094 | Loss: 0.6674\n",
            "Train Epoch: 001 Batch: 00005/00094 | Loss: 0.6831\n",
            "Train Epoch: 001 Batch: 00006/00094 | Loss: 0.6993\n",
            "Train Epoch: 001 Batch: 00007/00094 | Loss: 0.6502\n",
            "Train Epoch: 001 Batch: 00008/00094 | Loss: 0.6937\n",
            "Train Epoch: 001 Batch: 00009/00094 | Loss: 0.7112\n",
            "Train Epoch: 001 Batch: 00010/00094 | Loss: 0.7087\n",
            "Train Epoch: 001 Batch: 00011/00094 | Loss: 0.6787\n",
            "Train Epoch: 001 Batch: 00012/00094 | Loss: 0.6936\n",
            "Train Epoch: 001 Batch: 00013/00094 | Loss: 0.6969\n",
            "Train Epoch: 001 Batch: 00014/00094 | Loss: 0.7121\n",
            "Train Epoch: 001 Batch: 00015/00094 | Loss: 0.6847\n",
            "Train Epoch: 001 Batch: 00016/00094 | Loss: 0.7029\n",
            "Train Epoch: 001 Batch: 00017/00094 | Loss: 0.6582\n",
            "Train Epoch: 001 Batch: 00018/00094 | Loss: 0.6430\n",
            "Train Epoch: 001 Batch: 00019/00094 | Loss: 0.7025\n",
            "Train Epoch: 001 Batch: 00020/00094 | Loss: 0.6759\n",
            "Train Epoch: 001 Batch: 00021/00094 | Loss: 0.6786\n",
            "Train Epoch: 001 Batch: 00022/00094 | Loss: 0.6661\n",
            "Train Epoch: 001 Batch: 00023/00094 | Loss: 0.6811\n",
            "Train Epoch: 001 Batch: 00024/00094 | Loss: 0.6648\n",
            "Train Epoch: 001 Batch: 00025/00094 | Loss: 0.6437\n",
            "Train Epoch: 001 Batch: 00026/00094 | Loss: 0.6584\n",
            "Train Epoch: 001 Batch: 00027/00094 | Loss: 0.6573\n",
            "Train Epoch: 001 Batch: 00028/00094 | Loss: 0.6420\n",
            "Train Epoch: 001 Batch: 00029/00094 | Loss: 0.6465\n",
            "Train Epoch: 001 Batch: 00030/00094 | Loss: 0.6584\n",
            "Train Epoch: 001 Batch: 00031/00094 | Loss: 0.6043\n",
            "Train Epoch: 001 Batch: 00032/00094 | Loss: 0.6170\n",
            "Train Epoch: 001 Batch: 00033/00094 | Loss: 0.6676\n",
            "Train Epoch: 001 Batch: 00034/00094 | Loss: 0.6408\n",
            "Train Epoch: 001 Batch: 00035/00094 | Loss: 0.6311\n",
            "Train Epoch: 001 Batch: 00036/00094 | Loss: 0.6391\n",
            "Train Epoch: 001 Batch: 00037/00094 | Loss: 0.6580\n",
            "Train Epoch: 001 Batch: 00038/00094 | Loss: 0.6249\n",
            "Train Epoch: 001 Batch: 00039/00094 | Loss: 0.6128\n",
            "Train Epoch: 001 Batch: 00040/00094 | Loss: 0.6211\n",
            "Train Epoch: 001 Batch: 00041/00094 | Loss: 0.6201\n",
            "Train Epoch: 001 Batch: 00042/00094 | Loss: 0.6700\n",
            "Train Epoch: 001 Batch: 00043/00094 | Loss: 0.6918\n",
            "Train Epoch: 001 Batch: 00044/00094 | Loss: 0.6100\n",
            "Train Epoch: 001 Batch: 00045/00094 | Loss: 0.6064\n",
            "Train Epoch: 001 Batch: 00046/00094 | Loss: 0.6365\n",
            "Train Epoch: 001 Batch: 00047/00094 | Loss: 0.6308\n",
            "Train Epoch: 001 Batch: 00048/00094 | Loss: 0.6188\n",
            "Train Epoch: 001 Batch: 00049/00094 | Loss: 0.6350\n",
            "Train Epoch: 001 Batch: 00050/00094 | Loss: 0.6107\n",
            "Train Epoch: 001 Batch: 00051/00094 | Loss: 0.6230\n",
            "Train Epoch: 001 Batch: 00052/00094 | Loss: 0.6473\n",
            "Train Epoch: 001 Batch: 00053/00094 | Loss: 0.5784\n",
            "Train Epoch: 001 Batch: 00054/00094 | Loss: 0.5923\n",
            "Train Epoch: 001 Batch: 00055/00094 | Loss: 0.5948\n",
            "Train Epoch: 001 Batch: 00056/00094 | Loss: 0.6086\n",
            "Train Epoch: 001 Batch: 00057/00094 | Loss: 0.6154\n",
            "Train Epoch: 001 Batch: 00058/00094 | Loss: 0.6018\n",
            "Train Epoch: 001 Batch: 00059/00094 | Loss: 0.5836\n",
            "Train Epoch: 001 Batch: 00060/00094 | Loss: 0.6004\n",
            "Train Epoch: 001 Batch: 00061/00094 | Loss: 0.5614\n",
            "Train Epoch: 001 Batch: 00062/00094 | Loss: 0.5638\n",
            "Train Epoch: 001 Batch: 00063/00094 | Loss: 0.6237\n",
            "Train Epoch: 001 Batch: 00064/00094 | Loss: 0.5941\n",
            "Train Epoch: 001 Batch: 00065/00094 | Loss: 0.5871\n",
            "Train Epoch: 001 Batch: 00066/00094 | Loss: 0.5778\n",
            "Train Epoch: 001 Batch: 00067/00094 | Loss: 0.6468\n",
            "Train Epoch: 001 Batch: 00068/00094 | Loss: 0.5616\n",
            "Train Epoch: 001 Batch: 00069/00094 | Loss: 0.5912\n",
            "Train Epoch: 001 Batch: 00070/00094 | Loss: 0.5717\n",
            "Train Epoch: 001 Batch: 00071/00094 | Loss: 0.6411\n",
            "Train Epoch: 001 Batch: 00072/00094 | Loss: 0.5779\n",
            "Train Epoch: 001 Batch: 00073/00094 | Loss: 0.5855\n",
            "Train Epoch: 001 Batch: 00074/00094 | Loss: 0.6138\n",
            "Train Epoch: 001 Batch: 00075/00094 | Loss: 0.5717\n",
            "Train Epoch: 001 Batch: 00076/00094 | Loss: 0.5561\n",
            "Train Epoch: 001 Batch: 00077/00094 | Loss: 0.6138\n",
            "Train Epoch: 001 Batch: 00078/00094 | Loss: 0.5586\n",
            "Train Epoch: 001 Batch: 00079/00094 | Loss: 0.5803\n",
            "Train Epoch: 001 Batch: 00080/00094 | Loss: 0.6413\n",
            "Train Epoch: 001 Batch: 00081/00094 | Loss: 0.5979\n",
            "Train Epoch: 001 Batch: 00082/00094 | Loss: 0.5486\n",
            "Train Epoch: 001 Batch: 00083/00094 | Loss: 0.5667\n",
            "Train Epoch: 001 Batch: 00084/00094 | Loss: 0.5492\n",
            "Train Epoch: 001 Batch: 00085/00094 | Loss: 0.5901\n",
            "Train Epoch: 001 Batch: 00086/00094 | Loss: 0.5416\n",
            "Train Epoch: 001 Batch: 00087/00094 | Loss: 0.5853\n",
            "Train Epoch: 001 Batch: 00088/00094 | Loss: 0.5684\n",
            "Train Epoch: 001 Batch: 00089/00094 | Loss: 0.5802\n",
            "Train Epoch: 001 Batch: 00090/00094 | Loss: 0.5933\n",
            "Train Epoch: 001 Batch: 00091/00094 | Loss: 0.5848\n",
            "Train Epoch: 001 Batch: 00092/00094 | Loss: 0.5869\n",
            "Train Epoch: 001 Batch: 00093/00094 | Loss: 0.5982\n",
            "\n",
            "Epoch: 1/2 - CE_Loss: 0.6290 | ACC: 0.6497\n",
            "Adjusting learning rate of group 0 to 4.8779e-03.\n",
            "===> Starting TEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:05<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.5603 | MainLoss:0.5603 | top:73.8500\n",
            "Test accuracy: 73.85000000000001\n",
            "Save model ...\n",
            "Train Epoch: 002 Batch: 00000/00094 | Loss: 0.5465\n",
            "Train Epoch: 002 Batch: 00001/00094 | Loss: 0.5540\n",
            "Train Epoch: 002 Batch: 00002/00094 | Loss: 0.5722\n",
            "Train Epoch: 002 Batch: 00003/00094 | Loss: 0.5716\n",
            "Train Epoch: 002 Batch: 00004/00094 | Loss: 0.5952\n",
            "Train Epoch: 002 Batch: 00005/00094 | Loss: 0.5883\n",
            "Train Epoch: 002 Batch: 00006/00094 | Loss: 0.5417\n",
            "Train Epoch: 002 Batch: 00007/00094 | Loss: 0.5220\n",
            "Train Epoch: 002 Batch: 00008/00094 | Loss: 0.6050\n",
            "Train Epoch: 002 Batch: 00009/00094 | Loss: 0.5457\n",
            "Train Epoch: 002 Batch: 00010/00094 | Loss: 0.5994\n",
            "Train Epoch: 002 Batch: 00011/00094 | Loss: 0.4979\n",
            "Train Epoch: 002 Batch: 00012/00094 | Loss: 0.6124\n",
            "Train Epoch: 002 Batch: 00013/00094 | Loss: 0.5545\n",
            "Train Epoch: 002 Batch: 00014/00094 | Loss: 0.5787\n",
            "Train Epoch: 002 Batch: 00015/00094 | Loss: 0.5665\n",
            "Train Epoch: 002 Batch: 00016/00094 | Loss: 0.5628\n",
            "Train Epoch: 002 Batch: 00017/00094 | Loss: 0.5337\n",
            "Train Epoch: 002 Batch: 00018/00094 | Loss: 0.5484\n",
            "Train Epoch: 002 Batch: 00019/00094 | Loss: 0.6190\n",
            "Train Epoch: 002 Batch: 00020/00094 | Loss: 0.4904\n",
            "Train Epoch: 002 Batch: 00021/00094 | Loss: 0.6981\n",
            "Train Epoch: 002 Batch: 00022/00094 | Loss: 0.5297\n",
            "Train Epoch: 002 Batch: 00023/00094 | Loss: 0.5373\n",
            "Train Epoch: 002 Batch: 00024/00094 | Loss: 0.5340\n",
            "Train Epoch: 002 Batch: 00025/00094 | Loss: 0.5373\n",
            "Train Epoch: 002 Batch: 00026/00094 | Loss: 0.5634\n",
            "Train Epoch: 002 Batch: 00027/00094 | Loss: 0.6004\n",
            "Train Epoch: 002 Batch: 00028/00094 | Loss: 0.4759\n",
            "Train Epoch: 002 Batch: 00029/00094 | Loss: 0.4976\n",
            "Train Epoch: 002 Batch: 00030/00094 | Loss: 0.5732\n",
            "Train Epoch: 002 Batch: 00031/00094 | Loss: 0.5850\n",
            "Train Epoch: 002 Batch: 00032/00094 | Loss: 0.5282\n",
            "Train Epoch: 002 Batch: 00033/00094 | Loss: 0.5496\n",
            "Train Epoch: 002 Batch: 00034/00094 | Loss: 0.4610\n",
            "Train Epoch: 002 Batch: 00035/00094 | Loss: 0.5365\n",
            "Train Epoch: 002 Batch: 00036/00094 | Loss: 0.4420\n",
            "Train Epoch: 002 Batch: 00037/00094 | Loss: 0.5289\n",
            "Train Epoch: 002 Batch: 00038/00094 | Loss: 0.4962\n",
            "Train Epoch: 002 Batch: 00039/00094 | Loss: 0.5401\n",
            "Train Epoch: 002 Batch: 00040/00094 | Loss: 0.5181\n",
            "Train Epoch: 002 Batch: 00041/00094 | Loss: 0.4960\n",
            "Train Epoch: 002 Batch: 00042/00094 | Loss: 0.4733\n",
            "Train Epoch: 002 Batch: 00043/00094 | Loss: 0.5024\n",
            "Train Epoch: 002 Batch: 00044/00094 | Loss: 0.4822\n",
            "Train Epoch: 002 Batch: 00045/00094 | Loss: 0.4878\n",
            "Train Epoch: 002 Batch: 00046/00094 | Loss: 0.5898\n",
            "Train Epoch: 002 Batch: 00047/00094 | Loss: 0.5171\n",
            "Train Epoch: 002 Batch: 00048/00094 | Loss: 0.6057\n",
            "Train Epoch: 002 Batch: 00049/00094 | Loss: 0.4887\n",
            "Train Epoch: 002 Batch: 00050/00094 | Loss: 0.4913\n",
            "Train Epoch: 002 Batch: 00051/00094 | Loss: 0.4883\n",
            "Train Epoch: 002 Batch: 00052/00094 | Loss: 0.3936\n",
            "Train Epoch: 002 Batch: 00053/00094 | Loss: 0.4874\n",
            "Train Epoch: 002 Batch: 00054/00094 | Loss: 0.4645\n",
            "Train Epoch: 002 Batch: 00055/00094 | Loss: 0.4789\n",
            "Train Epoch: 002 Batch: 00056/00094 | Loss: 0.5440\n",
            "Train Epoch: 002 Batch: 00057/00094 | Loss: 0.5145\n",
            "Train Epoch: 002 Batch: 00058/00094 | Loss: 0.4638\n",
            "Train Epoch: 002 Batch: 00059/00094 | Loss: 0.5410\n",
            "Train Epoch: 002 Batch: 00060/00094 | Loss: 0.5543\n",
            "Train Epoch: 002 Batch: 00061/00094 | Loss: 0.4624\n",
            "Train Epoch: 002 Batch: 00062/00094 | Loss: 0.4193\n",
            "Train Epoch: 002 Batch: 00063/00094 | Loss: 0.5163\n",
            "Train Epoch: 002 Batch: 00064/00094 | Loss: 0.4109\n",
            "Train Epoch: 002 Batch: 00065/00094 | Loss: 0.5326\n",
            "Train Epoch: 002 Batch: 00066/00094 | Loss: 0.4187\n",
            "Train Epoch: 002 Batch: 00067/00094 | Loss: 0.5079\n",
            "Train Epoch: 002 Batch: 00068/00094 | Loss: 0.4371\n",
            "Train Epoch: 002 Batch: 00069/00094 | Loss: 0.4468\n",
            "Train Epoch: 002 Batch: 00070/00094 | Loss: 0.5210\n",
            "Train Epoch: 002 Batch: 00071/00094 | Loss: 0.5058\n",
            "Train Epoch: 002 Batch: 00072/00094 | Loss: 0.4426\n",
            "Train Epoch: 002 Batch: 00073/00094 | Loss: 0.4768\n",
            "Train Epoch: 002 Batch: 00074/00094 | Loss: 0.4213\n",
            "Train Epoch: 002 Batch: 00075/00094 | Loss: 0.4568\n",
            "Train Epoch: 002 Batch: 00076/00094 | Loss: 0.4734\n",
            "Train Epoch: 002 Batch: 00077/00094 | Loss: 0.5044\n",
            "Train Epoch: 002 Batch: 00078/00094 | Loss: 0.5449\n",
            "Train Epoch: 002 Batch: 00079/00094 | Loss: 0.5396\n",
            "Train Epoch: 002 Batch: 00080/00094 | Loss: 0.4607\n",
            "Train Epoch: 002 Batch: 00081/00094 | Loss: 0.5102\n",
            "Train Epoch: 002 Batch: 00082/00094 | Loss: 0.4866\n",
            "Train Epoch: 002 Batch: 00083/00094 | Loss: 0.4606\n",
            "Train Epoch: 002 Batch: 00084/00094 | Loss: 0.5120\n",
            "Train Epoch: 002 Batch: 00085/00094 | Loss: 0.4904\n",
            "Train Epoch: 002 Batch: 00086/00094 | Loss: 0.4791\n",
            "Train Epoch: 002 Batch: 00087/00094 | Loss: 0.4628\n",
            "Train Epoch: 002 Batch: 00088/00094 | Loss: 0.4897\n",
            "Train Epoch: 002 Batch: 00089/00094 | Loss: 0.4070\n",
            "Train Epoch: 002 Batch: 00090/00094 | Loss: 0.4715\n",
            "Train Epoch: 002 Batch: 00091/00094 | Loss: 0.4140\n",
            "Train Epoch: 002 Batch: 00092/00094 | Loss: 0.5107\n",
            "Train Epoch: 002 Batch: 00093/00094 | Loss: 0.4187\n",
            "\n",
            "Epoch: 2/2 - CE_Loss: 0.5129 | ACC: 0.7573\n",
            "Adjusting learning rate of group 0 to 4.5235e-03.\n",
            "===> Starting TEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:05<00:00,  5.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.4583 | MainLoss:0.4583 | top:79.1000\n",
            "Test accuracy: 79.10000000000001\n",
            "Save model ...\n"
          ]
        }
      ],
      "source": [
        "tl_train(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrQgEkqyqu8O"
      },
      "source": [
        "### Knowledge distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BWn0-vizej7"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild\",diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"gaugan\", \"biggan\", \"cyclegan\"] # List of all datasets from first to current task\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    \"name\":         \"tkd_gau_big_cycle\",    # Used for tagging the experiment on logs\n",
        "    \"data\":         \"/dataset\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"./checkpoints/KD/demo/gaugan_biggan/model_best_accuracy.pth\", # load weights of task i-1 from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"name_sources\": \"gaugan_biggan\",        # Ordered list of previous task: dataset1_dataset2_dataseti-1\n",
        "    \"name_target\":   \"cyclegan\",            # Task i dataset\n",
        "    \"checkpoints_dir\": \"./checkpoints/KD/demo\", # Save folder (the task subfolder is automatically created)\n",
        "    \"lr_schedule\":  \"cosine\",               # cosine, onecycle\n",
        "    \"test\":         False,                  # False\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"loss_schedule\": True,                  # True, False\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"KD_alpha\":     0.5,                    # alpha factor for kd loss\n",
        "    \"num_store\":    5,                      # Stores for representation loss\n",
        "    \"lr\":           0.005,                  # Learning rate\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"epochs\":       200,                    # Traning epochs\n",
        "    \"early_stop\":   True,                   # True, False\n",
        "    \"patience\":     25,                     # Early stop patience\n",
        "    \"ds_cfg\":       ds_cfg,\n",
        "}\n",
        "\n",
        "cfg = SimpleNamespace(**train_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwa1z_GMrqH5",
        "outputId": "e78b47a8-3c03-46b6-cc69-a246888f92ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset gaugan in /dataset/gaugan\n",
            "TOT (for each class): 30, train 10, val 10, test 10\n",
            "\n",
            "Extracting dataset biggan in /dataset/biggan\n",
            "TOT (for each class): 30, train 10, val 10, test 10\n",
            "\n",
            "Extracting dataset cyclegan in /dataset/cyclegan\n",
            "TOT (for each class): 30, train 10, val 10, test 10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# limit: Limit the number of extracted images (test, validation, test)\n",
        "build_cddb_dataset(cfg.ds_cfg[\"fake_ds\"], erase=True, continual_mode=True, limit=(1000,1000,1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykvxXn34S2dg",
        "outputId": "40d83eca-dedb-4861-8ead-11286f6718f9"
      },
      "outputs": [],
      "source": [
        "kd_train(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNC0GfK8WLIb"
      },
      "source": [
        "### Elastic Weight Consolidation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZIkMgGhaWXS8"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild\",diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"gaugan\", \"biggan\"] # List of all datasets from first to current task\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    \"name\":         \"tewc_gau_big\",         # Used for tagging the experiment on logs\n",
        "    \"data\":         \"/dataset\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"./checkpoints/EWC/demo/gaugan/model_best_accuracy.pth\", # load weights of task i-1 from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"name_sources\": \"gaugan\",               # Ordered list of previous task: dataset1_dataset2_dataseti-1\n",
        "    \"name_target\":   \"biggan\",              # Task i dataset\n",
        "    \"checkpoints_dir\": \"./checkpoints/EWC/demo\", # Save folder (the task subfolder is automatically created)\n",
        "    \"test\":         False,                  # False\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"importance\":  10,                      # Importance factor for ewc loss\n",
        "    \"sample_batch\":  4,                     # Number of batches for each past task used for inportance matrix computation\n",
        "    \"lr\":           0.005,                  # Learning rate\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"epochs\":       2,                    # Traning epochs\n",
        "    \"early_stop\":   True,                   # True, False\n",
        "    \"patience\":     25,                     # Early stop patience\n",
        "    \"ds_cfg\":       ds_cfg,\n",
        "}\n",
        "\n",
        "cfg = SimpleNamespace(**train_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhaf_zj9X4GB",
        "outputId": "5ac0167d-2dd2-4f4f-fb74-0ab22bcf16ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset gaugan in /dataset/gaugan\n",
            "TOT (for each class): 5000, train 3000, val 1000, test 1000\n",
            "\n",
            "Extracting dataset biggan in /dataset/biggan\n",
            "TOT (for each class): 2000, train 1200, val 400, test 400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "build_cddb_dataset(cfg.ds_cfg[\"fake_ds\"], erase=True, continual_mode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42FIdctiX5iP",
        "outputId": "9024ded5-4fdf-4ca6-b3db-451c2059cc3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from /dataset\n",
            "Source: gaugan\n",
            "Target: biggan\n",
            "\n",
            "---DATASET PATHS---\n",
            "Train dir: /dataset/biggan/train\n",
            "Validation Source dir /dataset/gaugan/val\n",
            "Validation Target dir /dataset/biggan/val\n",
            "Dataset available in dicLoader:  train_target / val_source / val_target / val_target_mix\n",
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet from ./checkpoints/TL/demo/gaugan/model_best_accuracy.pth\n",
            "Loaded\n",
            "Model summary\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [32, 64, 64, 64]           9,408\n",
            "       BatchNorm2d-2           [32, 64, 64, 64]             128\n",
            "              ReLU-3           [32, 64, 64, 64]               0\n",
            "         MaxPool2d-4           [32, 64, 32, 32]               0\n",
            "            Conv2d-5           [32, 64, 32, 32]           4,096\n",
            "       BatchNorm2d-6           [32, 64, 32, 32]             128\n",
            "              ReLU-7           [32, 64, 32, 32]               0\n",
            "            Conv2d-8           [32, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-9           [32, 64, 32, 32]             128\n",
            "             ReLU-10           [32, 64, 32, 32]               0\n",
            "           Conv2d-11          [32, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-12          [32, 256, 32, 32]             512\n",
            "           Conv2d-13          [32, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-14          [32, 256, 32, 32]             512\n",
            "             ReLU-15          [32, 256, 32, 32]               0\n",
            "       Bottleneck-16          [32, 256, 32, 32]               0\n",
            "           Conv2d-17           [32, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-18           [32, 64, 32, 32]             128\n",
            "             ReLU-19           [32, 64, 32, 32]               0\n",
            "           Conv2d-20           [32, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-21           [32, 64, 32, 32]             128\n",
            "             ReLU-22           [32, 64, 32, 32]               0\n",
            "           Conv2d-23          [32, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-24          [32, 256, 32, 32]             512\n",
            "             ReLU-25          [32, 256, 32, 32]               0\n",
            "       Bottleneck-26          [32, 256, 32, 32]               0\n",
            "           Conv2d-27           [32, 64, 32, 32]          16,384\n",
            "      BatchNorm2d-28           [32, 64, 32, 32]             128\n",
            "             ReLU-29           [32, 64, 32, 32]               0\n",
            "           Conv2d-30           [32, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-31           [32, 64, 32, 32]             128\n",
            "             ReLU-32           [32, 64, 32, 32]               0\n",
            "           Conv2d-33          [32, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-34          [32, 256, 32, 32]             512\n",
            "             ReLU-35          [32, 256, 32, 32]               0\n",
            "       Bottleneck-36          [32, 256, 32, 32]               0\n",
            "           Conv2d-37          [32, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-38          [32, 128, 32, 32]             256\n",
            "             ReLU-39          [32, 128, 32, 32]               0\n",
            "           Conv2d-40          [32, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-41          [32, 128, 16, 16]             256\n",
            "             ReLU-42          [32, 128, 16, 16]               0\n",
            "           Conv2d-43          [32, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-44          [32, 512, 16, 16]           1,024\n",
            "           Conv2d-45          [32, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-46          [32, 512, 16, 16]           1,024\n",
            "             ReLU-47          [32, 512, 16, 16]               0\n",
            "       Bottleneck-48          [32, 512, 16, 16]               0\n",
            "           Conv2d-49          [32, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-50          [32, 128, 16, 16]             256\n",
            "             ReLU-51          [32, 128, 16, 16]               0\n",
            "           Conv2d-52          [32, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-53          [32, 128, 16, 16]             256\n",
            "             ReLU-54          [32, 128, 16, 16]               0\n",
            "           Conv2d-55          [32, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-56          [32, 512, 16, 16]           1,024\n",
            "             ReLU-57          [32, 512, 16, 16]               0\n",
            "       Bottleneck-58          [32, 512, 16, 16]               0\n",
            "           Conv2d-59          [32, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-60          [32, 128, 16, 16]             256\n",
            "             ReLU-61          [32, 128, 16, 16]               0\n",
            "           Conv2d-62          [32, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-63          [32, 128, 16, 16]             256\n",
            "             ReLU-64          [32, 128, 16, 16]               0\n",
            "           Conv2d-65          [32, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-66          [32, 512, 16, 16]           1,024\n",
            "             ReLU-67          [32, 512, 16, 16]               0\n",
            "       Bottleneck-68          [32, 512, 16, 16]               0\n",
            "           Conv2d-69          [32, 128, 16, 16]          65,536\n",
            "      BatchNorm2d-70          [32, 128, 16, 16]             256\n",
            "             ReLU-71          [32, 128, 16, 16]               0\n",
            "           Conv2d-72          [32, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-73          [32, 128, 16, 16]             256\n",
            "             ReLU-74          [32, 128, 16, 16]               0\n",
            "           Conv2d-75          [32, 512, 16, 16]          65,536\n",
            "      BatchNorm2d-76          [32, 512, 16, 16]           1,024\n",
            "             ReLU-77          [32, 512, 16, 16]               0\n",
            "       Bottleneck-78          [32, 512, 16, 16]               0\n",
            "           Conv2d-79          [32, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-80          [32, 256, 16, 16]             512\n",
            "             ReLU-81          [32, 256, 16, 16]               0\n",
            "           Conv2d-82            [32, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-83            [32, 256, 8, 8]             512\n",
            "             ReLU-84            [32, 256, 8, 8]               0\n",
            "           Conv2d-85           [32, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-86           [32, 1024, 8, 8]           2,048\n",
            "           Conv2d-87           [32, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-88           [32, 1024, 8, 8]           2,048\n",
            "             ReLU-89           [32, 1024, 8, 8]               0\n",
            "       Bottleneck-90           [32, 1024, 8, 8]               0\n",
            "           Conv2d-91            [32, 256, 8, 8]         262,144\n",
            "      BatchNorm2d-92            [32, 256, 8, 8]             512\n",
            "             ReLU-93            [32, 256, 8, 8]               0\n",
            "           Conv2d-94            [32, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-95            [32, 256, 8, 8]             512\n",
            "             ReLU-96            [32, 256, 8, 8]               0\n",
            "           Conv2d-97           [32, 1024, 8, 8]         262,144\n",
            "      BatchNorm2d-98           [32, 1024, 8, 8]           2,048\n",
            "             ReLU-99           [32, 1024, 8, 8]               0\n",
            "      Bottleneck-100           [32, 1024, 8, 8]               0\n",
            "          Conv2d-101            [32, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-102            [32, 256, 8, 8]             512\n",
            "            ReLU-103            [32, 256, 8, 8]               0\n",
            "          Conv2d-104            [32, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-105            [32, 256, 8, 8]             512\n",
            "            ReLU-106            [32, 256, 8, 8]               0\n",
            "          Conv2d-107           [32, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-108           [32, 1024, 8, 8]           2,048\n",
            "            ReLU-109           [32, 1024, 8, 8]               0\n",
            "      Bottleneck-110           [32, 1024, 8, 8]               0\n",
            "          Conv2d-111            [32, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-112            [32, 256, 8, 8]             512\n",
            "            ReLU-113            [32, 256, 8, 8]               0\n",
            "          Conv2d-114            [32, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-115            [32, 256, 8, 8]             512\n",
            "            ReLU-116            [32, 256, 8, 8]               0\n",
            "          Conv2d-117           [32, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-118           [32, 1024, 8, 8]           2,048\n",
            "            ReLU-119           [32, 1024, 8, 8]               0\n",
            "      Bottleneck-120           [32, 1024, 8, 8]               0\n",
            "          Conv2d-121            [32, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-122            [32, 256, 8, 8]             512\n",
            "            ReLU-123            [32, 256, 8, 8]               0\n",
            "          Conv2d-124            [32, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-125            [32, 256, 8, 8]             512\n",
            "            ReLU-126            [32, 256, 8, 8]               0\n",
            "          Conv2d-127           [32, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-128           [32, 1024, 8, 8]           2,048\n",
            "            ReLU-129           [32, 1024, 8, 8]               0\n",
            "      Bottleneck-130           [32, 1024, 8, 8]               0\n",
            "          Conv2d-131            [32, 256, 8, 8]         262,144\n",
            "     BatchNorm2d-132            [32, 256, 8, 8]             512\n",
            "            ReLU-133            [32, 256, 8, 8]               0\n",
            "          Conv2d-134            [32, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-135            [32, 256, 8, 8]             512\n",
            "            ReLU-136            [32, 256, 8, 8]               0\n",
            "          Conv2d-137           [32, 1024, 8, 8]         262,144\n",
            "     BatchNorm2d-138           [32, 1024, 8, 8]           2,048\n",
            "            ReLU-139           [32, 1024, 8, 8]               0\n",
            "      Bottleneck-140           [32, 1024, 8, 8]               0\n",
            "          Conv2d-141            [32, 512, 8, 8]         524,288\n",
            "     BatchNorm2d-142            [32, 512, 8, 8]           1,024\n",
            "            ReLU-143            [32, 512, 8, 8]               0\n",
            "          Conv2d-144            [32, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-145            [32, 512, 4, 4]           1,024\n",
            "            ReLU-146            [32, 512, 4, 4]               0\n",
            "          Conv2d-147           [32, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-148           [32, 2048, 4, 4]           4,096\n",
            "          Conv2d-149           [32, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-150           [32, 2048, 4, 4]           4,096\n",
            "            ReLU-151           [32, 2048, 4, 4]               0\n",
            "      Bottleneck-152           [32, 2048, 4, 4]               0\n",
            "          Conv2d-153            [32, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-154            [32, 512, 4, 4]           1,024\n",
            "            ReLU-155            [32, 512, 4, 4]               0\n",
            "          Conv2d-156            [32, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-157            [32, 512, 4, 4]           1,024\n",
            "            ReLU-158            [32, 512, 4, 4]               0\n",
            "          Conv2d-159           [32, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-160           [32, 2048, 4, 4]           4,096\n",
            "            ReLU-161           [32, 2048, 4, 4]               0\n",
            "      Bottleneck-162           [32, 2048, 4, 4]               0\n",
            "          Conv2d-163            [32, 512, 4, 4]       1,048,576\n",
            "     BatchNorm2d-164            [32, 512, 4, 4]           1,024\n",
            "            ReLU-165            [32, 512, 4, 4]               0\n",
            "          Conv2d-166            [32, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-167            [32, 512, 4, 4]           1,024\n",
            "            ReLU-168            [32, 512, 4, 4]               0\n",
            "          Conv2d-169           [32, 2048, 4, 4]       1,048,576\n",
            "     BatchNorm2d-170           [32, 2048, 4, 4]           4,096\n",
            "            ReLU-171           [32, 2048, 4, 4]               0\n",
            "      Bottleneck-172           [32, 2048, 4, 4]               0\n",
            "AdaptiveAvgPool2d-173           [32, 2048, 1, 1]               0\n",
            "          Linear-174                    [32, 1]           2,049\n",
            "          Linear-175                    [32, 2]               2\n",
            "================================================================\n",
            "Total params: 23,510,083\n",
            "Trainable params: 23,510,081\n",
            "Non-trainable params: 2\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 6.00\n",
            "Forward/backward pass size (MB): 2994.50\n",
            "Params size (MB): 89.68\n",
            "Estimated Total Size (MB): 3090.18\n",
            "----------------------------------------------------------------\n",
            "===> Starting the dataset biggan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13/13 [00:02<00:00,  5.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.8666 | MainLoss:0.8666 | top:51.6250\n",
            "Start Target Validation ACC: 51.62%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training in 2 epochs\n",
            "\n",
            "\n",
            "---------- Starting epoch 0 ----------\n",
            "Train Epoch: 000 |Acc 57.29167 | Loss: 0.72133 | Task Loss 0.72116 | EWC Loss: 0.00017\n",
            "===> Starting the dataset biggan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13/13 [00:02<00:00,  5.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.7006 | MainLoss:0.7006 | top:55.7500\n",
            "[VAL Acc] Target: 55.75%\n",
            "===> Starting the dataset gaugan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:07<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.5350 | MainLoss:0.5350 | top:73.9000\n",
            "[VAL Acc] Source 1-th: 73.90%\n",
            "[VAL Acc] Avg 64.83%\n",
            "VAL Acc improve from 64.83% to 64.83%\n",
            "Save best model\n",
            "\n",
            "\n",
            "---------- Starting epoch 1 ----------\n",
            "Train Epoch: 001 |Acc 66.83333 | Loss: 0.61079 | Task Loss 0.61016 | EWC Loss: 0.00063\n",
            "===> Starting the dataset biggan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13/13 [00:03<00:00,  3.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.6523 | MainLoss:0.6523 | top:60.7500\n",
            "[VAL Acc] Target: 60.75%\n",
            "===> Starting the dataset gaugan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:06<00:00,  5.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test | Loss:0.5738 | MainLoss:0.5738 | top:69.7500\n",
            "[VAL Acc] Source 1-th: 69.75%\n",
            "[VAL Acc] Avg 65.25%\n",
            "VAL Acc improve from 65.25% to 65.25%\n",
            "Save best model\n"
          ]
        }
      ],
      "source": [
        "ewc_train(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjqcB8Vn_Dkj"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqLXKz3P_GuJ"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# Set the datasets, it will be used to build the dataset folder\n",
        "# Available datasets: biggan,crn,cyclegan,faceforensics,gaugan,glow,imle,san,stargan,stylegan,whichfaceisreal,wild,diffusionshort\n",
        "ds_cfg = {\n",
        "    \"type\":         \"cddb\",                 # cddb, guarnera\n",
        "    #\"real_ds\":      \"ffhq\",                # used only for guarnera: ffhq, celeba\n",
        "    \"fake_ds\":      [\"gaugan\", \"biggan\", \"cyclegan\"] # List of all datasets to test\n",
        "}\n",
        "\n",
        "\n",
        "evaluate_cfg = {\n",
        "\n",
        "    \"name\":         \"ekd_gau_big_cycle\",    # Used for tagging the experiment on logs\n",
        "    \"dataroot\":     \"/dataset\",             # Folder containing the EXTRACTED datasets\n",
        "    \"weight\":       \"./checkpoints/KD/demo/gaugan_biggan_cyclegan/model_best_accuracy.pth\", # load weights of task i from file .pth\n",
        "    \"network\":      \"ResNet\",               # Backbone: ResNet, ResNet18, Xception, MobileNet2\n",
        "    \"test\":         True,                   # True\n",
        "    \"use_gpu\":      True,                   # True, False\n",
        "    \"num_gpu\":      \"0\",                    # GPU id, used only if use_gpu=True\n",
        "    \"crop\":         True,                   # Crop images instead of resize\n",
        "    \"flip\":         False,                  # Random flip augmentation\n",
        "    \"resolution\":   128,                    # Crop/resize resolution\n",
        "    \"num_class\":    2,                      # classification classes, 2 for binary classification\n",
        "    \"batch_size\":   64,                     # Batch size\n",
        "    \"ds_cfg\":        ds_cfg,\n",
        "}\n",
        "\n",
        "evaluate_cfg = SimpleNamespace(**evaluate_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt2crCG9_In3",
        "outputId": "8a18622b-e69f-43d9-bfd3-bd3a2847aef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset gaugan in /dataset/gaugan\n",
            "TOT (for each class): 5000, train 3000, val 1000, test 1000\n",
            "\n",
            "Extracting dataset biggan in /dataset/biggan\n",
            "TOT (for each class): 2000, train 1200, val 400, test 400\n",
            "\n",
            "Extracting dataset cyclegan in /dataset/cyclegan\n",
            "TOT (for each class): 1319, train 784, val 262, test 273\n",
            "\n"
          ]
        }
      ],
      "source": [
        "build_cddb_dataset(evaluate_cfg.ds_cfg[\"fake_ds\"], erase=True, continual_mode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1hdy8PL_LyX",
        "outputId": "27ea677a-9f95-493b-ed4b-fa5c5e9e7e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " ------ Loading models ------\n",
            "Loading ResNet18 from ./checkpoints/KD/demo/gaugan_biggan_cyclegan/model_best_accuracy.pth\n",
            "Loaded\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from /dataset/gaugan/test\n",
            "Source: \n",
            "Target: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████| 32/32 [00:09<00:00,  3.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss:0.6381 | Acc:0.6350 | Acc Real:0.7534 | Acc Fake:0.5156 | Ap:0.5868\n",
            "Num reals: 1000, Num fakes: 1000\n",
            "\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6089    0.7550    0.6741      1000\n",
            "           1     0.6776    0.5150    0.5852      1000\n",
            "\n",
            "   micro avg     0.6350    0.6350    0.6350      2000\n",
            "   macro avg     0.6433    0.6350    0.6297      2000\n",
            "weighted avg     0.6433    0.6350    0.6297      2000\n",
            " samples avg     0.6350    0.6350    0.6350      2000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from /dataset/biggan/test\n",
            "Source: \n",
            "Target: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████| 13/13 [00:02<00:00,  5.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss:0.8496 | Acc:0.4875 | Acc Real:0.6017 | Acc Fake:0.3656 | Ap:0.4939\n",
            "Num reals: 400, Num fakes: 400\n",
            "\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4899    0.6050    0.5414       400\n",
            "           1     0.4837    0.3700    0.4193       400\n",
            "\n",
            "   micro avg     0.4875    0.4875    0.4875       800\n",
            "   macro avg     0.4868    0.4875    0.4803       800\n",
            "weighted avg     0.4868    0.4875    0.4803       800\n",
            " samples avg     0.4875    0.4875    0.4875       800\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------ Creating Loaders ------\n",
            "GPU num is 0\n",
            "\n",
            "===> Starting Task 1 loader from /dataset/cyclegan/test\n",
            "Source: \n",
            "Target: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████| 9/9 [00:01<00:00,  5.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss:0.4771 | Acc:0.7766 | Acc Real:0.7253 | Acc Fake:0.8245 | Ap:0.7156\n",
            "Num reals: 273, Num fakes: 273\n",
            "\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8082    0.7253    0.7645       273\n",
            "           1     0.7508    0.8278    0.7875       273\n",
            "\n",
            "   micro avg     0.7766    0.7766    0.7766       546\n",
            "   macro avg     0.7795    0.7766    0.7760       546\n",
            "weighted avg     0.7795    0.7766    0.7760       546\n",
            " samples avg     0.7766    0.7766    0.7766       546\n",
            "\n",
            "Avg: | Acc:0.6330 | Acc Real:0.6935 | Acc Fake:0.5685\n"
          ]
        }
      ],
      "source": [
        "evaluate(evaluate_cfg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
